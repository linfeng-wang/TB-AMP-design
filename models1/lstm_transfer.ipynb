{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual FASTA parsing (without Biopython)\n",
    "# fasta_path = \"../data//naturalAMPs_APD2024a-ADAM.fasta.txt\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\", index=False)\n",
    "\n",
    "\n",
    "# Manual FASTA parsing (without Biopython)\n",
    "# fasta_path = \"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# fasta_path = \"../data/uniprotkb_length_5_TO_30_NOT_antimicrob_2025_04_14.fasta (1)\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "uniprot_df = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\")\n",
    "uniprot_df1 = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\")\n",
    "uniprot_df = pd.concat([uniprot_df, uniprot_df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 2\n",
      "Range of sequence lengths: 181\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sequence length: 34.4038112522686\n",
      "Median sequence length: 29.0\n",
      "Standard deviation of sequence lengths: 23.719451486351453\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate sequence lengths\n",
    "sequence_lengths = adam_df['Sequence'].apply(len)\n",
    "\n",
    "# Calculate the range of sequence lengths\n",
    "length_range = sequence_lengths.max() - sequence_lengths.min()\n",
    "print(sequence_lengths.max(),sequence_lengths.min())\n",
    "print(f\"Range of sequence lengths: {length_range}\")\n",
    "\n",
    "# Draw a box plot\n",
    "plt.boxplot(sequence_lengths)\n",
    "plt.title(\"Box Plot of Sequence Lengths\")\n",
    "plt.ylabel(\"Sequence Length\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display distribution statistics\n",
    "mean_length = sequence_lengths.mean()\n",
    "median_length = sequence_lengths.median()\n",
    "std_dev_length = sequence_lengths.std()\n",
    "\n",
    "print(f\"Mean sequence length: {mean_length}\")\n",
    "print(f\"Median sequence length: {median_length}\")\n",
    "print(f\"Standard deviation of sequence lengths: {std_dev_length}\")\n",
    "\n",
    "# adam_df = adam_df.drop(columns=['Sequence Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_95341/3724575810.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_uniprot_df = uniprot_df.groupby('Sequence Length', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Peptide ID  \\\n",
      "81192  sp|P12997|BIOB_CITFR Biotin synthase (Fragment...   \n",
      "81715  sp|P85089|GTF2_LEUME Dextransucrase 2 (Fragmen...   \n",
      "81193  sp|P13071|BIOA_CITFR Adenosylmethionine-8-amin...   \n",
      "81176  sp|P0DKJ0|P160B_ARATH Peptide encoded by miPEP...   \n",
      "81362  sp|P41853|FARP_ARTTR FMRFamide-like neuropepti...   \n",
      "...                                                  ...   \n",
      "52463  tr|F5P1Z5|F5P1Z5_SHIFL ATP synthase subunit c ...   \n",
      "55078  sp|O28338|PURS_ARCFU Phosphoribosylformylglyci...   \n",
      "26968  tr|A0A4D5YML7|A0A4D5YML7_9ROSI ATP synthase su...   \n",
      "39606  tr|A0A7L3GVD8|A0A7L3GVD8_9PASS Serine palmitoy...   \n",
      "30647  tr|A0A5C1DC31|A0A5C1DC31_9ANNE Cytochrome c ox...   \n",
      "\n",
      "                                                Sequence  \n",
      "81192                                              MAHSS  \n",
      "81715                                              DSTNY  \n",
      "81193                                              MTTDD  \n",
      "81176                                              MFSPQ  \n",
      "81362                                              RYIRF  \n",
      "...                                                  ...  \n",
      "52463  MENLNMDLLYMAAAVMMGLAAIGAAIGIGILGGKFLEGAARQPDLI...  \n",
      "55078  MIADVYIELKEGVADPEGEATLKALRLLGFKRVKKVSTVKVFRIDI...  \n",
      "26968  NPLISAASVIAAGLAVGLASIGPGIGQGTAAGQAVEGIARQPEAEG...  \n",
      "39606  MDVRSTLSYLYWLFCQFELITCSYLMEPWEKVLFYSFNLAMLGLLL...  \n",
      "30647  GFGNWLVPLMLGAPDMAFPRINNLGFWLIPPAVILLVMSAFIEKGA...  \n",
      "\n",
      "[3739 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate sequence lengths for uniprot_df\n",
    "uniprot_df['Sequence Length'] = uniprot_df['Sequence'].apply(len)\n",
    "\n",
    "# Calculate sequence lengths for adam_df\n",
    "adam_df['Sequence Length'] = adam_df['Sequence'].apply(len)\n",
    "\n",
    "# Perform stratified sampling to select more samples\n",
    "sampled_uniprot_df = uniprot_df.groupby('Sequence Length', group_keys=False).apply(\n",
    "    lambda x: x.sample(\n",
    "        n=min(len(x), int(1.5 * adam_df['Sequence Length'].value_counts().get(x.name, 0))),  # Increase sample size\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# Drop the 'Sequence Length' column after sampling\n",
    "sampled_uniprot_df = sampled_uniprot_df.drop(columns=['Sequence Length'])\n",
    "adam_df = adam_df.drop(columns=['Sequence Length'])\n",
    "\n",
    "# Draw a box plot to visualize the distribution\n",
    "plt.boxplot(sampled_uniprot_df['Sequence'].apply(len))\n",
    "plt.title(\"Box Plot of Sequence Lengths (Sampled Negatives)\")\n",
    "plt.ylabel(\"Sequence Length\")\n",
    "plt.show()\n",
    "\n",
    "print(sampled_uniprot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "{'L', 'K', 'H', 'Y', 'Z', 'F', 'G', 'V', 'I', 'C', 'Q', 'W', 'T', 'X', 'D', 'A', 'E', 'R', 'S', 'N', 'P', 'B', 'M'}\n",
      "23\n",
      "{'B', 'Z'}\n",
      "Number of 'B' values: 2\n",
      "Number of sequences after filtering: 5592\n"
     ]
    }
   ],
   "source": [
    "\n",
    "adam_df['label'] = 1\n",
    "sampled_uniprot_df['label'] = 0\n",
    "adam_df.columns = [\"Peptide ID\", \"Sequences\", 'label']\n",
    "sampled_uniprot_df.columns = [\"Peptide ID\", \"Sequences\" , 'label']\n",
    "df = pd.concat([adam_df, sampled_uniprot_df], ignore_index=True)\n",
    "\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWYX\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "\n",
    "# Filter out sequences containing non-standard amino acids\n",
    "df = df[~df['Sequences'].str.contains('|'.join(non_standard_amino_acids))]\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "tb_df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df[~df['Sequences'].isin(tb_df['Sequences'])]\n",
    "\n",
    "print(f\"Number of sequences after filtering: {len(df)}\")\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define One-Hot Encoding Function for DNA Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        length = len(seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        return one_hot_torch(seq, dtype=self.one_hot_dtype), torch.tensor(label, dtype=torch.float32), length\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def collate_and_pack(batch):\n",
    "    # batch = list of (tensor_seq, label, length)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "\n",
    "    # lengths as tensor\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # Sort by descending length (required by pack_padded_sequence)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    labels = torch.tensor([labels[i] for i in sorted_indices])\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    # Stack to shape: (batch_size, 20, seq_len) and transpose for LSTM input\n",
    "    # LSTM expects input of shape (seq_len, batch_size, features)\n",
    "    sequences = [seq.T for seq in sequences]  # Transpose each [20, L] to [L, 20]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)  # shape: [max_len, batch, features]\n",
    "\n",
    "    # Pack the sequence\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 3914\n",
      "Validation: 839\n",
      "Test: 839\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        # packed_input: PackedSequence\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # hn: [num_layers, batch_size, hidden_dim]\n",
    "        # We'll use the **last layer's** hidden state as feature\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(last_hidden)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.6651, Val Loss: 0.6351, Val Acc: 0.6567, Val AUC: 0.8435\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 112\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, acc, auc\n\u001b[1;32m    111\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMClassifier(hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[104], line 39\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m     36\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[0;32m---> 39\u001b[0m val_loss, val_acc, val_auc \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;66;03m# Logging\u001b[39;00m\n\u001b[1;32m     42\u001b[0m writer\u001b[38;5;241m.\u001b[39madd_scalar(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLoss/Train\u001b[39m\u001b[38;5;124m'\u001b[39m, avg_train_loss, epoch)\n",
      "Cell \u001b[0;32mIn[104], line 72\u001b[0m, in \u001b[0;36mevaluate_model\u001b[0;34m(model, data_loader, criterion, device, verbose)\u001b[0m\n\u001b[1;32m     69\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 72\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m packed_input, labels \u001b[38;5;129;01min\u001b[39;00m data_loader:\n\u001b[1;32m     73\u001b[0m         labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     74\u001b[0m         packed_input \u001b[38;5;241m=\u001b[39m packed_input\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[101], line 25\u001b[0m, in \u001b[0;36mSequenceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[1;32m     24\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(seq\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# unpadded length\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mone_hot_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot_dtype\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), length\n",
      "Cell \u001b[0;32mIn[101], line 8\u001b[0m, in \u001b[0;36mone_hot_torch\u001b[0;34m(seq, dtype)\u001b[0m\n\u001b[1;32m      6\u001b[0m arr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(amino_acids), \u001b[38;5;28mlen\u001b[39m(seq_bytes), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, aa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(aa_bytes):\n\u001b[0;32m----> 8\u001b[0m     arr[i, seq_bytes \u001b[38;5;241m==\u001b[39m aa] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "    # Set up TensorBoard writer\n",
    "    log_dir = f\"runs/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Run evaluation\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return history\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # handle corner cases\n",
    "\n",
    "    # Sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        # Print metrics\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "model = LSTMClassifier(hidden_dim=64)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding in regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[401 160]\n",
      " [ 89 407]]\n",
      "Sensitivity (Recall for Positive Class): 0.8206\n",
      "Specificity (Recall for Negative Class): 0.7148\n",
      "Epoch [1/10] - Train Loss: 0.6630, Val Loss: 0.5471, Val Acc: 0.7644, Val AUC: 0.8299\n",
      "\n",
      "Confusion Matrix:\n",
      "[[460 101]\n",
      " [101 395]]\n",
      "Sensitivity (Recall for Positive Class): 0.7964\n",
      "Specificity (Recall for Negative Class): 0.8200\n",
      "Epoch [2/10] - Train Loss: 0.5161, Val Loss: 0.4849, Val Acc: 0.8089, Val AUC: 0.8625\n",
      "\n",
      "Confusion Matrix:\n",
      "[[192 369]\n",
      " [ 29 467]]\n",
      "Sensitivity (Recall for Positive Class): 0.9415\n",
      "Specificity (Recall for Negative Class): 0.3422\n",
      "Epoch [3/10] - Train Loss: 0.6564, Val Loss: 0.6466, Val Acc: 0.6235, Val AUC: 0.8047\n",
      "\n",
      "Confusion Matrix:\n",
      "[[424 137]\n",
      " [119 377]]\n",
      "Sensitivity (Recall for Positive Class): 0.7601\n",
      "Specificity (Recall for Negative Class): 0.7558\n",
      "Epoch [4/10] - Train Loss: 0.5960, Val Loss: 0.5372, Val Acc: 0.7578, Val AUC: 0.8364\n",
      "\n",
      "Confusion Matrix:\n",
      "[[458 103]\n",
      " [ 77 419]]\n",
      "Sensitivity (Recall for Positive Class): 0.8448\n",
      "Specificity (Recall for Negative Class): 0.8164\n",
      "Epoch [5/10] - Train Loss: 0.4724, Val Loss: 0.3893, Val Acc: 0.8297, Val AUC: 0.9082\n",
      "\n",
      "Confusion Matrix:\n",
      "[[515  46]\n",
      " [121 375]]\n",
      "Sensitivity (Recall for Positive Class): 0.7560\n",
      "Specificity (Recall for Negative Class): 0.9180\n",
      "Epoch [6/10] - Train Loss: 0.3992, Val Loss: 0.3821, Val Acc: 0.8420, Val AUC: 0.9220\n",
      "\n",
      "Confusion Matrix:\n",
      "[[476  85]\n",
      " [ 67 429]]\n",
      "Sensitivity (Recall for Positive Class): 0.8649\n",
      "Specificity (Recall for Negative Class): 0.8485\n",
      "Epoch [7/10] - Train Loss: 0.3674, Val Loss: 0.3593, Val Acc: 0.8562, Val AUC: 0.9305\n",
      "\n",
      "Confusion Matrix:\n",
      "[[512  49]\n",
      " [ 91 405]]\n",
      "Sensitivity (Recall for Positive Class): 0.8165\n",
      "Specificity (Recall for Negative Class): 0.9127\n",
      "Epoch [8/10] - Train Loss: 0.3570, Val Loss: 0.3330, Val Acc: 0.8675, Val AUC: 0.9375\n",
      "\n",
      "Confusion Matrix:\n",
      "[[451 110]\n",
      " [ 55 441]]\n",
      "Sensitivity (Recall for Positive Class): 0.8891\n",
      "Specificity (Recall for Negative Class): 0.8039\n",
      "Epoch [9/10] - Train Loss: 0.4099, Val Loss: 0.3379, Val Acc: 0.8439, Val AUC: 0.9359\n",
      "\n",
      "Confusion Matrix:\n",
      "[[464  97]\n",
      " [ 43 453]]\n",
      "Sensitivity (Recall for Positive Class): 0.9133\n",
      "Specificity (Recall for Negative Class): 0.8271\n",
      "Epoch [10/10] - Train Loss: 0.3665, Val Loss: 0.3215, Val Acc: 0.8675, Val AUC: 0.9421\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # L2 regularization\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "    # Set up TensorBoard writer\n",
    "    log_dir = f\"runs/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)   \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Run evaluation\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.4f}, \"\n",
    "              f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # handle corner cases\n",
    "\n",
    "    # Sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        # Print metrics\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=64, dropout=0.5)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3,\n",
    "                      weight_decay=1e-4, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling on general AMP data (bayesian optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-04-23 18:36:53,270] A new study created in memory with name: no-name-8e9d8653-f6fb-4fc8-a872-a8c63cd07d19\n",
      "[I 2025-04-23 18:37:59,180] Trial 0 finished with value: 0.3562810772231647 and parameters: {'hidden_dim': 87, 'num_layers': 1, 'dropout': 0.16245952171507475, 'lr': 0.006420442460121685, 'weight_decay': 0.0047193694355770095}. Best is trial 0 with value: 0.3562810772231647.\n",
      "[I 2025-04-23 18:39:32,808] Trial 1 finished with value: 0.6769623926707676 and parameters: {'hidden_dim': 73, 'num_layers': 3, 'dropout': 0.18122954970186447, 'lr': 0.00764397073778798, 'weight_decay': 0.004415837749229311}. Best is trial 0 with value: 0.3562810772231647.\n",
      "[I 2025-04-23 18:40:48,483] Trial 2 finished with value: 0.6766137736184257 and parameters: {'hidden_dim': 37, 'num_layers': 2, 'dropout': 0.2659247886274199, 'lr': 0.009501096661893998, 'weight_decay': 0.005629132479807707}. Best is trial 0 with value: 0.3562810772231647.\n",
      "[I 2025-04-23 18:41:53,609] Trial 3 finished with value: 0.3174825920058148 and parameters: {'hidden_dim': 48, 'num_layers': 2, 'dropout': 0.21619230364283887, 'lr': 0.0035798321418978236, 'weight_decay': 0.001827243430381888}. Best is trial 3 with value: 0.3174825920058148.\n",
      "[I 2025-04-23 18:42:56,191] Trial 4 finished with value: 0.6768956908157894 and parameters: {'hidden_dim': 84, 'num_layers': 2, 'dropout': 0.29885163653766655, 'lr': 0.006829446200486774, 'weight_decay': 0.009795015709482609}. Best is trial 3 with value: 0.3174825920058148.\n",
      "[I 2025-04-23 18:44:08,586] Trial 5 finished with value: 0.676875250680106 and parameters: {'hidden_dim': 106, 'num_layers': 3, 'dropout': 0.2700139057400544, 'lr': 0.00857520149252107, 'weight_decay': 0.009049760228971835}. Best is trial 3 with value: 0.3174825920058148.\n",
      "[I 2025-04-23 18:45:30,575] Trial 6 finished with value: 0.6774501757962363 and parameters: {'hidden_dim': 32, 'num_layers': 3, 'dropout': 0.1993787109273829, 'lr': 0.0004963319811935291, 'weight_decay': 0.0077666150451303475}. Best is trial 3 with value: 0.3174825920058148.\n",
      "[I 2025-04-23 18:46:39,644] Trial 7 finished with value: 0.4923818622316633 and parameters: {'hidden_dim': 49, 'num_layers': 2, 'dropout': 0.46290963736735724, 'lr': 0.007200538682228881, 'weight_decay': 0.0015668759601442977}. Best is trial 3 with value: 0.3174825920058148.\n",
      "[I 2025-04-23 18:47:52,555] Trial 8 finished with value: 0.6771207281521389 and parameters: {'hidden_dim': 56, 'num_layers': 3, 'dropout': 0.2843067610176564, 'lr': 0.0012504369011684035, 'weight_decay': 0.0069200561124542655}. Best is trial 3 with value: 0.3174825920058148.\n",
      "[I 2025-04-23 18:48:53,235] Trial 9 finished with value: 0.35146298312715124 and parameters: {'hidden_dim': 39, 'num_layers': 1, 'dropout': 0.45430917006781235, 'lr': 0.006013340723972141, 'weight_decay': 0.00436887785063164}. Best is trial 3 with value: 0.3174825920058148.\n",
      "[I 2025-04-23 18:49:52,325] Trial 10 finished with value: 0.24983559947993075 and parameters: {'hidden_dim': 122, 'num_layers': 1, 'dropout': 0.10849893192718553, 'lr': 0.0032818404851738315, 'weight_decay': 0.0003051101212223203}. Best is trial 10 with value: 0.24983559947993075.\n",
      "[I 2025-04-23 18:50:56,655] Trial 11 finished with value: 0.2780721943293299 and parameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.11415014400217946, 'lr': 0.0039009419620492742, 'weight_decay': 0.00014210402150517751}. Best is trial 10 with value: 0.24983559947993075.\n",
      "[I 2025-04-23 18:51:56,929] Trial 12 finished with value: 0.27586585709026884 and parameters: {'hidden_dim': 123, 'num_layers': 1, 'dropout': 0.11322896394923973, 'lr': 0.003677459435605959, 'weight_decay': 3.931325112064372e-06}. Best is trial 10 with value: 0.24983559947993075.\n",
      "[I 2025-04-23 18:52:54,369] Trial 13 finished with value: 0.2279745124812637 and parameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.10245101789054618, 'lr': 0.002820964179045024, 'weight_decay': 2.4839993944027442e-05}. Best is trial 13 with value: 0.2279745124812637.\n",
      "[I 2025-04-23 18:53:51,952] Trial 14 finished with value: 0.4182130130273955 and parameters: {'hidden_dim': 111, 'num_layers': 1, 'dropout': 0.36886228522763953, 'lr': 0.0022246282509594513, 'weight_decay': 0.0018321013068170633}. Best is trial 13 with value: 0.2279745124812637.\n",
      "[I 2025-04-23 18:54:47,857] Trial 15 finished with value: 0.2972503219332014 and parameters: {'hidden_dim': 104, 'num_layers': 1, 'dropout': 0.3780663398812869, 'lr': 0.004876006981400639, 'weight_decay': 0.0029160844162285674}. Best is trial 13 with value: 0.2279745124812637.\n",
      "[I 2025-04-23 18:55:45,650] Trial 16 finished with value: 0.38028922730258535 and parameters: {'hidden_dim': 115, 'num_layers': 1, 'dropout': 0.1403587437201603, 'lr': 0.0022361275386695682, 'weight_decay': 0.00293434194791631}. Best is trial 13 with value: 0.2279745124812637.\n",
      "[I 2025-04-23 18:56:50,250] Trial 17 finished with value: 0.33076839574745726 and parameters: {'hidden_dim': 95, 'num_layers': 2, 'dropout': 0.10104630047956503, 'lr': 0.0024075301886246543, 'weight_decay': 0.0009567958724748189}. Best is trial 13 with value: 0.2279745124812637.\n",
      "[I 2025-04-23 18:57:42,819] Trial 18 finished with value: 0.35324868506618906 and parameters: {'hidden_dim': 69, 'num_layers': 1, 'dropout': 0.2342434256574356, 'lr': 0.004941744930305893, 'weight_decay': 0.0031837302510231385}. Best is trial 13 with value: 0.2279745124812637.\n",
      "[I 2025-04-23 18:58:51,102] Trial 19 finished with value: 0.38121104346854345 and parameters: {'hidden_dim': 120, 'num_layers': 2, 'dropout': 0.36535862502510547, 'lr': 0.0001194247769529407, 'weight_decay': 0.000698363092653612}. Best is trial 13 with value: 0.2279745124812637.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.10245101789054618, 'lr': 0.002820964179045024, 'weight_decay': 2.4839993944027442e-05}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    # if not train:\n",
    "    #     model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_auc = train_model(model, train_loader, val_loader, num_epochs=20, lr=lr,\n",
    "                          weight_decay=weight_decay, verbose=False, train=True)\n",
    "    return val_auc\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20] - Train Loss: 0.6633, Val Loss: 0.5787, Val Acc: 0.6722, Val AUC: 0.8321\n",
      "Epoch [2/20] - Train Loss: 0.5102, Val Loss: 0.3905, Val Acc: 0.8308, Val AUC: 0.9070\n",
      "Epoch [3/20] - Train Loss: 0.4257, Val Loss: 0.4158, Val Acc: 0.8129, Val AUC: 0.9067\n",
      "Epoch [4/20] - Train Loss: 0.4490, Val Loss: 0.4154, Val Acc: 0.8153, Val AUC: 0.9019\n",
      "Epoch [5/20] - Train Loss: 0.4474, Val Loss: 0.4681, Val Acc: 0.7807, Val AUC: 0.8558\n",
      "Epoch [6/20] - Train Loss: 0.4793, Val Loss: 0.5048, Val Acc: 0.7282, Val AUC: 0.8241\n",
      "Epoch [7/20] - Train Loss: 0.5639, Val Loss: 0.5802, Val Acc: 0.6544, Val AUC: 0.7682\n",
      "Epoch [8/20] - Train Loss: 0.5398, Val Loss: 0.4760, Val Acc: 0.7628, Val AUC: 0.8597\n",
      "Epoch [9/20] - Train Loss: 0.5335, Val Loss: 0.5166, Val Acc: 0.7402, Val AUC: 0.8340\n",
      "Epoch [10/20] - Train Loss: 0.4375, Val Loss: 0.3372, Val Acc: 0.8582, Val AUC: 0.9252\n",
      "Epoch [11/20] - Train Loss: 0.3348, Val Loss: 0.3238, Val Acc: 0.8558, Val AUC: 0.9442\n",
      "Epoch [12/20] - Train Loss: 0.3087, Val Loss: 0.3027, Val Acc: 0.8701, Val AUC: 0.9442\n",
      "Epoch [13/20] - Train Loss: 0.2751, Val Loss: 0.2681, Val Acc: 0.8760, Val AUC: 0.9509\n",
      "Epoch [14/20] - Train Loss: 0.2614, Val Loss: 0.2627, Val Acc: 0.8844, Val AUC: 0.9520\n",
      "Epoch [15/20] - Train Loss: 0.2473, Val Loss: 0.2691, Val Acc: 0.8784, Val AUC: 0.9491\n",
      "Epoch [16/20] - Train Loss: 0.2347, Val Loss: 0.2596, Val Acc: 0.8903, Val AUC: 0.9535\n",
      "Epoch [17/20] - Train Loss: 0.2404, Val Loss: 0.2548, Val Acc: 0.8927, Val AUC: 0.9548\n",
      "Epoch [18/20] - Train Loss: 0.2229, Val Loss: 0.2533, Val Acc: 0.8951, Val AUC: 0.9573\n",
      "Epoch [19/20] - Train Loss: 0.2182, Val Loss: 0.2545, Val Acc: 0.8772, Val AUC: 0.9606\n",
      "Epoch [20/20] - Train Loss: 0.2043, Val Loss: 0.2538, Val Acc: 0.8856, Val AUC: 0.9566\n",
      "Test Loss: 0.2677, Test Accuracy: 0.8999, Test AUC: 0.9579\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout= lstm_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_best_param['lr'],\n",
    "                      weight_decay=lstm_best_param['weight_decay'], verbose=True, train=False)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'best_model_lstm_1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:27:52,255] A new study created in memory with name: no-name-9241524f-ae80-4dac-ba31-6f42f8b67913\n",
      "[I 2025-04-23 14:28:13,775] Trial 0 finished with value: 0.2386869029807193 and parameters: {'hidden_dim': 68, 'num_layers': 1, 'dropout': 0.195687961309635, 'lr': 0.007300367989080572, 'weight_decay': 0.0011771003251049882}. Best is trial 0 with value: 0.2386869029807193.\n",
      "[I 2025-04-23 14:28:35,647] Trial 1 finished with value: 0.27903481466429575 and parameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.4715352238716358, 'lr': 0.0025030040661374775, 'weight_decay': 0.006317832689759742}. Best is trial 0 with value: 0.2386869029807193.\n",
      "[I 2025-04-23 14:29:01,929] Trial 2 finished with value: 0.677487850189209 and parameters: {'hidden_dim': 122, 'num_layers': 3, 'dropout': 0.19804916387652308, 'lr': 0.005548039942523586, 'weight_decay': 0.009854436320957755}. Best is trial 0 with value: 0.2386869029807193.\n",
      "[I 2025-04-23 14:29:28,533] Trial 3 finished with value: 0.2672868006463562 and parameters: {'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.45410987890208876, 'lr': 0.004325785374818501, 'weight_decay': 0.00884067104936531}. Best is trial 0 with value: 0.2386869029807193.\n",
      "[I 2025-04-23 14:29:53,840] Trial 4 finished with value: 0.2648002481354134 and parameters: {'hidden_dim': 58, 'num_layers': 1, 'dropout': 0.36922669907894445, 'lr': 0.007790805738979943, 'weight_decay': 0.00425414206127686}. Best is trial 0 with value: 0.2386869029807193.\n",
      "[I 2025-04-23 14:30:17,987] Trial 5 finished with value: 0.26759185801659313 and parameters: {'hidden_dim': 73, 'num_layers': 1, 'dropout': 0.43798573731278123, 'lr': 0.003218151363531546, 'weight_decay': 0.005345881539670667}. Best is trial 0 with value: 0.2386869029807193.\n",
      "[I 2025-04-23 14:30:50,469] Trial 6 finished with value: 0.6774257378918784 and parameters: {'hidden_dim': 127, 'num_layers': 3, 'dropout': 0.3065310038823399, 'lr': 0.006589356441300781, 'weight_decay': 0.00839932755796806}. Best is trial 0 with value: 0.2386869029807193.\n",
      "[I 2025-04-23 14:31:18,700] Trial 7 finished with value: 0.3366913077022348 and parameters: {'hidden_dim': 123, 'num_layers': 2, 'dropout': 0.17019696504500614, 'lr': 0.0073295725874801, 'weight_decay': 0.009736382743253244}. Best is trial 0 with value: 0.2386869029807193.\n",
      "[I 2025-04-23 14:31:49,262] Trial 8 finished with value: 0.6772146991321019 and parameters: {'hidden_dim': 107, 'num_layers': 3, 'dropout': 0.34987512701778944, 'lr': 0.00728037966961378, 'weight_decay': 0.006428144170426836}. Best is trial 0 with value: 0.2386869029807193.\n",
      "[I 2025-04-23 14:32:16,144] Trial 9 finished with value: 0.6772476008960179 and parameters: {'hidden_dim': 58, 'num_layers': 3, 'dropout': 0.3941140259752486, 'lr': 0.007684222587831848, 'weight_decay': 0.009593168511492419}. Best is trial 0 with value: 0.2386869029807193.\n",
      "[I 2025-04-23 14:32:38,788] Trial 10 finished with value: 0.21210496167519263 and parameters: {'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.11835584327843407, 'lr': 0.009315020454619297, 'weight_decay': 0.00023131505962738913}. Best is trial 10 with value: 0.21210496167519263.\n",
      "[I 2025-04-23 14:33:06,171] Trial 11 finished with value: 0.20934398169629276 and parameters: {'hidden_dim': 36, 'num_layers': 2, 'dropout': 0.14356648631040383, 'lr': 0.009777068595997298, 'weight_decay': 5.114978782425517e-05}. Best is trial 11 with value: 0.20934398169629276.\n",
      "[I 2025-04-23 14:33:32,096] Trial 12 finished with value: 0.227530097988035 and parameters: {'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.10222739174864837, 'lr': 0.00977947406437901, 'weight_decay': 0.00044398564561081143}. Best is trial 11 with value: 0.20934398169629276.\n",
      "[I 2025-04-23 14:33:53,906] Trial 13 finished with value: 0.2905677690037659 and parameters: {'hidden_dim': 34, 'num_layers': 2, 'dropout': 0.10005143990875473, 'lr': 0.0003756389512897043, 'weight_decay': 0.002309791506190038}. Best is trial 11 with value: 0.20934398169629276.\n",
      "[I 2025-04-23 14:34:18,159] Trial 14 finished with value: 0.2547712121158838 and parameters: {'hidden_dim': 43, 'num_layers': 2, 'dropout': 0.2497181224621498, 'lr': 0.009982321956488707, 'weight_decay': 0.0026400979539836734}. Best is trial 11 with value: 0.20934398169629276.\n",
      "[I 2025-04-23 14:34:42,738] Trial 15 finished with value: 0.239236043899187 and parameters: {'hidden_dim': 89, 'num_layers': 2, 'dropout': 0.1604435954650339, 'lr': 0.008834115850941189, 'weight_decay': 0.0001383992261758088}. Best is trial 11 with value: 0.20934398169629276.\n",
      "[I 2025-04-23 14:35:05,869] Trial 16 finished with value: 0.25599632252539906 and parameters: {'hidden_dim': 47, 'num_layers': 2, 'dropout': 0.2749902999715396, 'lr': 0.008877579210169267, 'weight_decay': 0.002197146225489663}. Best is trial 11 with value: 0.20934398169629276.\n",
      "[I 2025-04-23 14:35:28,490] Trial 17 finished with value: 0.2436739999268736 and parameters: {'hidden_dim': 85, 'num_layers': 2, 'dropout': 0.1401637162172538, 'lr': 0.008755762728183871, 'weight_decay': 0.0013435680849068015}. Best is trial 11 with value: 0.20934398169629276.\n",
      "[I 2025-04-23 14:35:52,914] Trial 18 finished with value: 0.28475638532212805 and parameters: {'hidden_dim': 42, 'num_layers': 2, 'dropout': 0.2309564415723372, 'lr': 0.005797340080835652, 'weight_decay': 0.003431665732618866}. Best is trial 11 with value: 0.20934398169629276.\n",
      "[I 2025-04-23 14:36:15,436] Trial 19 finished with value: 0.21932934916445188 and parameters: {'hidden_dim': 51, 'num_layers': 3, 'dropout': 0.12806769278848018, 'lr': 0.004092420589625231, 'weight_decay': 2.1765759500883662e-05}. Best is trial 11 with value: 0.20934398169629276.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 36, 'num_layers': 2, 'dropout': 0.14356648631040383, 'lr': 0.009777068595997298, 'weight_decay': 5.114978782425517e-05}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# Updated BiLSTM with flatten layer as previously defined\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_flatten/BiLSTM_Flatten_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "    max_seq_len = 100  # fixed for now; match your padding/truncation\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage (uncomment and run in your local environment):\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_dim': 36,\n",
       " 'num_layers': 2,\n",
       " 'dropout': 0.14356648631040383,\n",
       " 'lr': 0.009777068595997298,\n",
       " 'weight_decay': 5.114978782425517e-05}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[298  53]\n",
      " [117 371]]\n",
      "Sensitivity: 0.7602, Specificity: 0.8490\n",
      "Epoch [1/30] - Train Loss: 0.5787, Val Loss: 0.4530, Val Acc: 0.7974, Val AUC: 0.8136\n",
      "\n",
      "Confusion Matrix:\n",
      "[[273  78]\n",
      " [ 61 427]]\n",
      "Sensitivity: 0.8750, Specificity: 0.7778\n",
      "Epoch [2/30] - Train Loss: 0.4419, Val Loss: 0.3902, Val Acc: 0.8343, Val AUC: 0.9035\n",
      "\n",
      "Confusion Matrix:\n",
      "[[329  22]\n",
      " [183 305]]\n",
      "Sensitivity: 0.6250, Specificity: 0.9373\n",
      "Epoch [3/30] - Train Loss: 0.4235, Val Loss: 0.5083, Val Acc: 0.7557, Val AUC: 0.8566\n",
      "\n",
      "Confusion Matrix:\n",
      "[[252  99]\n",
      " [ 47 441]]\n",
      "Sensitivity: 0.9037, Specificity: 0.7179\n",
      "Epoch [4/30] - Train Loss: 0.4779, Val Loss: 0.4072, Val Acc: 0.8260, Val AUC: 0.9010\n",
      "\n",
      "Confusion Matrix:\n",
      "[[235 116]\n",
      " [ 26 462]]\n",
      "Sensitivity: 0.9467, Specificity: 0.6695\n",
      "Epoch [5/30] - Train Loss: 0.3923, Val Loss: 0.3377, Val Acc: 0.8308, Val AUC: 0.9179\n",
      "\n",
      "Confusion Matrix:\n",
      "[[226 125]\n",
      " [ 12 476]]\n",
      "Sensitivity: 0.9754, Specificity: 0.6439\n",
      "Epoch [6/30] - Train Loss: 0.3744, Val Loss: 0.3504, Val Acc: 0.8367, Val AUC: 0.9194\n",
      "\n",
      "Confusion Matrix:\n",
      "[[262  89]\n",
      " [ 47 441]]\n",
      "Sensitivity: 0.9037, Specificity: 0.7464\n",
      "Epoch [7/30] - Train Loss: 0.3716, Val Loss: 0.3539, Val Acc: 0.8379, Val AUC: 0.9192\n",
      "\n",
      "Confusion Matrix:\n",
      "[[225 126]\n",
      " [ 12 476]]\n",
      "Sensitivity: 0.9754, Specificity: 0.6410\n",
      "Epoch [8/30] - Train Loss: 0.3673, Val Loss: 0.3380, Val Acc: 0.8355, Val AUC: 0.9297\n",
      "\n",
      "Confusion Matrix:\n",
      "[[234 117]\n",
      " [ 20 468]]\n",
      "Sensitivity: 0.9590, Specificity: 0.6667\n",
      "Epoch [9/30] - Train Loss: 0.3644, Val Loss: 0.3294, Val Acc: 0.8367, Val AUC: 0.9310\n",
      "\n",
      "Confusion Matrix:\n",
      "[[226 125]\n",
      " [ 12 476]]\n",
      "Sensitivity: 0.9754, Specificity: 0.6439\n",
      "Epoch [10/30] - Train Loss: 0.3913, Val Loss: 0.3895, Val Acc: 0.8367, Val AUC: 0.8997\n",
      "\n",
      "Confusion Matrix:\n",
      "[[227 124]\n",
      " [ 15 473]]\n",
      "Sensitivity: 0.9693, Specificity: 0.6467\n",
      "Epoch [11/30] - Train Loss: 0.4173, Val Loss: 0.3557, Val Acc: 0.8343, Val AUC: 0.9137\n",
      "\n",
      "Confusion Matrix:\n",
      "[[226 125]\n",
      " [ 12 476]]\n",
      "Sensitivity: 0.9754, Specificity: 0.6439\n",
      "Epoch [12/30] - Train Loss: 0.4193, Val Loss: 0.3481, Val Acc: 0.8367, Val AUC: 0.9166\n",
      "\n",
      "Confusion Matrix:\n",
      "[[290  61]\n",
      " [ 50 438]]\n",
      "Sensitivity: 0.8975, Specificity: 0.8262\n",
      "Epoch [13/30] - Train Loss: 0.3581, Val Loss: 0.3062, Val Acc: 0.8677, Val AUC: 0.9347\n",
      "\n",
      "Confusion Matrix:\n",
      "[[263  88]\n",
      " [ 28 460]]\n",
      "Sensitivity: 0.9426, Specificity: 0.7493\n",
      "Epoch [14/30] - Train Loss: 0.3419, Val Loss: 0.3019, Val Acc: 0.8617, Val AUC: 0.9435\n",
      "\n",
      "Confusion Matrix:\n",
      "[[277  74]\n",
      " [ 32 456]]\n",
      "Sensitivity: 0.9344, Specificity: 0.7892\n",
      "Epoch [15/30] - Train Loss: 0.3069, Val Loss: 0.2705, Val Acc: 0.8737, Val AUC: 0.9544\n",
      "\n",
      "Confusion Matrix:\n",
      "[[320  31]\n",
      " [ 62 426]]\n",
      "Sensitivity: 0.8730, Specificity: 0.9117\n",
      "Epoch [16/30] - Train Loss: 0.2971, Val Loss: 0.2499, Val Acc: 0.8892, Val AUC: 0.9616\n",
      "\n",
      "Confusion Matrix:\n",
      "[[303  48]\n",
      " [ 41 447]]\n",
      "Sensitivity: 0.9160, Specificity: 0.8632\n",
      "Epoch [17/30] - Train Loss: 0.2769, Val Loss: 0.2356, Val Acc: 0.8939, Val AUC: 0.9626\n",
      "\n",
      "Confusion Matrix:\n",
      "[[315  36]\n",
      " [ 47 441]]\n",
      "Sensitivity: 0.9037, Specificity: 0.8974\n",
      "Epoch [18/30] - Train Loss: 0.2521, Val Loss: 0.2270, Val Acc: 0.9011, Val AUC: 0.9649\n",
      "\n",
      "Confusion Matrix:\n",
      "[[294  57]\n",
      " [ 40 448]]\n",
      "Sensitivity: 0.9180, Specificity: 0.8376\n",
      "Epoch [19/30] - Train Loss: 0.2770, Val Loss: 0.2743, Val Acc: 0.8844, Val AUC: 0.9531\n",
      "\n",
      "Confusion Matrix:\n",
      "[[306  45]\n",
      " [ 51 437]]\n",
      "Sensitivity: 0.8955, Specificity: 0.8718\n",
      "Epoch [20/30] - Train Loss: 0.3286, Val Loss: 0.2685, Val Acc: 0.8856, Val AUC: 0.9532\n",
      "\n",
      "Confusion Matrix:\n",
      "[[305  46]\n",
      " [ 45 443]]\n",
      "Sensitivity: 0.9078, Specificity: 0.8689\n",
      "Epoch [21/30] - Train Loss: 0.2612, Val Loss: 0.2558, Val Acc: 0.8915, Val AUC: 0.9580\n",
      "\n",
      "Confusion Matrix:\n",
      "[[306  45]\n",
      " [ 37 451]]\n",
      "Sensitivity: 0.9242, Specificity: 0.8718\n",
      "Epoch [22/30] - Train Loss: 0.2456, Val Loss: 0.2425, Val Acc: 0.9023, Val AUC: 0.9602\n",
      "\n",
      "Confusion Matrix:\n",
      "[[307  44]\n",
      " [ 44 444]]\n",
      "Sensitivity: 0.9098, Specificity: 0.8746\n",
      "Epoch [23/30] - Train Loss: 0.2354, Val Loss: 0.2508, Val Acc: 0.8951, Val AUC: 0.9572\n",
      "\n",
      "Confusion Matrix:\n",
      "[[276  75]\n",
      " [ 35 453]]\n",
      "Sensitivity: 0.9283, Specificity: 0.7863\n",
      "Epoch [24/30] - Train Loss: 0.2348, Val Loss: 0.3001, Val Acc: 0.8689, Val AUC: 0.9443\n",
      "\n",
      "Confusion Matrix:\n",
      "[[295  56]\n",
      " [ 28 460]]\n",
      "Sensitivity: 0.9426, Specificity: 0.8405\n",
      "Epoch [25/30] - Train Loss: 0.2517, Val Loss: 0.2350, Val Acc: 0.8999, Val AUC: 0.9635\n",
      "\n",
      "Confusion Matrix:\n",
      "[[323  28]\n",
      " [ 52 436]]\n",
      "Sensitivity: 0.8934, Specificity: 0.9202\n",
      "Epoch [26/30] - Train Loss: 0.2182, Val Loss: 0.2308, Val Acc: 0.9046, Val AUC: 0.9647\n",
      "\n",
      "Confusion Matrix:\n",
      "[[307  44]\n",
      " [ 34 454]]\n",
      "Sensitivity: 0.9303, Specificity: 0.8746\n",
      "Epoch [27/30] - Train Loss: 0.2040, Val Loss: 0.2202, Val Acc: 0.9070, Val AUC: 0.9669\n",
      "\n",
      "Confusion Matrix:\n",
      "[[296  55]\n",
      " [ 29 459]]\n",
      "Sensitivity: 0.9406, Specificity: 0.8433\n",
      "Epoch [28/30] - Train Loss: 0.2082, Val Loss: 0.2179, Val Acc: 0.8999, Val AUC: 0.9681\n",
      "\n",
      "Confusion Matrix:\n",
      "[[307  44]\n",
      " [ 32 456]]\n",
      "Sensitivity: 0.9344, Specificity: 0.8746\n",
      "Epoch [29/30] - Train Loss: 0.1827, Val Loss: 0.2365, Val Acc: 0.9094, Val AUC: 0.9652\n",
      "\n",
      "Confusion Matrix:\n",
      "[[303  48]\n",
      " [ 37 451]]\n",
      "Sensitivity: 0.9242, Specificity: 0.8632\n",
      "Epoch [30/30] - Train Loss: 0.1867, Val Loss: 0.2216, Val Acc: 0.8987, Val AUC: 0.9676\n",
      "\n",
      "Confusion Matrix:\n",
      "[[302  49]\n",
      " [ 32 456]]\n",
      "Sensitivity: 0.9344, Specificity: 0.8604\n",
      "Test Loss: 0.2657, Test Accuracy: 0.9035, Test AUC: 0.9645\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout= bilstm_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=30, lr=bilstm_best_param['lr'],\n",
    "                      weight_decay=bilstm_best_param['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:41:04,620] A new study created in memory with name: no-name-6fbfc53a-2304-4daf-b466-d4fabb7cbf7c\n",
      "[I 2025-04-23 14:41:26,468] Trial 0 finished with value: 0.6771370938846043 and parameters: {'hidden_dim': 57, 'num_layers': 3, 'dropout': 0.2719621858583795, 'lr': 0.003919503099491344, 'weight_decay': 0.0026719207449166817}. Best is trial 0 with value: 0.6771370938846043.\n",
      "[I 2025-04-23 14:41:47,553] Trial 1 finished with value: 0.4312099292874336 and parameters: {'hidden_dim': 122, 'num_layers': 1, 'dropout': 0.11849876281251306, 'lr': 0.0019139454061706763, 'weight_decay': 0.0003071268093830577}. Best is trial 1 with value: 0.4312099292874336.\n",
      "[I 2025-04-23 14:42:13,152] Trial 2 finished with value: 0.5884787525449481 and parameters: {'hidden_dim': 92, 'num_layers': 1, 'dropout': 0.3782604877763943, 'lr': 0.00021246116322917881, 'weight_decay': 5.605650659026732e-06}. Best is trial 1 with value: 0.4312099292874336.\n",
      "[I 2025-04-23 14:42:35,870] Trial 3 finished with value: 0.41442096871989115 and parameters: {'hidden_dim': 72, 'num_layers': 2, 'dropout': 0.23475539608410978, 'lr': 0.0006580774483765377, 'weight_decay': 3.6019790083121027e-05}. Best is trial 3 with value: 0.41442096871989115.\n",
      "[I 2025-04-23 14:42:57,693] Trial 4 finished with value: 0.6769985812050956 and parameters: {'hidden_dim': 36, 'num_layers': 1, 'dropout': 0.20861590503184405, 'lr': 0.0010582158245236591, 'weight_decay': 0.008284166024279118}. Best is trial 3 with value: 0.41442096871989115.\n",
      "[I 2025-04-23 14:43:22,485] Trial 5 finished with value: 0.37373444863728117 and parameters: {'hidden_dim': 110, 'num_layers': 2, 'dropout': 0.24554156360407622, 'lr': 0.004646726057762118, 'weight_decay': 0.000310450434872429}. Best is trial 5 with value: 0.37373444863728117.\n",
      "[I 2025-04-23 14:43:44,753] Trial 6 finished with value: 0.6767672938959939 and parameters: {'hidden_dim': 59, 'num_layers': 2, 'dropout': 0.3574103385930014, 'lr': 0.009972960012552325, 'weight_decay': 0.0006381658763661}. Best is trial 5 with value: 0.37373444863728117.\n",
      "[I 2025-04-23 14:44:08,827] Trial 7 finished with value: 0.4386106250541551 and parameters: {'hidden_dim': 102, 'num_layers': 1, 'dropout': 0.37689481069897446, 'lr': 0.0003991647820641519, 'weight_decay': 4.107475054101857e-06}. Best is trial 5 with value: 0.37373444863728117.\n",
      "[I 2025-04-23 14:44:33,754] Trial 8 finished with value: 0.4388893874628203 and parameters: {'hidden_dim': 83, 'num_layers': 3, 'dropout': 0.40298432604436607, 'lr': 0.00018118199491959973, 'weight_decay': 5.477861808913703e-06}. Best is trial 5 with value: 0.37373444863728117.\n",
      "[I 2025-04-23 14:44:59,326] Trial 9 finished with value: 0.3674801465656076 and parameters: {'hidden_dim': 117, 'num_layers': 3, 'dropout': 0.19967883915454016, 'lr': 0.0002359027862181724, 'weight_decay': 7.782709379708303e-05}. Best is trial 9 with value: 0.3674801465656076.\n",
      "[I 2025-04-23 14:45:24,268] Trial 10 finished with value: 0.4348800576158932 and parameters: {'hidden_dim': 128, 'num_layers': 3, 'dropout': 0.10122372820464107, 'lr': 0.00010590673158141775, 'weight_decay': 3.969786273102152e-05}. Best is trial 9 with value: 0.3674801465656076.\n",
      "[I 2025-04-23 14:45:48,415] Trial 11 finished with value: 0.3859666170818465 and parameters: {'hidden_dim': 109, 'num_layers': 2, 'dropout': 0.48349190674587716, 'lr': 0.003445978870479817, 'weight_decay': 0.0001694640918745387}. Best is trial 9 with value: 0.3674801465656076.\n",
      "[I 2025-04-23 14:46:13,948] Trial 12 finished with value: 0.37646331425224033 and parameters: {'hidden_dim': 111, 'num_layers': 3, 'dropout': 0.18270619278867894, 'lr': 0.00785573586062184, 'weight_decay': 3.121857918529689e-05}. Best is trial 9 with value: 0.3674801465656076.\n",
      "[I 2025-04-23 14:46:35,369] Trial 13 finished with value: 0.6764730513095856 and parameters: {'hidden_dim': 97, 'num_layers': 2, 'dropout': 0.29349240755639394, 'lr': 0.0012386944111949893, 'weight_decay': 0.0010334828216781564}. Best is trial 9 with value: 0.3674801465656076.\n",
      "[I 2025-04-23 14:46:58,428] Trial 14 finished with value: 0.3655942699738911 and parameters: {'hidden_dim': 117, 'num_layers': 2, 'dropout': 0.1810060705619591, 'lr': 0.0004398244190274676, 'weight_decay': 1.115749024063186e-06}. Best is trial 14 with value: 0.3655942699738911.\n",
      "[I 2025-04-23 14:47:26,704] Trial 15 finished with value: 0.37068271211215426 and parameters: {'hidden_dim': 119, 'num_layers': 3, 'dropout': 0.16099127433898808, 'lr': 0.00047633341155286374, 'weight_decay': 1.0728977930791746e-06}. Best is trial 14 with value: 0.3655942699738911.\n",
      "[I 2025-04-23 14:47:49,170] Trial 16 finished with value: 0.5338889871324811 and parameters: {'hidden_dim': 86, 'num_layers': 2, 'dropout': 0.15481184025785505, 'lr': 0.00029365697328211847, 'weight_decay': 1.3065799849261652e-06}. Best is trial 14 with value: 0.3655942699738911.\n",
      "[I 2025-04-23 14:48:12,511] Trial 17 finished with value: 0.45800492806094034 and parameters: {'hidden_dim': 73, 'num_layers': 3, 'dropout': 0.33400173877633377, 'lr': 0.0001397654041554712, 'weight_decay': 1.3135609785083608e-05}. Best is trial 14 with value: 0.3655942699738911.\n",
      "[I 2025-04-23 14:48:37,405] Trial 18 finished with value: 0.4295989179185459 and parameters: {'hidden_dim': 127, 'num_layers': 2, 'dropout': 0.20023314732086597, 'lr': 0.0005855084313200487, 'weight_decay': 0.00010273842420674638}. Best is trial 14 with value: 0.3655942699738911.\n",
      "[I 2025-04-23 14:48:59,643] Trial 19 finished with value: 0.406305017215865 and parameters: {'hidden_dim': 103, 'num_layers': 3, 'dropout': 0.13874503941593364, 'lr': 0.0002844387016673139, 'weight_decay': 1.3257623119059027e-05}. Best is trial 14 with value: 0.3655942699738911.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 117, 'num_layers': 2, 'dropout': 0.1810060705619591, 'lr': 0.0004398244190274676, 'weight_decay': 1.115749024063186e-06}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# LSTM with Attention classifier\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)  # shape: [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)  # shape: [batch, seq_len]\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)  # normalize\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # shape: [batch, hidden_dim]\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-attn/LSTM_Attn_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-lstm_attention.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_attn_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[  0 351]\n",
      " [  0 488]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [1/20] - Train Loss: 0.6748, Val Loss: 0.6454, Val Acc: 0.5816, Val AUC: 0.8303\n",
      "\n",
      "Confusion Matrix:\n",
      "[[228 123]\n",
      " [ 54 434]]\n",
      "Sensitivity: 0.8893, Specificity: 0.6496\n",
      "Epoch [2/20] - Train Loss: 0.5463, Val Loss: 0.4461, Val Acc: 0.7890, Val AUC: 0.8692\n",
      "\n",
      "Confusion Matrix:\n",
      "[[254  97]\n",
      " [ 50 438]]\n",
      "Sensitivity: 0.8975, Specificity: 0.7236\n",
      "Epoch [3/20] - Train Loss: 0.4448, Val Loss: 0.4133, Val Acc: 0.8248, Val AUC: 0.8816\n",
      "\n",
      "Confusion Matrix:\n",
      "[[281  70]\n",
      " [ 66 422]]\n",
      "Sensitivity: 0.8648, Specificity: 0.8006\n",
      "Epoch [4/20] - Train Loss: 0.4158, Val Loss: 0.3833, Val Acc: 0.8379, Val AUC: 0.8956\n",
      "\n",
      "Confusion Matrix:\n",
      "[[301  50]\n",
      " [103 385]]\n",
      "Sensitivity: 0.7889, Specificity: 0.8575\n",
      "Epoch [5/20] - Train Loss: 0.4005, Val Loss: 0.4049, Val Acc: 0.8176, Val AUC: 0.9009\n",
      "\n",
      "Confusion Matrix:\n",
      "[[206 145]\n",
      " [ 20 468]]\n",
      "Sensitivity: 0.9590, Specificity: 0.5869\n",
      "Epoch [6/20] - Train Loss: 0.4259, Val Loss: 0.4279, Val Acc: 0.8033, Val AUC: 0.9054\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 98 253]\n",
      " [  5 483]]\n",
      "Sensitivity: 0.9898, Specificity: 0.2792\n",
      "Epoch [7/20] - Train Loss: 0.5918, Val Loss: 0.5581, Val Acc: 0.6925, Val AUC: 0.8879\n",
      "\n",
      "Confusion Matrix:\n",
      "[[273  78]\n",
      " [ 75 413]]\n",
      "Sensitivity: 0.8463, Specificity: 0.7778\n",
      "Epoch [8/20] - Train Loss: 0.4913, Val Loss: 0.4353, Val Acc: 0.8176, Val AUC: 0.8769\n",
      "\n",
      "Confusion Matrix:\n",
      "[[298  53]\n",
      " [ 99 389]]\n",
      "Sensitivity: 0.7971, Specificity: 0.8490\n",
      "Epoch [9/20] - Train Loss: 0.4466, Val Loss: 0.4238, Val Acc: 0.8188, Val AUC: 0.8803\n",
      "\n",
      "Confusion Matrix:\n",
      "[[287  64]\n",
      " [ 85 403]]\n",
      "Sensitivity: 0.8258, Specificity: 0.8177\n",
      "Epoch [10/20] - Train Loss: 0.4223, Val Loss: 0.4003, Val Acc: 0.8224, Val AUC: 0.8942\n",
      "\n",
      "Confusion Matrix:\n",
      "[[278  73]\n",
      " [ 59 429]]\n",
      "Sensitivity: 0.8791, Specificity: 0.7920\n",
      "Epoch [11/20] - Train Loss: 0.3979, Val Loss: 0.3965, Val Acc: 0.8427, Val AUC: 0.8824\n",
      "\n",
      "Confusion Matrix:\n",
      "[[273  78]\n",
      " [ 53 435]]\n",
      "Sensitivity: 0.8914, Specificity: 0.7778\n",
      "Epoch [12/20] - Train Loss: 0.3829, Val Loss: 0.3782, Val Acc: 0.8439, Val AUC: 0.9001\n",
      "\n",
      "Confusion Matrix:\n",
      "[[288  63]\n",
      " [ 61 427]]\n",
      "Sensitivity: 0.8750, Specificity: 0.8205\n",
      "Epoch [13/20] - Train Loss: 0.3790, Val Loss: 0.3722, Val Acc: 0.8522, Val AUC: 0.8987\n",
      "\n",
      "Confusion Matrix:\n",
      "[[273  78]\n",
      " [ 49 439]]\n",
      "Sensitivity: 0.8996, Specificity: 0.7778\n",
      "Epoch [14/20] - Train Loss: 0.3682, Val Loss: 0.3664, Val Acc: 0.8486, Val AUC: 0.9021\n",
      "\n",
      "Confusion Matrix:\n",
      "[[283  68]\n",
      " [ 54 434]]\n",
      "Sensitivity: 0.8893, Specificity: 0.8063\n",
      "Epoch [15/20] - Train Loss: 0.3494, Val Loss: 0.3638, Val Acc: 0.8546, Val AUC: 0.9035\n",
      "\n",
      "Confusion Matrix:\n",
      "[[273  78]\n",
      " [ 51 437]]\n",
      "Sensitivity: 0.8955, Specificity: 0.7778\n",
      "Epoch [16/20] - Train Loss: 0.3437, Val Loss: 0.3660, Val Acc: 0.8462, Val AUC: 0.9036\n",
      "\n",
      "Confusion Matrix:\n",
      "[[272  79]\n",
      " [ 48 440]]\n",
      "Sensitivity: 0.9016, Specificity: 0.7749\n",
      "Epoch [17/20] - Train Loss: 0.3407, Val Loss: 0.3521, Val Acc: 0.8486, Val AUC: 0.9104\n",
      "\n",
      "Confusion Matrix:\n",
      "[[310  41]\n",
      " [ 94 394]]\n",
      "Sensitivity: 0.8074, Specificity: 0.8832\n",
      "Epoch [18/20] - Train Loss: 0.3431, Val Loss: 0.3847, Val Acc: 0.8391, Val AUC: 0.9234\n",
      "\n",
      "Confusion Matrix:\n",
      "[[259  92]\n",
      " [ 46 442]]\n",
      "Sensitivity: 0.9057, Specificity: 0.7379\n",
      "Epoch [19/20] - Train Loss: 0.3246, Val Loss: 0.3467, Val Acc: 0.8355, Val AUC: 0.9140\n",
      "\n",
      "Confusion Matrix:\n",
      "[[289  62]\n",
      " [ 53 435]]\n",
      "Sensitivity: 0.8914, Specificity: 0.8234\n",
      "Epoch [20/20] - Train Loss: 0.3277, Val Loss: 0.3246, Val Acc: 0.8629, Val AUC: 0.9253\n",
      "\n",
      "Confusion Matrix:\n",
      "[[301  50]\n",
      " [ 55 433]]\n",
      "Sensitivity: 0.8873, Specificity: 0.8575\n",
      "Test Loss: 0.2956, Test Accuracy: 0.8749, Test AUC: 0.9449\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_attn_best_param['hidden_dim'], num_layers=lstm_attn_best_param['num_layers'], dropout= lstm_attn_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_attn_best_param['lr'],\n",
    "                      weight_decay=lstm_attn_best_param['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm + attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:50:01,603] A new study created in memory with name: no-name-9805538b-eb2a-493f-8eaf-fb1bd4cfa222\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:50:28,099] Trial 0 finished with value: 0.6770566361291068 and parameters: {'hidden_dim': 115, 'num_layers': 3, 'dropout': 0.46150723810346495, 'lr': 0.0010185257467787245, 'weight_decay': 0.005559229255197298}. Best is trial 0 with value: 0.6770566361291068.\n",
      "[I 2025-04-23 14:50:51,868] Trial 1 finished with value: 0.6768435026918139 and parameters: {'hidden_dim': 87, 'num_layers': 2, 'dropout': 0.4027320318239537, 'lr': 0.007936101287001942, 'weight_decay': 0.0015850207475419844}. Best is trial 1 with value: 0.6768435026918139.\n",
      "[I 2025-04-23 14:51:18,114] Trial 2 finished with value: 0.6768869842801776 and parameters: {'hidden_dim': 105, 'num_layers': 2, 'dropout': 0.41611386392848315, 'lr': 0.004484340104313318, 'weight_decay': 0.004763292565030228}. Best is trial 1 with value: 0.6768435026918139.\n",
      "[I 2025-04-23 14:51:39,643] Trial 3 finished with value: 0.6748069652489254 and parameters: {'hidden_dim': 37, 'num_layers': 1, 'dropout': 0.2889192568225892, 'lr': 0.00017341035882273436, 'weight_decay': 0.0016413993002851656}. Best is trial 3 with value: 0.6748069652489254.\n",
      "[I 2025-04-23 14:52:03,871] Trial 4 finished with value: 0.6767022907733917 and parameters: {'hidden_dim': 93, 'num_layers': 1, 'dropout': 0.4975971262047302, 'lr': 0.006966225608137499, 'weight_decay': 0.008782932723957913}. Best is trial 3 with value: 0.6748069652489254.\n",
      "[I 2025-04-23 14:52:30,342] Trial 5 finished with value: 0.6769324796540397 and parameters: {'hidden_dim': 72, 'num_layers': 2, 'dropout': 0.48944216272388286, 'lr': 0.000863221299151272, 'weight_decay': 0.0037611126826213345}. Best is trial 3 with value: 0.6748069652489254.\n",
      "[I 2025-04-23 14:52:52,523] Trial 6 finished with value: 0.6753495931625366 and parameters: {'hidden_dim': 79, 'num_layers': 1, 'dropout': 0.29249715935108866, 'lr': 0.004506963877258697, 'weight_decay': 0.0032996460370851312}. Best is trial 3 with value: 0.6748069652489254.\n",
      "[I 2025-04-23 14:53:15,300] Trial 7 finished with value: 0.6768749185970852 and parameters: {'hidden_dim': 36, 'num_layers': 2, 'dropout': 0.46083923142606276, 'lr': 0.009942783503232005, 'weight_decay': 0.007109913828491938}. Best is trial 3 with value: 0.6748069652489254.\n",
      "[I 2025-04-23 14:53:42,111] Trial 8 finished with value: 0.6774662690503257 and parameters: {'hidden_dim': 53, 'num_layers': 2, 'dropout': 0.19690159492213108, 'lr': 0.0005141768886346245, 'weight_decay': 0.009986340491838594}. Best is trial 3 with value: 0.6748069652489254.\n",
      "[I 2025-04-23 14:54:05,964] Trial 9 finished with value: 0.677062485899244 and parameters: {'hidden_dim': 49, 'num_layers': 1, 'dropout': 0.1691479372212128, 'lr': 0.00948259197277765, 'weight_decay': 0.009592639865423516}. Best is trial 3 with value: 0.6748069652489254.\n",
      "[I 2025-04-23 14:54:35,917] Trial 10 finished with value: 0.676977493933269 and parameters: {'hidden_dim': 128, 'num_layers': 3, 'dropout': 0.3011517281286277, 'lr': 0.0027370855660143467, 'weight_decay': 0.00029050163727432216}. Best is trial 3 with value: 0.6748069652489254.\n",
      "[I 2025-04-23 14:55:06,019] Trial 11 finished with value: 0.41990141783441814 and parameters: {'hidden_dim': 67, 'num_layers': 1, 'dropout': 0.2843297057078276, 'lr': 0.003892013255511007, 'weight_decay': 0.0024356851079415155}. Best is trial 11 with value: 0.41990141783441814.\n",
      "[I 2025-04-23 14:55:29,803] Trial 12 finished with value: 0.4322843477129936 and parameters: {'hidden_dim': 62, 'num_layers': 1, 'dropout': 0.27980267985684054, 'lr': 0.0025169785829769436, 'weight_decay': 0.0019842368970374204}. Best is trial 11 with value: 0.41990141783441814.\n",
      "[I 2025-04-23 14:55:51,522] Trial 13 finished with value: 0.6267192576612745 and parameters: {'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.24261039843805107, 'lr': 0.0027434053848080214, 'weight_decay': 0.0023356013366361435}. Best is trial 11 with value: 0.41990141783441814.\n",
      "[I 2025-04-23 14:56:12,767] Trial 14 finished with value: 0.29744882030146463 and parameters: {'hidden_dim': 57, 'num_layers': 1, 'dropout': 0.3466216842628215, 'lr': 0.002848168458219481, 'weight_decay': 0.0008267741158526356}. Best is trial 14 with value: 0.29744882030146463.\n",
      "[I 2025-04-23 14:56:37,066] Trial 15 finished with value: 0.26085375009902884 and parameters: {'hidden_dim': 57, 'num_layers': 1, 'dropout': 0.36021858978414467, 'lr': 0.005795624831498474, 'weight_decay': 1.6568212934199144e-05}. Best is trial 15 with value: 0.26085375009902884.\n",
      "[I 2025-04-23 14:56:59,014] Trial 16 finished with value: 0.2830094833459173 and parameters: {'hidden_dim': 50, 'num_layers': 1, 'dropout': 0.36071061600174464, 'lr': 0.006305563483554014, 'weight_decay': 0.0002687398783010051}. Best is trial 15 with value: 0.26085375009902884.\n",
      "[I 2025-04-23 14:57:24,900] Trial 17 finished with value: 0.29062163616929737 and parameters: {'hidden_dim': 46, 'num_layers': 3, 'dropout': 0.10149760304109334, 'lr': 0.0063347467463512755, 'weight_decay': 3.789782590208742e-05}. Best is trial 15 with value: 0.26085375009902884.\n",
      "[I 2025-04-23 14:57:51,417] Trial 18 finished with value: 0.676862107855933 and parameters: {'hidden_dim': 43, 'num_layers': 2, 'dropout': 0.3767823480179073, 'lr': 0.00594076657701881, 'weight_decay': 0.006432907883742266}. Best is trial 15 with value: 0.26085375009902884.\n",
      "[I 2025-04-23 14:58:12,692] Trial 19 finished with value: 0.6760380182947431 and parameters: {'hidden_dim': 78, 'num_layers': 1, 'dropout': 0.3453856261322912, 'lr': 0.008198958406947206, 'weight_decay': 0.003472487762623494}. Best is trial 15 with value: 0.26085375009902884.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 57, 'num_layers': 1, 'dropout': 0.36021858978414467, 'lr': 0.005795624831498474, 'weight_decay': 1.6568212934199144e-05}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# BiLSTM with Attention Classifier\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[286  65]\n",
      " [ 83 405]]\n",
      "Sensitivity: 0.8299, Specificity: 0.8148\n",
      "Epoch [1/30] - Train Loss: 0.6521, Val Loss: 0.4506, Val Acc: 0.8236, Val AUC: 0.8791\n",
      "\n",
      "Confusion Matrix:\n",
      "[[319  32]\n",
      " [ 88 400]]\n",
      "Sensitivity: 0.8197, Specificity: 0.9088\n",
      "Epoch [2/30] - Train Loss: 0.3696, Val Loss: 0.3281, Val Acc: 0.8570, Val AUC: 0.9336\n",
      "\n",
      "Confusion Matrix:\n",
      "[[262  89]\n",
      " [ 20 468]]\n",
      "Sensitivity: 0.9590, Specificity: 0.7464\n",
      "Epoch [3/30] - Train Loss: 0.3307, Val Loss: 0.2898, Val Acc: 0.8701, Val AUC: 0.9432\n",
      "\n",
      "Confusion Matrix:\n",
      "[[308  43]\n",
      " [ 45 443]]\n",
      "Sensitivity: 0.9078, Specificity: 0.8775\n",
      "Epoch [4/30] - Train Loss: 0.2826, Val Loss: 0.2682, Val Acc: 0.8951, Val AUC: 0.9489\n",
      "\n",
      "Confusion Matrix:\n",
      "[[302  49]\n",
      " [ 39 449]]\n",
      "Sensitivity: 0.9201, Specificity: 0.8604\n",
      "Epoch [5/30] - Train Loss: 0.2771, Val Loss: 0.2622, Val Acc: 0.8951, Val AUC: 0.9513\n",
      "\n",
      "Confusion Matrix:\n",
      "[[280  71]\n",
      " [ 28 460]]\n",
      "Sensitivity: 0.9426, Specificity: 0.7977\n",
      "Epoch [6/30] - Train Loss: 0.2594, Val Loss: 0.2992, Val Acc: 0.8820, Val AUC: 0.9415\n",
      "\n",
      "Confusion Matrix:\n",
      "[[321  30]\n",
      " [ 70 418]]\n",
      "Sensitivity: 0.8566, Specificity: 0.9145\n",
      "Epoch [7/30] - Train Loss: 0.2660, Val Loss: 0.2747, Val Acc: 0.8808, Val AUC: 0.9552\n",
      "\n",
      "Confusion Matrix:\n",
      "[[300  51]\n",
      " [ 37 451]]\n",
      "Sensitivity: 0.9242, Specificity: 0.8547\n",
      "Epoch [8/30] - Train Loss: 0.2502, Val Loss: 0.2517, Val Acc: 0.8951, Val AUC: 0.9529\n",
      "\n",
      "Confusion Matrix:\n",
      "[[294  57]\n",
      " [ 26 462]]\n",
      "Sensitivity: 0.9467, Specificity: 0.8376\n",
      "Epoch [9/30] - Train Loss: 0.2374, Val Loss: 0.2600, Val Acc: 0.9011, Val AUC: 0.9536\n",
      "\n",
      "Confusion Matrix:\n",
      "[[309  42]\n",
      " [ 39 449]]\n",
      "Sensitivity: 0.9201, Specificity: 0.8803\n",
      "Epoch [10/30] - Train Loss: 0.2401, Val Loss: 0.2424, Val Acc: 0.9035, Val AUC: 0.9567\n",
      "\n",
      "Confusion Matrix:\n",
      "[[296  55]\n",
      " [ 21 467]]\n",
      "Sensitivity: 0.9570, Specificity: 0.8433\n",
      "Epoch [11/30] - Train Loss: 0.2253, Val Loss: 0.2462, Val Acc: 0.9094, Val AUC: 0.9572\n",
      "\n",
      "Confusion Matrix:\n",
      "[[302  49]\n",
      " [ 24 464]]\n",
      "Sensitivity: 0.9508, Specificity: 0.8604\n",
      "Epoch [12/30] - Train Loss: 0.2173, Val Loss: 0.2446, Val Acc: 0.9130, Val AUC: 0.9569\n",
      "\n",
      "Confusion Matrix:\n",
      "[[319  32]\n",
      " [ 47 441]]\n",
      "Sensitivity: 0.9037, Specificity: 0.9088\n",
      "Epoch [13/30] - Train Loss: 0.2222, Val Loss: 0.2298, Val Acc: 0.9058, Val AUC: 0.9642\n",
      "\n",
      "Confusion Matrix:\n",
      "[[275  76]\n",
      " [ 18 470]]\n",
      "Sensitivity: 0.9631, Specificity: 0.7835\n",
      "Epoch [14/30] - Train Loss: 0.2438, Val Loss: 0.2512, Val Acc: 0.8880, Val AUC: 0.9589\n",
      "\n",
      "Confusion Matrix:\n",
      "[[305  46]\n",
      " [ 32 456]]\n",
      "Sensitivity: 0.9344, Specificity: 0.8689\n",
      "Epoch [15/30] - Train Loss: 0.2193, Val Loss: 0.2416, Val Acc: 0.9070, Val AUC: 0.9600\n",
      "\n",
      "Confusion Matrix:\n",
      "[[310  41]\n",
      " [ 31 457]]\n",
      "Sensitivity: 0.9365, Specificity: 0.8832\n",
      "Epoch [16/30] - Train Loss: 0.1954, Val Loss: 0.2237, Val Acc: 0.9142, Val AUC: 0.9631\n",
      "\n",
      "Confusion Matrix:\n",
      "[[310  41]\n",
      " [ 41 447]]\n",
      "Sensitivity: 0.9160, Specificity: 0.8832\n",
      "Epoch [17/30] - Train Loss: 0.1922, Val Loss: 0.2361, Val Acc: 0.9023, Val AUC: 0.9608\n",
      "\n",
      "Confusion Matrix:\n",
      "[[306  45]\n",
      " [ 33 455]]\n",
      "Sensitivity: 0.9324, Specificity: 0.8718\n",
      "Epoch [18/30] - Train Loss: 0.1749, Val Loss: 0.2353, Val Acc: 0.9070, Val AUC: 0.9595\n",
      "\n",
      "Confusion Matrix:\n",
      "[[301  50]\n",
      " [ 27 461]]\n",
      "Sensitivity: 0.9447, Specificity: 0.8575\n",
      "Epoch [19/30] - Train Loss: 0.1687, Val Loss: 0.2333, Val Acc: 0.9082, Val AUC: 0.9633\n",
      "\n",
      "Confusion Matrix:\n",
      "[[319  32]\n",
      " [ 45 443]]\n",
      "Sensitivity: 0.9078, Specificity: 0.9088\n",
      "Epoch [20/30] - Train Loss: 0.1547, Val Loss: 0.2362, Val Acc: 0.9082, Val AUC: 0.9641\n",
      "\n",
      "Confusion Matrix:\n",
      "[[308  43]\n",
      " [ 31 457]]\n",
      "Sensitivity: 0.9365, Specificity: 0.8775\n",
      "Epoch [21/30] - Train Loss: 0.1478, Val Loss: 0.2401, Val Acc: 0.9118, Val AUC: 0.9622\n",
      "\n",
      "Confusion Matrix:\n",
      "[[312  39]\n",
      " [ 33 455]]\n",
      "Sensitivity: 0.9324, Specificity: 0.8889\n",
      "Epoch [22/30] - Train Loss: 0.1329, Val Loss: 0.2313, Val Acc: 0.9142, Val AUC: 0.9671\n",
      "\n",
      "Confusion Matrix:\n",
      "[[307  44]\n",
      " [ 27 461]]\n",
      "Sensitivity: 0.9447, Specificity: 0.8746\n",
      "Epoch [23/30] - Train Loss: 0.1447, Val Loss: 0.2344, Val Acc: 0.9154, Val AUC: 0.9638\n",
      "\n",
      "Confusion Matrix:\n",
      "[[326  25]\n",
      " [ 95 393]]\n",
      "Sensitivity: 0.8053, Specificity: 0.9288\n",
      "Epoch [24/30] - Train Loss: 0.1518, Val Loss: 0.3198, Val Acc: 0.8570, Val AUC: 0.9551\n",
      "\n",
      "Confusion Matrix:\n",
      "[[260  91]\n",
      " [ 23 465]]\n",
      "Sensitivity: 0.9529, Specificity: 0.7407\n",
      "Epoch [25/30] - Train Loss: 0.3590, Val Loss: 0.3145, Val Acc: 0.8641, Val AUC: 0.9242\n",
      "\n",
      "Confusion Matrix:\n",
      "[[304  47]\n",
      " [ 41 447]]\n",
      "Sensitivity: 0.9160, Specificity: 0.8661\n",
      "Epoch [26/30] - Train Loss: 0.3102, Val Loss: 0.2669, Val Acc: 0.8951, Val AUC: 0.9457\n",
      "\n",
      "Confusion Matrix:\n",
      "[[282  69]\n",
      " [ 26 462]]\n",
      "Sensitivity: 0.9467, Specificity: 0.8034\n",
      "Epoch [27/30] - Train Loss: 0.2797, Val Loss: 0.2624, Val Acc: 0.8868, Val AUC: 0.9543\n",
      "\n",
      "Confusion Matrix:\n",
      "[[319  32]\n",
      " [ 47 441]]\n",
      "Sensitivity: 0.9037, Specificity: 0.9088\n",
      "Epoch [28/30] - Train Loss: 0.2452, Val Loss: 0.2322, Val Acc: 0.9058, Val AUC: 0.9622\n",
      "\n",
      "Confusion Matrix:\n",
      "[[300  51]\n",
      " [ 21 467]]\n",
      "Sensitivity: 0.9570, Specificity: 0.8547\n",
      "Epoch [29/30] - Train Loss: 0.2219, Val Loss: 0.2284, Val Acc: 0.9142, Val AUC: 0.9641\n",
      "\n",
      "Confusion Matrix:\n",
      "[[316  35]\n",
      " [ 46 442]]\n",
      "Sensitivity: 0.9057, Specificity: 0.9003\n",
      "Epoch [30/30] - Train Loss: 0.2079, Val Loss: 0.2206, Val Acc: 0.9035, Val AUC: 0.9653\n",
      "\n",
      "Confusion Matrix:\n",
      "[[322  29]\n",
      " [ 46 442]]\n",
      "Sensitivity: 0.9057, Specificity: 0.9174\n",
      "Test Loss: 0.2299, Test Accuracy: 0.9106, Test AUC: 0.9684\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model-bilstm_attention.pt')\n",
    "\n",
    "\n",
    "model = BiLSTMWithAttentionClassifier(\n",
    "    input_dim=20,\n",
    "    hidden_dim=bilstm_attn_best_param['hidden_dim'],\n",
    "    num_layers=bilstm_attn_best_param['num_layers'],\n",
    "    dropout=bilstm_attn_best_param['dropout']\n",
    ")\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=30, lr=bilstm_attn_best_param['lr'],\n",
    "                      weight_decay=bilstm_attn_best_param['weight_decay'], verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning to TB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'L', 'K', 'H', 'Y', 'F', 'V', 'G', 'I', 'C', 'Q', 'W', 'T', 'X', 'D', 'A', 'E', 'R', 'N', 'S', 'P', 'M'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 264\n",
      "Validation: 88\n",
      "Test: 88\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 19:07:42,281] A new study created in memory with name: no-name-d7c71c9f-59c8-4e1a-a9e6-b7aa0d23dd3b\n",
      "[I 2025-04-23 19:07:45,561] Trial 0 finished with value: 0.36615941921869916 and parameters: {'lr': 0.0003421494037646235, 'weight_decay': 0.0004250163658044531, 'dropout': 0.13677730546064848}. Best is trial 0 with value: 0.36615941921869916.\n",
      "[I 2025-04-23 19:07:48,349] Trial 1 finished with value: 0.36754122376441956 and parameters: {'lr': 2.5077722361751643e-05, 'weight_decay': 0.0015172041766497702, 'dropout': 0.41458997666720165}. Best is trial 0 with value: 0.36615941921869916.\n",
      "[I 2025-04-23 19:07:51,376] Trial 2 finished with value: 0.3668985068798065 and parameters: {'lr': 0.0002971047036254324, 'weight_decay': 3.964686129915509e-06, 'dropout': 0.4306700073327977}. Best is trial 0 with value: 0.36615941921869916.\n",
      "[I 2025-04-23 19:07:54,556] Trial 3 finished with value: 0.360005646944046 and parameters: {'lr': 0.0015067224145193059, 'weight_decay': 1.5878315049002529e-06, 'dropout': 0.49447759793049473}. Best is trial 3 with value: 0.360005646944046.\n",
      "[I 2025-04-23 19:07:57,617] Trial 4 finished with value: 0.36754026015599567 and parameters: {'lr': 1.119748660635548e-05, 'weight_decay': 0.004490060798350587, 'dropout': 0.2412198672864433}. Best is trial 3 with value: 0.360005646944046.\n",
      "[I 2025-04-23 19:08:00,689] Trial 5 finished with value: 0.3587215344111125 and parameters: {'lr': 0.0015120171374454062, 'weight_decay': 0.006318963209860363, 'dropout': 0.3223013291101864}. Best is trial 5 with value: 0.3587215344111125.\n",
      "[I 2025-04-23 19:08:03,511] Trial 6 finished with value: 0.36756696303685504 and parameters: {'lr': 1.7679488558564144e-05, 'weight_decay': 0.00028596974102045595, 'dropout': 0.17335371965147545}. Best is trial 5 with value: 0.3587215344111125.\n",
      "[I 2025-04-23 19:08:06,603] Trial 7 finished with value: 0.3618978063265483 and parameters: {'lr': 0.0012247946808830136, 'weight_decay': 5.105991352611759e-06, 'dropout': 0.1251804882694264}. Best is trial 5 with value: 0.3587215344111125.\n",
      "[I 2025-04-23 19:08:09,858] Trial 8 finished with value: 0.3675224880377452 and parameters: {'lr': 2.533000040236616e-05, 'weight_decay': 0.0015071635613465768, 'dropout': 0.10049143559466756}. Best is trial 5 with value: 0.3587215344111125.\n",
      "[I 2025-04-23 19:08:13,662] Trial 9 finished with value: 0.3587099413077037 and parameters: {'lr': 0.0023234636012357678, 'weight_decay': 2.744594070410518e-06, 'dropout': 0.3532428934431129}. Best is trial 9 with value: 0.3587099413077037.\n",
      "[I 2025-04-23 19:08:16,600] Trial 10 finished with value: 0.34070682525634766 and parameters: {'lr': 0.007266830441509175, 'weight_decay': 1.807168287319578e-05, 'dropout': 0.3146691419392218}. Best is trial 10 with value: 0.34070682525634766.\n",
      "[I 2025-04-23 19:08:19,804] Trial 11 finished with value: 0.33668726682662964 and parameters: {'lr': 0.008237152384374042, 'weight_decay': 2.7872343756178457e-05, 'dropout': 0.32120572110083057}. Best is trial 11 with value: 0.33668726682662964.\n",
      "[I 2025-04-23 19:08:23,635] Trial 12 finished with value: 0.3313956956068675 and parameters: {'lr': 0.009776019820156315, 'weight_decay': 2.7638039363217505e-05, 'dropout': 0.26704999697372}. Best is trial 12 with value: 0.3313956956068675.\n",
      "[I 2025-04-23 19:08:27,898] Trial 13 finished with value: 0.3367028733094533 and parameters: {'lr': 0.00766322460806435, 'weight_decay': 2.6076607080303907e-05, 'dropout': 0.2342189474247619}. Best is trial 12 with value: 0.3313956956068675.\n",
      "[I 2025-04-23 19:08:30,940] Trial 14 finished with value: 0.3306034157673518 and parameters: {'lr': 0.00937817644509991, 'weight_decay': 4.896518418786971e-05, 'dropout': 0.2483648137668667}. Best is trial 14 with value: 0.3306034157673518.\n",
      "[I 2025-04-23 19:08:33,749] Trial 15 finished with value: 0.34801144401232403 and parameters: {'lr': 0.003509378638608163, 'weight_decay': 9.608006529387152e-05, 'dropout': 0.23639545158328562}. Best is trial 14 with value: 0.3306034157673518.\n",
      "[I 2025-04-23 19:08:36,846] Trial 16 finished with value: 0.3673021097977956 and parameters: {'lr': 0.0001466876861063266, 'weight_decay': 8.775306783640095e-05, 'dropout': 0.19637591895593934}. Best is trial 14 with value: 0.3306034157673518.\n",
      "[I 2025-04-23 19:08:39,969] Trial 17 finished with value: 0.3467513422171275 and parameters: {'lr': 0.003949284280797894, 'weight_decay': 9.899250901336487e-06, 'dropout': 0.2770682009398341}. Best is trial 14 with value: 0.3306034157673518.\n",
      "[I 2025-04-23 19:08:43,039] Trial 18 finished with value: 0.3643195827802022 and parameters: {'lr': 0.0006446662736010023, 'weight_decay': 5.958538869701079e-05, 'dropout': 0.35882885663094927}. Best is trial 14 with value: 0.3306034157673518.\n",
      "[I 2025-04-23 19:08:46,140] Trial 19 finished with value: 0.3672949473063151 and parameters: {'lr': 0.0001306685031156137, 'weight_decay': 0.0002340517613237621, 'dropout': 0.2776076217130521}. Best is trial 14 with value: 0.3306034157673518.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.00937817644509991, 'weight_decay': 4.896518418786971e-05, 'dropout': 0.2483648137668667}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation function with detailed output\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    # print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    # print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    # print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function for frozen encoder\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-lstm-trans-frozen/FrozenEncoder_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        # if val_auc > best_val_auc:\n",
    "        #     best_val_auc = val_auc\n",
    "        #     torch.save(model.state_dict(), 'best_model_frozen.pt')\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=dropout)\n",
    "    model.load_state_dict(torch.load('best_model_lstm_1.pt'))\n",
    "    \n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_frozen_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.009551633347012976,\n",
       " 'weight_decay': 3.398187179038976e-05,\n",
       " 'dropout': 0.3700572272856807}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_frozen_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8068, AUC: 0.9253\n",
      "Sensitivity: 0.8780, Specificity: 0.7447\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 5 36]]\n",
      "Epoch [1/20] - Train Loss: 0.4642, Val Loss: 0.3667, Val Acc: 0.8068, Val AUC: 0.9253\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8182, AUC: 0.9248\n",
      "Sensitivity: 0.9024, Specificity: 0.7447\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 4 37]]\n",
      "Epoch [2/20] - Train Loss: 0.4323, Val Loss: 0.3651, Val Acc: 0.8182, Val AUC: 0.9248\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8068, AUC: 0.9273\n",
      "Sensitivity: 0.8780, Specificity: 0.7447\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 5 36]]\n",
      "Epoch [3/20] - Train Loss: 0.4276, Val Loss: 0.3609, Val Acc: 0.8068, Val AUC: 0.9273\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8068, AUC: 0.9273\n",
      "Sensitivity: 0.8780, Specificity: 0.7447\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 5 36]]\n",
      "Epoch [4/20] - Train Loss: 0.4091, Val Loss: 0.3616, Val Acc: 0.8068, Val AUC: 0.9273\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8295, AUC: 0.9273\n",
      "Sensitivity: 0.8537, Specificity: 0.8085\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 6 35]]\n",
      "Epoch [5/20] - Train Loss: 0.4014, Val Loss: 0.3556, Val Acc: 0.8295, Val AUC: 0.9273\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8182, AUC: 0.9279\n",
      "Sensitivity: 0.8049, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 8 33]]\n",
      "Epoch [6/20] - Train Loss: 0.4272, Val Loss: 0.3498, Val Acc: 0.8182, Val AUC: 0.9279\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8182, AUC: 0.9294\n",
      "Sensitivity: 0.8049, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 8 33]]\n",
      "Epoch [7/20] - Train Loss: 0.3960, Val Loss: 0.3485, Val Acc: 0.8182, Val AUC: 0.9294\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8295, AUC: 0.9279\n",
      "Sensitivity: 0.8293, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 7 34]]\n",
      "Epoch [8/20] - Train Loss: 0.3925, Val Loss: 0.3477, Val Acc: 0.8295, Val AUC: 0.9279\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8182, AUC: 0.9279\n",
      "Sensitivity: 0.8049, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 8 33]]\n",
      "Epoch [9/20] - Train Loss: 0.3635, Val Loss: 0.3460, Val Acc: 0.8182, Val AUC: 0.9279\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8295, AUC: 0.9284\n",
      "Sensitivity: 0.8293, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 7 34]]\n",
      "Epoch [10/20] - Train Loss: 0.3817, Val Loss: 0.3455, Val Acc: 0.8295, Val AUC: 0.9284\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8295, AUC: 0.9273\n",
      "Sensitivity: 0.8293, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 7 34]]\n",
      "Epoch [11/20] - Train Loss: 0.4203, Val Loss: 0.3448, Val Acc: 0.8295, Val AUC: 0.9273\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8182, AUC: 0.9279\n",
      "Sensitivity: 0.8049, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 8 33]]\n",
      "Epoch [12/20] - Train Loss: 0.3886, Val Loss: 0.3412, Val Acc: 0.8182, Val AUC: 0.9279\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8182, AUC: 0.9294\n",
      "Sensitivity: 0.7805, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 9 32]]\n",
      "Epoch [13/20] - Train Loss: 0.3715, Val Loss: 0.3372, Val Acc: 0.8182, Val AUC: 0.9294\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8182, AUC: 0.9325\n",
      "Sensitivity: 0.8049, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 8 33]]\n",
      "Epoch [14/20] - Train Loss: 0.3603, Val Loss: 0.3378, Val Acc: 0.8182, Val AUC: 0.9325\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8295, AUC: 0.9362\n",
      "Sensitivity: 0.8293, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 7 34]]\n",
      "Epoch [15/20] - Train Loss: 0.3623, Val Loss: 0.3355, Val Acc: 0.8295, Val AUC: 0.9362\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8409, AUC: 0.9377\n",
      "Sensitivity: 0.8537, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 6 35]]\n",
      "Epoch [16/20] - Train Loss: 0.3655, Val Loss: 0.3332, Val Acc: 0.8409, Val AUC: 0.9377\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8409, AUC: 0.9403\n",
      "Sensitivity: 0.8293, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 7 34]]\n",
      "Epoch [17/20] - Train Loss: 0.3580, Val Loss: 0.3299, Val Acc: 0.8409, Val AUC: 0.9403\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8409, AUC: 0.9398\n",
      "Sensitivity: 0.8293, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 7 34]]\n",
      "Epoch [18/20] - Train Loss: 0.3407, Val Loss: 0.3272, Val Acc: 0.8409, Val AUC: 0.9398\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8523, AUC: 0.9414\n",
      "Sensitivity: 0.8537, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 6 35]]\n",
      "Epoch [19/20] - Train Loss: 0.3866, Val Loss: 0.3265, Val Acc: 0.8523, Val AUC: 0.9414\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8409, AUC: 0.9419\n",
      "Sensitivity: 0.8293, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 7 34]]\n",
      "Epoch [20/20] - Train Loss: 0.3631, Val Loss: 0.3255, Val Acc: 0.8409, Val AUC: 0.9419\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7614, AUC: 0.8635\n",
      "Sensitivity: 0.6829, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [13 28]]\n",
      "Test Loss: 0.4711, Test Accuracy: 0.7614, Test AUC: 0.8635\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-trans-frozen/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Best hyperparameters: {'lr': 0.009940295438316211, 'weight_decay': 1.4383289881186473e-05, 'dropout': 0.22563027249521914}\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=lstm_frozen_best_param['dropout'])\n",
    "model.load_state_dict(torch.load('best_model_lstm_1.pt'))\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_frozen_best_param['lr'],\n",
    "                      weight_decay=lstm_frozen_best_param['weight_decay'], verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 19:10:00,236] A new study created in memory with name: no-name-47b00de3-4ff9-4117-98e5-c439150c42ae\n",
      "[I 2025-04-23 19:10:04,130] Trial 0 finished with value: 0.2731936772664388 and parameters: {'lr': 0.0008719156911495195, 'weight_decay': 0.00021053686616933335, 'dropout': 0.1690739061239185}. Best is trial 0 with value: 0.2731936772664388.\n",
      "[I 2025-04-23 19:10:07,631] Trial 1 finished with value: 0.2821635405222575 and parameters: {'lr': 0.0011333976808246989, 'weight_decay': 2.0410947626281652e-06, 'dropout': 0.4850335313011026}. Best is trial 0 with value: 0.2731936772664388.\n",
      "[I 2025-04-23 19:10:11,054] Trial 2 finished with value: 0.2769099672635396 and parameters: {'lr': 0.007759789758562864, 'weight_decay': 0.006043468518985285, 'dropout': 0.19854968228576558}. Best is trial 0 with value: 0.2731936772664388.\n",
      "[I 2025-04-23 19:10:14,595] Trial 3 finished with value: 0.3614562253157298 and parameters: {'lr': 2.0135206000433684e-05, 'weight_decay': 6.603300441324877e-06, 'dropout': 0.17445961393134893}. Best is trial 0 with value: 0.2731936772664388.\n",
      "[I 2025-04-23 19:10:18,037] Trial 4 finished with value: 0.3263178567091624 and parameters: {'lr': 0.00017017805794972886, 'weight_decay': 7.592602314477376e-06, 'dropout': 0.382212784084737}. Best is trial 0 with value: 0.2731936772664388.\n",
      "[I 2025-04-23 19:10:21,464] Trial 5 finished with value: 0.2672518938779831 and parameters: {'lr': 0.002410951910044168, 'weight_decay': 0.0012224463013911875, 'dropout': 0.2101444515762618}. Best is trial 5 with value: 0.2672518938779831.\n",
      "[I 2025-04-23 19:10:24,886] Trial 6 finished with value: 0.3044931987921397 and parameters: {'lr': 0.009396785326855001, 'weight_decay': 0.000873251463960479, 'dropout': 0.12083698158960816}. Best is trial 5 with value: 0.2672518938779831.\n",
      "[I 2025-04-23 19:10:28,209] Trial 7 finished with value: 0.36401309569676715 and parameters: {'lr': 1.3771012553977263e-05, 'weight_decay': 4.157310585365412e-06, 'dropout': 0.22081869136009571}. Best is trial 5 with value: 0.2672518938779831.\n",
      "[I 2025-04-23 19:10:31,629] Trial 8 finished with value: 0.3360666235287984 and parameters: {'lr': 0.00010687660936988119, 'weight_decay': 0.0001336065413437596, 'dropout': 0.1707052620273783}. Best is trial 5 with value: 0.2672518938779831.\n",
      "[I 2025-04-23 19:10:35,112] Trial 9 finished with value: 0.34867780407269794 and parameters: {'lr': 9.261595052118744e-05, 'weight_decay': 0.00024576433155123833, 'dropout': 0.3643441768409038}. Best is trial 5 with value: 0.2672518938779831.\n",
      "[I 2025-04-23 19:10:38,650] Trial 10 finished with value: 0.2334538996219635 and parameters: {'lr': 0.0022198189193588687, 'weight_decay': 0.009965975951580993, 'dropout': 0.2836752045455191}. Best is trial 10 with value: 0.2334538996219635.\n",
      "[I 2025-04-23 19:10:42,261] Trial 11 finished with value: 0.24734390527009964 and parameters: {'lr': 0.0020725848644746344, 'weight_decay': 0.007314786703298354, 'dropout': 0.2838151634536179}. Best is trial 10 with value: 0.2334538996219635.\n",
      "[I 2025-04-23 19:10:45,823] Trial 12 finished with value: 0.23981788257757822 and parameters: {'lr': 0.002147154175019864, 'weight_decay': 0.008746468733917151, 'dropout': 0.28832054417745967}. Best is trial 10 with value: 0.2334538996219635.\n",
      "[I 2025-04-23 19:10:49,307] Trial 13 finished with value: 0.2808384746313095 and parameters: {'lr': 0.00044527595616865307, 'weight_decay': 0.0021720219888017355, 'dropout': 0.30089916696740326}. Best is trial 10 with value: 0.2334538996219635.\n",
      "[I 2025-04-23 19:10:52,888] Trial 14 finished with value: 0.2596953213214874 and parameters: {'lr': 0.004256243636722789, 'weight_decay': 0.008426975822144717, 'dropout': 0.30637794353166037}. Best is trial 10 with value: 0.2334538996219635.\n",
      "[I 2025-04-23 19:10:56,420] Trial 15 finished with value: 0.28719839453697205 and parameters: {'lr': 0.0005017268699673915, 'weight_decay': 2.5362976099218873e-05, 'dropout': 0.2606805171513502}. Best is trial 10 with value: 0.2334538996219635.\n",
      "[I 2025-04-23 19:10:59,877] Trial 16 finished with value: 0.2678905824820201 and parameters: {'lr': 0.0021253791618316715, 'weight_decay': 0.002948659033516662, 'dropout': 0.3619244747711947}. Best is trial 10 with value: 0.2334538996219635.\n",
      "[I 2025-04-23 19:11:03,245] Trial 17 finished with value: 0.30520788828531903 and parameters: {'lr': 0.004381506665901164, 'weight_decay': 0.0005484037367683119, 'dropout': 0.43317735380943045}. Best is trial 10 with value: 0.2334538996219635.\n",
      "[I 2025-04-23 19:11:06,706] Trial 18 finished with value: 0.26054680844148 and parameters: {'lr': 0.0010353851866247367, 'weight_decay': 3.726366382785499e-05, 'dropout': 0.25450038056602353}. Best is trial 10 with value: 0.2334538996219635.\n",
      "[I 2025-04-23 19:11:10,148] Trial 19 finished with value: 0.3125241200129191 and parameters: {'lr': 0.0002565193037518684, 'weight_decay': 0.0024159717570396504, 'dropout': 0.33112542292978664}. Best is trial 10 with value: 0.2334538996219635.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.0022198189193588687, 'weight_decay': 0.009965975951580993, 'dropout': 0.2836752045455191}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation function with detailed output\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    # print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    # print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    # print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function for frozen encoder\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    log_dir = f\"runs-lstm-transfer-fullbackprop/fullbackprop_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        # if val_auc > best_val_auc:\n",
    "        #     best_val_auc = val_auc\n",
    "        #     torch.save(model.state_dict(), 'best_model_frozen.pt')\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_full_backprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# # Load the best pretrained model and fine-tune\n",
    "# def finetune_with_frozen_encoder(pretrained_model_path, train_loader, val_loader, hidden_dim, num_layers, dropout):\n",
    "#     model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "#     model.load_state_dict(torch.load(pretrained_model_path))\n",
    "#     # freeze_encoder(model)\n",
    "\n",
    "#     best_auc = train_finetune_model(\n",
    "#         model=model,\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         num_epochs=10,\n",
    "#         lr=1e-3,\n",
    "#         weight_decay=1e-4\n",
    "#     )\n",
    "\n",
    "#     model.load_state_dict(torch.load('best_model_frozen.pt'))\n",
    "#     evaluate_model(model, val_loader, nn.BCELoss())\n",
    "\n",
    "#     return model, best_auc\n",
    "\n",
    "# model, best_auc = finetune_with_frozen_encoder(\n",
    "#     pretrained_model_path='best_model-lstm.pt',\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     hidden_dim=47,  # or from Optuna\n",
    "#     num_layers=2,\n",
    "#     dropout=0.3\n",
    "# )\n",
    "\n",
    "\n",
    "# lstm_best_param = {'hidden_dim': 74,\n",
    "#  'num_layers': 3,\n",
    "#  'dropout': 0.3037059572844035,\n",
    "#  'lr': 0.00774103421243492,\n",
    "#  'weight_decay': 2.4221276513292614e-05}\n",
    "\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=dropout)\n",
    "    model.load_state_dict(torch.load('best_model_lstm_1.pt'))\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_fullbackprop_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5455, AUC: 0.7203\n",
      "Sensitivity: 0.0488, Specificity: 0.9787\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [39  2]]\n",
      "Epoch [1/19] - Train Loss: 0.6917, Val Loss: 334.0234, Val Acc: 0.5455, Val AUC: 0.7203\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7483\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [2/19] - Train Loss: 0.6869, Val Loss: 334.0203, Val Acc: 0.5341, Val AUC: 0.7483\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7514\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [3/19] - Train Loss: 0.6866, Val Loss: 334.0181, Val Acc: 0.5341, Val AUC: 0.7514\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7577\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [4/19] - Train Loss: 0.6814, Val Loss: 334.0157, Val Acc: 0.5341, Val AUC: 0.7577\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5455, AUC: 0.7566\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Epoch [5/19] - Train Loss: 0.6786, Val Loss: 334.0130, Val Acc: 0.5455, Val AUC: 0.7566\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5455, AUC: 0.7675\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Epoch [6/19] - Train Loss: 0.6702, Val Loss: 334.0086, Val Acc: 0.5455, Val AUC: 0.7675\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5455, AUC: 0.7903\n",
      "Sensitivity: 0.0732, Specificity: 0.9574\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [38  3]]\n",
      "Epoch [7/19] - Train Loss: 0.6674, Val Loss: 334.0004, Val Acc: 0.5455, Val AUC: 0.7903\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6705, AUC: 0.7981\n",
      "Sensitivity: 0.4146, Specificity: 0.8936\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [24 17]]\n",
      "Epoch [8/19] - Train Loss: 0.6518, Val Loss: 333.9798, Val Acc: 0.6705, Val AUC: 0.7981\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6477, AUC: 0.8127\n",
      "Sensitivity: 1.0000, Specificity: 0.3404\n",
      "Confusion Matrix:\n",
      "[[16 31]\n",
      " [ 0 41]]\n",
      "Epoch [9/19] - Train Loss: 0.6066, Val Loss: 333.9900, Val Acc: 0.6477, Val AUC: 0.8127\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5568, AUC: 0.8028\n",
      "Sensitivity: 1.0000, Specificity: 0.1702\n",
      "Confusion Matrix:\n",
      "[[ 8 39]\n",
      " [ 0 41]]\n",
      "Epoch [10/19] - Train Loss: 0.6585, Val Loss: 334.0025, Val Acc: 0.5568, Val AUC: 0.8028\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6932, AUC: 0.7789\n",
      "Sensitivity: 0.8537, Specificity: 0.5532\n",
      "Confusion Matrix:\n",
      "[[26 21]\n",
      " [ 6 35]]\n",
      "Epoch [11/19] - Train Loss: 0.6577, Val Loss: 333.9990, Val Acc: 0.6932, Val AUC: 0.7789\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7273, AUC: 0.7841\n",
      "Sensitivity: 0.7073, Specificity: 0.7447\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [12 29]]\n",
      "Epoch [12/19] - Train Loss: 0.6567, Val Loss: 333.9952, Val Acc: 0.7273, Val AUC: 0.7841\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6705, AUC: 0.8012\n",
      "Sensitivity: 0.5854, Specificity: 0.7447\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [17 24]]\n",
      "Epoch [13/19] - Train Loss: 0.6512, Val Loss: 333.9885, Val Acc: 0.6705, Val AUC: 0.8012\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7159, AUC: 0.8179\n",
      "Sensitivity: 0.5854, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [17 24]]\n",
      "Epoch [14/19] - Train Loss: 0.6384, Val Loss: 333.9763, Val Acc: 0.7159, Val AUC: 0.8179\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7386, AUC: 0.7732\n",
      "Sensitivity: 0.5854, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [17 24]]\n",
      "Epoch [15/19] - Train Loss: 0.5999, Val Loss: 333.9440, Val Acc: 0.7386, Val AUC: 0.7732\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7727, AUC: 0.8454\n",
      "Sensitivity: 0.8537, Specificity: 0.7021\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [ 6 35]]\n",
      "Epoch [16/19] - Train Loss: 0.5300, Val Loss: 333.8761, Val Acc: 0.7727, Val AUC: 0.8454\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7727, AUC: 0.8599\n",
      "Sensitivity: 0.9024, Specificity: 0.6596\n",
      "Confusion Matrix:\n",
      "[[31 16]\n",
      " [ 4 37]]\n",
      "Epoch [17/19] - Train Loss: 0.5381, Val Loss: 333.8571, Val Acc: 0.7727, Val AUC: 0.8599\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8068, AUC: 0.8646\n",
      "Sensitivity: 0.8537, Specificity: 0.7660\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 6 35]]\n",
      "Epoch [18/19] - Train Loss: 0.4797, Val Loss: 333.8427, Val Acc: 0.8068, Val AUC: 0.8646\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8068, AUC: 0.8480\n",
      "Sensitivity: 0.8537, Specificity: 0.7660\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 6 35]]\n",
      "Epoch [19/19] - Train Loss: 0.4806, Val Loss: 333.8343, Val Acc: 0.8068, Val AUC: 0.8480\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8182, AUC: 0.8563\n",
      "Sensitivity: 0.7561, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [10 31]]\n",
      "Test Loss: 333.8182, Test Accuracy: 0.8182, Test AUC: 0.8563\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 1000.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Best hyperparameters: {'lr': 0.008986542560528932, 'weight_decay': 2.3033044758439348e-06, 'dropout': 0.17164705350229123}\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=113, num_layers=1, dropout=lstm_fullbackprop_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=19, lr=lstm_fullbackprop_best_param['lr'],\n",
    "                      weight_decay=lstm_fullbackprop_best_param['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:10,612] A new study created in memory with name: no-name-7f3452ef-93c1-4031-a4b3-40a6e5f6b86c\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:13,870] Trial 0 finished with value: 0.33782506982485455 and parameters: {'lr': 0.0017796059973143941, 'weight_decay': 0.0006860970999920002, 'dropout': 0.24971109052865048}. Best is trial 0 with value: 0.33782506982485455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:16,988] Trial 1 finished with value: 0.47190314531326294 and parameters: {'lr': 0.00017329513203733415, 'weight_decay': 0.0003277855200596469, 'dropout': 0.3319246594049984}. Best is trial 0 with value: 0.33782506982485455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:20,294] Trial 2 finished with value: 0.6631995042165121 and parameters: {'lr': 1.1594290503343794e-05, 'weight_decay': 2.6363332455964796e-06, 'dropout': 0.4756554488885625}. Best is trial 0 with value: 0.33782506982485455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:23,248] Trial 3 finished with value: 0.45222267508506775 and parameters: {'lr': 0.0002295415132426954, 'weight_decay': 1.128547454948887e-06, 'dropout': 0.36176939040013434}. Best is trial 0 with value: 0.33782506982485455.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:26,315] Trial 4 finished with value: 0.31914270917574566 and parameters: {'lr': 0.009020543969954435, 'weight_decay': 0.00030106082830825, 'dropout': 0.3291155724256972}. Best is trial 4 with value: 0.31914270917574566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:29,273] Trial 5 finished with value: 0.3927656610806783 and parameters: {'lr': 0.0003444172431024544, 'weight_decay': 6.317017202442747e-06, 'dropout': 0.10967923814581684}. Best is trial 4 with value: 0.31914270917574566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:32,199] Trial 6 finished with value: 0.3388697604338328 and parameters: {'lr': 0.0018423104676231136, 'weight_decay': 4.6192508965538966e-05, 'dropout': 0.42688370558908373}. Best is trial 4 with value: 0.31914270917574566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:35,160] Trial 7 finished with value: 0.4336910843849182 and parameters: {'lr': 0.0003003883031895655, 'weight_decay': 4.591804215169564e-06, 'dropout': 0.45547495827890383}. Best is trial 4 with value: 0.31914270917574566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:38,121] Trial 8 finished with value: 0.34522779285907745 and parameters: {'lr': 0.001266145225660757, 'weight_decay': 0.0013558270308641907, 'dropout': 0.19271992342554523}. Best is trial 4 with value: 0.31914270917574566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:41,078] Trial 9 finished with value: 0.34467100103696185 and parameters: {'lr': 0.0010487150320152832, 'weight_decay': 0.0003529319360927194, 'dropout': 0.349736545850439}. Best is trial 4 with value: 0.31914270917574566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:44,034] Trial 10 finished with value: 0.3269038846095403 and parameters: {'lr': 0.005264790330463692, 'weight_decay': 0.008597220731054012, 'dropout': 0.27038432008715263}. Best is trial 4 with value: 0.31914270917574566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:46,976] Trial 11 finished with value: 0.32173876464366913 and parameters: {'lr': 0.008748265807767885, 'weight_decay': 0.007916138100659312, 'dropout': 0.24086835563091996}. Best is trial 4 with value: 0.31914270917574566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:49,932] Trial 12 finished with value: 0.35712339480717975 and parameters: {'lr': 0.009266687410741811, 'weight_decay': 0.006055079642655287, 'dropout': 0.20934850490505155}. Best is trial 4 with value: 0.31914270917574566.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:52,890] Trial 13 finished with value: 0.3142252465089162 and parameters: {'lr': 0.00491353830118281, 'weight_decay': 5.231631318620915e-05, 'dropout': 0.1661628877750591}. Best is trial 13 with value: 0.3142252465089162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:55,842] Trial 14 finished with value: 0.3552434742450714 and parameters: {'lr': 0.00398265291361465, 'weight_decay': 3.664676526658605e-05, 'dropout': 0.11866639357097115}. Best is trial 13 with value: 0.3142252465089162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:18:58,834] Trial 15 finished with value: 0.5744276841481527 and parameters: {'lr': 6.210660620524913e-05, 'weight_decay': 2.4721741996841613e-05, 'dropout': 0.16849286694881843}. Best is trial 13 with value: 0.3142252465089162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:19:01,821] Trial 16 finished with value: 0.3293154885371526 and parameters: {'lr': 0.0029856580771336636, 'weight_decay': 0.00011289756187139816, 'dropout': 0.3044798318782619}. Best is trial 13 with value: 0.3142252465089162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:19:05,468] Trial 17 finished with value: 0.3585430483023326 and parameters: {'lr': 0.0007886097945175969, 'weight_decay': 9.54087965348904e-05, 'dropout': 0.39939353770752495}. Best is trial 13 with value: 0.3142252465089162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:19:08,625] Trial 18 finished with value: 0.3300894796848297 and parameters: {'lr': 0.009936845871955969, 'weight_decay': 1.4876732036569162e-05, 'dropout': 0.16981542824018322}. Best is trial 13 with value: 0.3142252465089162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:19:11,833] Trial 19 finished with value: 0.33923161029815674 and parameters: {'lr': 0.003350592139587679, 'weight_decay': 0.0015952353229986833, 'dropout': 0.28796133159502}. Best is trial 13 with value: 0.3142252465089162.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.00491353830118281, 'weight_decay': 5.231631318620915e-05, 'dropout': 0.1661628877750591}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm-trans-frozen/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\" Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=dropout, max_seq_len=100)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.009277756892612359,\n",
       " 'weight_decay': 0.007087210931281583,\n",
       " 'dropout': 0.17885881665986722}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_frozen_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 10 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.8085\n",
      "Epoch [1/15] - Train Loss: 0.4553, Val Loss: 0.3799, Val Acc: 0.8409, Val AUC: 0.9196\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.8085\n",
      "Epoch [2/15] - Train Loss: 0.4534, Val Loss: 0.3738, Val Acc: 0.8409, Val AUC: 0.9206\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.8085\n",
      "Epoch [3/15] - Train Loss: 0.4803, Val Loss: 0.3688, Val Acc: 0.8409, Val AUC: 0.9216\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8085\n",
      "Epoch [4/15] - Train Loss: 0.4556, Val Loss: 0.3632, Val Acc: 0.8295, Val AUC: 0.9206\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8085\n",
      "Epoch [5/15] - Train Loss: 0.3995, Val Loss: 0.3591, Val Acc: 0.8295, Val AUC: 0.9206\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8085\n",
      "Epoch [6/15] - Train Loss: 0.4033, Val Loss: 0.3563, Val Acc: 0.8182, Val AUC: 0.9211\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8085\n",
      "Epoch [7/15] - Train Loss: 0.3967, Val Loss: 0.3538, Val Acc: 0.8068, Val AUC: 0.9196\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8298\n",
      "Epoch [8/15] - Train Loss: 0.3920, Val Loss: 0.3516, Val Acc: 0.8182, Val AUC: 0.9206\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8298\n",
      "Epoch [9/15] - Train Loss: 0.4021, Val Loss: 0.3512, Val Acc: 0.8068, Val AUC: 0.9222\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8298\n",
      "Epoch [10/15] - Train Loss: 0.4181, Val Loss: 0.3490, Val Acc: 0.8068, Val AUC: 0.9211\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8298\n",
      "Epoch [11/15] - Train Loss: 0.3959, Val Loss: 0.3467, Val Acc: 0.8068, Val AUC: 0.9216\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8298\n",
      "Epoch [12/15] - Train Loss: 0.3787, Val Loss: 0.3445, Val Acc: 0.8068, Val AUC: 0.9216\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8298\n",
      "Epoch [13/15] - Train Loss: 0.3919, Val Loss: 0.3430, Val Acc: 0.8068, Val AUC: 0.9232\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.8298\n",
      "Epoch [14/15] - Train Loss: 0.3746, Val Loss: 0.3426, Val Acc: 0.7955, Val AUC: 0.9216\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.8298\n",
      "Epoch [15/15] - Train Loss: 0.4037, Val Loss: 0.3420, Val Acc: 0.7955, Val AUC: 0.9222\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.9362\n",
      "Test Loss: 0.3659, Test Accuracy: 0.8523, Test AUC: 0.9294\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=15, lr=bilstm_frozen_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:49:27,583] A new study created in memory with name: no-name-2aedfca4-db32-469f-ba31-13e6d311fc63\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:49:32,000] Trial 0 finished with value: 0.3112034499645233 and parameters: {'lr': 0.0001630809222721005, 'weight_decay': 0.0014185998512110736, 'dropout': 0.3094651336127348}. Best is trial 0 with value: 0.3112034499645233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:49:36,447] Trial 1 finished with value: 0.6052219867706299 and parameters: {'lr': 2.17970935508799e-05, 'weight_decay': 0.00012195594491309195, 'dropout': 0.3324465246496219}. Best is trial 0 with value: 0.3112034499645233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:49:41,483] Trial 2 finished with value: 0.31596994400024414 and parameters: {'lr': 8.633050305716488e-05, 'weight_decay': 0.00010327828087684382, 'dropout': 0.153362603302576}. Best is trial 0 with value: 0.3112034499645233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:49:46,077] Trial 3 finished with value: 0.6251201430956522 and parameters: {'lr': 1.7773577855966884e-05, 'weight_decay': 0.004039824748652052, 'dropout': 0.11601707621568083}. Best is trial 0 with value: 0.3112034499645233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:49:50,608] Trial 4 finished with value: 0.2063147028287252 and parameters: {'lr': 0.008649702897983806, 'weight_decay': 7.343799085764715e-05, 'dropout': 0.2627943398207111}. Best is trial 4 with value: 0.2063147028287252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:49:55,150] Trial 5 finished with value: 0.20751512547334036 and parameters: {'lr': 0.0009908680310138712, 'weight_decay': 2.6343705639715834e-06, 'dropout': 0.47960890072696416}. Best is trial 4 with value: 0.2063147028287252.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:49:59,744] Trial 6 finished with value: 0.17355355123678842 and parameters: {'lr': 0.0022969046140146362, 'weight_decay': 0.00020692205694312508, 'dropout': 0.2653725139193013}. Best is trial 6 with value: 0.17355355123678842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:04,290] Trial 7 finished with value: 0.5980440974235535 and parameters: {'lr': 2.6233200642325638e-05, 'weight_decay': 0.00037746257703974906, 'dropout': 0.17501687939480243}. Best is trial 6 with value: 0.17355355123678842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:08,820] Trial 8 finished with value: 0.6252854665120443 and parameters: {'lr': 1.8815912100371568e-05, 'weight_decay': 0.00048664444611804355, 'dropout': 0.45535738535915604}. Best is trial 6 with value: 0.17355355123678842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:13,324] Trial 9 finished with value: 0.32983121275901794 and parameters: {'lr': 9.930882925600388e-05, 'weight_decay': 4.271261332060684e-06, 'dropout': 0.4756577041007801}. Best is trial 6 with value: 0.17355355123678842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:17,840] Trial 10 finished with value: 0.14472179114818573 and parameters: {'lr': 0.001315725792745674, 'weight_decay': 1.7103931520673425e-05, 'dropout': 0.2352209978719757}. Best is trial 10 with value: 0.14472179114818573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:22,418] Trial 11 finished with value: 0.21829814463853836 and parameters: {'lr': 0.0013617437923293956, 'weight_decay': 9.8592221935793e-06, 'dropout': 0.2375051349582395}. Best is trial 10 with value: 0.14472179114818573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:26,942] Trial 12 finished with value: 0.16366789738337198 and parameters: {'lr': 0.003201668498454252, 'weight_decay': 2.1493110730768297e-05, 'dropout': 0.37733465532025456}. Best is trial 10 with value: 0.14472179114818573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:31,462] Trial 13 finished with value: 0.12681863456964493 and parameters: {'lr': 0.005922507544333748, 'weight_decay': 1.8843479188414696e-05, 'dropout': 0.384651040546839}. Best is trial 13 with value: 0.12681863456964493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:35,916] Trial 14 finished with value: 0.16718310117721558 and parameters: {'lr': 0.009507967044425674, 'weight_decay': 2.517113606649571e-05, 'dropout': 0.3618072662296803}. Best is trial 13 with value: 0.12681863456964493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:40,393] Trial 15 finished with value: 0.27748754620552063 and parameters: {'lr': 0.0005017308912893484, 'weight_decay': 1.232268517759142e-06, 'dropout': 0.4056539646237916}. Best is trial 13 with value: 0.12681863456964493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:45,057] Trial 16 finished with value: 0.13791860764225325 and parameters: {'lr': 0.0037726683316992075, 'weight_decay': 3.129448126363808e-05, 'dropout': 0.20827031042253003}. Best is trial 13 with value: 0.12681863456964493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:49,606] Trial 17 finished with value: 0.14744721601406732 and parameters: {'lr': 0.004488945212229494, 'weight_decay': 4.631893897484662e-05, 'dropout': 0.19643010882613368}. Best is trial 13 with value: 0.12681863456964493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:54,233] Trial 18 finished with value: 0.2612822155157725 and parameters: {'lr': 0.0004724208956804686, 'weight_decay': 6.813720363520621e-06, 'dropout': 0.4076099344021139}. Best is trial 13 with value: 0.12681863456964493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 8 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:50:58,967] Trial 19 finished with value: 0.11863645383467276 and parameters: {'lr': 0.005139257627001614, 'weight_decay': 1.58936096912024e-06, 'dropout': 0.10381630405473648}. Best is trial 19 with value: 0.11863645383467276.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.005139257627001614, 'weight_decay': 1.58936096912024e-06, 'dropout': 0.10381630405473648}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm-trans-fullbackprop/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\" Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=dropout, max_seq_len=100)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=20, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 10 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8723\n",
      "Epoch [1/10] - Train Loss: 0.3904, Val Loss: 0.3236, Val Acc: 0.8295, Val AUC: 0.9258\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.8511\n",
      "Epoch [2/10] - Train Loss: 0.4071, Val Loss: 0.3029, Val Acc: 0.8750, Val AUC: 0.9429\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.7660\n",
      "Epoch [3/10] - Train Loss: 0.2641, Val Loss: 0.2714, Val Acc: 0.8636, Val AUC: 0.9611\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Epoch [4/10] - Train Loss: 0.2490, Val Loss: 0.2050, Val Acc: 0.9091, Val AUC: 0.9766\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8936\n",
      "Epoch [5/10] - Train Loss: 0.1890, Val Loss: 0.1722, Val Acc: 0.9318, Val AUC: 0.9891\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.9149\n",
      "Epoch [6/10] - Train Loss: 0.1746, Val Loss: 0.1491, Val Acc: 0.9432, Val AUC: 0.9927\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.9149\n",
      "Epoch [7/10] - Train Loss: 0.1379, Val Loss: 0.1219, Val Acc: 0.9432, Val AUC: 0.9948\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.9149\n",
      "Epoch [8/10] - Train Loss: 0.1877, Val Loss: 0.1329, Val Acc: 0.9318, Val AUC: 0.9896\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.9149\n",
      "Epoch [9/10] - Train Loss: 0.1187, Val Loss: 0.1518, Val Acc: 0.9318, Val AUC: 0.9870\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.9149\n",
      "Epoch [10/10] - Train Loss: 0.1022, Val Loss: 0.1164, Val Acc: 0.9432, Val AUC: 0.9938\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.8511\n",
      "Test Loss: 0.4061, Test Accuracy: 0.8636, Test AUC: 0.9497\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_fullbackprop_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "# freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10, lr=bilstm_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:28:51,438] A new study created in memory with name: no-name-7fe782fe-7653-4b2c-bf07-2f463f54019e\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:28:55,410] Trial 0 finished with value: 0.6924989223480225 and parameters: {'lr': 1.0580136216771827e-05, 'weight_decay': 0.0009512716234453152, 'dropout': 0.3953395628451303}. Best is trial 0 with value: 0.6924989223480225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:28:59,447] Trial 1 finished with value: 0.6928654511769613 and parameters: {'lr': 2.847710109219892e-05, 'weight_decay': 7.3712995153454e-05, 'dropout': 0.20889720525185643}. Best is trial 0 with value: 0.6924989223480225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:29:02,659] Trial 2 finished with value: 0.6935778260231018 and parameters: {'lr': 1.583331176332376e-05, 'weight_decay': 0.0003543898578538054, 'dropout': 0.19224481367883386}. Best is trial 0 with value: 0.6924989223480225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:29:05,700] Trial 3 finished with value: 0.6927499572436014 and parameters: {'lr': 0.00012827221086175963, 'weight_decay': 0.0003131589863213475, 'dropout': 0.22429838041364558}. Best is trial 0 with value: 0.6924989223480225.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:29:09,488] Trial 4 finished with value: 0.6922715703646342 and parameters: {'lr': 0.00021150963987984675, 'weight_decay': 2.6529918505428453e-05, 'dropout': 0.3155001938105344}. Best is trial 4 with value: 0.6922715703646342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:29:12,550] Trial 5 finished with value: 0.6924922664960226 and parameters: {'lr': 0.00024138342960457648, 'weight_decay': 0.00019154645874221038, 'dropout': 0.42426275429085436}. Best is trial 4 with value: 0.6922715703646342.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:29:15,575] Trial 6 finished with value: 0.6911651889483134 and parameters: {'lr': 0.00995296041769307, 'weight_decay': 0.006376606974890003, 'dropout': 0.35492210608982966}. Best is trial 6 with value: 0.6911651889483134.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:29:18,618] Trial 7 finished with value: 0.6911958456039429 and parameters: {'lr': 0.0011455631548746318, 'weight_decay': 0.0004313434404314324, 'dropout': 0.4587199767331821}. Best is trial 6 with value: 0.6911651889483134.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:29:21,694] Trial 8 finished with value: 0.6923718055089315 and parameters: {'lr': 3.544237899891655e-05, 'weight_decay': 0.003752596411821664, 'dropout': 0.30358643594891777}. Best is trial 6 with value: 0.6911651889483134.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:29:24,740] Trial 9 finished with value: 0.6935398777325948 and parameters: {'lr': 1.737412596930617e-05, 'weight_decay': 0.0007114309441361794, 'dropout': 0.43111019336944756}. Best is trial 6 with value: 0.6911651889483134.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:29:27,777] Trial 10 finished with value: 0.6910457412401835 and parameters: {'lr': 0.006007981239342854, 'weight_decay': 2.070457120160532e-06, 'dropout': 0.11014022979526603}. Best is trial 10 with value: 0.6910457412401835.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:29:30,854] Trial 11 finished with value: 0.6911030411720276 and parameters: {'lr': 0.008147455174583559, 'weight_decay': 1.0195666644097763e-06, 'dropout': 0.11206693697639139}. Best is trial 10 with value: 0.6910457412401835.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.006007981239342854, 'weight_decay': 2.070457120160532e-06, 'dropout': 0.11014022979526603}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM with Attention\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze LSTM encoder\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-att-trans/LSTMAttTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_att_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Load compatible weights\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\" Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=dropout)\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=12)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_att_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.00810126296687975,\n",
       " 'weight_decay': 0.0004883045455184517,\n",
       " 'dropout': 0.48419922377224167}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_att_frozen_best_parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [1/20] - Train Loss: 0.6966, Val Loss: 0.6936, Val Acc: 0.4659, Val AUC: 0.4556\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/20] - Train Loss: 0.6918, Val Loss: 0.6915, Val Acc: 0.5341, Val AUC: 0.5558\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/20] - Train Loss: 0.6920, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6508\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/20] - Train Loss: 0.6899, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.6850\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/20] - Train Loss: 0.6859, Val Loss: 0.6906, Val Acc: 0.5341, Val AUC: 0.7115\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/20] - Train Loss: 0.6953, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.7239\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/20] - Train Loss: 0.6919, Val Loss: 0.6901, Val Acc: 0.5341, Val AUC: 0.7369\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [8/20] - Train Loss: 0.6887, Val Loss: 0.6898, Val Acc: 0.5341, Val AUC: 0.7410\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [9/20] - Train Loss: 0.6914, Val Loss: 0.6896, Val Acc: 0.5341, Val AUC: 0.7494\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [10/20] - Train Loss: 0.6910, Val Loss: 0.6895, Val Acc: 0.5341, Val AUC: 0.7592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [11/20] - Train Loss: 0.6879, Val Loss: 0.6894, Val Acc: 0.5341, Val AUC: 0.7582\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [12/20] - Train Loss: 0.6875, Val Loss: 0.6890, Val Acc: 0.5341, Val AUC: 0.7587\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [13/20] - Train Loss: 0.6872, Val Loss: 0.6888, Val Acc: 0.5341, Val AUC: 0.7613\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [14/20] - Train Loss: 0.6875, Val Loss: 0.6886, Val Acc: 0.5341, Val AUC: 0.7602\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [15/20] - Train Loss: 0.6905, Val Loss: 0.6885, Val Acc: 0.5341, Val AUC: 0.7602\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [16/20] - Train Loss: 0.6872, Val Loss: 0.6882, Val Acc: 0.5341, Val AUC: 0.7639\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [17/20] - Train Loss: 0.6876, Val Loss: 0.6880, Val Acc: 0.5341, Val AUC: 0.7660\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [18/20] - Train Loss: 0.6876, Val Loss: 0.6878, Val Acc: 0.5341, Val AUC: 0.7683\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [19/20] - Train Loss: 0.6832, Val Loss: 0.6876, Val Acc: 0.5341, Val AUC: 0.7680\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [20/20] - Train Loss: 0.6833, Val Loss: 0.6876, Val Acc: 0.5341, Val AUC: 0.7701\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Test Loss: 0.6899, Test Accuracy: 0.5341, Test AUC: 0.7462\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=lstm_att_frozen_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_att_frozen_best_parameters['lr'],\n",
    "                      weight_decay=lstm_att_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full back prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:42:43,184] A new study created in memory with name: no-name-3825c6e9-f91e-4d1c-b0d5-6d96a7dc3193\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:42:46,771] Trial 0 finished with value: 0.6911980112393697 and parameters: {'lr': 0.004873649656152712, 'weight_decay': 0.0018066159944397585, 'dropout': 0.4499048480713421}. Best is trial 0 with value: 0.6911980112393697.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:42:50,089] Trial 1 finished with value: 0.6908102631568909 and parameters: {'lr': 0.00025824032175719594, 'weight_decay': 1.6350171426021518e-05, 'dropout': 0.48441601880092366}. Best is trial 1 with value: 0.6908102631568909.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:42:53,438] Trial 2 finished with value: 0.6909380356470743 and parameters: {'lr': 0.0001744808175292812, 'weight_decay': 7.384515809631804e-06, 'dropout': 0.11418645849108114}. Best is trial 1 with value: 0.6908102631568909.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:42:56,780] Trial 3 finished with value: 0.6763949990272522 and parameters: {'lr': 0.0006822424753366178, 'weight_decay': 2.784206787326874e-06, 'dropout': 0.2955324462588349}. Best is trial 3 with value: 0.6763949990272522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:43:00,119] Trial 4 finished with value: 0.6919279098510742 and parameters: {'lr': 7.018573809497234e-05, 'weight_decay': 0.0003551762930137449, 'dropout': 0.1047542967371915}. Best is trial 3 with value: 0.6763949990272522.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:43:03,909] Trial 5 finished with value: 0.44153182705243427 and parameters: {'lr': 0.008533473355820425, 'weight_decay': 6.0585356805829684e-05, 'dropout': 0.4254629953316178}. Best is trial 5 with value: 0.44153182705243427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:43:07,316] Trial 6 finished with value: 0.6911950508753458 and parameters: {'lr': 0.0042639645632647025, 'weight_decay': 0.003101136196325409, 'dropout': 0.10518946383002553}. Best is trial 5 with value: 0.44153182705243427.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W 2025-04-23 16:43:08,458] Trial 7 failed with parameters: {'lr': 0.0021917385902615813, 'weight_decay': 2.8760754204001105e-06, 'dropout': 0.2553925515213533} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_57934/4764393.py\", line 142, in objective\n",
      "    return train_finetune_model(\n",
      "  File \"/tmp/ipykernel_57934/4764393.py\", line 96, in train_finetune_model\n",
      "    for packed_input, labels in train_loader:\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 708, in __next__\n",
      "    data = self._next_data()\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py\", line 764, in _next_data\n",
      "    data = self._dataset_fetcher.fetch(index)  # may raise StopIteration\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py\", line 52, in <listcomp>\n",
      "    data = [self.dataset[idx] for idx in possibly_batched_index]\n",
      "  File \"/tmp/ipykernel_57934/3995740769.py\", line 25, in __getitem__\n",
      "    return one_hot_torch(seq, dtype=self.one_hot_dtype), torch.tensor(label, dtype=torch.float32), length\n",
      "  File \"/tmp/ipykernel_57934/3995740769.py\", line 8, in one_hot_torch\n",
      "    arr[i, seq_bytes == aa] = 1\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-23 16:43:08,462] Trial 7 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[72], line 149\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[38;5;66;03m# Run optimization\u001b[39;00m\n\u001b[1;32m    148\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 149\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m12\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    152\u001b[0m lstm_att_fullbackprop_best_parameters \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[72], line 142\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    139\u001b[0m model \u001b[38;5;241m=\u001b[39m load_partial_weights(model, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbest_model-lstm_attention.pt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    140\u001b[0m \u001b[38;5;66;03m# freeze_encoder(model)\u001b[39;00m\n\u001b[0;32m--> 142\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrain_finetune_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\n\u001b[1;32m    145\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[72], line 96\u001b[0m, in \u001b[0;36mtrain_finetune_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr, weight_decay, device)\u001b[0m\n\u001b[1;32m     93\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     94\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 96\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m packed_input, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     97\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     98\u001b[0m     packed_input \u001b[38;5;241m=\u001b[39m packed_input\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[42], line 25\u001b[0m, in \u001b[0;36mSequenceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[1;32m     24\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(seq\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# unpadded length\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mone_hot_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot_dtype\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), length\n",
      "Cell \u001b[0;32mIn[42], line 8\u001b[0m, in \u001b[0;36mone_hot_torch\u001b[0;34m(seq, dtype)\u001b[0m\n\u001b[1;32m      6\u001b[0m arr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(amino_acids), \u001b[38;5;28mlen\u001b[39m(seq_bytes), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, aa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(aa_bytes):\n\u001b[0;32m----> 8\u001b[0m     arr[i, seq_bytes \u001b[38;5;241m==\u001b[39m aa] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM with Attention\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze LSTM encoder\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-att-trans-fullback/LSTMAttTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_att_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Load compatible weights\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\" Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=dropout)\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=12)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_att_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/20] - Train Loss: 0.6922, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6668\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/20] - Train Loss: 0.6897, Val Loss: 0.6898, Val Acc: 0.5341, Val AUC: 0.7229\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/20] - Train Loss: 0.6916, Val Loss: 0.6882, Val Acc: 0.5341, Val AUC: 0.7265\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/20] - Train Loss: 0.6832, Val Loss: 0.6853, Val Acc: 0.5341, Val AUC: 0.7395\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/20] - Train Loss: 0.6817, Val Loss: 0.6773, Val Acc: 0.5341, Val AUC: 0.7172\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [28 13]]\n",
      "Sensitivity: 0.3171, Specificity: 0.9149\n",
      "Epoch [6/20] - Train Loss: 0.6532, Val Loss: 0.6450, Val Acc: 0.6364, Val AUC: 0.7177\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.8298\n",
      "Epoch [7/20] - Train Loss: 0.6381, Val Loss: 0.6896, Val Acc: 0.7045, Val AUC: 0.7135\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1 46]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0213\n",
      "Epoch [8/20] - Train Loss: 0.7011, Val Loss: 0.7384, Val Acc: 0.4773, Val AUC: 0.7488\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 4 43]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0851\n",
      "Epoch [9/20] - Train Loss: 0.7054, Val Loss: 0.6810, Val Acc: 0.5114, Val AUC: 0.8121\n",
      "\n",
      "Confusion Matrix:\n",
      "[[26 21]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.5532\n",
      "Epoch [10/20] - Train Loss: 0.6684, Val Loss: 0.6652, Val Acc: 0.7159, Val AUC: 0.8142\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [23 18]]\n",
      "Sensitivity: 0.4390, Specificity: 0.9149\n",
      "Epoch [11/20] - Train Loss: 0.6439, Val Loss: 0.6368, Val Acc: 0.6932, Val AUC: 0.8018\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.8936\n",
      "Epoch [12/20] - Train Loss: 0.6038, Val Loss: 0.5960, Val Acc: 0.7045, Val AUC: 0.8142\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8085\n",
      "Epoch [13/20] - Train Loss: 0.5784, Val Loss: 0.5470, Val Acc: 0.7955, Val AUC: 0.8293\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.7234\n",
      "Epoch [14/20] - Train Loss: 0.5313, Val Loss: 0.5011, Val Acc: 0.7955, Val AUC: 0.8505\n",
      "\n",
      "Confusion Matrix:\n",
      "[[24 23]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.5106\n",
      "Epoch [15/20] - Train Loss: 0.5203, Val Loss: 0.5469, Val Acc: 0.7159, Val AUC: 0.8417\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.7447\n",
      "Epoch [16/20] - Train Loss: 0.5556, Val Loss: 0.4667, Val Acc: 0.8182, Val AUC: 0.8568\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.7447\n",
      "Epoch [17/20] - Train Loss: 0.5520, Val Loss: 0.4864, Val Acc: 0.8068, Val AUC: 0.8485\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.7234\n",
      "Epoch [18/20] - Train Loss: 0.4880, Val Loss: 0.4662, Val Acc: 0.8068, Val AUC: 0.8651\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [14 27]]\n",
      "Sensitivity: 0.6585, Specificity: 0.8723\n",
      "Epoch [19/20] - Train Loss: 0.4949, Val Loss: 0.5208, Val Acc: 0.7727, Val AUC: 0.8339\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.7447\n",
      "Epoch [20/20] - Train Loss: 0.4314, Val Loss: 0.4657, Val Acc: 0.8182, Val AUC: 0.8661\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8085\n",
      "Test Loss: 0.4630, Test Accuracy: 0.8182, Test AUC: 0.8672\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=lstm_att_fullbackprop_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_att_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=lstm_att_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:57:48,251] A new study created in memory with name: no-name-d1c679e9-df7e-45fc-bf27-f1ab0e634fe8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:57:51,431] Trial 0 finished with value: 0.6912128726641337 and parameters: {'lr': 0.001213335490262817, 'weight_decay': 1.6501065190473084e-06, 'dropout': 0.17599660706302178}. Best is trial 0 with value: 0.6912128726641337.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:57:54,715] Trial 1 finished with value: 0.6904135942459106 and parameters: {'lr': 0.006095077057210512, 'weight_decay': 0.0002538086261707292, 'dropout': 0.33182821306421917}. Best is trial 1 with value: 0.6904135942459106.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:57:57,934] Trial 2 finished with value: 0.6911675731341044 and parameters: {'lr': 0.0015189508263390619, 'weight_decay': 0.00028545740833203445, 'dropout': 0.1270048146855595}. Best is trial 1 with value: 0.6904135942459106.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:01,036] Trial 3 finished with value: 0.6968292395273844 and parameters: {'lr': 9.901283437468789e-05, 'weight_decay': 0.0017012221509590758, 'dropout': 0.4912434478569728}. Best is trial 1 with value: 0.6904135942459106.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:04,254] Trial 4 finished with value: 0.6904257535934448 and parameters: {'lr': 0.0062877906546631125, 'weight_decay': 0.00208091177051165, 'dropout': 0.3906594449633165}. Best is trial 1 with value: 0.6904135942459106.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:07,852] Trial 5 finished with value: 0.692499041557312 and parameters: {'lr': 0.0005704978849014396, 'weight_decay': 0.00011605815834299935, 'dropout': 0.353138472850712}. Best is trial 1 with value: 0.6904135942459106.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:11,443] Trial 6 finished with value: 0.6958618561426798 and parameters: {'lr': 9.791778988019342e-05, 'weight_decay': 5.8235224679129276e-06, 'dropout': 0.43076980976193524}. Best is trial 1 with value: 0.6904135942459106.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:14,956] Trial 7 finished with value: 0.699084202448527 and parameters: {'lr': 2.8227750410959282e-05, 'weight_decay': 0.004705796619082758, 'dropout': 0.2939967490004889}. Best is trial 1 with value: 0.6904135942459106.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:17,965] Trial 8 finished with value: 0.6912475029627482 and parameters: {'lr': 0.0013458065708753483, 'weight_decay': 0.0005230067282938586, 'dropout': 0.15885094156979435}. Best is trial 1 with value: 0.6904135942459106.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:21,130] Trial 9 finished with value: 0.6923113067944845 and parameters: {'lr': 0.0006765593813336239, 'weight_decay': 1.0156555010231644e-06, 'dropout': 0.24246815703645327}. Best is trial 1 with value: 0.6904135942459106.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:24,207] Trial 10 finished with value: 0.6851838032404581 and parameters: {'lr': 0.00941696333900365, 'weight_decay': 2.053936264011462e-05, 'dropout': 0.273686714206745}. Best is trial 10 with value: 0.6851838032404581.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:27,204] Trial 11 finished with value: 0.6891086300214132 and parameters: {'lr': 0.008709294148699708, 'weight_decay': 1.1132399613762646e-05, 'dropout': 0.27117294949514725}. Best is trial 10 with value: 0.6851838032404581.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:30,208] Trial 12 finished with value: 0.6831917762756348 and parameters: {'lr': 0.009696671957702303, 'weight_decay': 1.9344957847779875e-05, 'dropout': 0.25060681073952185}. Best is trial 12 with value: 0.6831917762756348.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:33,139] Trial 13 finished with value: 0.6906208992004395 and parameters: {'lr': 0.0029094304622560596, 'weight_decay': 3.331156723610327e-05, 'dropout': 0.21872091991839038}. Best is trial 12 with value: 0.6831917762756348.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:36,255] Trial 14 finished with value: 0.694275697072347 and parameters: {'lr': 0.00018364319389385563, 'weight_decay': 2.4649187076003066e-05, 'dropout': 0.22283955902126248}. Best is trial 12 with value: 0.6831917762756348.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:39,314] Trial 15 finished with value: 0.6905526717503866 and parameters: {'lr': 0.0030172672887332115, 'weight_decay': 4.707483939780255e-05, 'dropout': 0.32788829234097294}. Best is trial 12 with value: 0.6831917762756348.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:42,777] Trial 16 finished with value: 0.7026496330897013 and parameters: {'lr': 1.1581762515163134e-05, 'weight_decay': 4.542275399753267e-06, 'dropout': 0.10262846017362962}. Best is trial 12 with value: 0.6831917762756348.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:46,713] Trial 17 finished with value: 0.6871206561724345 and parameters: {'lr': 0.00988344198243183, 'weight_decay': 1.3946810968519412e-05, 'dropout': 0.2682063135252046}. Best is trial 12 with value: 0.6831917762756348.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:49,726] Trial 18 finished with value: 0.6905550559361776 and parameters: {'lr': 0.0035087324782476035, 'weight_decay': 6.663277363391753e-05, 'dropout': 0.1839232034415129}. Best is trial 12 with value: 0.6831917762756348.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 16:58:52,792] Trial 19 finished with value: 0.6939802765846252 and parameters: {'lr': 0.000436956874076432, 'weight_decay': 2.432709300122711e-06, 'dropout': 0.38766459982215173}. Best is trial 12 with value: 0.6831917762756348.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.009696671957702303, 'weight_decay': 1.9344957847779875e-05, 'dropout': 0.25060681073952185}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm_attn-trans-frozen/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_attn_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\" Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=dropout)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [24 17]]\n",
      "Sensitivity: 0.4146, Specificity: 0.7021\n",
      "Epoch [1/50] - Train Loss: 0.6974, Val Loss: 0.6917, Val Acc: 0.5682, Val AUC: 0.6367\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/50] - Train Loss: 0.6932, Val Loss: 0.6877, Val Acc: 0.5341, Val AUC: 0.7125\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/50] - Train Loss: 0.6871, Val Loss: 0.6852, Val Acc: 0.5341, Val AUC: 0.7442\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Epoch [4/50] - Train Loss: 0.6829, Val Loss: 0.6827, Val Acc: 0.5455, Val AUC: 0.7535\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Epoch [5/50] - Train Loss: 0.6824, Val Loss: 0.6801, Val Acc: 0.5455, Val AUC: 0.7540\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/50] - Train Loss: 0.6756, Val Loss: 0.6782, Val Acc: 0.5341, Val AUC: 0.7519\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Epoch [7/50] - Train Loss: 0.6725, Val Loss: 0.6761, Val Acc: 0.5455, Val AUC: 0.7530\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Epoch [8/50] - Train Loss: 0.6817, Val Loss: 0.6744, Val Acc: 0.5455, Val AUC: 0.7519\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [33  8]]\n",
      "Sensitivity: 0.1951, Specificity: 0.9149\n",
      "Epoch [9/50] - Train Loss: 0.6700, Val Loss: 0.6719, Val Acc: 0.5795, Val AUC: 0.7556\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [32  9]]\n",
      "Sensitivity: 0.2195, Specificity: 0.9149\n",
      "Epoch [10/50] - Train Loss: 0.6709, Val Loss: 0.6700, Val Acc: 0.5909, Val AUC: 0.7530\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [28 13]]\n",
      "Sensitivity: 0.3171, Specificity: 0.9149\n",
      "Epoch [11/50] - Train Loss: 0.6613, Val Loss: 0.6681, Val Acc: 0.6364, Val AUC: 0.7535\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [28 13]]\n",
      "Sensitivity: 0.3171, Specificity: 0.9149\n",
      "Epoch [12/50] - Train Loss: 0.6631, Val Loss: 0.6660, Val Acc: 0.6364, Val AUC: 0.7530\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [29 12]]\n",
      "Sensitivity: 0.2927, Specificity: 0.9149\n",
      "Epoch [13/50] - Train Loss: 0.6612, Val Loss: 0.6642, Val Acc: 0.6250, Val AUC: 0.7519\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [27 14]]\n",
      "Sensitivity: 0.3415, Specificity: 0.9149\n",
      "Epoch [14/50] - Train Loss: 0.6602, Val Loss: 0.6624, Val Acc: 0.6477, Val AUC: 0.7535\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [29 12]]\n",
      "Sensitivity: 0.2927, Specificity: 0.9149\n",
      "Epoch [15/50] - Train Loss: 0.6649, Val Loss: 0.6609, Val Acc: 0.6250, Val AUC: 0.7545\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [23 18]]\n",
      "Sensitivity: 0.4390, Specificity: 0.9149\n",
      "Epoch [16/50] - Train Loss: 0.6625, Val Loss: 0.6593, Val Acc: 0.6932, Val AUC: 0.7545\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [17/50] - Train Loss: 0.6531, Val Loss: 0.6580, Val Acc: 0.6705, Val AUC: 0.7519\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [18/50] - Train Loss: 0.6496, Val Loss: 0.6566, Val Acc: 0.6705, Val AUC: 0.7519\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [19/50] - Train Loss: 0.6495, Val Loss: 0.6546, Val Acc: 0.6705, Val AUC: 0.7530\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8936\n",
      "Epoch [20/50] - Train Loss: 0.6412, Val Loss: 0.6530, Val Acc: 0.6932, Val AUC: 0.7530\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [25 16]]\n",
      "Sensitivity: 0.3902, Specificity: 0.9149\n",
      "Epoch [21/50] - Train Loss: 0.6437, Val Loss: 0.6516, Val Acc: 0.6705, Val AUC: 0.7530\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8936\n",
      "Epoch [22/50] - Train Loss: 0.6557, Val Loss: 0.6498, Val Acc: 0.6932, Val AUC: 0.7535\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [23/50] - Train Loss: 0.6414, Val Loss: 0.6485, Val Acc: 0.6705, Val AUC: 0.7540\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [24/50] - Train Loss: 0.6517, Val Loss: 0.6472, Val Acc: 0.6705, Val AUC: 0.7535\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.7021\n",
      "Epoch [25/50] - Train Loss: 0.6415, Val Loss: 0.6467, Val Acc: 0.6364, Val AUC: 0.7540\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [19 22]]\n",
      "Sensitivity: 0.5366, Specificity: 0.8085\n",
      "Epoch [26/50] - Train Loss: 0.6439, Val Loss: 0.6451, Val Acc: 0.6818, Val AUC: 0.7530\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [19 22]]\n",
      "Sensitivity: 0.5366, Specificity: 0.7872\n",
      "Epoch [27/50] - Train Loss: 0.6306, Val Loss: 0.6439, Val Acc: 0.6705, Val AUC: 0.7525\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32 15]\n",
      " [17 24]]\n",
      "Sensitivity: 0.5854, Specificity: 0.6809\n",
      "Epoch [28/50] - Train Loss: 0.6372, Val Loss: 0.6431, Val Acc: 0.6364, Val AUC: 0.7540\n",
      "\n",
      "Confusion Matrix:\n",
      "[[31 16]\n",
      " [15 26]]\n",
      "Sensitivity: 0.6341, Specificity: 0.6596\n",
      "Epoch [29/50] - Train Loss: 0.6521, Val Loss: 0.6423, Val Acc: 0.6477, Val AUC: 0.7540\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32 15]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.6809\n",
      "Epoch [30/50] - Train Loss: 0.6384, Val Loss: 0.6411, Val Acc: 0.6477, Val AUC: 0.7535\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [19 22]]\n",
      "Sensitivity: 0.5366, Specificity: 0.7872\n",
      "Epoch [31/50] - Train Loss: 0.6322, Val Loss: 0.6395, Val Acc: 0.6705, Val AUC: 0.7525\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [32/50] - Train Loss: 0.6477, Val Loss: 0.6383, Val Acc: 0.6705, Val AUC: 0.7540\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [33/50] - Train Loss: 0.6320, Val Loss: 0.6373, Val Acc: 0.6705, Val AUC: 0.7540\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [34/50] - Train Loss: 0.6266, Val Loss: 0.6363, Val Acc: 0.6705, Val AUC: 0.7551\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [35/50] - Train Loss: 0.6352, Val Loss: 0.6352, Val Acc: 0.6705, Val AUC: 0.7535\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [36/50] - Train Loss: 0.6287, Val Loss: 0.6343, Val Acc: 0.6705, Val AUC: 0.7540\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [20 21]]\n",
      "Sensitivity: 0.5122, Specificity: 0.7872\n",
      "Epoch [37/50] - Train Loss: 0.6263, Val Loss: 0.6335, Val Acc: 0.6591, Val AUC: 0.7525\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [38/50] - Train Loss: 0.6359, Val Loss: 0.6325, Val Acc: 0.6705, Val AUC: 0.7535\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [39/50] - Train Loss: 0.6327, Val Loss: 0.6318, Val Acc: 0.6705, Val AUC: 0.7509\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.8511\n",
      "Epoch [40/50] - Train Loss: 0.6317, Val Loss: 0.6312, Val Acc: 0.6818, Val AUC: 0.7519\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.8298\n",
      "Epoch [41/50] - Train Loss: 0.6271, Val Loss: 0.6305, Val Acc: 0.6705, Val AUC: 0.7519\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [42/50] - Train Loss: 0.6217, Val Loss: 0.6300, Val Acc: 0.6705, Val AUC: 0.7525\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [43/50] - Train Loss: 0.6252, Val Loss: 0.6290, Val Acc: 0.6705, Val AUC: 0.7530\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.8298\n",
      "Epoch [44/50] - Train Loss: 0.6270, Val Loss: 0.6281, Val Acc: 0.6705, Val AUC: 0.7530\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [19 22]]\n",
      "Sensitivity: 0.5366, Specificity: 0.7447\n",
      "Epoch [45/50] - Train Loss: 0.6177, Val Loss: 0.6275, Val Acc: 0.6477, Val AUC: 0.7509\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [20 21]]\n",
      "Sensitivity: 0.5122, Specificity: 0.7872\n",
      "Epoch [46/50] - Train Loss: 0.6186, Val Loss: 0.6267, Val Acc: 0.6591, Val AUC: 0.7509\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.7234\n",
      "Epoch [47/50] - Train Loss: 0.6261, Val Loss: 0.6263, Val Acc: 0.6477, Val AUC: 0.7504\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.8298\n",
      "Epoch [48/50] - Train Loss: 0.6111, Val Loss: 0.6253, Val Acc: 0.6705, Val AUC: 0.7499\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [49/50] - Train Loss: 0.6383, Val Loss: 0.6246, Val Acc: 0.6705, Val AUC: 0.7499\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [50/50] - Train Loss: 0.6130, Val Loss: 0.6242, Val Acc: 0.6705, Val AUC: 0.7509\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [20 21]]\n",
      "Sensitivity: 0.5122, Specificity: 0.7447\n",
      "Test Loss: 0.6527, Test Accuracy: 0.6364, Test AUC: 0.6876\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attn-trans-frozen/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=bilstm_attn_frozen_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=50, lr=bilstm_attn_frozen_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_attn_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full back prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:08:36,706] A new study created in memory with name: no-name-d0df64be-1b1f-4da8-9370-09bc8365b399\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:08:39,677] Trial 0 finished with value: 0.6910742521286011 and parameters: {'lr': 0.0028688078785574665, 'weight_decay': 0.00409532503499045, 'dropout': 0.25816961288579376}. Best is trial 0 with value: 0.6910742521286011.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:08:42,677] Trial 1 finished with value: 0.6997012495994568 and parameters: {'lr': 3.164896046158462e-05, 'weight_decay': 1.4278099877242348e-06, 'dropout': 0.38281511880183394}. Best is trial 0 with value: 0.6910742521286011.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:08:45,664] Trial 2 finished with value: 0.6974846919377645 and parameters: {'lr': 0.00018587125945051665, 'weight_decay': 4.589559909232043e-06, 'dropout': 0.14865344247749382}. Best is trial 0 with value: 0.6910742521286011.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:08:48,659] Trial 3 finished with value: 0.6983677347501119 and parameters: {'lr': 8.280417739447928e-05, 'weight_decay': 3.019471560877485e-05, 'dropout': 0.33327952007426764}. Best is trial 0 with value: 0.6910742521286011.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:08:51,653] Trial 4 finished with value: 0.6902627150217692 and parameters: {'lr': 0.0029681942379113763, 'weight_decay': 7.255721243673613e-05, 'dropout': 0.25338426410762904}. Best is trial 4 with value: 0.6902627150217692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:08:54,656] Trial 5 finished with value: 0.7005416949590048 and parameters: {'lr': 4.8454798026388156e-05, 'weight_decay': 2.176014339644594e-06, 'dropout': 0.3349249689564769}. Best is trial 4 with value: 0.6902627150217692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:08:57,689] Trial 6 finished with value: 0.6909839113553365 and parameters: {'lr': 0.001674654767355343, 'weight_decay': 1.2786459258723156e-05, 'dropout': 0.46320057821681226}. Best is trial 4 with value: 0.6902627150217692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:00,698] Trial 7 finished with value: 0.7014480233192444 and parameters: {'lr': 6.094911993617183e-05, 'weight_decay': 0.00013896791084261567, 'dropout': 0.29257460558022486}. Best is trial 4 with value: 0.6902627150217692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:03,729] Trial 8 finished with value: 0.6905463536580404 and parameters: {'lr': 0.009519224162700115, 'weight_decay': 0.0054727297786471, 'dropout': 0.25511759892675673}. Best is trial 4 with value: 0.6902627150217692.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:06,743] Trial 9 finished with value: 0.6899136503537496 and parameters: {'lr': 0.0071591694396381566, 'weight_decay': 0.00028633767941907656, 'dropout': 0.3824060894393755}. Best is trial 9 with value: 0.6899136503537496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:09,804] Trial 10 finished with value: 0.6917299628257751 and parameters: {'lr': 0.0007776761964037633, 'weight_decay': 0.00045400793242690883, 'dropout': 0.4586981008677134}. Best is trial 9 with value: 0.6899136503537496.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:12,863] Trial 11 finished with value: 0.6888040701548258 and parameters: {'lr': 0.008169746585538725, 'weight_decay': 0.00038721168043384034, 'dropout': 0.1686285797760359}. Best is trial 11 with value: 0.6888040701548258.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:15,872] Trial 12 finished with value: 0.688623309135437 and parameters: {'lr': 0.009782304004121438, 'weight_decay': 0.0005023349826674154, 'dropout': 0.1160343322344825}. Best is trial 12 with value: 0.688623309135437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:18,878] Trial 13 finished with value: 0.692325234413147 and parameters: {'lr': 0.0007100406910401789, 'weight_decay': 0.0011241344811070727, 'dropout': 0.10580440178739639}. Best is trial 12 with value: 0.688623309135437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:21,917] Trial 14 finished with value: 0.6900460322697958 and parameters: {'lr': 0.004941970239127406, 'weight_decay': 0.001117158951527526, 'dropout': 0.1811853083377467}. Best is trial 12 with value: 0.688623309135437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:24,844] Trial 15 finished with value: 0.6912442247072855 and parameters: {'lr': 0.0012166294756938492, 'weight_decay': 0.0012415690740160993, 'dropout': 0.1837551143746166}. Best is trial 12 with value: 0.688623309135437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:27,939] Trial 16 finished with value: 0.6941462755203247 and parameters: {'lr': 0.00030887768927204254, 'weight_decay': 9.04062418184831e-05, 'dropout': 0.10419588024472728}. Best is trial 12 with value: 0.688623309135437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:31,113] Trial 17 finished with value: 0.7018484671910604 and parameters: {'lr': 1.7430951060348488e-05, 'weight_decay': 0.00037331821932153796, 'dropout': 0.19669691486598456}. Best is trial 12 with value: 0.688623309135437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:34,435] Trial 18 finished with value: 0.6910401980082194 and parameters: {'lr': 0.0035466873484743676, 'weight_decay': 0.0023679882715902886, 'dropout': 0.15804344652415683}. Best is trial 12 with value: 0.688623309135437.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:09:37,429] Trial 19 finished with value: 0.6863773266474406 and parameters: {'lr': 0.008026441373823662, 'weight_decay': 3.2378568151095094e-05, 'dropout': 0.21376336929775247}. Best is trial 19 with value: 0.6863773266474406.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.008026441373823662, 'weight_decay': 3.2378568151095094e-05, 'dropout': 0.21376336929775247}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm_attn-trans-fullbackprop/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_attn_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\" Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=dropout)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded 1 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 4 43]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.0851\n",
      "Epoch [1/120] - Train Loss: 0.6972, Val Loss: 0.6923, Val Acc: 0.4773, Val AUC: 0.6715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [34  7]]\n",
      "Sensitivity: 0.1707, Specificity: 0.9787\n",
      "Epoch [2/120] - Train Loss: 0.6919, Val Loss: 0.6890, Val Acc: 0.6023, Val AUC: 0.7135\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/120] - Train Loss: 0.6860, Val Loss: 0.6860, Val Acc: 0.5341, Val AUC: 0.7234\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/120] - Train Loss: 0.6819, Val Loss: 0.6840, Val Acc: 0.5341, Val AUC: 0.7270\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/120] - Train Loss: 0.6865, Val Loss: 0.6827, Val Acc: 0.5341, Val AUC: 0.7218\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/120] - Train Loss: 0.6834, Val Loss: 0.6815, Val Acc: 0.5341, Val AUC: 0.7281\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/120] - Train Loss: 0.6759, Val Loss: 0.6800, Val Acc: 0.5341, Val AUC: 0.7286\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [8/120] - Train Loss: 0.6780, Val Loss: 0.6786, Val Acc: 0.5341, Val AUC: 0.7270\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Epoch [9/120] - Train Loss: 0.6728, Val Loss: 0.6769, Val Acc: 0.5455, Val AUC: 0.7239\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Epoch [10/120] - Train Loss: 0.6748, Val Loss: 0.6757, Val Acc: 0.5455, Val AUC: 0.7255\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [39  2]]\n",
      "Sensitivity: 0.0488, Specificity: 1.0000\n",
      "Epoch [11/120] - Train Loss: 0.6730, Val Loss: 0.6739, Val Acc: 0.5568, Val AUC: 0.7218\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [36  5]]\n",
      "Sensitivity: 0.1220, Specificity: 1.0000\n",
      "Epoch [12/120] - Train Loss: 0.6771, Val Loss: 0.6726, Val Acc: 0.5909, Val AUC: 0.7234\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [34  7]]\n",
      "Sensitivity: 0.1707, Specificity: 0.9149\n",
      "Epoch [13/120] - Train Loss: 0.6764, Val Loss: 0.6712, Val Acc: 0.5682, Val AUC: 0.7229\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [24 17]]\n",
      "Sensitivity: 0.4146, Specificity: 0.9149\n",
      "Epoch [14/120] - Train Loss: 0.6727, Val Loss: 0.6703, Val Acc: 0.6818, Val AUC: 0.7208\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [23 18]]\n",
      "Sensitivity: 0.4390, Specificity: 0.8298\n",
      "Epoch [15/120] - Train Loss: 0.6701, Val Loss: 0.6694, Val Acc: 0.6477, Val AUC: 0.7224\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [23 18]]\n",
      "Sensitivity: 0.4390, Specificity: 0.8298\n",
      "Epoch [16/120] - Train Loss: 0.6689, Val Loss: 0.6681, Val Acc: 0.6477, Val AUC: 0.7250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [26 15]]\n",
      "Sensitivity: 0.3659, Specificity: 0.9149\n",
      "Epoch [17/120] - Train Loss: 0.6629, Val Loss: 0.6665, Val Acc: 0.6591, Val AUC: 0.7255\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [26 15]]\n",
      "Sensitivity: 0.3659, Specificity: 0.9149\n",
      "Epoch [18/120] - Train Loss: 0.6619, Val Loss: 0.6651, Val Acc: 0.6591, Val AUC: 0.7260\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [28 13]]\n",
      "Sensitivity: 0.3171, Specificity: 0.9149\n",
      "Epoch [19/120] - Train Loss: 0.6617, Val Loss: 0.6638, Val Acc: 0.6364, Val AUC: 0.7286\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [34  7]]\n",
      "Sensitivity: 0.1707, Specificity: 0.9149\n",
      "Epoch [20/120] - Train Loss: 0.6613, Val Loss: 0.6629, Val Acc: 0.5682, Val AUC: 0.7296\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [34  7]]\n",
      "Sensitivity: 0.1707, Specificity: 0.9149\n",
      "Epoch [21/120] - Train Loss: 0.6623, Val Loss: 0.6620, Val Acc: 0.5682, Val AUC: 0.7281\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [32  9]]\n",
      "Sensitivity: 0.2195, Specificity: 0.9149\n",
      "Epoch [22/120] - Train Loss: 0.6566, Val Loss: 0.6608, Val Acc: 0.5909, Val AUC: 0.7250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [31 10]]\n",
      "Sensitivity: 0.2439, Specificity: 0.9149\n",
      "Epoch [23/120] - Train Loss: 0.6550, Val Loss: 0.6596, Val Acc: 0.6023, Val AUC: 0.7250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [26 15]]\n",
      "Sensitivity: 0.3659, Specificity: 0.9149\n",
      "Epoch [24/120] - Train Loss: 0.6596, Val Loss: 0.6584, Val Acc: 0.6591, Val AUC: 0.7250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [26 15]]\n",
      "Sensitivity: 0.3659, Specificity: 0.9149\n",
      "Epoch [25/120] - Train Loss: 0.6597, Val Loss: 0.6573, Val Acc: 0.6591, Val AUC: 0.7265\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [25 16]]\n",
      "Sensitivity: 0.3902, Specificity: 0.9149\n",
      "Epoch [26/120] - Train Loss: 0.6507, Val Loss: 0.6561, Val Acc: 0.6705, Val AUC: 0.7276\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [24 17]]\n",
      "Sensitivity: 0.4146, Specificity: 0.8723\n",
      "Epoch [27/120] - Train Loss: 0.6473, Val Loss: 0.6551, Val Acc: 0.6591, Val AUC: 0.7234\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [23 18]]\n",
      "Sensitivity: 0.4390, Specificity: 0.8723\n",
      "Epoch [28/120] - Train Loss: 0.6512, Val Loss: 0.6541, Val Acc: 0.6705, Val AUC: 0.7239\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8723\n",
      "Epoch [29/120] - Train Loss: 0.6467, Val Loss: 0.6531, Val Acc: 0.6818, Val AUC: 0.7250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8723\n",
      "Epoch [30/120] - Train Loss: 0.6452, Val Loss: 0.6520, Val Acc: 0.6818, Val AUC: 0.7255\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [24 17]]\n",
      "Sensitivity: 0.4146, Specificity: 0.9149\n",
      "Epoch [31/120] - Train Loss: 0.6417, Val Loss: 0.6508, Val Acc: 0.6818, Val AUC: 0.7255\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [24 17]]\n",
      "Sensitivity: 0.4146, Specificity: 0.9149\n",
      "Epoch [32/120] - Train Loss: 0.6465, Val Loss: 0.6498, Val Acc: 0.6818, Val AUC: 0.7244\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [24 17]]\n",
      "Sensitivity: 0.4146, Specificity: 0.8723\n",
      "Epoch [33/120] - Train Loss: 0.6383, Val Loss: 0.6489, Val Acc: 0.6591, Val AUC: 0.7234\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [24 17]]\n",
      "Sensitivity: 0.4146, Specificity: 0.8723\n",
      "Epoch [34/120] - Train Loss: 0.6490, Val Loss: 0.6479, Val Acc: 0.6591, Val AUC: 0.7234\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [25 16]]\n",
      "Sensitivity: 0.3902, Specificity: 0.9149\n",
      "Epoch [35/120] - Train Loss: 0.6447, Val Loss: 0.6473, Val Acc: 0.6705, Val AUC: 0.7239\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [25 16]]\n",
      "Sensitivity: 0.3902, Specificity: 0.9149\n",
      "Epoch [36/120] - Train Loss: 0.6469, Val Loss: 0.6467, Val Acc: 0.6705, Val AUC: 0.7250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [25 16]]\n",
      "Sensitivity: 0.3902, Specificity: 0.9149\n",
      "Epoch [37/120] - Train Loss: 0.6454, Val Loss: 0.6458, Val Acc: 0.6705, Val AUC: 0.7260\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [25 16]]\n",
      "Sensitivity: 0.3902, Specificity: 0.9149\n",
      "Epoch [38/120] - Train Loss: 0.6475, Val Loss: 0.6447, Val Acc: 0.6705, Val AUC: 0.7270\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8723\n",
      "Epoch [39/120] - Train Loss: 0.6530, Val Loss: 0.6439, Val Acc: 0.6818, Val AUC: 0.7244\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.8298\n",
      "Epoch [40/120] - Train Loss: 0.6365, Val Loss: 0.6434, Val Acc: 0.6705, Val AUC: 0.7239\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [20 21]]\n",
      "Sensitivity: 0.5122, Specificity: 0.7872\n",
      "Epoch [41/120] - Train Loss: 0.6322, Val Loss: 0.6428, Val Acc: 0.6591, Val AUC: 0.7239\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.8511\n",
      "Epoch [42/120] - Train Loss: 0.6489, Val Loss: 0.6419, Val Acc: 0.6818, Val AUC: 0.7239\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8723\n",
      "Epoch [43/120] - Train Loss: 0.6384, Val Loss: 0.6412, Val Acc: 0.6818, Val AUC: 0.7250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8723\n",
      "Epoch [44/120] - Train Loss: 0.6331, Val Loss: 0.6407, Val Acc: 0.6818, Val AUC: 0.7255\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [24 17]]\n",
      "Sensitivity: 0.4146, Specificity: 0.9149\n",
      "Epoch [45/120] - Train Loss: 0.6452, Val Loss: 0.6403, Val Acc: 0.6818, Val AUC: 0.7260\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [24 17]]\n",
      "Sensitivity: 0.4146, Specificity: 0.9149\n",
      "Epoch [46/120] - Train Loss: 0.6400, Val Loss: 0.6397, Val Acc: 0.6818, Val AUC: 0.7244\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8936\n",
      "Epoch [47/120] - Train Loss: 0.6213, Val Loss: 0.6390, Val Acc: 0.6932, Val AUC: 0.7234\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8723\n",
      "Epoch [48/120] - Train Loss: 0.6334, Val Loss: 0.6381, Val Acc: 0.6818, Val AUC: 0.7229\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8723\n",
      "Epoch [49/120] - Train Loss: 0.6356, Val Loss: 0.6373, Val Acc: 0.6818, Val AUC: 0.7234\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [50/120] - Train Loss: 0.6178, Val Loss: 0.6366, Val Acc: 0.6591, Val AUC: 0.7239\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [51/120] - Train Loss: 0.6292, Val Loss: 0.6358, Val Acc: 0.6591, Val AUC: 0.7255\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [52/120] - Train Loss: 0.6219, Val Loss: 0.6349, Val Acc: 0.6477, Val AUC: 0.7250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [53/120] - Train Loss: 0.6256, Val Loss: 0.6341, Val Acc: 0.6591, Val AUC: 0.7239\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [20 21]]\n",
      "Sensitivity: 0.5122, Specificity: 0.7660\n",
      "Epoch [54/120] - Train Loss: 0.6249, Val Loss: 0.6335, Val Acc: 0.6477, Val AUC: 0.7234\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [55/120] - Train Loss: 0.6265, Val Loss: 0.6327, Val Acc: 0.6591, Val AUC: 0.7250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8936\n",
      "Epoch [56/120] - Train Loss: 0.6293, Val Loss: 0.6323, Val Acc: 0.6932, Val AUC: 0.7255\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [57/120] - Train Loss: 0.6299, Val Loss: 0.6315, Val Acc: 0.6705, Val AUC: 0.7265\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [58/120] - Train Loss: 0.6172, Val Loss: 0.6307, Val Acc: 0.6477, Val AUC: 0.7276\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [59/120] - Train Loss: 0.6230, Val Loss: 0.6301, Val Acc: 0.6591, Val AUC: 0.7260\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.8085\n",
      "Epoch [60/120] - Train Loss: 0.6332, Val Loss: 0.6294, Val Acc: 0.6591, Val AUC: 0.7260\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.7447\n",
      "Epoch [61/120] - Train Loss: 0.6278, Val Loss: 0.6291, Val Acc: 0.6591, Val AUC: 0.7265\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [62/120] - Train Loss: 0.6217, Val Loss: 0.6287, Val Acc: 0.6477, Val AUC: 0.7276\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [63/120] - Train Loss: 0.6127, Val Loss: 0.6283, Val Acc: 0.6477, Val AUC: 0.7276\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [64/120] - Train Loss: 0.6123, Val Loss: 0.6279, Val Acc: 0.6591, Val AUC: 0.7276\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [65/120] - Train Loss: 0.6130, Val Loss: 0.6272, Val Acc: 0.6477, Val AUC: 0.7270\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.7872\n",
      "Epoch [66/120] - Train Loss: 0.6408, Val Loss: 0.6266, Val Acc: 0.6477, Val AUC: 0.7270\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.7447\n",
      "Epoch [67/120] - Train Loss: 0.6261, Val Loss: 0.6262, Val Acc: 0.6591, Val AUC: 0.7276\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.7872\n",
      "Epoch [68/120] - Train Loss: 0.6172, Val Loss: 0.6256, Val Acc: 0.6477, Val AUC: 0.7291\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [69/120] - Train Loss: 0.6204, Val Loss: 0.6252, Val Acc: 0.6591, Val AUC: 0.7296\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [70/120] - Train Loss: 0.6234, Val Loss: 0.6249, Val Acc: 0.6591, Val AUC: 0.7302\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.7872\n",
      "Epoch [71/120] - Train Loss: 0.6211, Val Loss: 0.6245, Val Acc: 0.6364, Val AUC: 0.7296\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [17 24]]\n",
      "Sensitivity: 0.5854, Specificity: 0.7447\n",
      "Epoch [72/120] - Train Loss: 0.6228, Val Loss: 0.6241, Val Acc: 0.6705, Val AUC: 0.7302\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [17 24]]\n",
      "Sensitivity: 0.5854, Specificity: 0.7447\n",
      "Epoch [73/120] - Train Loss: 0.6266, Val Loss: 0.6239, Val Acc: 0.6705, Val AUC: 0.7312\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [17 24]]\n",
      "Sensitivity: 0.5854, Specificity: 0.7447\n",
      "Epoch [74/120] - Train Loss: 0.6084, Val Loss: 0.6234, Val Acc: 0.6705, Val AUC: 0.7312\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [17 24]]\n",
      "Sensitivity: 0.5854, Specificity: 0.7447\n",
      "Epoch [75/120] - Train Loss: 0.6246, Val Loss: 0.6228, Val Acc: 0.6705, Val AUC: 0.7327\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [17 24]]\n",
      "Sensitivity: 0.5854, Specificity: 0.7447\n",
      "Epoch [76/120] - Train Loss: 0.6100, Val Loss: 0.6223, Val Acc: 0.6705, Val AUC: 0.7338\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.7660\n",
      "Epoch [77/120] - Train Loss: 0.6251, Val Loss: 0.6218, Val Acc: 0.6705, Val AUC: 0.7343\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.7872\n",
      "Epoch [78/120] - Train Loss: 0.6127, Val Loss: 0.6214, Val Acc: 0.6818, Val AUC: 0.7343\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [79/120] - Train Loss: 0.6349, Val Loss: 0.6211, Val Acc: 0.6477, Val AUC: 0.7348\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [80/120] - Train Loss: 0.6153, Val Loss: 0.6210, Val Acc: 0.6591, Val AUC: 0.7353\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [81/120] - Train Loss: 0.6112, Val Loss: 0.6205, Val Acc: 0.6477, Val AUC: 0.7364\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [82/120] - Train Loss: 0.5973, Val Loss: 0.6203, Val Acc: 0.6591, Val AUC: 0.7359\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [83/120] - Train Loss: 0.6009, Val Loss: 0.6200, Val Acc: 0.6705, Val AUC: 0.7369\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [84/120] - Train Loss: 0.6041, Val Loss: 0.6199, Val Acc: 0.6705, Val AUC: 0.7374\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [85/120] - Train Loss: 0.6242, Val Loss: 0.6197, Val Acc: 0.6705, Val AUC: 0.7369\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [86/120] - Train Loss: 0.6039, Val Loss: 0.6189, Val Acc: 0.6705, Val AUC: 0.7364\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [20 21]]\n",
      "Sensitivity: 0.5122, Specificity: 0.7872\n",
      "Epoch [87/120] - Train Loss: 0.6041, Val Loss: 0.6179, Val Acc: 0.6591, Val AUC: 0.7364\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.7447\n",
      "Epoch [88/120] - Train Loss: 0.6252, Val Loss: 0.6175, Val Acc: 0.6591, Val AUC: 0.7359\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [17 24]]\n",
      "Sensitivity: 0.5854, Specificity: 0.7447\n",
      "Epoch [89/120] - Train Loss: 0.6271, Val Loss: 0.6173, Val Acc: 0.6705, Val AUC: 0.7374\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [17 24]]\n",
      "Sensitivity: 0.5854, Specificity: 0.7447\n",
      "Epoch [90/120] - Train Loss: 0.6171, Val Loss: 0.6170, Val Acc: 0.6705, Val AUC: 0.7374\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [19 22]]\n",
      "Sensitivity: 0.5366, Specificity: 0.7872\n",
      "Epoch [91/120] - Train Loss: 0.5996, Val Loss: 0.6168, Val Acc: 0.6705, Val AUC: 0.7379\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.7660\n",
      "Epoch [92/120] - Train Loss: 0.6218, Val Loss: 0.6165, Val Acc: 0.6705, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.7660\n",
      "Epoch [93/120] - Train Loss: 0.6135, Val Loss: 0.6164, Val Acc: 0.6705, Val AUC: 0.7379\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [19 22]]\n",
      "Sensitivity: 0.5366, Specificity: 0.7872\n",
      "Epoch [94/120] - Train Loss: 0.6143, Val Loss: 0.6162, Val Acc: 0.6705, Val AUC: 0.7379\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [95/120] - Train Loss: 0.6234, Val Loss: 0.6164, Val Acc: 0.6477, Val AUC: 0.7374\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [96/120] - Train Loss: 0.5944, Val Loss: 0.6162, Val Acc: 0.6477, Val AUC: 0.7379\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [97/120] - Train Loss: 0.6151, Val Loss: 0.6158, Val Acc: 0.6477, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [98/120] - Train Loss: 0.6118, Val Loss: 0.6155, Val Acc: 0.6477, Val AUC: 0.7379\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [99/120] - Train Loss: 0.5960, Val Loss: 0.6155, Val Acc: 0.6591, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.8085\n",
      "Epoch [100/120] - Train Loss: 0.5990, Val Loss: 0.6146, Val Acc: 0.6591, Val AUC: 0.7374\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [101/120] - Train Loss: 0.5955, Val Loss: 0.6145, Val Acc: 0.6477, Val AUC: 0.7374\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [102/120] - Train Loss: 0.6222, Val Loss: 0.6146, Val Acc: 0.6477, Val AUC: 0.7379\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8511\n",
      "Epoch [103/120] - Train Loss: 0.6219, Val Loss: 0.6147, Val Acc: 0.6705, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [104/120] - Train Loss: 0.5997, Val Loss: 0.6145, Val Acc: 0.6591, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8298\n",
      "Epoch [105/120] - Train Loss: 0.6107, Val Loss: 0.6140, Val Acc: 0.6591, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8085\n",
      "Epoch [106/120] - Train Loss: 0.6044, Val Loss: 0.6134, Val Acc: 0.6477, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.7872\n",
      "Epoch [107/120] - Train Loss: 0.6167, Val Loss: 0.6129, Val Acc: 0.6477, Val AUC: 0.7390\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.7234\n",
      "Epoch [108/120] - Train Loss: 0.6101, Val Loss: 0.6126, Val Acc: 0.6705, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32 15]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.6809\n",
      "Epoch [109/120] - Train Loss: 0.6135, Val Loss: 0.6126, Val Acc: 0.6477, Val AUC: 0.7400\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.7021\n",
      "Epoch [110/120] - Train Loss: 0.5987, Val Loss: 0.6123, Val Acc: 0.6591, Val AUC: 0.7390\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.7660\n",
      "Epoch [111/120] - Train Loss: 0.6063, Val Loss: 0.6119, Val Acc: 0.6932, Val AUC: 0.7390\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [19 22]]\n",
      "Sensitivity: 0.5366, Specificity: 0.7872\n",
      "Epoch [112/120] - Train Loss: 0.6415, Val Loss: 0.6118, Val Acc: 0.6705, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [19 22]]\n",
      "Sensitivity: 0.5366, Specificity: 0.7660\n",
      "Epoch [113/120] - Train Loss: 0.6043, Val Loss: 0.6115, Val Acc: 0.6591, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [17 24]]\n",
      "Sensitivity: 0.5854, Specificity: 0.7660\n",
      "Epoch [114/120] - Train Loss: 0.5936, Val Loss: 0.6112, Val Acc: 0.6818, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.7234\n",
      "Epoch [115/120] - Train Loss: 0.6210, Val Loss: 0.6110, Val Acc: 0.6705, Val AUC: 0.7390\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.7234\n",
      "Epoch [116/120] - Train Loss: 0.6052, Val Loss: 0.6109, Val Acc: 0.6705, Val AUC: 0.7379\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.7660\n",
      "Epoch [117/120] - Train Loss: 0.6151, Val Loss: 0.6106, Val Acc: 0.6932, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [19 22]]\n",
      "Sensitivity: 0.5366, Specificity: 0.7660\n",
      "Epoch [118/120] - Train Loss: 0.6128, Val Loss: 0.6104, Val Acc: 0.6591, Val AUC: 0.7390\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [19 22]]\n",
      "Sensitivity: 0.5366, Specificity: 0.7660\n",
      "Epoch [119/120] - Train Loss: 0.5953, Val Loss: 0.6103, Val Acc: 0.6591, Val AUC: 0.7385\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.8085\n",
      "Epoch [120/120] - Train Loss: 0.6133, Val Loss: 0.6105, Val Acc: 0.6591, Val AUC: 0.7395\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.7234\n",
      "Test Loss: 0.6326, Test Accuracy: 0.6705, Test AUC: 0.7260\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attn-trans-fullbackprop/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=bilstm_attn_fullbackprop_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=120, lr=bilstm_attn_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_attn_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
