{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility across numpy, torch, and random.\"\"\"\n",
    "    random.seed(seed_value)  # Python's built-in random module\n",
    "    np.random.seed(seed_value)  # NumPy random seed\n",
    "\n",
    "    torch.manual_seed(seed_value)  # PyTorch random seed for CPU\n",
    "    torch.cuda.manual_seed(seed_value)  # PyTorch random seed for CUDA\n",
    "    torch.cuda.manual_seed_all(seed_value)  # PyTorch seed for all GPUs\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior in cuDNN\n",
    "    torch.backends.cudnn.benchmark = False  # Disables cuDNN auto-optimization\n",
    "\n",
    "# Usage\n",
    "set_seed(42)  # Call this function before running models or data splits to ensure reproducibility\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('all_seq722.csv')\n",
    "\n",
    "    \n",
    "df = df[~df[\"Sequences\"].str.contains('-')]\n",
    "df['Sequences'] = df['Sequences'].str.upper()\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E', 'K', 'R', 'P', 'W', 'S', 'I', 'C', 'V', 'Q', 'X', 'Y', 'M', 'F', 'G', 'N', 'T', 'L', 'A', 'D', 'H'}\n",
      "21\n",
      "{'X'}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries after execution state reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "\n",
    "# Define One-Hot Encoding Function for DNA Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "# Define custom dataset class with transformation\n",
    "\n",
    "# Updating the Dataset class with the OneHotEncoder function at the end\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences  # Raw sequences\n",
    "        self.labels = labels  # Labels\n",
    "        self.one_hot_dtype = one_hot_dtype  # Data type for one-hot encoding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seqs_comb = self.sequences.iloc[idx]  # Get sequence\n",
    "        amp_label = self.labels.iloc[idx]    # Get corresponding label\n",
    "        # Apply one-hot encoding transformation at the end\n",
    "        return one_hot_torch(seqs_comb, dtype=self.one_hot_dtype), torch.tensor(amp_label, dtype=torch.float32)\n",
    "\n",
    "\n",
    "# Convert dataset into PyTorch Dataset\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.15, random_state=42, stratify=y_train_val\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Convert back to PyTorch datasets\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "# for x,y in train_loader:\n",
    "#     print(x)\n",
    "#     print(y)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 20, 128])\n",
      "tensor([1., 0., 0., 1., 0., 0., 0., 1., 0., 1., 1., 1., 0., 1., 0., 0.],\n",
      "       dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "for x,y in train_loader:\n",
    "    print(x.shape)\n",
    "    print(y)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21, TrainLoss: 0.6935, ValLoss: 0.6904, Accuracy: 0.5495, AUC: 0.5444, Sensitivity: 0.0889, Specificity: 1.0000\n",
      "Epoch 6/21, TrainLoss: 0.5593, ValLoss: 0.4415, Accuracy: 0.8132, AUC: 0.8126, Sensitivity: 0.7556, Specificity: 0.8696\n",
      "Epoch 11/21, TrainLoss: 0.5188, ValLoss: 0.4786, Accuracy: 0.7473, AUC: 0.7488, Sensitivity: 0.8889, Specificity: 0.6087\n",
      "Epoch 16/21, TrainLoss: 0.4357, ValLoss: 0.3672, Accuracy: 0.8462, AUC: 0.8457, Sensitivity: 0.8000, Specificity: 0.8913\n",
      "Epoch 21/21, TrainLoss: 0.2891, ValLoss: 0.3293, Accuracy: 0.8791, AUC: 0.8795, Sensitivity: 0.9111, Specificity: 0.8478\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "ic.disable()\n",
    "# Define the Basic LSTM Model\n",
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        ic(h_n.shape)\n",
    "        ic(h_n[-1].shape)\n",
    "        ic(h_n[-1])\n",
    "        out = self.fc(h_n[-1])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "# Function to calculate specificity\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "# Training and Evaluation Function\n",
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_,num_epochs=100):\n",
    "    writer = SummaryWriter(tensorboard_)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            ic(sequences.shape)\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "                val_losses.append(loss.item())\n",
    "                preds = (outputs > 0.5).float()\n",
    "                all_labels.extend(labels.numpy())\n",
    "                all_preds.extend(preds.numpy())\n",
    "                \n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            writer.add_scalar('Loss/Val', avg_val_loss, epoch)\n",
    "\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "        sensitivity = recall_score(all_labels, all_preds)\n",
    "        specificity = specificity_score(all_labels, all_preds)\n",
    "\n",
    "        writer.add_scalar('Metrics/Accuracy', accuracy, epoch)\n",
    "        writer.add_scalar('Metrics/AUC', auc, epoch)\n",
    "        writer.add_scalar('Metrics/Sensitivity', sensitivity, epoch)\n",
    "        writer.add_scalar('Metrics/Specificity', specificity, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, TrainLoss: {avg_train_loss:.4f}, ValLoss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 128       \n",
    "hidden_size = 128    # Number of features in the hidden state\n",
    "num_layers = 2       # Number of stacked LSTM layers\n",
    "output_size = 1      # Binary classification (AMP or not)\n",
    "batch_size = 16\n",
    "num_epochs = 21\n",
    "learning_rate = 0.001\n",
    "tensorboard_ = f\"runs/basic_lstm/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = BasicLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_,num_epochs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8505\n",
      "Test AUC: 0.8506\n",
      "Test Sensitivity (Recall): 0.8679\n",
      "Test Specificity: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8504672897196262, 0.85062893081761, 0.8679245283018868, 0.8333333333333334)"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for sequences, labels in test_loader:\n",
    "            outputs = model(sequences)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_preds.extend(preds.numpy())\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    sensitivity = recall_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Test AUC: {auc:.4f}')\n",
    "    print(f'Test Sensitivity (Recall): {sensitivity:.4f}')\n",
    "    print(f'Test Specificity: {specificity:.4f}')\n",
    "    \n",
    "    return accuracy, auc, sensitivity, specificity\n",
    "# After training is complete\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lstm with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale_factor = torch.sqrt(torch.tensor(hidden_size, dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        attention_scores = torch.bmm(query, key.transpose(1, 2))  # [batch, seq_len, seq_len]\n",
    "        attention_scores = attention_scores / self.scale_factor\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # Normalize scores\n",
    "        context_vector = torch.bmm(attention_weights, value)  # Weighted sum\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class LSTM_ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM_ScaledDotAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.attention = ScaledDotProductAttention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # LSTM output shape: [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Using last hidden state as Query\n",
    "        query = lstm_out[:, -1, :].unsqueeze(1)  # [batch_size, 1, hidden_size]\n",
    "\n",
    "        # Key and Value are the full sequence outputs\n",
    "        key = lstm_out  # [batch_size, seq_len, hidden_size]\n",
    "        value = lstm_out\n",
    "\n",
    "        context_vector, attention_weights = self.attention(query, key, value)\n",
    "        context_vector = context_vector.squeeze(1)  # [batch_size, hidden_size]\n",
    "\n",
    "        out = self.fc(context_vector)  # [batch_size, output_size]\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/26, TrainLoss: 0.6951, ValLoss: 0.6895, Accuracy: 0.5275, AUC: 0.5222, Sensitivity: 0.0444, Specificity: 1.0000\n",
      "Epoch 6/26, TrainLoss: 0.5163, ValLoss: 0.4518, Accuracy: 0.8022, AUC: 0.8012, Sensitivity: 0.7111, Specificity: 0.8913\n",
      "Epoch 11/26, TrainLoss: 0.4682, ValLoss: 0.4259, Accuracy: 0.8352, AUC: 0.8350, Sensitivity: 0.8222, Specificity: 0.8478\n",
      "Epoch 16/26, TrainLoss: 0.4533, ValLoss: 0.3931, Accuracy: 0.8352, AUC: 0.8341, Sensitivity: 0.7333, Specificity: 0.9348\n",
      "Epoch 21/26, TrainLoss: 0.4512, ValLoss: 0.3829, Accuracy: 0.8571, AUC: 0.8570, Sensitivity: 0.8444, Specificity: 0.8696\n",
      "Epoch 26/26, TrainLoss: 0.4153, ValLoss: 0.4528, Accuracy: 0.8352, AUC: 0.8353, Sensitivity: 0.8444, Specificity: 0.8261\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs=25):\n",
    "    writer = SummaryWriter(tensorboard_log_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "                val_losses.append(loss.item())\n",
    "                preds = (outputs > 0.5).float()\n",
    "                all_labels.extend(labels.numpy())\n",
    "                all_preds.extend(preds.numpy())\n",
    "\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            writer.add_scalar('Loss/Val', avg_val_loss, epoch)\n",
    "\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "        sensitivity = recall_score(all_labels, all_preds)\n",
    "        specificity = specificity_score(all_labels, all_preds)\n",
    "\n",
    "        writer.add_scalar('Metrics/Accuracy', accuracy, epoch)\n",
    "        writer.add_scalar('Metrics/AUC', auc, epoch)\n",
    "        writer.add_scalar('Metrics/Sensitivity', sensitivity, epoch)\n",
    "        writer.add_scalar('Metrics/Specificity', specificity, epoch)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, TrainLoss: {avg_train_loss:.4f}, ValLoss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "\n",
    "    writer.close()\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 128  # One-hot encoding size or embedding dimension\n",
    "hidden_size = 128  # LSTM hidden state size\n",
    "num_layers = 2  # Stacked LSTM layers\n",
    "output_size = 1  # Binary classification\n",
    "batch_size = 16\n",
    "num_epochs = 26\n",
    "learning_rate = 0.001\n",
    "tensorboard_log_dir = f\"runs/lstm_attention/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = LSTM_ScaledDotAttention(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8505\n",
      "Test AUC: 0.8506\n",
      "Test Sensitivity (Recall): 0.8679\n",
      "Test Specificity: 0.8333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8504672897196262, 0.85062893081761, 0.8679245283018868, 0.8333333333333334)"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            outputs = model(sequences)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_preds.extend(preds.numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    sensitivity = recall_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Test AUC: {auc:.4f}')\n",
    "    print(f'Test Sensitivity (Recall): {sensitivity:.4f}')\n",
    "    print(f'Test Specificity: {specificity:.4f}')\n",
    "\n",
    "    return accuracy, auc, sensitivity, specificity\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "ic.disable()\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_weights = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: [batch_size, seq_len, hidden_dim]\n",
    "        attention_scores = self.attention_weights(lstm_output)  # [batch_size, seq_len, 1]\n",
    "        attention_scores = attention_scores.squeeze(-1)  # [batch_size, seq_len]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # [batch_size, seq_len]\n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), lstm_output)  # [batch_size, 1, hidden_dim]\n",
    "        context_vector = context_vector.squeeze(1)  # [batch_size, hidden_dim]\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class CNN_LSTM_Attention(nn.Module):\n",
    "    def __init__(self, input_channels, cnn_output_dim, lstm_hidden_dim, lstm_layers, output_dim):\n",
    "        super(CNN_LSTM_Attention, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(cnn_output_dim, lstm_hidden_dim, lstm_layers, batch_first=True)\n",
    "        self.attention = Attention(lstm_hidden_dim)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_channels]\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        ic(x.shape)\n",
    "        x = x.view(batch_size * seq_len, -1, x.size(2))  # [batch_size * seq_len, input_channels, feature_dim]\n",
    "        ic(x.shape)\n",
    "        cnn_out = self.cnn(x)  # [batch_size * seq_len, 128, feature_dim/4]\n",
    "        cnn_out = cnn_out.view(batch_size, seq_len, -1)  # [batch_size, seq_len, cnn_output_dim]\n",
    "        lstm_out, _ = self.lstm(cnn_out)  # [batch_size, seq_len, lstm_hidden_dim]\n",
    "        context_vector, attention_weights = self.attention(lstm_out)  # [batch_size, lstm_hidden_dim]\n",
    "        out = self.fc(context_vector)  # [batch_size, output_dim]\n",
    "        return self.sigmoid(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "input_channels = 1  # Number of input channels\n",
    "cnn_output_dim = 128 * (input_size // 4)  # Adjust based on CNN architecture\n",
    "lstm_hidden_dim = 128\n",
    "lstm_layers = 2\n",
    "output_dim = 1  # Binary classification\n",
    "batch_size = 16\n",
    "num_epochs = 26\n",
    "learning_rate = 0.001\n",
    "tensorboard_log_dir = f\"runs/cnn_lstm_attention/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = CNN_LSTM_Attention(input_channels, cnn_output_dim, lstm_hidden_dim, lstm_layers, output_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Multiply hidden_size by 2 for bidirectional\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)  # LSTM output: (batch_size, seq_len, hidden_size * 2)\n",
    "        # Concatenate the last hidden states from both directions\n",
    "        out = self.fc(torch.cat((h_n[-2], h_n[-1]), dim=1))  \n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate specificity\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs=25):\n",
    "    writer = SummaryWriter(tensorboard_log_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "                val_losses.append(loss.item())\n",
    "                preds = (outputs > 0.5).float()\n",
    "                all_labels.extend(labels.numpy())\n",
    "                all_preds.extend(preds.numpy())\n",
    "\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            writer.add_scalar('Loss/Val', avg_val_loss, epoch)\n",
    "\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "        sensitivity = recall_score(all_labels, all_preds)\n",
    "        specificity = specificity_score(all_labels, all_preds)\n",
    "\n",
    "        writer.add_scalar('Metrics/Accuracy', accuracy, epoch)\n",
    "        writer.add_scalar('Metrics/AUC', auc, epoch)\n",
    "        writer.add_scalar('Metrics/Sensitivity', sensitivity, epoch)\n",
    "        writer.add_scalar('Metrics/Specificity', specificity, epoch)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, TrainLoss: {avg_train_loss:.4f}, ValLoss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/26, TrainLoss: 0.6900, ValLoss: 0.6719, Accuracy: 0.7363, AUC: 0.7338, Sensitivity: 0.5111, Specificity: 0.9565\n",
      "Epoch 6/26, TrainLoss: 0.4284, ValLoss: 0.4699, Accuracy: 0.7473, AUC: 0.7473, Sensitivity: 0.7556, Specificity: 0.7391\n",
      "Epoch 11/26, TrainLoss: 0.3351, ValLoss: 0.3742, Accuracy: 0.8242, AUC: 0.8242, Sensitivity: 0.8222, Specificity: 0.8261\n",
      "Epoch 16/26, TrainLoss: 0.2488, ValLoss: 0.3774, Accuracy: 0.8352, AUC: 0.8353, Sensitivity: 0.8444, Specificity: 0.8261\n",
      "Epoch 21/26, TrainLoss: 0.2203, ValLoss: 0.3713, Accuracy: 0.8681, AUC: 0.8671, Sensitivity: 0.7778, Specificity: 0.9565\n",
      "Epoch 26/26, TrainLoss: 0.1320, ValLoss: 0.3322, Accuracy: 0.8791, AUC: 0.8785, Sensitivity: 0.8222, Specificity: 0.9348\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 128  # Number of features per time step (e.g., One-hot encoding size)\n",
    "hidden_size = 128  # Number of LSTM units\n",
    "num_layers = 2  # Stacked LSTM layers\n",
    "output_size = 1  # Binary classification (AMP or not)\n",
    "batch_size = 16\n",
    "num_epochs = 26\n",
    "learning_rate = 0.001\n",
    "tensorboard_log_dir = f\"runs/bilstm/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = BiLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.9065\n",
      "Test AUC: 0.9064\n",
      "Test Sensitivity (Recall): 0.8868\n",
      "Test Specificity: 0.9259\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.9065420560747663,\n",
       " 0.9063591893780573,\n",
       " 0.8867924528301887,\n",
       " 0.9259259259259259)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            outputs = model(sequences)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_preds.extend(preds.numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    sensitivity = recall_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Test AUC: {auc:.4f}')\n",
    "    print(f'Test Sensitivity (Recall): {sensitivity:.4f}')\n",
    "    print(f'Test Specificity: {specificity:.4f}')\n",
    "\n",
    "    return accuracy, auc, sensitivity, specificity\n",
    "\n",
    "# Evaluate on Test Set\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biLSTM with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale_factor = torch.sqrt(torch.tensor(hidden_size, dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        attention_scores = torch.bmm(query, key.transpose(1, 2))  # [batch, seq_len, seq_len]\n",
    "        attention_scores = attention_scores / self.scale_factor\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # Normalize scores\n",
    "        context_vector = torch.bmm(attention_weights, value)  # Weighted sum\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class BiLSTM_ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BiLSTM_ScaledDotAttention, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.attention = ScaledDotProductAttention(hidden_size * 2)  # Bidirectional -> 2x hidden size\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.bilstm(x)  # LSTM output shape: [batch_size, seq_len, hidden_size * 2]\n",
    "        \n",
    "        # Using last hidden state as Query\n",
    "        query = lstm_out[:, -1, :].unsqueeze(1)  # [batch_size, 1, hidden_size * 2]\n",
    "\n",
    "        # Key and Value are the full sequence outputs\n",
    "        key = lstm_out  # [batch_size, seq_len, hidden_size * 2]\n",
    "        value = lstm_out\n",
    "\n",
    "        context_vector, attention_weights = self.attention(query, key, value)\n",
    "        context_vector = context_vector.squeeze(1)  # [batch_size, hidden_size * 2]\n",
    "\n",
    "        out = self.fc(context_vector)  # [batch_size, output_size]\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40, TrainLoss: 0.6938, ValLoss: 0.6926, Accuracy: 0.5055, AUC: 0.5000, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch 6/40, TrainLoss: 0.6839, ValLoss: 0.6774, Accuracy: 0.5824, AUC: 0.5867, Sensitivity: 0.9778, Specificity: 0.1957\n",
      "Epoch 11/40, TrainLoss: 0.4866, ValLoss: 0.3833, Accuracy: 0.8462, AUC: 0.8464, Sensitivity: 0.8667, Specificity: 0.8261\n",
      "Epoch 16/40, TrainLoss: 0.4589, ValLoss: 0.3749, Accuracy: 0.8462, AUC: 0.8464, Sensitivity: 0.8667, Specificity: 0.8261\n",
      "Epoch 21/40, TrainLoss: 0.4347, ValLoss: 0.3704, Accuracy: 0.8462, AUC: 0.8454, Sensitivity: 0.7778, Specificity: 0.9130\n",
      "Epoch 26/40, TrainLoss: 0.4217, ValLoss: 0.3631, Accuracy: 0.8462, AUC: 0.8464, Sensitivity: 0.8667, Specificity: 0.8261\n",
      "Epoch 31/40, TrainLoss: 0.4437, ValLoss: 0.3875, Accuracy: 0.8132, AUC: 0.8123, Sensitivity: 0.7333, Specificity: 0.8913\n",
      "Epoch 36/40, TrainLoss: 0.4204, ValLoss: 0.3636, Accuracy: 0.8462, AUC: 0.8457, Sensitivity: 0.8000, Specificity: 0.8913\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs=25):\n",
    "    writer = SummaryWriter(tensorboard_log_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "                val_losses.append(loss.item())\n",
    "                preds = (outputs > 0.5).float()\n",
    "                all_labels.extend(labels.numpy())\n",
    "                all_preds.extend(preds.numpy())\n",
    "\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            writer.add_scalar('Loss/Val', avg_val_loss, epoch)\n",
    "\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "        sensitivity = recall_score(all_labels, all_preds)\n",
    "        specificity = specificity_score(all_labels, all_preds)\n",
    "\n",
    "        writer.add_scalar('Metrics/Accuracy', accuracy, epoch)\n",
    "        writer.add_scalar('Metrics/AUC', auc, epoch)\n",
    "        writer.add_scalar('Metrics/Sensitivity', sensitivity, epoch)\n",
    "        writer.add_scalar('Metrics/Specificity', specificity, epoch)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, TrainLoss: {avg_train_loss:.4f}, ValLoss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 128  # One-hot encoding size or embedding dimension\n",
    "hidden_size = 128  # LSTM hidden state size\n",
    "num_layers = 2  # Stacked BiLSTM layers\n",
    "output_size = 1  # Binary classification\n",
    "batch_size = 16\n",
    "num_epochs = 40\n",
    "learning_rate = 0.0001\n",
    "tensorboard_log_dir = f\"runs/bilstm_attention/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = BiLSTM_ScaledDotAttention(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8318\n",
      "Test AUC: 0.8326\n",
      "Test Sensitivity (Recall): 0.9245\n",
      "Test Specificity: 0.7407\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8317757009345794,\n",
       " 0.8326345213137666,\n",
       " 0.9245283018867925,\n",
       " 0.7407407407407407)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            outputs = model(sequences)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_preds.extend(preds.numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    sensitivity = recall_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Test AUC: {auc:.4f}')\n",
    "    print(f'Test Sensitivity (Recall): {sensitivity:.4f}')\n",
    "    print(f'Test Specificity: {specificity:.4f}')\n",
    "\n",
    "    return accuracy, auc, sensitivity, specificity\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # LSTM output: (batch_size, seq_len, hidden_size)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take the last time step's output\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/26, TrainLoss: 0.6936, ValLoss: 0.6927, Accuracy: 0.5055, AUC: 0.5000, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch 6/26, TrainLoss: 0.6873, ValLoss: 0.6814, Accuracy: 0.5495, AUC: 0.5541, Sensitivity: 0.9778, Specificity: 0.1304\n",
      "Epoch 11/26, TrainLoss: 0.5466, ValLoss: 0.4706, Accuracy: 0.7802, AUC: 0.7802, Sensitivity: 0.7778, Specificity: 0.7826\n",
      "Epoch 16/26, TrainLoss: 0.5107, ValLoss: 0.4543, Accuracy: 0.7912, AUC: 0.7923, Sensitivity: 0.8889, Specificity: 0.6957\n",
      "Epoch 21/26, TrainLoss: 0.4844, ValLoss: 0.4219, Accuracy: 0.8132, AUC: 0.8135, Sensitivity: 0.8444, Specificity: 0.7826\n",
      "Epoch 26/26, TrainLoss: 0.4759, ValLoss: 0.4417, Accuracy: 0.8022, AUC: 0.8002, Sensitivity: 0.6222, Specificity: 0.9783\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate specificity\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs=25):\n",
    "    writer = SummaryWriter(tensorboard_log_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "                val_losses.append(loss.item())\n",
    "                preds = (outputs > 0.5).float()\n",
    "                all_labels.extend(labels.numpy())\n",
    "                all_preds.extend(preds.numpy())\n",
    "\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            writer.add_scalar('Loss/Val', avg_val_loss, epoch)\n",
    "\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "        sensitivity = recall_score(all_labels, all_preds)\n",
    "        specificity = specificity_score(all_labels, all_preds)\n",
    "\n",
    "        writer.add_scalar('Metrics/Accuracy', accuracy, epoch)\n",
    "        writer.add_scalar('Metrics/AUC', auc, epoch)\n",
    "        writer.add_scalar('Metrics/Sensitivity', sensitivity, epoch)\n",
    "        writer.add_scalar('Metrics/Specificity', specificity, epoch)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, TrainLoss: {avg_train_loss:.4f}, ValLoss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "\n",
    "    writer.close()\n",
    "    \n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 128  # Feature size (e.g., one-hot encoding or embedding)\n",
    "hidden_size = 128  # LSTM hidden state size\n",
    "num_layers = 3  # Stacked LSTM layers\n",
    "output_size = 1  # Binary classification\n",
    "batch_size = 16\n",
    "num_epochs = 26\n",
    "learning_rate = 0.0001\n",
    "tensorboard_log_dir = f\"runs/stacked_lstm/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = StackedLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.8411\n",
      "Test AUC: 0.8405\n",
      "Test Sensitivity (Recall): 0.7736\n",
      "Test Specificity: 0.9074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8411214953271028,\n",
       " 0.8404961565338924,\n",
       " 0.7735849056603774,\n",
       " 0.9074074074074074)"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for sequences, labels in test_loader:\n",
    "            outputs = model(sequences)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_preds.extend(preds.numpy())\n",
    "\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_preds)\n",
    "    sensitivity = recall_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Test AUC: {auc:.4f}')\n",
    "    print(f'Test Sensitivity (Recall): {sensitivity:.4f}')\n",
    "    print(f'Test Specificity: {specificity:.4f}')\n",
    "\n",
    "    return accuracy, auc, sensitivity, specificity\n",
    "\n",
    "# Evaluate\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train': 255, 'Validation': 45, 'Test': 54}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load Dataset\n",
    "df = pd.read_csv('all_seq722.csv')\n",
    "df = df[df['Sequences'].str.len() >= 3]  # Remove rows with less than 3 letters in sequences\n",
    "df = df[df['AMP'] != 0]  # Remove rows where AMP is 0\n",
    "\n",
    "# Preprocess sequences\n",
    "df = df[~df[\"Sequences\"].str.contains('-')]\n",
    "df['Sequences'] = df['Sequences'].str.upper()\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))  # Pad sequences\n",
    "\n",
    "# Define One-Hot Encoding Function for Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"  # 20 standard amino acids\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)  # One-hot encoded matrix\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "def preprocess_sequence(full_sequence, max_length):\n",
    "    \"\"\"\n",
    "    Prepares input and target sequences while avoiding padding ('X') during training.\n",
    "    \n",
    "    Args:\n",
    "        full_sequence (str): The original sequence including 'X' padding.\n",
    "        max_length (int): Maximum sequence length (for padding).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (input sequence, target sequence) after removing padding.\n",
    "    \"\"\"\n",
    "    # Remove trailing 'X' (padding) before slicing\n",
    "    trimmed_sequence = full_sequence.rstrip('X')\n",
    "\n",
    "    # Ensure there's at least 2 valid residues left for training\n",
    "    if len(trimmed_sequence) < 2:\n",
    "        return None, None  # Skip sequences that are too short to predict anything\n",
    "\n",
    "    # Create input & target sequences (without padding)\n",
    "    input_seq = trimmed_sequence[:-1]  # Exclude last valid residue\n",
    "    target_seq = trimmed_sequence[1:]  # Shifted by one position\n",
    "\n",
    "    # Pad back to maintain fixed-length format\n",
    "    input_seq = input_seq.ljust(max_length - 1, 'X')  # Pad to max_length - 1\n",
    "    target_seq = target_seq.ljust(max_length - 1, 'X')  # Target also gets 'X' padding\n",
    "\n",
    "    return input_seq, target_seq\n",
    "\n",
    "# Custom Dataset for Generative Model\n",
    "class GenerativeSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, max_length, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.max_length = max_length\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        full_sequence = self.sequences.iloc[idx]  # Get sequence\n",
    "\n",
    "        # Process input & target while handling padding correctly\n",
    "        input_seq, target_seq = preprocess_sequence(full_sequence, self.max_length)\n",
    "        if input_seq is None or target_seq is None:\n",
    "            return None  # Skip sequences that are too short\n",
    "\n",
    "        # Convert to one-hot encoding\n",
    "        input_one_hot = one_hot_torch(input_seq, dtype=self.one_hot_dtype)\n",
    "        target_one_hot = one_hot_torch(target_seq, dtype=self.one_hot_dtype)\n",
    "\n",
    "        return input_one_hot, target_one_hot\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train_val, X_test = train_test_split(df[\"Sequences\"], test_size=0.15, random_state=42)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=0.15, random_state=42)  \n",
    "\n",
    "# Convert to PyTorch Dataset\n",
    "train_dataset = GenerativeSequenceDataset(X_train, max_length)\n",
    "val_dataset = GenerativeSequenceDataset(X_val, max_length)\n",
    "test_dataset = GenerativeSequenceDataset(X_test, max_length)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(dataset_sizes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded Input Sequence: RWRRKWWWXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Decoded Target Sequence: WRRKWWWWXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n"
     ]
    }
   ],
   "source": [
    "def one_hot_to_sequence(one_hot_tensor):\n",
    "    \"\"\"\n",
    "    Converts a one-hot encoded tensor back into an amino acid sequence,\n",
    "    handling fully zero vectors (padding 'X').\n",
    "\n",
    "    Args:\n",
    "        one_hot_tensor (torch.Tensor): One-hot encoded tensor of shape (num_amino_acids, seq_length)\n",
    "\n",
    "    Returns:\n",
    "        str: Decoded amino acid sequence\n",
    "    \"\"\"\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"  # 20 standard amino acids\n",
    "    seq_length = one_hot_tensor.shape[1]  # Get sequence length\n",
    "    \n",
    "    decoded_sequence = \"\"\n",
    "    for i in range(seq_length):\n",
    "        column = one_hot_tensor[:, i]  # Get one-hot vector for residue\n",
    "        \n",
    "        if torch.sum(column) == 0:  # If it's fully zero, assume it's padding\n",
    "            decoded_sequence += \"X\"\n",
    "        else:\n",
    "            amino_acid_index = torch.argmax(column).item()  # Get highest probability index\n",
    "            decoded_sequence += amino_acids[amino_acid_index]  # Convert index to amino acid\n",
    "    \n",
    "    return decoded_sequence\n",
    "\n",
    "for x, y in train_loader:\n",
    "    # print(\"One-Hot Encoded Input:\\n\", x)\n",
    "    # print(\"One-Hot Encoded Target:\\n\", y)\n",
    "    \n",
    "    # Convert back to amino acid sequences\n",
    "    input_seq_decoded = one_hot_to_sequence(x[0])  # Decode first sample in batch\n",
    "    target_seq_decoded = one_hot_to_sequence(y[0])  # Decode first target sequence\n",
    "    \n",
    "    print(\"\\nDecoded Input Sequence:\", input_seq_decoded)\n",
    "    print(\"Decoded Target Sequence:\", target_seq_decoded)\n",
    "    break  # Print one batch and exit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "ic.enable()\n",
    "class LSTM_Generative(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM_Generative, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)  # Softmax for probability distribution\n",
    "\n",
    "    def forward(self, x):\n",
    "        ic(x.shape)\n",
    "        lstm_out, _ = self.lstm(x)  # Shape: (batch_size, seq_length, hidden_size)\n",
    "        out = self.fc(lstm_out)  # Shape: (batch_size, seq_length, output_size)\n",
    "        return self.softmax(out)  # Probability distribution over amino acids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([15, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([15, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([15, 20])\n",
      "ic| outputs.shape: torch.Size([15, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([13, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, TrainLoss: 4.8296, ValLoss: 4.6929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([15, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([15, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([15, 20])\n",
      "ic| outputs.shape: torch.Size([15, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([13, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([15, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([15, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([15, 20])\n",
      "ic| outputs.shape: torch.Size([15, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([13, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([15, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([15, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([15, 20])\n",
      "ic| outputs.shape: torch.Size([15, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([13, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([15, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([15, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([15, 20])\n",
      "ic| outputs.shape: torch.Size([15, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([13, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([15, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([15, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([15, 20])\n",
      "ic| outputs.shape: torch.Size([15, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([13, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, TrainLoss: 4.1966, ValLoss: 4.2045\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([15, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([15, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([15, 20])\n",
      "ic| outputs.shape: torch.Size([15, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([13, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([15, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([15, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([15, 20])\n",
      "ic| outputs.shape: torch.Size([15, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([13, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([15, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([15, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([15, 20])\n",
      "ic| outputs.shape: torch.Size([15, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([13, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([15, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([15, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([15, 20])\n",
      "ic| outputs.shape: torch.Size([15, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([13, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([15, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([15, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([15, 20])\n",
      "ic| outputs.shape: torch.Size([15, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([13, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, TrainLoss: 4.1914, ValLoss: 4.1993\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, 127])\n",
      "ic| target_indices.shape: torch.Size([16, 20])\n",
      "ic| outputs.shape: torch.Size([16, 20, 127])\n",
      "ic| x.shape: torch.Size([16, 20, 127])\n",
      "ic| target_seq.shape: torch.Size([16, 20, "
     ]
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "ic.disable()\n",
    "def train_lstm(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs=25):\n",
    "    writer = SummaryWriter(tensorboard_log_dir)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for input_seq, target_seq in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_seq)  # Forward pass\n",
    "\n",
    "            # Convert one-hot target to class indices (needed for CrossEntropyLoss)\n",
    "            ic(target_seq.shape)\n",
    "            target_indices = torch.argmax(target_seq, dim=-1)  # Shape: (batch_size, seq_length\n",
    "            ic(target_indices.shape)\n",
    "            ic(outputs.shape)\n",
    "            # Compute loss (CrossEntropy expects target of shape (batch_size, seq_length))\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), target_indices.view(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "\n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for input_seq, target_seq in val_loader:\n",
    "                outputs = model(input_seq)\n",
    "\n",
    "                # Convert one-hot target to class indices\n",
    "                target_indices = torch.argmax(target_seq, dim=-1)\n",
    "\n",
    "                loss = criterion(outputs.view(-1, outputs.shape[-1]), target_indices.view(-1))\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        writer.add_scalar('Loss/Val', avg_val_loss, epoch)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, TrainLoss: {avg_train_loss:.4f}, ValLoss: {avg_val_loss:.4f}')\n",
    "\n",
    "    writer.close()\n",
    "\n",
    "\n",
    "import time\n",
    "\n",
    "# Define Amino Acid Encoding\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "char_to_index = {aa: i for i, aa in enumerate(amino_acids)}\n",
    "index_to_char = {i: aa for i, aa in enumerate(amino_acids)}\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 127 #len(amino_acids)  # Number of amino acids (20)\n",
    "hidden_size = 128  # LSTM hidden state size\n",
    "num_layers = 2  # Stacked LSTM layers\n",
    "output_size = 127#len(amino_acids)  # Predicting one of 20 amino acids\n",
    "batch_size = 16\n",
    "num_epochs = 30\n",
    "learning_rate = 0.001\n",
    "tensorboard_log_dir = f\"runs/generative_lstm/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = LSTM_Generative(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=20)\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the Model\n",
    "train_lstm(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'outputs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[116], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43moutputs\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'outputs' is not defined"
     ]
    }
   ],
   "source": [
    "outputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    ", target_indices.view(-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def evaluate_lstm(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    total_perplexity = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in test_loader:\n",
    "            outputs = model(input_seq)  # Predictions (batch_size, seq_length, num_classes)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), target_seq.view(-1, target_seq.shape[-1]))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy (compare predicted residues with ground truth)\n",
    "            predicted_indices = torch.argmax(outputs, dim=-1)  # Get highest probability residues\n",
    "            target_indices = torch.argmax(target_seq, dim=-1)  # Get actual residues\n",
    "            batch_accuracy = (predicted_indices == target_indices).float().mean().item()\n",
    "            total_accuracy += batch_accuracy\n",
    "\n",
    "            # Compute perplexity (exp of cross-entropy loss)\n",
    "            batch_perplexity = torch.exp(loss).item()\n",
    "            total_perplexity += batch_perplexity\n",
    "\n",
    "            num_samples += 1\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_loss = total_loss / num_samples\n",
    "    avg_accuracy = total_accu\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
