{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual FASTA parsing (without Biopython)\n",
    "# fasta_path = \"../data//naturalAMPs_APD2024a-ADAM.fasta.txt\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\", index=False)\n",
    "\n",
    "\n",
    "# Manual FASTA parsing (without Biopython)\n",
    "# fasta_path = \"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# fasta_path = \"../data/uniprotkb_length_5_TO_30_NOT_antimicrob_2025_04_14.fasta (1)\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Bio import SeqIO\n",
    "\n",
    "def read_fasta_sequences(fasta_path):\n",
    "    \"\"\"\n",
    "    Reads sequences from a FASTA file and returns them as a list of strings.\n",
    "\n",
    "    Args:\n",
    "        fasta_path (str): Path to the FASTA file.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of amino acid or nucleotide sequences as strings.\n",
    "    \"\"\"\n",
    "    sequences = [str(record.seq) for record in SeqIO.parse(fasta_path, \"fasta\")]\n",
    "    return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbbasp = pd.read_csv(\"../models1/database_check/DBAASP_peptides.csv\")\n",
    "dbbasp = dbbasp[[\"ID\", \"SEQUENCE\"]]\n",
    "dbbasp.columns = [\"Peptide ID\", \"Sequence\"]\n",
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "adam_df = pd.concat([adam_df, dbbasp], ignore_index=True)\n",
    "uniprot_df = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\")\n",
    "uniprot_df1 = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\")\n",
    "uniprot_df2 = pd.read_csv(\"/mnt/storageG1/lwang/Projects/TB-AMP-design/data/uniprotkb_length_5_TO_30_NOT_antimicrob_2025_04_23 (1).tsv\", sep=\"\\t\")\n",
    "uniprot_df2\n",
    "uniprot_df2 = uniprot_df2[['Entry', 'Sequence']]\n",
    "uniprot_df2.columns = [\"Peptide ID\", \"Sequence\"]\n",
    "uniprot_df = pd.concat([uniprot_df, uniprot_df1, uniprot_df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Peptide ID</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr|A0A009HCC0|A0A009HCC0_9GAMM Acyl carrier pr...</td>\n",
       "      <td>MSDIEQRVKQAVAEQLGLKAEEIKNEASFMDDLGADSLDLVELVMS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tr|A0A009YF97|A0A009YF97_9GAMM Acyl carrier pr...</td>\n",
       "      <td>MSDIEQRVKQAVAEQLGMKVEEIKNEASFMDDLGADSLDLVELVMS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tr|A0A010SZ96|A0A010SZ96_PSEFL Acyl carrier pr...</td>\n",
       "      <td>MSTIEERVKKIVAEQLGVKEEEVVNTASFVEDLGADSLDTVELVMA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tr|A0A011NAB1|A0A011NAB1_9PAST Acyl carrier pr...</td>\n",
       "      <td>MSIEERVKKIIVDQLGAKAEDVKPEASFIEDLGADSLDTVELVMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tr|A0A011NTH9|A0A011NTH9_9PROT DNA-directed RN...</td>\n",
       "      <td>MARVTVDDCLTRIPNRFQMTLAATYRARQITAGASPLIDANRDKPT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104968</th>\n",
       "      <td>P9WEJ1</td>\n",
       "      <td>MPEQKANCSPNGNITVDSMIMSLGSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105566</th>\n",
       "      <td>Q6LEB3</td>\n",
       "      <td>LFNKYISRPRRVELAVMLNLTERHIKI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105568</th>\n",
       "      <td>Q6QLL8</td>\n",
       "      <td>VKGFSFKYGNGVWIGRTKSTNSRSGFQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105789</th>\n",
       "      <td>Q9DTV7</td>\n",
       "      <td>HFPGFGQSLLFGYPVYVFGDCVQGDWCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106056</th>\n",
       "      <td>T0YYA3</td>\n",
       "      <td>EPGLPESFNVLIKEMQSLALDIELLKTREK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38787 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Peptide ID  \\\n",
       "0       tr|A0A009HCC0|A0A009HCC0_9GAMM Acyl carrier pr...   \n",
       "6       tr|A0A009YF97|A0A009YF97_9GAMM Acyl carrier pr...   \n",
       "8       tr|A0A010SZ96|A0A010SZ96_PSEFL Acyl carrier pr...   \n",
       "9       tr|A0A011NAB1|A0A011NAB1_9PAST Acyl carrier pr...   \n",
       "10      tr|A0A011NTH9|A0A011NTH9_9PROT DNA-directed RN...   \n",
       "...                                                   ...   \n",
       "104968                                             P9WEJ1   \n",
       "105566                                             Q6LEB3   \n",
       "105568                                             Q6QLL8   \n",
       "105789                                             Q9DTV7   \n",
       "106056                                             T0YYA3   \n",
       "\n",
       "                                                 Sequence  \n",
       "0       MSDIEQRVKQAVAEQLGLKAEEIKNEASFMDDLGADSLDLVELVMS...  \n",
       "6       MSDIEQRVKQAVAEQLGMKVEEIKNEASFMDDLGADSLDLVELVMS...  \n",
       "8       MSTIEERVKKIVAEQLGVKEEEVVNTASFVEDLGADSLDTVELVMA...  \n",
       "9       MSIEERVKKIIVDQLGAKAEDVKPEASFIEDLGADSLDTVELVMAL...  \n",
       "10      MARVTVDDCLTRIPNRFQMTLAATYRARQITAGASPLIDANRDKPT...  \n",
       "...                                                   ...  \n",
       "104968                         MPEQKANCSPNGNITVDSMIMSLGSS  \n",
       "105566                        LFNKYISRPRRVELAVMLNLTERHIKI  \n",
       "105568                        VKGFSFKYGNGVWIGRTKSTNSRSGFQ  \n",
       "105789                       HFPGFGQSLLFGYPVYVFGDCVQGDWCR  \n",
       "106056                     EPGLPESFNVLIKEMQSLALDIELLKTREK  \n",
       "\n",
       "[38787 rows x 2 columns]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniprot_df = uniprot_df[uniprot_df['Sequence'].apply(lambda s: isinstance(s, str) and len(s) >= 10)]\n",
    "uniprot_df = uniprot_df.drop_duplicates(subset=\"Sequence\")\n",
    "uniprot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove rows where the Sequence is not a string or is shorter than 10 characters\n",
    "adam_df = adam_df[adam_df['Sequence'].apply(lambda x: isinstance(x, str) and len(x) >= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 10\n",
      "Range of sequence lengths: 180\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sequence length: 23.250698890792677\n",
      "Median sequence length: 19.0\n",
      "Standard deviation of sequence lengths: 15.501457216027081\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate sequence lengths\n",
    "sequence_lengths = adam_df['Sequence'].apply(len)\n",
    "\n",
    "# Calculate the range of sequence lengths\n",
    "length_range = sequence_lengths.max() - sequence_lengths.min()\n",
    "print(sequence_lengths.max(),sequence_lengths.min())\n",
    "print(f\"Range of sequence lengths: {length_range}\")\n",
    "\n",
    "# Draw a box plot\n",
    "plt.boxplot(sequence_lengths)\n",
    "plt.title(\"Box Plot of Sequence Lengths\")\n",
    "plt.ylabel(\"Sequence Length\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display distribution statistics\n",
    "mean_length = sequence_lengths.mean()\n",
    "median_length = sequence_lengths.median()\n",
    "std_dev_length = sequence_lengths.std()\n",
    "\n",
    "print(f\"Mean sequence length: {mean_length}\")\n",
    "print(f\"Median sequence length: {median_length}\")\n",
    "print(f\"Standard deviation of sequence lengths: {std_dev_length}\")\n",
    "\n",
    "# adam_df = adam_df.drop(columns=['Sequence Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_100926/242586896.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_uniprot_df = uniprot_df.groupby('Sequence Length', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Peptide ID  \\\n",
      "0      sp|C0HJE2|AMYG_BACLI Glucoamylase (Fragment) O...   \n",
      "1      sp|Q03367|PSBF_CAPAN Cytochrome b559 subunit b...   \n",
      "2      sp|Q10997|SPI_HALRO Serine proteinase inhibito...   \n",
      "3      sp|P85962|RLA2_PSEMZ Large ribosomal subunit p...   \n",
      "4      sp|P02728|GLEM_HUMAN Erythrocyte membrane glyc...   \n",
      "...                                                  ...   \n",
      "15418  sp|Q6B8L8|RPOZ_GRATL Putative DNA-directed RNA...   \n",
      "15419  tr|A0A3N5N678|A0A3N5N678_9HYPH Acyl carrier pr...   \n",
      "15420  tr|A0A7L0FFZ7|A0A7L0FFZ7_CORCN Serine palmitoy...   \n",
      "15421  tr|A0A258UY98|A0A258UY98_9SPHN ATP synthase su...   \n",
      "15422  tr|A0A3C0S7Q9|A0A3C0S7Q9_9BACT DNA-directed RN...   \n",
      "\n",
      "                                                Sequence  \n",
      "0                                             SSNKLTTSWG  \n",
      "1                                             SISAMQFIQR  \n",
      "2                                             TKKDGEEKVA  \n",
      "3                                             DITEVIAAGR  \n",
      "4                                             CEGHSHDHGA  \n",
      "...                                                  ...  \n",
      "15418  MDNQNIKIYNHLKVHEIIYKTEELLNISDNRYKITIQIANRAKRKK...  \n",
      "15419  MSDVAERVKKIVVEHLGVEAEKVTESASFIDDLGADSLDTVELVMA...  \n",
      "15420  MDVKSMKNYLYWLYCQFELITCSYLMEPWEKLLFYTFNIAMLVTVL...  \n",
      "15421  MNWIHLISIMGAALAISLGAIAPALAQGKAVAAAMDAIARQPEAAA...  \n",
      "15422  MALQIQDTLGLAQKALDVVKSRYLLCILISQRIHQLEQGAQPTIDV...  \n",
      "\n",
      "[15423 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate sequence lengths for uniprot_df\n",
    "uniprot_df['Sequence Length'] = uniprot_df['Sequence'].apply(len)\n",
    "\n",
    "# Calculate sequence lengths for adam_df\n",
    "adam_df['Sequence Length'] = adam_df['Sequences'].apply(len)\n",
    "\n",
    "# Perform stratified sampling to select more samples\n",
    "default_min_samples = 30\n",
    "\n",
    "# Perform lenient stratified sampling\n",
    "sampled_uniprot_df = uniprot_df.groupby('Sequence Length', group_keys=False).apply(\n",
    "    lambda x: x.sample(\n",
    "        n=min(\n",
    "            len(x), \n",
    "            int(14 * adam_df['Sequence Length'].value_counts().get(x.name, default_min_samples))\n",
    "        ),\n",
    "        random_state=42\n",
    "    )\n",
    ").reset_index(drop=True)\n",
    "# Drop the 'Sequence Length' column after sampling\n",
    "sampled_uniprot_df = sampled_uniprot_df.drop(columns=['Sequence Length'])\n",
    "adam_df = adam_df.drop(columns=['Sequence Length'])\n",
    "\n",
    "# Draw a box plot to visualize the distribution\n",
    "plt.boxplot(sampled_uniprot_df['Sequence'].apply(len))\n",
    "plt.title(\"Box Plot of Sequence Lengths (Sampled Negatives)\")\n",
    "plt.ylabel(\"Sequence Length\")\n",
    "plt.show()\n",
    "\n",
    "print(sampled_uniprot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n",
      "{'n', 'k', 'U', 'm', 'V', 'w', 'D', 'M', 'T', 'K', 'f', 'd', 'a', 'r', 'L', 'A', 't', 'q', 'Y', ' ', 'h', 'y', 'W', 'G', 'E', 'C', '9', 's', 'I', 'Z', 'N', 'Q', 'e', 'F', 'p', 'x', 'v', 'X', 'O', 'P', 'i', 'H', 'l', 'g', 'B', 'c', 'S', 'R'}\n",
      "48\n",
      "{'n', 'k', 'U', 'm', 'w', 'f', 'd', 'a', 'r', 'q', ' ', 'h', 'y', '9', 's', 'Z', 'e', 'p', 'x', 'v', 'O', 'i', 'l', 'g', 'B', 'c', 't'}\n",
      "Number of 'B' values: 11\n",
      "Number of sequences after filtering: 31029\n",
      "label\n",
      "1    15839\n",
      "0    15190\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "\n",
    "adam_df['label'] = 1\n",
    "sampled_uniprot_df['label'] = 0\n",
    "adam_df.columns = [\"Peptide ID\", \"Sequences\", 'label']\n",
    "sampled_uniprot_df.columns = [\"Peptide ID\", \"Sequences\" , 'label']\n",
    "df = pd.concat([adam_df, sampled_uniprot_df], ignore_index=True)\n",
    "\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWYX\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "\n",
    "# Filter out sequences containing non-standard amino acids\n",
    "df = df[~df['Sequences'].str.contains('|'.join(non_standard_amino_acids))]\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "tb_df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df[~df['Sequences'].isin(tb_df['Sequences'])]\n",
    "df = df[df['Sequences'].apply(lambda x: isinstance(x, str) and len(x) >= 10)]\n",
    "print(f\"Number of sequences after filtering: {len(df)}\")\n",
    "print(df['label'].value_counts())\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# lengths = [len(seq) for seq in df['Sequences']]\n",
    "# print(lengths[lengths <= 0])\n",
    "\n",
    "lengths = np.array([len(seq) for seq in df['Sequences']])\n",
    "print(lengths[lengths <= 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define One-Hot Encoding Function for DNA Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        length = len(seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        return one_hot_torch(seq, dtype=self.one_hot_dtype), torch.tensor(label, dtype=torch.float32), length\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def collate_and_pack(batch):\n",
    "    # batch = list of (tensor_seq, label, length)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "\n",
    "    # Filter out sequences with zero length\n",
    "    filtered_batch = [(seq, lbl, l) for seq, lbl, l in zip(sequences, labels, lengths) if l > 0]\n",
    "\n",
    "    if len(filtered_batch) == 0:\n",
    "        raise ValueError(\"All sequences in the batch have zero length.\")\n",
    "\n",
    "    sequences, labels, lengths = zip(*filtered_batch)\n",
    "\n",
    "    # Convert lengths to tensor\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # Sort by descending length (required by pack_padded_sequence)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    labels = torch.tensor([labels[i] for i in sorted_indices])\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    # Stack to shape: (batch_size, 20, seq_len) and transpose for LSTM input\n",
    "    # LSTM expects input of shape (seq_len, batch_size, features)\n",
    "    sequences = [seq.T for seq in sequences]  # Transpose each [20, L] to [L, 20]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)  # shape: [max_len, batch, features]\n",
    "\n",
    "    # Pack the sequence\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 21720\n",
      "Validation: 4655\n",
      "Test: 4654\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        # packed_input: PackedSequence\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # hn: [num_layers, batch_size, hidden_dim]\n",
    "        # We'll use the **last layer's** hidden state as feature\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(last_hidden)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.5227, Val Loss: 0.3842, Val Acc: 0.8451, Val AUC: 0.9235\n",
      "Epoch [2/10] - Train Loss: 0.3084, Val Loss: 0.2863, Val Acc: 0.8876, Val AUC: 0.9638\n",
      "Epoch [3/10] - Train Loss: 0.2366, Val Loss: 0.2234, Val Acc: 0.9149, Val AUC: 0.9711\n",
      "Epoch [4/10] - Train Loss: 0.2087, Val Loss: 0.2155, Val Acc: 0.9218, Val AUC: 0.9741\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[75], line 112\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, acc, auc\n\u001b[1;32m    111\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMClassifier(hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[75], line 33\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[1;32m     32\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m---> 33\u001b[0m     \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     34\u001b[0m     epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     36\u001b[0m avg_train_loss \u001b[38;5;241m=\u001b[39m epoch_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader)\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/optim/optimizer.py:493\u001b[0m, in \u001b[0;36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    489\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    490\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    491\u001b[0m             )\n\u001b[0;32m--> 493\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_optimizer_step_code()\n\u001b[1;32m    496\u001b[0m \u001b[38;5;66;03m# call optimizer step post hooks\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/optim/optimizer.py:91\u001b[0m, in \u001b[0;36m_use_grad_for_differentiable.<locals>._use_grad\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     89\u001b[0m     torch\u001b[38;5;241m.\u001b[39mset_grad_enabled(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdefaults[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdifferentiable\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     90\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n\u001b[0;32m---> 91\u001b[0m     ret \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     93\u001b[0m     torch\u001b[38;5;241m.\u001b[39m_dynamo\u001b[38;5;241m.\u001b[39mgraph_break()\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/optim/adam.py:244\u001b[0m, in \u001b[0;36mAdam.step\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    232\u001b[0m     beta1, beta2 \u001b[38;5;241m=\u001b[39m group[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbetas\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    234\u001b[0m     has_complex \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_init_group(\n\u001b[1;32m    235\u001b[0m         group,\n\u001b[1;32m    236\u001b[0m         params_with_grad,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    241\u001b[0m         state_steps,\n\u001b[1;32m    242\u001b[0m     )\n\u001b[0;32m--> 244\u001b[0m     \u001b[43madam\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams_with_grad\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avgs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmax_exp_avg_sqs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    250\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstate_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    251\u001b[0m \u001b[43m        \u001b[49m\u001b[43mamsgrad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mamsgrad\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhas_complex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhas_complex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta1\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[43mbeta2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbeta2\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m        \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m        \u001b[49m\u001b[43meps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43meps\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmaximize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmaximize\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m        \u001b[49m\u001b[43mforeach\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mforeach\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    260\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcapturable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcapturable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    261\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdifferentiable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mdifferentiable\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    262\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfused\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgroup\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfused\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    263\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgrad_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgrad_scale\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    264\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfound_inf\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfound_inf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    265\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    267\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/optim/optimizer.py:154\u001b[0m, in \u001b[0;36m_disable_dynamo_if_unsupported.<locals>.wrapper.<locals>.maybe_fallback\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    152\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m disabled_func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 154\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/optim/adam.py:848\u001b[0m, in \u001b[0;36madam\u001b[0;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, foreach, capturable, differentiable, fused, grad_scale, found_inf, has_complex, amsgrad, beta1, beta2, lr, weight_decay, eps, maximize)\u001b[0m\n\u001b[1;32m    844\u001b[0m     _, foreach \u001b[38;5;241m=\u001b[39m _default_to_fused_or_foreach(\n\u001b[1;32m    845\u001b[0m         params, differentiable, use_fused\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    846\u001b[0m     )\n\u001b[1;32m    847\u001b[0m     \u001b[38;5;66;03m# Do not flip on foreach for the unsupported case where lr is a Tensor and capturable=False.\u001b[39;00m\n\u001b[0;32m--> 848\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m foreach \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTensor\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m capturable:\n\u001b[1;32m    849\u001b[0m         foreach \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m fused \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "    # Set up TensorBoard writer\n",
    "    log_dir = f\"runs/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Run evaluation\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return history\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # handle corner cases\n",
    "\n",
    "    # Sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        # Print metrics\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "model = LSTMClassifier(hidden_dim=64)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding in regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[401 160]\n",
      " [ 89 407]]\n",
      "Sensitivity (Recall for Positive Class): 0.8206\n",
      "Specificity (Recall for Negative Class): 0.7148\n",
      "Epoch [1/10] - Train Loss: 0.6630, Val Loss: 0.5471, Val Acc: 0.7644, Val AUC: 0.8299\n",
      "\n",
      "Confusion Matrix:\n",
      "[[460 101]\n",
      " [101 395]]\n",
      "Sensitivity (Recall for Positive Class): 0.7964\n",
      "Specificity (Recall for Negative Class): 0.8200\n",
      "Epoch [2/10] - Train Loss: 0.5161, Val Loss: 0.4849, Val Acc: 0.8089, Val AUC: 0.8625\n",
      "\n",
      "Confusion Matrix:\n",
      "[[192 369]\n",
      " [ 29 467]]\n",
      "Sensitivity (Recall for Positive Class): 0.9415\n",
      "Specificity (Recall for Negative Class): 0.3422\n",
      "Epoch [3/10] - Train Loss: 0.6564, Val Loss: 0.6466, Val Acc: 0.6235, Val AUC: 0.8047\n",
      "\n",
      "Confusion Matrix:\n",
      "[[424 137]\n",
      " [119 377]]\n",
      "Sensitivity (Recall for Positive Class): 0.7601\n",
      "Specificity (Recall for Negative Class): 0.7558\n",
      "Epoch [4/10] - Train Loss: 0.5960, Val Loss: 0.5372, Val Acc: 0.7578, Val AUC: 0.8364\n",
      "\n",
      "Confusion Matrix:\n",
      "[[458 103]\n",
      " [ 77 419]]\n",
      "Sensitivity (Recall for Positive Class): 0.8448\n",
      "Specificity (Recall for Negative Class): 0.8164\n",
      "Epoch [5/10] - Train Loss: 0.4724, Val Loss: 0.3893, Val Acc: 0.8297, Val AUC: 0.9082\n",
      "\n",
      "Confusion Matrix:\n",
      "[[515  46]\n",
      " [121 375]]\n",
      "Sensitivity (Recall for Positive Class): 0.7560\n",
      "Specificity (Recall for Negative Class): 0.9180\n",
      "Epoch [6/10] - Train Loss: 0.3992, Val Loss: 0.3821, Val Acc: 0.8420, Val AUC: 0.9220\n",
      "\n",
      "Confusion Matrix:\n",
      "[[476  85]\n",
      " [ 67 429]]\n",
      "Sensitivity (Recall for Positive Class): 0.8649\n",
      "Specificity (Recall for Negative Class): 0.8485\n",
      "Epoch [7/10] - Train Loss: 0.3674, Val Loss: 0.3593, Val Acc: 0.8562, Val AUC: 0.9305\n",
      "\n",
      "Confusion Matrix:\n",
      "[[512  49]\n",
      " [ 91 405]]\n",
      "Sensitivity (Recall for Positive Class): 0.8165\n",
      "Specificity (Recall for Negative Class): 0.9127\n",
      "Epoch [8/10] - Train Loss: 0.3570, Val Loss: 0.3330, Val Acc: 0.8675, Val AUC: 0.9375\n",
      "\n",
      "Confusion Matrix:\n",
      "[[451 110]\n",
      " [ 55 441]]\n",
      "Sensitivity (Recall for Positive Class): 0.8891\n",
      "Specificity (Recall for Negative Class): 0.8039\n",
      "Epoch [9/10] - Train Loss: 0.4099, Val Loss: 0.3379, Val Acc: 0.8439, Val AUC: 0.9359\n",
      "\n",
      "Confusion Matrix:\n",
      "[[464  97]\n",
      " [ 43 453]]\n",
      "Sensitivity (Recall for Positive Class): 0.9133\n",
      "Specificity (Recall for Negative Class): 0.8271\n",
      "Epoch [10/10] - Train Loss: 0.3665, Val Loss: 0.3215, Val Acc: 0.8675, Val AUC: 0.9421\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # L2 regularization\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "    # Set up TensorBoard writer\n",
    "    log_dir = f\"runs/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)   \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Run evaluation\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.4f}, \"\n",
    "              f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # handle corner cases\n",
    "\n",
    "    # Sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        # Print metrics\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=64, dropout=0.5)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3,\n",
    "                      weight_decay=1e-4, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling on general AMP data (bayesian optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 22:29:56,492] A new study created in memory with name: no-name-578f1e60-413e-4f31-aa4b-6525a19e5672\n",
      "[I 2025-04-23 22:39:19,317] Trial 0 finished with value: 0.20186862716936085 and parameters: {'hidden_dim': 118, 'num_layers': 2, 'dropout': 0.15770098555716838, 'lr': 0.0033631098397860566, 'weight_decay': 0.0016651826858557562}. Best is trial 0 with value: 0.20186862716936085.\n",
      "[I 2025-04-23 22:47:05,995] Trial 1 finished with value: 0.20391635106850978 and parameters: {'hidden_dim': 86, 'num_layers': 1, 'dropout': 0.3730028207259446, 'lr': 0.006315513405964547, 'weight_decay': 0.004771900430672791}. Best is trial 0 with value: 0.20186862716936085.\n",
      "[I 2025-04-23 22:55:50,323] Trial 2 finished with value: 0.6929459988254391 and parameters: {'hidden_dim': 91, 'num_layers': 2, 'dropout': 0.18846678538578907, 'lr': 0.009849482264248585, 'weight_decay': 0.006762646719950194}. Best is trial 0 with value: 0.20186862716936085.\n",
      "[I 2025-04-23 23:03:10,159] Trial 3 finished with value: 0.20052941112893902 and parameters: {'hidden_dim': 55, 'num_layers': 1, 'dropout': 0.1920259318831156, 'lr': 0.0010686403231261604, 'weight_decay': 0.0026747380559445184}. Best is trial 3 with value: 0.20052941112893902.\n",
      "[I 2025-04-23 23:12:35,970] Trial 4 finished with value: 0.6929492223752688 and parameters: {'hidden_dim': 102, 'num_layers': 3, 'dropout': 0.2070482086261586, 'lr': 0.00562182141049264, 'weight_decay': 0.0038652870962443938}. Best is trial 3 with value: 0.20052941112893902.\n",
      "[I 2025-04-23 23:20:14,627] Trial 5 finished with value: 0.6929452533591284 and parameters: {'hidden_dim': 127, 'num_layers': 2, 'dropout': 0.344064291204961, 'lr': 0.0073094590842470875, 'weight_decay': 0.005738215930160666}. Best is trial 3 with value: 0.20052941112893902.\n",
      "[I 2025-04-23 23:27:50,588] Trial 6 finished with value: 0.6929460282195105 and parameters: {'hidden_dim': 126, 'num_layers': 3, 'dropout': 0.12092811789339747, 'lr': 0.002477318734643991, 'weight_decay': 0.0035298166666060584}. Best is trial 3 with value: 0.20052941112893902.\n",
      "[I 2025-04-23 23:34:51,367] Trial 7 finished with value: 0.6929460812921393 and parameters: {'hidden_dim': 73, 'num_layers': 2, 'dropout': 0.182915843538377, 'lr': 0.0018595615096800328, 'weight_decay': 0.009336943069450527}. Best is trial 3 with value: 0.20052941112893902.\n",
      "[I 2025-04-23 23:40:58,120] Trial 8 finished with value: 0.19294931341523994 and parameters: {'hidden_dim': 90, 'num_layers': 1, 'dropout': 0.3883518993022814, 'lr': 0.003079724987138381, 'weight_decay': 0.0032083013065655687}. Best is trial 8 with value: 0.19294931341523994.\n",
      "[I 2025-04-23 23:47:05,513] Trial 9 finished with value: 0.6834461484869866 and parameters: {'hidden_dim': 121, 'num_layers': 2, 'dropout': 0.3611067455892515, 'lr': 0.008599343597567106, 'weight_decay': 0.0023697293730452033}. Best is trial 8 with value: 0.19294931341523994.\n",
      "[I 2025-04-23 23:52:34,027] Trial 10 finished with value: 0.15068996559879552 and parameters: {'hidden_dim': 35, 'num_layers': 1, 'dropout': 0.49187413485190673, 'lr': 0.003714299509741798, 'weight_decay': 0.0003265723576364255}. Best is trial 10 with value: 0.15068996559879552.\n",
      "[I 2025-04-23 23:57:53,378] Trial 11 finished with value: 0.19020529540434275 and parameters: {'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.4993470704900185, 'lr': 0.0039575474383692095, 'weight_decay': 0.0006664079928594491}. Best is trial 10 with value: 0.15068996559879552.\n",
      "[I 2025-04-24 00:03:06,596] Trial 12 finished with value: 0.14063468677540347 and parameters: {'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.4902444099362521, 'lr': 0.0043057438074332, 'weight_decay': 0.00013769582938473473}. Best is trial 12 with value: 0.14063468677540347.\n",
      "[I 2025-04-24 00:08:30,881] Trial 13 finished with value: 0.14079919911614835 and parameters: {'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.4999309252972477, 'lr': 0.004643665445338893, 'weight_decay': 7.562145516561019e-05}. Best is trial 12 with value: 0.14063468677540347.\n",
      "[I 2025-04-24 00:15:27,621] Trial 14 finished with value: 0.14385655374355513 and parameters: {'hidden_dim': 50, 'num_layers': 1, 'dropout': 0.4325765282523394, 'lr': 0.004933813598522111, 'weight_decay': 9.07445691172788e-05}. Best is trial 12 with value: 0.14063468677540347.\n",
      "[I 2025-04-24 00:22:32,905] Trial 15 finished with value: 0.16656890613575503 and parameters: {'hidden_dim': 51, 'num_layers': 1, 'dropout': 0.27522098528657646, 'lr': 0.0047549431708245465, 'weight_decay': 0.0013619409945396224}. Best is trial 12 with value: 0.14063468677540347.\n",
      "[I 2025-04-24 00:28:58,581] Trial 16 finished with value: 0.25136904091867684 and parameters: {'hidden_dim': 69, 'num_layers': 1, 'dropout': 0.44276605789481993, 'lr': 0.00028082211068058624, 'weight_decay': 0.00835467679407784}. Best is trial 12 with value: 0.14063468677540347.\n",
      "[I 2025-04-24 00:37:40,250] Trial 17 finished with value: 0.6929480131358317 and parameters: {'hidden_dim': 43, 'num_layers': 3, 'dropout': 0.4482185319831601, 'lr': 0.006879705244908604, 'weight_decay': 0.0015204712532224597}. Best is trial 12 with value: 0.14063468677540347.\n",
      "[I 2025-04-24 00:44:27,909] Trial 18 finished with value: 0.28841755181959233 and parameters: {'hidden_dim': 63, 'num_layers': 1, 'dropout': 0.2870032488081937, 'lr': 0.005667986026622578, 'weight_decay': 0.0066696235347515915}. Best is trial 12 with value: 0.14063468677540347.\n",
      "[I 2025-04-24 00:52:25,184] Trial 19 finished with value: 0.692945906560715 and parameters: {'hidden_dim': 42, 'num_layers': 2, 'dropout': 0.4132787207932833, 'lr': 0.007530042850015101, 'weight_decay': 0.004560682521475082}. Best is trial 12 with value: 0.14063468677540347.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.4902444099362521, 'lr': 0.0043057438074332, 'weight_decay': 0.00013769582938473473}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    # if not train:\n",
    "    #     model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_auc = train_model(model, train_loader, val_loader, num_epochs=20, lr=lr,\n",
    "                          weight_decay=weight_decay, verbose=False, train=True)\n",
    "    return val_auc\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[1852  427]\n",
      " [  99 2277]]\n",
      "Sensitivity: 0.9583, Specificity: 0.8126\n",
      "Epoch [1/20] - Train Loss: 0.5112, Val Loss: 0.3194, Val Acc: 0.8870, Val AUC: 0.9587\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2006  273]\n",
      " [ 104 2272]]\n",
      "Sensitivity: 0.9562, Specificity: 0.8802\n",
      "Epoch [2/20] - Train Loss: 0.2733, Val Loss: 0.2144, Val Acc: 0.9190, Val AUC: 0.9715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2095  184]\n",
      " [ 175 2201]]\n",
      "Sensitivity: 0.9263, Specificity: 0.9193\n",
      "Epoch [3/20] - Train Loss: 0.2374, Val Loss: 0.2003, Val Acc: 0.9229, Val AUC: 0.9755\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2001  278]\n",
      " [  91 2285]]\n",
      "Sensitivity: 0.9617, Specificity: 0.8780\n",
      "Epoch [4/20] - Train Loss: 0.2140, Val Loss: 0.1964, Val Acc: 0.9207, Val AUC: 0.9766\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2123  156]\n",
      " [ 190 2186]]\n",
      "Sensitivity: 0.9200, Specificity: 0.9315\n",
      "Epoch [5/20] - Train Loss: 0.1940, Val Loss: 0.1948, Val Acc: 0.9257, Val AUC: 0.9779\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2097  182]\n",
      " [ 143 2233]]\n",
      "Sensitivity: 0.9398, Specificity: 0.9201\n",
      "Epoch [6/20] - Train Loss: 0.1859, Val Loss: 0.1792, Val Acc: 0.9302, Val AUC: 0.9802\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2040  239]\n",
      " [  81 2295]]\n",
      "Sensitivity: 0.9659, Specificity: 0.8951\n",
      "Epoch [7/20] - Train Loss: 0.1774, Val Loss: 0.1749, Val Acc: 0.9313, Val AUC: 0.9813\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2080  199]\n",
      " [ 113 2263]]\n",
      "Sensitivity: 0.9524, Specificity: 0.9127\n",
      "Epoch [8/20] - Train Loss: 0.1733, Val Loss: 0.1670, Val Acc: 0.9330, Val AUC: 0.9825\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2130  149]\n",
      " [ 155 2221]]\n",
      "Sensitivity: 0.9348, Specificity: 0.9346\n",
      "Epoch [9/20] - Train Loss: 0.1677, Val Loss: 0.1694, Val Acc: 0.9347, Val AUC: 0.9832\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2006  273]\n",
      " [  64 2312]]\n",
      "Sensitivity: 0.9731, Specificity: 0.8802\n",
      "Epoch [10/20] - Train Loss: 0.1627, Val Loss: 0.1725, Val Acc: 0.9276, Val AUC: 0.9840\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2039  240]\n",
      " [  78 2298]]\n",
      "Sensitivity: 0.9672, Specificity: 0.8947\n",
      "Epoch [11/20] - Train Loss: 0.1570, Val Loss: 0.1578, Val Acc: 0.9317, Val AUC: 0.9850\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2208   71]\n",
      " [ 278 2098]]\n",
      "Sensitivity: 0.8830, Specificity: 0.9688\n",
      "Epoch [12/20] - Train Loss: 0.1552, Val Loss: 0.1876, Val Acc: 0.9250, Val AUC: 0.9836\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2086  193]\n",
      " [ 100 2276]]\n",
      "Sensitivity: 0.9579, Specificity: 0.9153\n",
      "Epoch [13/20] - Train Loss: 0.1497, Val Loss: 0.1508, Val Acc: 0.9371, Val AUC: 0.9863\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2147  132]\n",
      " [ 136 2240]]\n",
      "Sensitivity: 0.9428, Specificity: 0.9421\n",
      "Epoch [14/20] - Train Loss: 0.1451, Val Loss: 0.1468, Val Acc: 0.9424, Val AUC: 0.9867\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2188   91]\n",
      " [ 199 2177]]\n",
      "Sensitivity: 0.9162, Specificity: 0.9601\n",
      "Epoch [15/20] - Train Loss: 0.1437, Val Loss: 0.1573, Val Acc: 0.9377, Val AUC: 0.9867\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2068  211]\n",
      " [  75 2301]]\n",
      "Sensitivity: 0.9684, Specificity: 0.9074\n",
      "Epoch [16/20] - Train Loss: 0.1400, Val Loss: 0.1467, Val Acc: 0.9386, Val AUC: 0.9880\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2117  162]\n",
      " [ 108 2268]]\n",
      "Sensitivity: 0.9545, Specificity: 0.9289\n",
      "Epoch [17/20] - Train Loss: 0.1383, Val Loss: 0.1388, Val Acc: 0.9420, Val AUC: 0.9882\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2056  223]\n",
      " [  62 2314]]\n",
      "Sensitivity: 0.9739, Specificity: 0.9022\n",
      "Epoch [18/20] - Train Loss: 0.1327, Val Loss: 0.1478, Val Acc: 0.9388, Val AUC: 0.9890\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2123  156]\n",
      " [ 110 2266]]\n",
      "Sensitivity: 0.9537, Specificity: 0.9315\n",
      "Epoch [19/20] - Train Loss: 0.1312, Val Loss: 0.1452, Val Acc: 0.9429, Val AUC: 0.9882\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2090  189]\n",
      " [  82 2294]]\n",
      "Sensitivity: 0.9655, Specificity: 0.9171\n",
      "Epoch [20/20] - Train Loss: 0.1280, Val Loss: 0.1340, Val Acc: 0.9418, Val AUC: 0.9897\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2119  159]\n",
      " [  85 2290]]\n",
      "Sensitivity: 0.9642, Specificity: 0.9302\n",
      "Test Loss: 0.1172, Test Accuracy: 0.9476, Test AUC: 0.9921\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout= lstm_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_best_param['lr'],\n",
    "                      weight_decay=lstm_best_param['weight_decay'], verbose=True, train=False)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'best_model_lstm_1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 00:59:49,644] A new study created in memory with name: no-name-db27d220-723a-4c38-b7b1-3cd6955bfa35\n",
      "[I 2025-04-24 01:03:13,275] Trial 0 finished with value: 0.19840536062439826 and parameters: {'hidden_dim': 100, 'num_layers': 2, 'dropout': 0.21295967703838065, 'lr': 0.00575844680005971, 'weight_decay': 0.004380793434831622}. Best is trial 0 with value: 0.19840536062439826.\n",
      "[I 2025-04-24 01:06:43,201] Trial 1 finished with value: 0.1979307351659422 and parameters: {'hidden_dim': 66, 'num_layers': 3, 'dropout': 0.17505347426176818, 'lr': 0.006200857156169697, 'weight_decay': 0.004379496497472087}. Best is trial 1 with value: 0.1979307351659422.\n",
      "[I 2025-04-24 01:10:04,464] Trial 2 finished with value: 0.22604743340244032 and parameters: {'hidden_dim': 97, 'num_layers': 3, 'dropout': 0.43234003386883213, 'lr': 0.0055050768054949175, 'weight_decay': 0.007911101000066968}. Best is trial 1 with value: 0.1979307351659422.\n",
      "[I 2025-04-24 01:13:42,665] Trial 3 finished with value: 0.215105667812367 and parameters: {'hidden_dim': 105, 'num_layers': 3, 'dropout': 0.3739320133907871, 'lr': 0.009201613030603073, 'weight_decay': 0.00516488695219434}. Best is trial 1 with value: 0.1979307351659422.\n",
      "[I 2025-04-24 01:17:13,198] Trial 4 finished with value: 0.20750799489347901 and parameters: {'hidden_dim': 97, 'num_layers': 3, 'dropout': 0.11056801498186682, 'lr': 0.0008808945255055895, 'weight_decay': 0.003233493697249263}. Best is trial 1 with value: 0.1979307351659422.\n",
      "[I 2025-04-24 01:20:45,703] Trial 5 finished with value: 0.14166774233318355 and parameters: {'hidden_dim': 117, 'num_layers': 1, 'dropout': 0.20658251418870632, 'lr': 0.003162250835000041, 'weight_decay': 0.00047758677816097224}. Best is trial 5 with value: 0.14166774233318355.\n",
      "[I 2025-04-24 01:24:15,273] Trial 6 finished with value: 0.22121723751499228 and parameters: {'hidden_dim': 85, 'num_layers': 3, 'dropout': 0.3543385507704986, 'lr': 0.005668191198588545, 'weight_decay': 0.006395931723240956}. Best is trial 5 with value: 0.14166774233318355.\n",
      "[I 2025-04-24 01:27:29,687] Trial 7 finished with value: 0.21643316255856868 and parameters: {'hidden_dim': 91, 'num_layers': 2, 'dropout': 0.15255977773314644, 'lr': 0.007807247186836619, 'weight_decay': 0.008773261241064785}. Best is trial 5 with value: 0.14166774233318355.\n",
      "[I 2025-04-24 01:30:33,369] Trial 8 finished with value: 0.19216416663911245 and parameters: {'hidden_dim': 42, 'num_layers': 1, 'dropout': 0.37263481750029637, 'lr': 0.0063601340362954535, 'weight_decay': 0.006199832232382486}. Best is trial 5 with value: 0.14166774233318355.\n",
      "[I 2025-04-24 01:33:49,891] Trial 9 finished with value: 0.20867018809873764 and parameters: {'hidden_dim': 80, 'num_layers': 1, 'dropout': 0.23712310137816056, 'lr': 0.0020131134266083155, 'weight_decay': 0.007925513924522726}. Best is trial 5 with value: 0.14166774233318355.\n",
      "[I 2025-04-24 01:37:37,317] Trial 10 finished with value: 0.14137877095235538 and parameters: {'hidden_dim': 124, 'num_layers': 1, 'dropout': 0.28503703241610984, 'lr': 0.0030256641935430067, 'weight_decay': 0.0004201433793522731}. Best is trial 10 with value: 0.14137877095235538.\n",
      "[I 2025-04-24 01:40:49,449] Trial 11 finished with value: 0.12357484814647125 and parameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.2794048413737868, 'lr': 0.0030378137748946626, 'weight_decay': 0.00010269830007820016}. Best is trial 11 with value: 0.12357484814647125.\n",
      "[I 2025-04-24 01:44:15,131] Trial 12 finished with value: 0.13254810282832954 and parameters: {'hidden_dim': 126, 'num_layers': 1, 'dropout': 0.28242335295171106, 'lr': 0.003666598620194872, 'weight_decay': 6.727948269314777e-05}. Best is trial 11 with value: 0.12357484814647125.\n",
      "[I 2025-04-24 01:47:42,231] Trial 13 finished with value: 0.166952592256951 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.28595181264326824, 'lr': 0.003965536448206724, 'weight_decay': 0.0020713541667680985}. Best is trial 11 with value: 0.12357484814647125.\n",
      "[I 2025-04-24 01:50:40,195] Trial 14 finished with value: 0.19294244306136485 and parameters: {'hidden_dim': 115, 'num_layers': 1, 'dropout': 0.49242280377847514, 'lr': 0.00010056479466835416, 'weight_decay': 0.0020883778173864166}. Best is trial 11 with value: 0.12357484814647125.\n",
      "[I 2025-04-24 01:53:40,014] Trial 15 finished with value: 0.12706143036484718 and parameters: {'hidden_dim': 70, 'num_layers': 1, 'dropout': 0.32590539076436176, 'lr': 0.003715689046360429, 'weight_decay': 8.362582585089315e-05}. Best is trial 11 with value: 0.12357484814647125.\n",
      "[I 2025-04-24 01:56:42,282] Trial 16 finished with value: 0.15437163453396052 and parameters: {'hidden_dim': 62, 'num_layers': 2, 'dropout': 0.32982086740069105, 'lr': 0.001814629138954511, 'weight_decay': 0.0018228168484590956}. Best is trial 11 with value: 0.12357484814647125.\n",
      "[I 2025-04-24 01:59:27,911] Trial 17 finished with value: 0.15350543023788765 and parameters: {'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.41083578875657234, 'lr': 0.0043729468095069935, 'weight_decay': 0.0013490005941751882}. Best is trial 11 with value: 0.12357484814647125.\n",
      "[I 2025-04-24 02:02:26,546] Trial 18 finished with value: 0.17536667897684932 and parameters: {'hidden_dim': 67, 'num_layers': 2, 'dropout': 0.2562670672328116, 'lr': 0.002222534186944956, 'weight_decay': 0.002988438866197673}. Best is trial 11 with value: 0.12357484814647125.\n",
      "[I 2025-04-24 02:05:21,810] Trial 19 finished with value: 0.169091243756144 and parameters: {'hidden_dim': 54, 'num_layers': 1, 'dropout': 0.31986037698261577, 'lr': 0.0044965696089531455, 'weight_decay': 0.002958520598308232}. Best is trial 11 with value: 0.12357484814647125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.2794048413737868, 'lr': 0.0030378137748946626, 'weight_decay': 0.00010269830007820016}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# Updated BiLSTM with flatten layer as previously defined\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_flatten/BiLSTM_Flatten_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "    max_seq_len = 100  # fixed for now; match your padding/truncation\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage (uncomment and run in your local environment):\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_dim': 127,\n",
       " 'num_layers': 1,\n",
       " 'dropout': 0.2794048413737868,\n",
       " 'lr': 0.0030378137748946626,\n",
       " 'weight_decay': 0.00010269830007820016}"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[1955  324]\n",
      " [ 148 2228]]\n",
      "Sensitivity: 0.9377, Specificity: 0.8578\n",
      "Epoch [1/30] - Train Loss: 0.4831, Val Loss: 0.3297, Val Acc: 0.8986, Val AUC: 0.9449\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2030  249]\n",
      " [ 134 2242]]\n",
      "Sensitivity: 0.9436, Specificity: 0.8907\n",
      "Epoch [2/30] - Train Loss: 0.2328, Val Loss: 0.2184, Val Acc: 0.9177, Val AUC: 0.9732\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2055  224]\n",
      " [ 145 2231]]\n",
      "Sensitivity: 0.9390, Specificity: 0.9017\n",
      "Epoch [3/30] - Train Loss: 0.2088, Val Loss: 0.2019, Val Acc: 0.9207, Val AUC: 0.9755\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2086  193]\n",
      " [ 166 2210]]\n",
      "Sensitivity: 0.9301, Specificity: 0.9153\n",
      "Epoch [4/30] - Train Loss: 0.2030, Val Loss: 0.1905, Val Acc: 0.9229, Val AUC: 0.9777\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2106  173]\n",
      " [ 159 2217]]\n",
      "Sensitivity: 0.9331, Specificity: 0.9241\n",
      "Epoch [5/30] - Train Loss: 0.1824, Val Loss: 0.1785, Val Acc: 0.9287, Val AUC: 0.9811\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2086  193]\n",
      " [ 118 2258]]\n",
      "Sensitivity: 0.9503, Specificity: 0.9153\n",
      "Epoch [6/30] - Train Loss: 0.1686, Val Loss: 0.1761, Val Acc: 0.9332, Val AUC: 0.9808\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2051  228]\n",
      " [ 102 2274]]\n",
      "Sensitivity: 0.9571, Specificity: 0.9000\n",
      "Epoch [7/30] - Train Loss: 0.1682, Val Loss: 0.1726, Val Acc: 0.9291, Val AUC: 0.9824\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2073  206]\n",
      " [  98 2278]]\n",
      "Sensitivity: 0.9588, Specificity: 0.9096\n",
      "Epoch [8/30] - Train Loss: 0.1614, Val Loss: 0.1615, Val Acc: 0.9347, Val AUC: 0.9849\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2111  168]\n",
      " [ 121 2255]]\n",
      "Sensitivity: 0.9491, Specificity: 0.9263\n",
      "Epoch [9/30] - Train Loss: 0.1502, Val Loss: 0.1608, Val Acc: 0.9379, Val AUC: 0.9849\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2097  182]\n",
      " [ 108 2268]]\n",
      "Sensitivity: 0.9545, Specificity: 0.9201\n",
      "Epoch [10/30] - Train Loss: 0.1382, Val Loss: 0.1572, Val Acc: 0.9377, Val AUC: 0.9856\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2117  162]\n",
      " [  92 2284]]\n",
      "Sensitivity: 0.9613, Specificity: 0.9289\n",
      "Epoch [11/30] - Train Loss: 0.1344, Val Loss: 0.1370, Val Acc: 0.9454, Val AUC: 0.9888\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2140  139]\n",
      " [ 109 2267]]\n",
      "Sensitivity: 0.9541, Specificity: 0.9390\n",
      "Epoch [12/30] - Train Loss: 0.1296, Val Loss: 0.1348, Val Acc: 0.9467, Val AUC: 0.9891\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2091  188]\n",
      " [  82 2294]]\n",
      "Sensitivity: 0.9655, Specificity: 0.9175\n",
      "Epoch [13/30] - Train Loss: 0.1235, Val Loss: 0.1458, Val Acc: 0.9420, Val AUC: 0.9878\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2200   79]\n",
      " [ 135 2241]]\n",
      "Sensitivity: 0.9432, Specificity: 0.9653\n",
      "Epoch [14/30] - Train Loss: 0.1217, Val Loss: 0.1268, Val Acc: 0.9540, Val AUC: 0.9902\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2169  110]\n",
      " [  93 2283]]\n",
      "Sensitivity: 0.9609, Specificity: 0.9517\n",
      "Epoch [15/30] - Train Loss: 0.1135, Val Loss: 0.1186, Val Acc: 0.9564, Val AUC: 0.9912\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2199   80]\n",
      " [ 126 2250]]\n",
      "Sensitivity: 0.9470, Specificity: 0.9649\n",
      "Epoch [16/30] - Train Loss: 0.1087, Val Loss: 0.1173, Val Acc: 0.9557, Val AUC: 0.9919\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2213   66]\n",
      " [ 145 2231]]\n",
      "Sensitivity: 0.9390, Specificity: 0.9710\n",
      "Epoch [17/30] - Train Loss: 0.1075, Val Loss: 0.1177, Val Acc: 0.9547, Val AUC: 0.9920\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2204   75]\n",
      " [ 118 2258]]\n",
      "Sensitivity: 0.9503, Specificity: 0.9671\n",
      "Epoch [18/30] - Train Loss: 0.1048, Val Loss: 0.1121, Val Acc: 0.9585, Val AUC: 0.9923\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2043  236]\n",
      " [  77 2299]]\n",
      "Sensitivity: 0.9676, Specificity: 0.8964\n",
      "Epoch [19/30] - Train Loss: 0.1227, Val Loss: 0.1675, Val Acc: 0.9328, Val AUC: 0.9866\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2198   81]\n",
      " [ 126 2250]]\n",
      "Sensitivity: 0.9470, Specificity: 0.9645\n",
      "Epoch [20/30] - Train Loss: 0.1237, Val Loss: 0.1239, Val Acc: 0.9555, Val AUC: 0.9906\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2226   53]\n",
      " [ 189 2187]]\n",
      "Sensitivity: 0.9205, Specificity: 0.9767\n",
      "Epoch [21/30] - Train Loss: 0.1047, Val Loss: 0.1377, Val Acc: 0.9480, Val AUC: 0.9914\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2227   52]\n",
      " [ 168 2208]]\n",
      "Sensitivity: 0.9293, Specificity: 0.9772\n",
      "Epoch [22/30] - Train Loss: 0.0973, Val Loss: 0.1216, Val Acc: 0.9527, Val AUC: 0.9917\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2202   77]\n",
      " [ 122 2254]]\n",
      "Sensitivity: 0.9487, Specificity: 0.9662\n",
      "Epoch [23/30] - Train Loss: 0.0921, Val Loss: 0.1156, Val Acc: 0.9573, Val AUC: 0.9919\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2208   71]\n",
      " [ 133 2243]]\n",
      "Sensitivity: 0.9440, Specificity: 0.9688\n",
      "Epoch [24/30] - Train Loss: 0.0900, Val Loss: 0.1177, Val Acc: 0.9562, Val AUC: 0.9917\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2188   91]\n",
      " [ 121 2255]]\n",
      "Sensitivity: 0.9491, Specificity: 0.9601\n",
      "Epoch [25/30] - Train Loss: 0.0880, Val Loss: 0.1201, Val Acc: 0.9545, Val AUC: 0.9919\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2167  112]\n",
      " [ 113 2263]]\n",
      "Sensitivity: 0.9524, Specificity: 0.9509\n",
      "Epoch [26/30] - Train Loss: 0.0846, Val Loss: 0.1215, Val Acc: 0.9517, Val AUC: 0.9913\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2190   89]\n",
      " [ 112 2264]]\n",
      "Sensitivity: 0.9529, Specificity: 0.9609\n",
      "Epoch [27/30] - Train Loss: 0.0839, Val Loss: 0.1100, Val Acc: 0.9568, Val AUC: 0.9925\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2183   96]\n",
      " [  93 2283]]\n",
      "Sensitivity: 0.9609, Specificity: 0.9579\n",
      "Epoch [28/30] - Train Loss: 0.0805, Val Loss: 0.1153, Val Acc: 0.9594, Val AUC: 0.9924\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2151  128]\n",
      " [  79 2297]]\n",
      "Sensitivity: 0.9668, Specificity: 0.9438\n",
      "Epoch [29/30] - Train Loss: 0.0766, Val Loss: 0.1190, Val Acc: 0.9555, Val AUC: 0.9928\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2221   58]\n",
      " [ 155 2221]]\n",
      "Sensitivity: 0.9348, Specificity: 0.9746\n",
      "Epoch [30/30] - Train Loss: 0.0739, Val Loss: 0.1223, Val Acc: 0.9542, Val AUC: 0.9926\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2220   58]\n",
      " [ 150 2225]]\n",
      "Sensitivity: 0.9368, Specificity: 0.9745\n",
      "Test Loss: 0.1212, Test Accuracy: 0.9553, Test AUC: 0.9930\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout= bilstm_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=30, lr=bilstm_best_param['lr'],\n",
    "                      weight_decay=bilstm_best_param['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 02:14:04,336] A new study created in memory with name: no-name-584e9dbd-ffd9-44dd-afec-856078a359b1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 02:16:43,388] Trial 0 finished with value: 0.18178581778113156 and parameters: {'hidden_dim': 43, 'num_layers': 3, 'dropout': 0.16688084395799957, 'lr': 0.0016967913084958185, 'weight_decay': 2.1368172284833312e-06}. Best is trial 0 with value: 0.18178581778113156.\n",
      "[I 2025-04-24 02:19:22,199] Trial 1 finished with value: 0.2197459757532159 and parameters: {'hidden_dim': 52, 'num_layers': 2, 'dropout': 0.30214792017447845, 'lr': 0.0010077852489961392, 'weight_decay': 0.00045081063760837794}. Best is trial 0 with value: 0.18178581778113156.\n",
      "[I 2025-04-24 02:22:04,103] Trial 2 finished with value: 0.17075231564167428 and parameters: {'hidden_dim': 111, 'num_layers': 2, 'dropout': 0.29727846759967036, 'lr': 0.0015935281764741956, 'weight_decay': 1.4521347161197535e-05}. Best is trial 2 with value: 0.17075231564167428.\n",
      "[I 2025-04-24 02:24:37,915] Trial 3 finished with value: 0.1632602711348501 and parameters: {'hidden_dim': 119, 'num_layers': 2, 'dropout': 0.4296783485284802, 'lr': 0.002913585310615679, 'weight_decay': 2.1271206146142976e-05}. Best is trial 3 with value: 0.1632602711348501.\n",
      "[I 2025-04-24 02:27:08,846] Trial 4 finished with value: 0.18257386563983682 and parameters: {'hidden_dim': 74, 'num_layers': 2, 'dropout': 0.35243315544865794, 'lr': 0.0014431419332604392, 'weight_decay': 0.00021316569376238326}. Best is trial 3 with value: 0.1632602711348501.\n",
      "[I 2025-04-24 02:29:47,076] Trial 5 finished with value: 0.1964183636520007 and parameters: {'hidden_dim': 107, 'num_layers': 3, 'dropout': 0.10967216276126335, 'lr': 0.0008710914319490338, 'weight_decay': 1.7800464665867004e-05}. Best is trial 3 with value: 0.1632602711348501.\n",
      "[I 2025-04-24 02:32:25,030] Trial 6 finished with value: 0.6929458600201018 and parameters: {'hidden_dim': 92, 'num_layers': 3, 'dropout': 0.12045547243308433, 'lr': 0.0004085095740486011, 'weight_decay': 0.004610564723142926}. Best is trial 3 with value: 0.1632602711348501.\n",
      "[I 2025-04-24 02:35:27,291] Trial 7 finished with value: 0.17275535708216772 and parameters: {'hidden_dim': 126, 'num_layers': 3, 'dropout': 0.13885802720728826, 'lr': 0.0019307203183299387, 'weight_decay': 1.1908530998404362e-06}. Best is trial 3 with value: 0.1632602711348501.\n",
      "[I 2025-04-24 02:38:34,636] Trial 8 finished with value: 0.21353571841569796 and parameters: {'hidden_dim': 116, 'num_layers': 3, 'dropout': 0.4607900856900814, 'lr': 0.0019149622181749999, 'weight_decay': 0.00013984512161384807}. Best is trial 3 with value: 0.1632602711348501.\n",
      "[I 2025-04-24 02:41:35,504] Trial 9 finished with value: 0.6929455570978661 and parameters: {'hidden_dim': 56, 'num_layers': 3, 'dropout': 0.37177417365615095, 'lr': 0.0008638539372370661, 'weight_decay': 0.009638510131849784}. Best is trial 3 with value: 0.1632602711348501.\n",
      "[I 2025-04-24 02:44:44,753] Trial 10 finished with value: 0.1637516407015389 and parameters: {'hidden_dim': 86, 'num_layers': 1, 'dropout': 0.48422984634983834, 'lr': 0.009040500338391197, 'weight_decay': 1.5551788486793456e-05}. Best is trial 3 with value: 0.1632602711348501.\n",
      "[I 2025-04-24 02:48:03,472] Trial 11 finished with value: 0.1749408366031026 and parameters: {'hidden_dim': 86, 'num_layers': 1, 'dropout': 0.48849032785452734, 'lr': 0.009507064846120024, 'weight_decay': 1.802442702693056e-05}. Best is trial 3 with value: 0.1632602711348501.\n",
      "[I 2025-04-24 02:51:16,599] Trial 12 finished with value: 0.1378282050359739 and parameters: {'hidden_dim': 73, 'num_layers': 1, 'dropout': 0.42257973470705656, 'lr': 0.007949617623932719, 'weight_decay': 7.416073340484421e-06}. Best is trial 12 with value: 0.1378282050359739.\n",
      "[I 2025-04-24 02:54:38,833] Trial 13 finished with value: 0.18135554884394553 and parameters: {'hidden_dim': 65, 'num_layers': 1, 'dropout': 0.41384974296667704, 'lr': 0.0041901246718249905, 'weight_decay': 4.535149555291026e-06}. Best is trial 12 with value: 0.1378282050359739.\n",
      "[I 2025-04-24 02:58:07,757] Trial 14 finished with value: 0.17336131785422154 and parameters: {'hidden_dim': 98, 'num_layers': 1, 'dropout': 0.40555785413861933, 'lr': 0.004693287549098405, 'weight_decay': 4.895319110598858e-05}. Best is trial 12 with value: 0.1378282050359739.\n",
      "[I 2025-04-24 03:01:17,373] Trial 15 finished with value: 0.23816182841993358 and parameters: {'hidden_dim': 75, 'num_layers': 2, 'dropout': 0.25268437066004124, 'lr': 0.0001143343210295138, 'weight_decay': 4.915993401754474e-06}. Best is trial 12 with value: 0.1378282050359739.\n",
      "[I 2025-04-24 03:04:21,917] Trial 16 finished with value: 0.20202720114221312 and parameters: {'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.4258038864609356, 'lr': 0.004402607779068766, 'weight_decay': 0.0013803754196951687}. Best is trial 12 with value: 0.1378282050359739.\n",
      "[I 2025-04-24 03:07:30,633] Trial 17 finished with value: 0.17585056259819906 and parameters: {'hidden_dim': 101, 'num_layers': 2, 'dropout': 0.3364434592641825, 'lr': 0.003308702139873009, 'weight_decay': 4.85922662436251e-05}. Best is trial 12 with value: 0.1378282050359739.\n",
      "[I 2025-04-24 03:10:17,571] Trial 18 finished with value: 0.2731192561045085 and parameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.23763166290807297, 'lr': 0.0063445845558282275, 'weight_decay': 3.558443004966182e-06}. Best is trial 12 with value: 0.1378282050359739.\n",
      "[I 2025-04-24 03:13:00,113] Trial 19 finished with value: 0.17436581569379322 and parameters: {'hidden_dim': 66, 'num_layers': 2, 'dropout': 0.44801712584838094, 'lr': 0.003005910976344389, 'weight_decay': 4.855555878720996e-05}. Best is trial 12 with value: 0.1378282050359739.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 73, 'num_layers': 1, 'dropout': 0.42257973470705656, 'lr': 0.007949617623932719, 'weight_decay': 7.416073340484421e-06}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# LSTM with Attention classifier\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)  # shape: [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)  # shape: [batch, seq_len]\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)  # normalize\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # shape: [batch, hidden_dim]\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-attn/LSTM_Attn_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-lstm_attention.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_attn_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[1768  511]\n",
      " [ 250 2126]]\n",
      "Sensitivity: 0.8948, Specificity: 0.7758\n",
      "Epoch [1/20] - Train Loss: 0.5481, Val Loss: 0.4131, Val Acc: 0.8365, Val AUC: 0.9009\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1300  979]\n",
      " [ 133 2243]]\n",
      "Sensitivity: 0.9440, Specificity: 0.5704\n",
      "Epoch [2/20] - Train Loss: 0.4977, Val Loss: 0.5078, Val Acc: 0.7611, Val AUC: 0.8885\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1968  311]\n",
      " [  88 2288]]\n",
      "Sensitivity: 0.9630, Specificity: 0.8635\n",
      "Epoch [3/20] - Train Loss: 0.2632, Val Loss: 0.2269, Val Acc: 0.9143, Val AUC: 0.9743\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2078  201]\n",
      " [ 144 2232]]\n",
      "Sensitivity: 0.9394, Specificity: 0.9118\n",
      "Epoch [4/20] - Train Loss: 0.2052, Val Loss: 0.1968, Val Acc: 0.9259, Val AUC: 0.9770\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1987  292]\n",
      " [  90 2286]]\n",
      "Sensitivity: 0.9621, Specificity: 0.8719\n",
      "Epoch [5/20] - Train Loss: 0.1880, Val Loss: 0.1873, Val Acc: 0.9179, Val AUC: 0.9800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2131  148]\n",
      " [ 149 2227]]\n",
      "Sensitivity: 0.9373, Specificity: 0.9351\n",
      "Epoch [6/20] - Train Loss: 0.1726, Val Loss: 0.1626, Val Acc: 0.9362, Val AUC: 0.9838\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2167  112]\n",
      " [ 168 2208]]\n",
      "Sensitivity: 0.9293, Specificity: 0.9509\n",
      "Epoch [7/20] - Train Loss: 0.1585, Val Loss: 0.1578, Val Acc: 0.9398, Val AUC: 0.9855\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2173  106]\n",
      " [ 167 2209]]\n",
      "Sensitivity: 0.9297, Specificity: 0.9535\n",
      "Epoch [8/20] - Train Loss: 0.1500, Val Loss: 0.1490, Val Acc: 0.9414, Val AUC: 0.9867\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2081  198]\n",
      " [  83 2293]]\n",
      "Sensitivity: 0.9651, Specificity: 0.9131\n",
      "Epoch [9/20] - Train Loss: 0.1454, Val Loss: 0.1558, Val Acc: 0.9396, Val AUC: 0.9860\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2104  175]\n",
      " [  93 2283]]\n",
      "Sensitivity: 0.9609, Specificity: 0.9232\n",
      "Epoch [10/20] - Train Loss: 0.1334, Val Loss: 0.1457, Val Acc: 0.9424, Val AUC: 0.9875\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2153  126]\n",
      " [ 115 2261]]\n",
      "Sensitivity: 0.9516, Specificity: 0.9447\n",
      "Epoch [11/20] - Train Loss: 0.1299, Val Loss: 0.1466, Val Acc: 0.9482, Val AUC: 0.9880\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2122  157]\n",
      " [  91 2285]]\n",
      "Sensitivity: 0.9617, Specificity: 0.9311\n",
      "Epoch [12/20] - Train Loss: 0.1255, Val Loss: 0.1364, Val Acc: 0.9467, Val AUC: 0.9891\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2185   94]\n",
      " [ 142 2234]]\n",
      "Sensitivity: 0.9402, Specificity: 0.9588\n",
      "Epoch [13/20] - Train Loss: 0.1194, Val Loss: 0.1359, Val Acc: 0.9493, Val AUC: 0.9894\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2169  110]\n",
      " [ 138 2238]]\n",
      "Sensitivity: 0.9419, Specificity: 0.9517\n",
      "Epoch [14/20] - Train Loss: 0.1153, Val Loss: 0.1344, Val Acc: 0.9467, Val AUC: 0.9892\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2108  171]\n",
      " [ 101 2275]]\n",
      "Sensitivity: 0.9575, Specificity: 0.9250\n",
      "Epoch [15/20] - Train Loss: 0.1076, Val Loss: 0.1453, Val Acc: 0.9416, Val AUC: 0.9880\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2175  104]\n",
      " [ 152 2224]]\n",
      "Sensitivity: 0.9360, Specificity: 0.9544\n",
      "Epoch [16/20] - Train Loss: 0.1029, Val Loss: 0.1423, Val Acc: 0.9450, Val AUC: 0.9888\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2161  118]\n",
      " [  92 2284]]\n",
      "Sensitivity: 0.9613, Specificity: 0.9482\n",
      "Epoch [17/20] - Train Loss: 0.1023, Val Loss: 0.1332, Val Acc: 0.9549, Val AUC: 0.9896\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2148  131]\n",
      " [  97 2279]]\n",
      "Sensitivity: 0.9592, Specificity: 0.9425\n",
      "Epoch [18/20] - Train Loss: 0.1004, Val Loss: 0.1391, Val Acc: 0.9510, Val AUC: 0.9894\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2176  103]\n",
      " [ 135 2241]]\n",
      "Sensitivity: 0.9432, Specificity: 0.9548\n",
      "Epoch [19/20] - Train Loss: 0.0911, Val Loss: 0.1368, Val Acc: 0.9489, Val AUC: 0.9892\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2188   91]\n",
      " [ 144 2232]]\n",
      "Sensitivity: 0.9394, Specificity: 0.9601\n",
      "Epoch [20/20] - Train Loss: 0.0876, Val Loss: 0.1621, Val Acc: 0.9495, Val AUC: 0.9894\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2185   93]\n",
      " [ 137 2238]]\n",
      "Sensitivity: 0.9423, Specificity: 0.9592\n",
      "Test Loss: 0.1492, Test Accuracy: 0.9506, Test AUC: 0.9917\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_attn_best_param['hidden_dim'], num_layers=lstm_attn_best_param['num_layers'], dropout= lstm_attn_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_attn_best_param['lr'],\n",
    "                      weight_decay=lstm_attn_best_param['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm + attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 03:19:50,499] A new study created in memory with name: no-name-4b004da4-2ef1-49f0-9209-827009ce76f7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 03:22:51,419] Trial 0 finished with value: 0.19018186618612237 and parameters: {'hidden_dim': 43, 'num_layers': 1, 'dropout': 0.13900372495935556, 'lr': 0.005216606119325261, 'weight_decay': 0.0027449152028254267}. Best is trial 0 with value: 0.19018186618612237.\n",
      "[I 2025-04-24 03:25:55,543] Trial 1 finished with value: 0.6929475861052944 and parameters: {'hidden_dim': 107, 'num_layers': 2, 'dropout': 0.29980588033125843, 'lr': 0.005322469898777397, 'weight_decay': 0.003111427563486947}. Best is trial 0 with value: 0.19018186618612237.\n",
      "[I 2025-04-24 03:28:42,499] Trial 2 finished with value: 0.22952822299852763 and parameters: {'hidden_dim': 40, 'num_layers': 1, 'dropout': 0.30692636212754404, 'lr': 0.0031868149143787885, 'weight_decay': 0.007963465800881734}. Best is trial 0 with value: 0.19018186618612237.\n",
      "[I 2025-04-24 03:31:32,531] Trial 3 finished with value: 0.21738621092414204 and parameters: {'hidden_dim': 75, 'num_layers': 1, 'dropout': 0.19629973431601655, 'lr': 0.007145490039502117, 'weight_decay': 0.004942712942324684}. Best is trial 0 with value: 0.19018186618612237.\n",
      "[I 2025-04-24 03:34:36,215] Trial 4 finished with value: 0.6929458787996475 and parameters: {'hidden_dim': 45, 'num_layers': 3, 'dropout': 0.28366041887700494, 'lr': 0.005894313044422328, 'weight_decay': 0.008606505032028708}. Best is trial 0 with value: 0.19018186618612237.\n",
      "[I 2025-04-24 03:37:17,626] Trial 5 finished with value: 0.2192859099539992 and parameters: {'hidden_dim': 124, 'num_layers': 1, 'dropout': 0.3634642536708068, 'lr': 0.002135685041738899, 'weight_decay': 0.005774192487796736}. Best is trial 0 with value: 0.19018186618612237.\n",
      "[I 2025-04-24 03:39:57,843] Trial 6 finished with value: 0.18810269930591322 and parameters: {'hidden_dim': 96, 'num_layers': 1, 'dropout': 0.42892690364705177, 'lr': 0.0009546464086789204, 'weight_decay': 0.0006962431482729197}. Best is trial 6 with value: 0.18810269930591322.\n",
      "[I 2025-04-24 03:42:30,291] Trial 7 finished with value: 0.2308090995639971 and parameters: {'hidden_dim': 91, 'num_layers': 1, 'dropout': 0.13654395430123115, 'lr': 0.009250285572265836, 'weight_decay': 0.0072500620533522036}. Best is trial 6 with value: 0.18810269930591322.\n",
      "[I 2025-04-24 03:45:31,493] Trial 8 finished with value: 0.30150961671790033 and parameters: {'hidden_dim': 120, 'num_layers': 1, 'dropout': 0.2689075494346391, 'lr': 0.008367228582570567, 'weight_decay': 0.007743557481239333}. Best is trial 6 with value: 0.18810269930591322.\n",
      "[I 2025-04-24 03:48:51,149] Trial 9 finished with value: 0.6929499572270537 and parameters: {'hidden_dim': 86, 'num_layers': 3, 'dropout': 0.11219706677430939, 'lr': 0.008221832199884675, 'weight_decay': 0.008133119675844134}. Best is trial 6 with value: 0.18810269930591322.\n",
      "[I 2025-04-24 03:51:55,005] Trial 10 finished with value: 0.16757463806704298 and parameters: {'hidden_dim': 65, 'num_layers': 2, 'dropout': 0.4848231696137831, 'lr': 0.0006840255223523315, 'weight_decay': 0.0002811577932575873}. Best is trial 10 with value: 0.16757463806704298.\n",
      "[I 2025-04-24 03:55:02,418] Trial 11 finished with value: 0.20510954848707538 and parameters: {'hidden_dim': 64, 'num_layers': 2, 'dropout': 0.47905556168048063, 'lr': 0.00014490678140422278, 'weight_decay': 0.0004086549696276563}. Best is trial 10 with value: 0.16757463806704298.\n",
      "[I 2025-04-24 03:57:57,052] Trial 12 finished with value: 0.17675191796805761 and parameters: {'hidden_dim': 60, 'num_layers': 2, 'dropout': 0.4996386142001808, 'lr': 0.00028471289622716833, 'weight_decay': 1.3798310420026143e-05}. Best is trial 10 with value: 0.16757463806704298.\n",
      "[I 2025-04-24 04:00:48,257] Trial 13 finished with value: 0.20653141698200408 and parameters: {'hidden_dim': 60, 'num_layers': 2, 'dropout': 0.4926535480679353, 'lr': 0.0023679727244795817, 'weight_decay': 0.0021801033527657373}. Best is trial 10 with value: 0.16757463806704298.\n",
      "[I 2025-04-24 04:03:49,737] Trial 14 finished with value: 0.17883072201519795 and parameters: {'hidden_dim': 61, 'num_layers': 2, 'dropout': 0.4208186134083465, 'lr': 0.003801343979094086, 'weight_decay': 0.001364442020906402}. Best is trial 10 with value: 0.16757463806704298.\n",
      "[I 2025-04-24 04:07:03,835] Trial 15 finished with value: 0.1547731656121881 and parameters: {'hidden_dim': 72, 'num_layers': 3, 'dropout': 0.4154040883157904, 'lr': 0.0011999905002511623, 'weight_decay': 1.9408679083588733e-05}. Best is trial 15 with value: 0.1547731656121881.\n",
      "[I 2025-04-24 04:10:32,363] Trial 16 finished with value: 0.6929454583011262 and parameters: {'hidden_dim': 73, 'num_layers': 3, 'dropout': 0.41308817804138775, 'lr': 0.0014383682500099912, 'weight_decay': 0.004357573911867046}. Best is trial 15 with value: 0.1547731656121881.\n",
      "[I 2025-04-24 04:13:35,006] Trial 17 finished with value: 0.6929492338062966 and parameters: {'hidden_dim': 33, 'num_layers': 3, 'dropout': 0.36313561017968543, 'lr': 0.0038011592223274474, 'weight_decay': 0.0015250946124740303}. Best is trial 15 with value: 0.1547731656121881.\n",
      "[I 2025-04-24 04:16:24,520] Trial 18 finished with value: 0.692945999641941 and parameters: {'hidden_dim': 104, 'num_layers': 3, 'dropout': 0.45027532931486847, 'lr': 0.0024209982194059827, 'weight_decay': 0.00971608384234205}. Best is trial 15 with value: 0.1547731656121881.\n",
      "[I 2025-04-24 04:19:02,050] Trial 19 finished with value: 0.6929458485890742 and parameters: {'hidden_dim': 52, 'num_layers': 2, 'dropout': 0.3885776020715546, 'lr': 0.0013222070024702043, 'weight_decay': 0.003937810668926782}. Best is trial 15 with value: 0.1547731656121881.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 72, 'num_layers': 3, 'dropout': 0.4154040883157904, 'lr': 0.0011999905002511623, 'weight_decay': 1.9408679083588733e-05}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# BiLSTM with Attention Classifier\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[1988  291]\n",
      " [ 108 2268]]\n",
      "Sensitivity: 0.9545, Specificity: 0.8723\n",
      "Epoch [1/30] - Train Loss: 0.3178, Val Loss: 0.2226, Val Acc: 0.9143, Val AUC: 0.9711\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2121  158]\n",
      " [ 201 2175]]\n",
      "Sensitivity: 0.9154, Specificity: 0.9307\n",
      "Epoch [2/30] - Train Loss: 0.2155, Val Loss: 0.1942, Val Acc: 0.9229, Val AUC: 0.9775\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1960  319]\n",
      " [ 103 2273]]\n",
      "Sensitivity: 0.9566, Specificity: 0.8600\n",
      "Epoch [3/30] - Train Loss: 0.1894, Val Loss: 0.2038, Val Acc: 0.9093, Val AUC: 0.9762\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2025  254]\n",
      " [  89 2287]]\n",
      "Sensitivity: 0.9625, Specificity: 0.8885\n",
      "Epoch [4/30] - Train Loss: 0.1801, Val Loss: 0.1754, Val Acc: 0.9263, Val AUC: 0.9813\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2105  174]\n",
      " [ 148 2228]]\n",
      "Sensitivity: 0.9377, Specificity: 0.9237\n",
      "Epoch [5/30] - Train Loss: 0.1679, Val Loss: 0.1704, Val Acc: 0.9308, Val AUC: 0.9818\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2025  254]\n",
      " [  97 2279]]\n",
      "Sensitivity: 0.9592, Specificity: 0.8885\n",
      "Epoch [6/30] - Train Loss: 0.1630, Val Loss: 0.1808, Val Acc: 0.9246, Val AUC: 0.9811\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1983  296]\n",
      " [ 136 2240]]\n",
      "Sensitivity: 0.9428, Specificity: 0.8701\n",
      "Epoch [7/30] - Train Loss: 0.1746, Val Loss: 0.2121, Val Acc: 0.9072, Val AUC: 0.9734\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2097  182]\n",
      " [ 138 2238]]\n",
      "Sensitivity: 0.9419, Specificity: 0.9201\n",
      "Epoch [8/30] - Train Loss: 0.1686, Val Loss: 0.1608, Val Acc: 0.9313, Val AUC: 0.9841\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2096  183]\n",
      " [ 138 2238]]\n",
      "Sensitivity: 0.9419, Specificity: 0.9197\n",
      "Epoch [9/30] - Train Loss: 0.1520, Val Loss: 0.1646, Val Acc: 0.9310, Val AUC: 0.9836\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2092  187]\n",
      " [ 107 2269]]\n",
      "Sensitivity: 0.9550, Specificity: 0.9179\n",
      "Epoch [10/30] - Train Loss: 0.1483, Val Loss: 0.1554, Val Acc: 0.9368, Val AUC: 0.9859\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2102  177]\n",
      " [ 108 2268]]\n",
      "Sensitivity: 0.9545, Specificity: 0.9223\n",
      "Epoch [11/30] - Train Loss: 0.1466, Val Loss: 0.1537, Val Acc: 0.9388, Val AUC: 0.9862\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2154  125]\n",
      " [ 156 2220]]\n",
      "Sensitivity: 0.9343, Specificity: 0.9452\n",
      "Epoch [12/30] - Train Loss: 0.1429, Val Loss: 0.1481, Val Acc: 0.9396, Val AUC: 0.9865\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2049  230]\n",
      " [  74 2302]]\n",
      "Sensitivity: 0.9689, Specificity: 0.8991\n",
      "Epoch [13/30] - Train Loss: 0.1408, Val Loss: 0.1611, Val Acc: 0.9347, Val AUC: 0.9856\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2125  154]\n",
      " [ 131 2245]]\n",
      "Sensitivity: 0.9449, Specificity: 0.9324\n",
      "Epoch [14/30] - Train Loss: 0.1365, Val Loss: 0.1454, Val Acc: 0.9388, Val AUC: 0.9865\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2204   75]\n",
      " [ 288 2088]]\n",
      "Sensitivity: 0.8788, Specificity: 0.9671\n",
      "Epoch [15/30] - Train Loss: 0.1328, Val Loss: 0.1748, Val Acc: 0.9220, Val AUC: 0.9852\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2130  149]\n",
      " [ 107 2269]]\n",
      "Sensitivity: 0.9550, Specificity: 0.9346\n",
      "Epoch [16/30] - Train Loss: 0.1303, Val Loss: 0.1344, Val Acc: 0.9450, Val AUC: 0.9890\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2162  117]\n",
      " [ 146 2230]]\n",
      "Sensitivity: 0.9386, Specificity: 0.9487\n",
      "Epoch [17/30] - Train Loss: 0.1263, Val Loss: 0.1312, Val Acc: 0.9435, Val AUC: 0.9892\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2072  207]\n",
      " [  81 2295]]\n",
      "Sensitivity: 0.9659, Specificity: 0.9092\n",
      "Epoch [18/30] - Train Loss: 0.1223, Val Loss: 0.1599, Val Acc: 0.9381, Val AUC: 0.9876\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2137  142]\n",
      " [ 126 2250]]\n",
      "Sensitivity: 0.9470, Specificity: 0.9377\n",
      "Epoch [19/30] - Train Loss: 0.1203, Val Loss: 0.1353, Val Acc: 0.9424, Val AUC: 0.9886\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2147  132]\n",
      " [ 122 2254]]\n",
      "Sensitivity: 0.9487, Specificity: 0.9421\n",
      "Epoch [20/30] - Train Loss: 0.1189, Val Loss: 0.1368, Val Acc: 0.9454, Val AUC: 0.9886\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2183   96]\n",
      " [ 141 2235]]\n",
      "Sensitivity: 0.9407, Specificity: 0.9579\n",
      "Epoch [21/30] - Train Loss: 0.1153, Val Loss: 0.1295, Val Acc: 0.9491, Val AUC: 0.9899\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2123  156]\n",
      " [  92 2284]]\n",
      "Sensitivity: 0.9613, Specificity: 0.9315\n",
      "Epoch [22/30] - Train Loss: 0.1116, Val Loss: 0.1301, Val Acc: 0.9467, Val AUC: 0.9899\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2183   96]\n",
      " [ 140 2236]]\n",
      "Sensitivity: 0.9411, Specificity: 0.9579\n",
      "Epoch [23/30] - Train Loss: 0.1080, Val Loss: 0.1274, Val Acc: 0.9493, Val AUC: 0.9903\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2104  175]\n",
      " [  94 2282]]\n",
      "Sensitivity: 0.9604, Specificity: 0.9232\n",
      "Epoch [24/30] - Train Loss: 0.1083, Val Loss: 0.1392, Val Acc: 0.9422, Val AUC: 0.9892\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2044  235]\n",
      " [  70 2306]]\n",
      "Sensitivity: 0.9705, Specificity: 0.8969\n",
      "Epoch [25/30] - Train Loss: 0.1045, Val Loss: 0.1533, Val Acc: 0.9345, Val AUC: 0.9871\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2118  161]\n",
      " [  80 2296]]\n",
      "Sensitivity: 0.9663, Specificity: 0.9294\n",
      "Epoch [26/30] - Train Loss: 0.1071, Val Loss: 0.1297, Val Acc: 0.9482, Val AUC: 0.9909\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2197   82]\n",
      " [ 167 2209]]\n",
      "Sensitivity: 0.9297, Specificity: 0.9640\n",
      "Epoch [27/30] - Train Loss: 0.1024, Val Loss: 0.1345, Val Acc: 0.9465, Val AUC: 0.9892\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2170  109]\n",
      " [ 119 2257]]\n",
      "Sensitivity: 0.9499, Specificity: 0.9522\n",
      "Epoch [28/30] - Train Loss: 0.0946, Val Loss: 0.1238, Val Acc: 0.9510, Val AUC: 0.9908\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2190   89]\n",
      " [ 118 2258]]\n",
      "Sensitivity: 0.9503, Specificity: 0.9609\n",
      "Epoch [29/30] - Train Loss: 0.0964, Val Loss: 0.1144, Val Acc: 0.9555, Val AUC: 0.9917\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2186   93]\n",
      " [ 148 2228]]\n",
      "Sensitivity: 0.9377, Specificity: 0.9592\n",
      "Epoch [30/30] - Train Loss: 0.0919, Val Loss: 0.1353, Val Acc: 0.9482, Val AUC: 0.9896\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2200   78]\n",
      " [ 170 2205]]\n",
      "Sensitivity: 0.9284, Specificity: 0.9658\n",
      "Test Loss: 0.1247, Test Accuracy: 0.9467, Test AUC: 0.9920\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model-bilstm_attention.pt')\n",
    "\n",
    "\n",
    "model = BiLSTMWithAttentionClassifier(\n",
    "    input_dim=20,\n",
    "    hidden_dim=bilstm_attn_best_param['hidden_dim'],\n",
    "    num_layers=bilstm_attn_best_param['num_layers'],\n",
    "    dropout=bilstm_attn_best_param['dropout']\n",
    ")\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=30, lr=bilstm_attn_best_param['lr'],\n",
    "                      weight_decay=bilstm_attn_best_param['weight_decay'], verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning to TB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'W', 'P', 'K', 'X', 'H', 'F', 'A', 'V', 'G', 'T', 'N', 'I', 'M', 'S', 'Q', 'R', 'Y', 'C', 'D', 'L', 'E'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AMP\n",
       "0.0    235\n",
       "1.0    205\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"AMP\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 264\n",
      "Validation: 88\n",
      "Test: 88\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:26:40,908] A new study created in memory with name: no-name-63868d4a-02aa-47f1-bd28-96c49a0a516d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:26:44,271] Trial 0 finished with value: 0.29968616366386414 and parameters: {'lr': 0.003965124150798926, 'weight_decay': 4.843807430377461e-05, 'dropout': 0.3212122237525864}. Best is trial 0 with value: 0.29968616366386414.\n",
      "[I 2025-04-24 04:26:47,398] Trial 1 finished with value: 0.506826917330424 and parameters: {'lr': 5.141802555767809e-05, 'weight_decay': 1.3693120935274958e-06, 'dropout': 0.4920347687027704}. Best is trial 0 with value: 0.29968616366386414.\n",
      "[I 2025-04-24 04:26:50,492] Trial 2 finished with value: 0.5105321804682413 and parameters: {'lr': 1.6530847407386235e-05, 'weight_decay': 9.06201046128733e-06, 'dropout': 0.4428268477985905}. Best is trial 0 with value: 0.29968616366386414.\n",
      "[I 2025-04-24 04:26:53,583] Trial 3 finished with value: 0.4718224306901296 and parameters: {'lr': 0.00039388040983086184, 'weight_decay': 0.0004552538230940001, 'dropout': 0.3635698941400992}. Best is trial 0 with value: 0.29968616366386414.\n",
      "[I 2025-04-24 04:26:56,658] Trial 4 finished with value: 0.29102861881256104 and parameters: {'lr': 0.009705639641200896, 'weight_decay': 4.025579723140578e-06, 'dropout': 0.2968233499802121}. Best is trial 4 with value: 0.29102861881256104.\n",
      "[I 2025-04-24 04:26:59,719] Trial 5 finished with value: 0.49290527900060016 and parameters: {'lr': 0.00016956993390259976, 'weight_decay': 0.0002498732559438478, 'dropout': 0.20145509104941386}. Best is trial 4 with value: 0.29102861881256104.\n",
      "[I 2025-04-24 04:27:02,786] Trial 6 finished with value: 0.45862042903900146 and parameters: {'lr': 0.0004809327595324119, 'weight_decay': 0.00042941291757855394, 'dropout': 0.1445934480678801}. Best is trial 4 with value: 0.29102861881256104.\n",
      "[I 2025-04-24 04:27:05,847] Trial 7 finished with value: 0.3735789656639099 and parameters: {'lr': 0.0014888994679197617, 'weight_decay': 1.2795249481314729e-05, 'dropout': 0.19472179724006092}. Best is trial 4 with value: 0.29102861881256104.\n",
      "[I 2025-04-24 04:27:08,903] Trial 8 finished with value: 0.49726348121960956 and parameters: {'lr': 0.00012694214889815417, 'weight_decay': 0.0004860943596537412, 'dropout': 0.13433547204102553}. Best is trial 4 with value: 0.29102861881256104.\n",
      "[I 2025-04-24 04:27:11,962] Trial 9 finished with value: 0.5079472561677297 and parameters: {'lr': 4.009147908501952e-05, 'weight_decay': 0.0006818858492191734, 'dropout': 0.409562369034534}. Best is trial 4 with value: 0.29102861881256104.\n",
      "[I 2025-04-24 04:27:15,009] Trial 10 finished with value: 0.30041199425856274 and parameters: {'lr': 0.007142998990105154, 'weight_decay': 0.007710227432036176, 'dropout': 0.2425557614672233}. Best is trial 4 with value: 0.29102861881256104.\n",
      "[I 2025-04-24 04:27:18,086] Trial 11 finished with value: 0.29431089758872986 and parameters: {'lr': 0.008124450741379366, 'weight_decay': 2.6057096917030846e-05, 'dropout': 0.3117021598753943}. Best is trial 4 with value: 0.29102861881256104.\n",
      "[I 2025-04-24 04:27:21,154] Trial 12 finished with value: 0.3502925932407379 and parameters: {'lr': 0.0019968207474225287, 'weight_decay': 1.1700894864665964e-06, 'dropout': 0.2856279711175848}. Best is trial 4 with value: 0.29102861881256104.\n",
      "[I 2025-04-24 04:27:24,207] Trial 13 finished with value: 0.2934248248736064 and parameters: {'lr': 0.007702779207829573, 'weight_decay': 7.3309435902775925e-06, 'dropout': 0.31540797035822726}. Best is trial 4 with value: 0.29102861881256104.\n",
      "[I 2025-04-24 04:27:27,281] Trial 14 finished with value: 0.3977941373984019 and parameters: {'lr': 0.0011994508544485834, 'weight_decay': 3.1336588481681263e-06, 'dropout': 0.36727818518181315}. Best is trial 4 with value: 0.29102861881256104.\n",
      "[I 2025-04-24 04:27:30,340] Trial 15 finished with value: 0.3132350842158 and parameters: {'lr': 0.0030695485674339337, 'weight_decay': 6.005943158455112e-06, 'dropout': 0.2841948108099726}. Best is trial 4 with value: 0.29102861881256104.\n",
      "[I 2025-04-24 04:27:33,410] Trial 16 finished with value: 0.28319938480854034 and parameters: {'lr': 0.009830738282818238, 'weight_decay': 0.00010986177815344734, 'dropout': 0.23636753992110987}. Best is trial 16 with value: 0.28319938480854034.\n",
      "[I 2025-04-24 04:27:36,474] Trial 17 finished with value: 0.43600623806317645 and parameters: {'lr': 0.0007367091562890534, 'weight_decay': 0.00010080724821812565, 'dropout': 0.23015502038730684}. Best is trial 16 with value: 0.28319938480854034.\n",
      "[I 2025-04-24 04:27:39,531] Trial 18 finished with value: 0.30456817150115967 and parameters: {'lr': 0.003276042176226118, 'weight_decay': 0.0019082552903346278, 'dropout': 0.1696522290307985}. Best is trial 16 with value: 0.28319938480854034.\n",
      "[I 2025-04-24 04:27:42,598] Trial 19 finished with value: 0.2938552995522817 and parameters: {'lr': 0.004383881089397302, 'weight_decay': 0.00010715031192931589, 'dropout': 0.10961270316732141}. Best is trial 16 with value: 0.28319938480854034.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.009830738282818238, 'weight_decay': 0.00010986177815344734, 'dropout': 0.23636753992110987}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation function with detailed output\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    # print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    # print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    # print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function for frozen encoder\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-lstm-trans-frozen/FrozenEncoder_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        # if val_auc > best_val_auc:\n",
    "        #     best_val_auc = val_auc\n",
    "        #     torch.save(model.state_dict(), 'best_model_frozen.pt')\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=dropout)\n",
    "    model.load_state_dict(torch.load('best_model_lstm_1.pt'))\n",
    "    \n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_frozen_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.009830738282818238,\n",
       " 'weight_decay': 0.00010986177815344734,\n",
       " 'dropout': 0.23636753992110987}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_frozen_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8295, AUC: 0.9507\n",
      "Sensitivity: 1.0000, Specificity: 0.6809\n",
      "Confusion Matrix:\n",
      "[[32 15]\n",
      " [ 0 41]]\n",
      "Epoch [1/20] - Train Loss: 0.6295, Val Loss: 0.4410, Val Acc: 0.8295, Val AUC: 0.9507\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8523, AUC: 0.9502\n",
      "Sensitivity: 1.0000, Specificity: 0.7234\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 0 41]]\n",
      "Epoch [2/20] - Train Loss: 0.5424, Val Loss: 0.3936, Val Acc: 0.8523, Val AUC: 0.9502\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8523, AUC: 0.9502\n",
      "Sensitivity: 0.9756, Specificity: 0.7447\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 1 40]]\n",
      "Epoch [3/20] - Train Loss: 0.4616, Val Loss: 0.3542, Val Acc: 0.8523, Val AUC: 0.9502\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8523, AUC: 0.9512\n",
      "Sensitivity: 0.9512, Specificity: 0.7660\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 2 39]]\n",
      "Epoch [4/20] - Train Loss: 0.4877, Val Loss: 0.3255, Val Acc: 0.8523, Val AUC: 0.9512\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8636, AUC: 0.9517\n",
      "Sensitivity: 0.9512, Specificity: 0.7872\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 2 39]]\n",
      "Epoch [5/20] - Train Loss: 0.4114, Val Loss: 0.3069, Val Acc: 0.8636, Val AUC: 0.9517\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8636, AUC: 0.9502\n",
      "Sensitivity: 0.9512, Specificity: 0.7872\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 2 39]]\n",
      "Epoch [6/20] - Train Loss: 0.3998, Val Loss: 0.2988, Val Acc: 0.8636, Val AUC: 0.9502\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8523, AUC: 0.9538\n",
      "Sensitivity: 0.9268, Specificity: 0.7872\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 3 38]]\n",
      "Epoch [7/20] - Train Loss: 0.4128, Val Loss: 0.2959, Val Acc: 0.8523, Val AUC: 0.9538\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8750, AUC: 0.9569\n",
      "Sensitivity: 0.9268, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 3 38]]\n",
      "Epoch [8/20] - Train Loss: 0.4032, Val Loss: 0.2952, Val Acc: 0.8750, Val AUC: 0.9569\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8750, AUC: 0.9574\n",
      "Sensitivity: 0.9268, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 3 38]]\n",
      "Epoch [9/20] - Train Loss: 0.4304, Val Loss: 0.2958, Val Acc: 0.8750, Val AUC: 0.9574\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8750, AUC: 0.9580\n",
      "Sensitivity: 0.9268, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 3 38]]\n",
      "Epoch [10/20] - Train Loss: 0.4132, Val Loss: 0.2963, Val Acc: 0.8750, Val AUC: 0.9580\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8750, AUC: 0.9590\n",
      "Sensitivity: 0.9268, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 3 38]]\n",
      "Epoch [11/20] - Train Loss: 0.3991, Val Loss: 0.2967, Val Acc: 0.8750, Val AUC: 0.9590\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9611\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [12/20] - Train Loss: 0.4133, Val Loss: 0.2963, Val Acc: 0.8864, Val AUC: 0.9611\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9632\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [13/20] - Train Loss: 0.3810, Val Loss: 0.2963, Val Acc: 0.8864, Val AUC: 0.9632\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9642\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [14/20] - Train Loss: 0.3909, Val Loss: 0.2944, Val Acc: 0.8864, Val AUC: 0.9642\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9657\n",
      "Sensitivity: 0.9756, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 1 40]]\n",
      "Epoch [15/20] - Train Loss: 0.3932, Val Loss: 0.2927, Val Acc: 0.8977, Val AUC: 0.9657\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9673\n",
      "Sensitivity: 0.9756, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 1 40]]\n",
      "Epoch [16/20] - Train Loss: 0.3644, Val Loss: 0.2907, Val Acc: 0.8977, Val AUC: 0.9673\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9683\n",
      "Sensitivity: 0.9756, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 1 40]]\n",
      "Epoch [17/20] - Train Loss: 0.4055, Val Loss: 0.2889, Val Acc: 0.8977, Val AUC: 0.9683\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9678\n",
      "Sensitivity: 0.9756, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 1 40]]\n",
      "Epoch [18/20] - Train Loss: 0.3633, Val Loss: 0.2893, Val Acc: 0.8977, Val AUC: 0.9678\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9678\n",
      "Sensitivity: 0.9756, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 1 40]]\n",
      "Epoch [19/20] - Train Loss: 0.3858, Val Loss: 0.2863, Val Acc: 0.8977, Val AUC: 0.9678\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9683\n",
      "Sensitivity: 0.9756, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 1 40]]\n",
      "Epoch [20/20] - Train Loss: 0.3821, Val Loss: 0.2835, Val Acc: 0.8977, Val AUC: 0.9683\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8750, AUC: 0.9450\n",
      "Sensitivity: 0.9024, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 4 37]]\n",
      "Test Loss: 0.3176, Test Accuracy: 0.8750, Test AUC: 0.9450\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-trans-frozen/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Best hyperparameters: {'lr': 0.009940295438316211, 'weight_decay': 1.4383289881186473e-05, 'dropout': 0.22563027249521914}\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=lstm_frozen_best_param['dropout'])\n",
    "model.load_state_dict(torch.load('best_model_lstm_1.pt'))\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_frozen_best_param['lr'],\n",
    "                      weight_decay=lstm_frozen_best_param['weight_decay'], verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:27:46,810] A new study created in memory with name: no-name-13015497-b33e-4d06-87cb-b0864f1b02e4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:27:49,895] Trial 0 finished with value: 0.1774317423502604 and parameters: {'lr': 0.001025891617117429, 'weight_decay': 1.1645688809531886e-06, 'dropout': 0.47164158775400755}. Best is trial 0 with value: 0.1774317423502604.\n",
      "[I 2025-04-24 04:27:53,376] Trial 1 finished with value: 0.17016056180000305 and parameters: {'lr': 0.000647881621177116, 'weight_decay': 0.006301578377335753, 'dropout': 0.45113349794553415}. Best is trial 1 with value: 0.17016056180000305.\n",
      "[I 2025-04-24 04:27:56,870] Trial 2 finished with value: 0.27499156693617505 and parameters: {'lr': 0.00013338084335559627, 'weight_decay': 0.0002453316452267952, 'dropout': 0.349075357435251}. Best is trial 1 with value: 0.17016056180000305.\n",
      "[I 2025-04-24 04:28:00,244] Trial 3 finished with value: 0.39594284693400067 and parameters: {'lr': 2.9182380419929234e-05, 'weight_decay': 2.554008805380876e-05, 'dropout': 0.1766061331454447}. Best is trial 1 with value: 0.17016056180000305.\n",
      "[I 2025-04-24 04:28:03,628] Trial 4 finished with value: 0.18807255973418555 and parameters: {'lr': 0.005955253834427649, 'weight_decay': 1.8708624624954433e-06, 'dropout': 0.47828544241868853}. Best is trial 1 with value: 0.17016056180000305.\n",
      "[I 2025-04-24 04:28:07,004] Trial 5 finished with value: 0.18092663089434305 and parameters: {'lr': 0.0011050824437600151, 'weight_decay': 2.845572581422537e-06, 'dropout': 0.24555121138098765}. Best is trial 1 with value: 0.17016056180000305.\n",
      "[I 2025-04-24 04:28:10,470] Trial 6 finished with value: 0.25520341098308563 and parameters: {'lr': 0.00017320430955154986, 'weight_decay': 0.0006677245145622635, 'dropout': 0.47351605436290745}. Best is trial 1 with value: 0.17016056180000305.\n",
      "[I 2025-04-24 04:28:13,905] Trial 7 finished with value: 0.16874153912067413 and parameters: {'lr': 0.0015508933290271692, 'weight_decay': 5.106518228388959e-05, 'dropout': 0.15266870198817917}. Best is trial 7 with value: 0.16874153912067413.\n",
      "[I 2025-04-24 04:28:17,296] Trial 8 finished with value: 0.4052680532137553 and parameters: {'lr': 2.696485054742835e-05, 'weight_decay': 1.9282747843007036e-05, 'dropout': 0.2554202481290665}. Best is trial 7 with value: 0.16874153912067413.\n",
      "[I 2025-04-24 04:28:20,724] Trial 9 finished with value: 0.2697640061378479 and parameters: {'lr': 0.0001433594800734995, 'weight_decay': 0.00035250199220727356, 'dropout': 0.31119872518854963}. Best is trial 7 with value: 0.16874153912067413.\n",
      "[I 2025-04-24 04:28:24,146] Trial 10 finished with value: 0.1902714048822721 and parameters: {'lr': 0.0054534549482595784, 'weight_decay': 2.312249070894771e-05, 'dropout': 0.10633136332367564}. Best is trial 7 with value: 0.16874153912067413.\n",
      "[I 2025-04-24 04:28:27,572] Trial 11 finished with value: 0.18562098840872446 and parameters: {'lr': 0.0009386389844735213, 'weight_decay': 0.005264125510679145, 'dropout': 0.38315352585395246}. Best is trial 7 with value: 0.16874153912067413.\n",
      "[I 2025-04-24 04:28:31,001] Trial 12 finished with value: 0.16526329020659128 and parameters: {'lr': 0.0024327208300422964, 'weight_decay': 0.005267981516856891, 'dropout': 0.10928487433767066}. Best is trial 12 with value: 0.16526329020659128.\n",
      "[I 2025-04-24 04:28:34,177] Trial 13 finished with value: 0.17521349092324576 and parameters: {'lr': 0.003017234379312858, 'weight_decay': 0.001759173857432036, 'dropout': 0.10665925202156112}. Best is trial 12 with value: 0.16526329020659128.\n",
      "[I 2025-04-24 04:28:37,402] Trial 14 finished with value: 0.16963441421588263 and parameters: {'lr': 0.002662218309430017, 'weight_decay': 7.801788692747109e-05, 'dropout': 0.17715561446037437}. Best is trial 12 with value: 0.16526329020659128.\n",
      "[I 2025-04-24 04:28:40,380] Trial 15 finished with value: 0.19118628650903702 and parameters: {'lr': 0.0004071843783863944, 'weight_decay': 8.518538436893477e-05, 'dropout': 0.1682054006280686}. Best is trial 12 with value: 0.16526329020659128.\n",
      "[I 2025-04-24 04:28:43,349] Trial 16 finished with value: 0.22104120751221976 and parameters: {'lr': 0.009839765405199263, 'weight_decay': 6.703637122685737e-06, 'dropout': 0.14484377096385373}. Best is trial 12 with value: 0.16526329020659128.\n",
      "[I 2025-04-24 04:28:46,769] Trial 17 finished with value: 0.16745844235022864 and parameters: {'lr': 0.002173876946978871, 'weight_decay': 0.001591315577016138, 'dropout': 0.22753839688409153}. Best is trial 12 with value: 0.16526329020659128.\n",
      "[I 2025-04-24 04:28:50,192] Trial 18 finished with value: 0.17538109918435416 and parameters: {'lr': 0.0025960656096739245, 'weight_decay': 0.0018599100149666078, 'dropout': 0.2435767518685954}. Best is trial 12 with value: 0.16526329020659128.\n",
      "[I 2025-04-24 04:28:53,635] Trial 19 finished with value: 0.1893850862979889 and parameters: {'lr': 0.0004292487029812241, 'weight_decay': 0.0015740833579102583, 'dropout': 0.23153169379259134}. Best is trial 12 with value: 0.16526329020659128.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.0024327208300422964, 'weight_decay': 0.005267981516856891, 'dropout': 0.10928487433767066}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation function with detailed output\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    # print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    # print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    # print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function for frozen encoder\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    log_dir = f\"runs-lstm-transfer-fullbackprop/fullbackprop_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        # if val_auc > best_val_auc:\n",
    "        #     best_val_auc = val_auc\n",
    "        #     torch.save(model.state_dict(), 'best_model_frozen.pt')\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_full_backprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# # Load the best pretrained model and fine-tune\n",
    "# def finetune_with_frozen_encoder(pretrained_model_path, train_loader, val_loader, hidden_dim, num_layers, dropout):\n",
    "#     model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "#     model.load_state_dict(torch.load(pretrained_model_path))\n",
    "#     # freeze_encoder(model)\n",
    "\n",
    "#     best_auc = train_finetune_model(\n",
    "#         model=model,\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         num_epochs=10,\n",
    "#         lr=1e-3,\n",
    "#         weight_decay=1e-4\n",
    "#     )\n",
    "\n",
    "#     model.load_state_dict(torch.load('best_model_frozen.pt'))\n",
    "#     evaluate_model(model, val_loader, nn.BCELoss())\n",
    "\n",
    "#     return model, best_auc\n",
    "\n",
    "# model, best_auc = finetune_with_frozen_encoder(\n",
    "#     pretrained_model_path='best_model-lstm.pt',\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     hidden_dim=47,  # or from Optuna\n",
    "#     num_layers=2,\n",
    "#     dropout=0.3\n",
    "# )\n",
    "\n",
    "\n",
    "# lstm_best_param = {'hidden_dim': 74,\n",
    "#  'num_layers': 3,\n",
    "#  'dropout': 0.3037059572844035,\n",
    "#  'lr': 0.00774103421243492,\n",
    "#  'weight_decay': 2.4221276513292614e-05}\n",
    "\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=dropout)\n",
    "    model.load_state_dict(torch.load('best_model_lstm_1.pt'))\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_fullbackprop_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5114, AUC: 0.7488\n",
      "Sensitivity: 1.0000, Specificity: 0.0851\n",
      "Confusion Matrix:\n",
      "[[ 4 43]\n",
      " [ 0 41]]\n",
      "Epoch [1/19] - Train Loss: 0.6959, Val Loss: 334.0241, Val Acc: 0.5114, Val AUC: 0.7488\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5455, AUC: 0.7717\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Epoch [2/19] - Train Loss: 0.6860, Val Loss: 334.0180, Val Acc: 0.5455, Val AUC: 0.7717\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7753\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [3/19] - Train Loss: 0.6809, Val Loss: 334.0131, Val Acc: 0.5341, Val AUC: 0.7753\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5455, AUC: 0.7852\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Epoch [4/19] - Train Loss: 0.6719, Val Loss: 334.0061, Val Acc: 0.5455, Val AUC: 0.7852\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5909, AUC: 0.7878\n",
      "Sensitivity: 0.1951, Specificity: 0.9362\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [33  8]]\n",
      "Epoch [5/19] - Train Loss: 0.6614, Val Loss: 333.9834, Val Acc: 0.5909, Val AUC: 0.7878\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7386, AUC: 0.7763\n",
      "Sensitivity: 0.7805, Specificity: 0.7021\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [ 9 32]]\n",
      "Epoch [6/19] - Train Loss: 0.5848, Val Loss: 333.9319, Val Acc: 0.7386, Val AUC: 0.7763\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6705, AUC: 0.8137\n",
      "Sensitivity: 0.8537, Specificity: 0.5106\n",
      "Confusion Matrix:\n",
      "[[24 23]\n",
      " [ 6 35]]\n",
      "Epoch [7/19] - Train Loss: 0.6642, Val Loss: 333.9760, Val Acc: 0.6705, Val AUC: 0.8137\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6932, AUC: 0.8070\n",
      "Sensitivity: 0.5122, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [20 21]]\n",
      "Epoch [8/19] - Train Loss: 0.6302, Val Loss: 333.9723, Val Acc: 0.6932, Val AUC: 0.8070\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6250, AUC: 0.7717\n",
      "Sensitivity: 0.2683, Specificity: 0.9362\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [30 11]]\n",
      "Epoch [9/19] - Train Loss: 0.6135, Val Loss: 333.9746, Val Acc: 0.6250, Val AUC: 0.7717\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7955, AUC: 0.8355\n",
      "Sensitivity: 0.8293, Specificity: 0.7660\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 7 34]]\n",
      "Epoch [10/19] - Train Loss: 0.5618, Val Loss: 333.8624, Val Acc: 0.7955, Val AUC: 0.8355\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7727, AUC: 0.8085\n",
      "Sensitivity: 0.7561, Specificity: 0.7872\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [10 31]]\n",
      "Epoch [11/19] - Train Loss: 0.5034, Val Loss: 333.8852, Val Acc: 0.7727, Val AUC: 0.8085\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7727, AUC: 0.8417\n",
      "Sensitivity: 0.6829, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [13 28]]\n",
      "Epoch [12/19] - Train Loss: 0.5639, Val Loss: 333.9192, Val Acc: 0.7727, Val AUC: 0.8417\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8182, AUC: 0.8588\n",
      "Sensitivity: 0.8780, Specificity: 0.7660\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 5 36]]\n",
      "Epoch [13/19] - Train Loss: 0.5427, Val Loss: 333.8940, Val Acc: 0.8182, Val AUC: 0.8588\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7841, AUC: 0.8396\n",
      "Sensitivity: 0.7805, Specificity: 0.7872\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 9 32]]\n",
      "Epoch [14/19] - Train Loss: 0.4756, Val Loss: 333.8409, Val Acc: 0.7841, Val AUC: 0.8396\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7386, AUC: 0.8832\n",
      "Sensitivity: 1.0000, Specificity: 0.5106\n",
      "Confusion Matrix:\n",
      "[[24 23]\n",
      " [ 0 41]]\n",
      "Epoch [15/19] - Train Loss: 0.5123, Val Loss: 333.8853, Val Acc: 0.7386, Val AUC: 0.8832\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8182, AUC: 0.8324\n",
      "Sensitivity: 0.8049, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 8 33]]\n",
      "Epoch [16/19] - Train Loss: 0.5620, Val Loss: 333.8652, Val Acc: 0.8182, Val AUC: 0.8324\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8295, AUC: 0.8755\n",
      "Sensitivity: 0.9512, Specificity: 0.7234\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 2 39]]\n",
      "Epoch [17/19] - Train Loss: 0.4533, Val Loss: 333.7983, Val Acc: 0.8295, Val AUC: 0.8755\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7727, AUC: 0.8101\n",
      "Sensitivity: 0.6585, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [14 27]]\n",
      "Epoch [18/19] - Train Loss: 0.4680, Val Loss: 333.8628, Val Acc: 0.7727, Val AUC: 0.8101\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6818, AUC: 0.8168\n",
      "Sensitivity: 0.3902, Specificity: 0.9362\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [25 16]]\n",
      "Epoch [19/19] - Train Loss: 0.5310, Val Loss: 333.9391, Val Acc: 0.6818, Val AUC: 0.8168\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7500, AUC: 0.8765\n",
      "Sensitivity: 0.5122, Specificity: 0.9574\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [20 21]]\n",
      "Test Loss: 333.8678, Test Accuracy: 0.7500, Test AUC: 0.8765\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 1000.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Best hyperparameters: {'lr': 0.008986542560528932, 'weight_decay': 2.3033044758439348e-06, 'dropout': 0.17164705350229123}\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=113, num_layers=1, dropout=lstm_fullbackprop_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=19, lr=lstm_fullbackprop_best_param['lr'],\n",
    "                      weight_decay=lstm_fullbackprop_best_param['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:28:58,305] A new study created in memory with name: no-name-9c2dfa54-d3ad-497f-b045-fa488f394ca4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:01,328] Trial 0 finished with value: 0.4065636694431305 and parameters: {'lr': 2.941883162379877e-05, 'weight_decay': 2.38401413349995e-06, 'dropout': 0.3611287904108649}. Best is trial 0 with value: 0.4065636694431305.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:04,326] Trial 1 finished with value: 0.3861815929412842 and parameters: {'lr': 3.126108720031123e-05, 'weight_decay': 5.105532045094588e-05, 'dropout': 0.1801349752297332}. Best is trial 1 with value: 0.3861815929412842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:07,303] Trial 2 finished with value: 0.500784158706665 and parameters: {'lr': 1.6164721140170504e-05, 'weight_decay': 2.1461322317349832e-05, 'dropout': 0.38477409144990926}. Best is trial 1 with value: 0.3861815929412842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:10,314] Trial 3 finished with value: 0.24605623881022134 and parameters: {'lr': 0.00018443347660790368, 'weight_decay': 7.977007061219181e-05, 'dropout': 0.14230523643713197}. Best is trial 3 with value: 0.24605623881022134.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:13,260] Trial 4 finished with value: 0.22883491714795431 and parameters: {'lr': 0.0061099446096271135, 'weight_decay': 3.0135271691678703e-06, 'dropout': 0.1787140099369593}. Best is trial 4 with value: 0.22883491714795431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:15,955] Trial 5 finished with value: 0.3525185485680898 and parameters: {'lr': 4.590244271559067e-05, 'weight_decay': 0.0002353833772507725, 'dropout': 0.28435516637630487}. Best is trial 4 with value: 0.22883491714795431.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:18,623] Trial 6 finished with value: 0.19085046152273813 and parameters: {'lr': 0.00351351384465214, 'weight_decay': 8.13270383745535e-06, 'dropout': 0.3785117166466354}. Best is trial 6 with value: 0.19085046152273813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:21,604] Trial 7 finished with value: 0.24080145359039307 and parameters: {'lr': 0.00024071030090479776, 'weight_decay': 8.810981582276138e-05, 'dropout': 0.4711885180352632}. Best is trial 6 with value: 0.19085046152273813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:24,576] Trial 8 finished with value: 0.22407925128936768 and parameters: {'lr': 0.0004280416782465189, 'weight_decay': 0.0014945383954607639, 'dropout': 0.2034292284734714}. Best is trial 6 with value: 0.19085046152273813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:27,480] Trial 9 finished with value: 0.23370718955993652 and parameters: {'lr': 0.0002617723538110202, 'weight_decay': 0.0020276839400153617, 'dropout': 0.16386430044255978}. Best is trial 6 with value: 0.19085046152273813.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:30,120] Trial 10 finished with value: 0.16953222950299582 and parameters: {'lr': 0.009461722123963946, 'weight_decay': 1.0021431195753282e-05, 'dropout': 0.4873168440741515}. Best is trial 10 with value: 0.16953222950299582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:32,798] Trial 11 finished with value: 0.2082110991080602 and parameters: {'lr': 0.009676634594786132, 'weight_decay': 8.781548411795359e-06, 'dropout': 0.4997159132364129}. Best is trial 10 with value: 0.16953222950299582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:35,739] Trial 12 finished with value: 0.20111846923828125 and parameters: {'lr': 0.002751981526817789, 'weight_decay': 1.1332426344482245e-06, 'dropout': 0.42017825682740967}. Best is trial 10 with value: 0.16953222950299582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:38,515] Trial 13 finished with value: 0.1974184438586235 and parameters: {'lr': 0.001850429758762826, 'weight_decay': 9.77599382092076e-06, 'dropout': 0.3088205165236688}. Best is trial 10 with value: 0.16953222950299582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:41,274] Trial 14 finished with value: 0.2020294889807701 and parameters: {'lr': 0.0012270690981436006, 'weight_decay': 0.00028879178556120977, 'dropout': 0.44621752608957704}. Best is trial 10 with value: 0.16953222950299582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:43,934] Trial 15 finished with value: 0.175786222020785 and parameters: {'lr': 0.004227442002956507, 'weight_decay': 1.3434533703872294e-05, 'dropout': 0.3275914305341107}. Best is trial 10 with value: 0.16953222950299582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:46,917] Trial 16 finished with value: 0.20844531059265137 and parameters: {'lr': 0.0007753109609233819, 'weight_decay': 2.4801588546478014e-05, 'dropout': 0.2613837496622038}. Best is trial 10 with value: 0.16953222950299582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:49,883] Trial 17 finished with value: 0.18984831869602203 and parameters: {'lr': 0.004973158178298022, 'weight_decay': 0.008157577464170413, 'dropout': 0.32779460993818715}. Best is trial 10 with value: 0.16953222950299582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:52,553] Trial 18 finished with value: 0.18944088369607925 and parameters: {'lr': 0.00919847950492456, 'weight_decay': 3.150577490298991e-06, 'dropout': 0.10166544516291334}. Best is trial 10 with value: 0.16953222950299582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:55,551] Trial 19 finished with value: 0.27923723061879474 and parameters: {'lr': 9.828969224030686e-05, 'weight_decay': 0.00029707984788363543, 'dropout': 0.23922380986769484}. Best is trial 10 with value: 0.16953222950299582.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.009461722123963946, 'weight_decay': 1.0021431195753282e-05, 'dropout': 0.4873168440741515}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm-trans-frozen/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=dropout, max_seq_len=100)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.009461722123963946,\n",
       " 'weight_decay': 1.0021431195753282e-05,\n",
       " 'dropout': 0.4873168440741515}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_frozen_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.7021\n",
      "Epoch [1/15] - Train Loss: 0.4772, Val Loss: 0.3931, Val Acc: 0.8409, Val AUC: 0.9626\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.7447\n",
      "Epoch [2/15] - Train Loss: 0.4043, Val Loss: 0.3435, Val Acc: 0.8636, Val AUC: 0.9632\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.7872\n",
      "Epoch [3/15] - Train Loss: 0.4075, Val Loss: 0.3061, Val Acc: 0.8864, Val AUC: 0.9642\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.8085\n",
      "Epoch [4/15] - Train Loss: 0.3651, Val Loss: 0.2820, Val Acc: 0.8977, Val AUC: 0.9663\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.8723\n",
      "Epoch [5/15] - Train Loss: 0.3642, Val Loss: 0.2648, Val Acc: 0.9318, Val AUC: 0.9699\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [6/15] - Train Loss: 0.3965, Val Loss: 0.2537, Val Acc: 0.9205, Val AUC: 0.9699\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [7/15] - Train Loss: 0.3451, Val Loss: 0.2438, Val Acc: 0.9205, Val AUC: 0.9725\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [8/15] - Train Loss: 0.3375, Val Loss: 0.2386, Val Acc: 0.9205, Val AUC: 0.9746\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [9/15] - Train Loss: 0.3137, Val Loss: 0.2340, Val Acc: 0.9205, Val AUC: 0.9766\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [10/15] - Train Loss: 0.3937, Val Loss: 0.2319, Val Acc: 0.9205, Val AUC: 0.9772\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [11/15] - Train Loss: 0.3463, Val Loss: 0.2317, Val Acc: 0.9205, Val AUC: 0.9766\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [12/15] - Train Loss: 0.3285, Val Loss: 0.2296, Val Acc: 0.9205, Val AUC: 0.9787\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [13/15] - Train Loss: 0.3378, Val Loss: 0.2318, Val Acc: 0.9205, Val AUC: 0.9787\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [14/15] - Train Loss: 0.3217, Val Loss: 0.2293, Val Acc: 0.9205, Val AUC: 0.9798\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [15/15] - Train Loss: 0.3391, Val Loss: 0.2295, Val Acc: 0.9205, Val AUC: 0.9798\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Test Loss: 0.2662, Test Accuracy: 0.9205, Test AUC: 0.9564\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=15, lr=bilstm_frozen_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:29:58,926] A new study created in memory with name: no-name-04b3c54a-f409-489f-baf3-dfce4b91a9c9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:03,229] Trial 0 finished with value: 0.23378469049930573 and parameters: {'lr': 8.615523044255915e-05, 'weight_decay': 0.00022821096329728405, 'dropout': 0.3412029242746114}. Best is trial 0 with value: 0.23378469049930573.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:06,985] Trial 1 finished with value: 0.17824952552715936 and parameters: {'lr': 0.0011969659881692508, 'weight_decay': 0.00010603636306227973, 'dropout': 0.4497486402324601}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:11,080] Trial 2 finished with value: 0.23912528405586878 and parameters: {'lr': 0.005426769169707186, 'weight_decay': 0.00038662679788534297, 'dropout': 0.1445101748746708}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:15,345] Trial 3 finished with value: 0.3023711641629537 and parameters: {'lr': 3.088386075400775e-05, 'weight_decay': 2.3850162221105293e-05, 'dropout': 0.23581283017462407}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:19,548] Trial 4 finished with value: 0.20559068024158478 and parameters: {'lr': 0.004655561696348063, 'weight_decay': 0.007318610115052156, 'dropout': 0.11527077645860935}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:23,781] Trial 5 finished with value: 0.27270273367563885 and parameters: {'lr': 4.873205312883345e-05, 'weight_decay': 1.5993323963453507e-06, 'dropout': 0.42960336548941724}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:28,158] Trial 6 finished with value: 0.2400266875823339 and parameters: {'lr': 7.71430799390238e-05, 'weight_decay': 1.3005909606757845e-05, 'dropout': 0.11254583860203389}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:32,592] Trial 7 finished with value: 0.28970739742120105 and parameters: {'lr': 3.407927976224004e-05, 'weight_decay': 0.00013862609203328427, 'dropout': 0.29196052859792454}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:37,030] Trial 8 finished with value: 0.4383583565553029 and parameters: {'lr': 1.2578922841444797e-05, 'weight_decay': 0.005074285925511259, 'dropout': 0.24786368973393702}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:41,444] Trial 9 finished with value: 0.1783747375011444 and parameters: {'lr': 0.0007727205522040633, 'weight_decay': 0.007499098875076084, 'dropout': 0.3950428802163941}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:45,803] Trial 10 finished with value: 0.18805007139841715 and parameters: {'lr': 0.0007673961898928216, 'weight_decay': 0.0011355646224629784, 'dropout': 0.489438777802107}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:50,042] Trial 11 finished with value: 0.2017118309934934 and parameters: {'lr': 0.0007872135982195883, 'weight_decay': 0.001685862911761774, 'dropout': 0.40805796457096666}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:54,414] Trial 12 finished with value: 0.1941604862610499 and parameters: {'lr': 0.0012409435439425388, 'weight_decay': 2.0555232817106267e-05, 'dropout': 0.3977355938633499}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:30:58,823] Trial 13 finished with value: 0.1881937012076378 and parameters: {'lr': 0.0002537034655498442, 'weight_decay': 1.0621188909898993e-06, 'dropout': 0.49882146221253587}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:03,198] Trial 14 finished with value: 0.18106545756260553 and parameters: {'lr': 0.0021601765452660625, 'weight_decay': 5.66985791400219e-06, 'dropout': 0.36257139577286307}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:07,576] Trial 15 finished with value: 0.20993159773449102 and parameters: {'lr': 0.00027875811413595914, 'weight_decay': 5.8417453271551356e-05, 'dropout': 0.44890654264453983}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:11,930] Trial 16 finished with value: 0.20863357931375504 and parameters: {'lr': 0.002360935019546193, 'weight_decay': 0.0012081221024121214, 'dropout': 0.34383688945289026}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:16,253] Trial 17 finished with value: 0.2128475954135259 and parameters: {'lr': 0.0003913847237533904, 'weight_decay': 0.0005220375302147344, 'dropout': 0.45883766477090354}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:20,195] Trial 18 finished with value: 0.22092175483703613 and parameters: {'lr': 0.00016737003606181311, 'weight_decay': 7.161407008628216e-05, 'dropout': 0.3789519192774441}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:23,969] Trial 19 finished with value: 0.3165133943160375 and parameters: {'lr': 0.009993610005600825, 'weight_decay': 0.003209783392656217, 'dropout': 0.30817548692479724}. Best is trial 1 with value: 0.17824952552715936.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.0011969659881692508, 'weight_decay': 0.00010603636306227973, 'dropout': 0.4497486402324601}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm-trans-fullbackprop/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=dropout, max_seq_len=100)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=20, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8298\n",
      "Epoch [1/10] - Train Loss: 0.4264, Val Loss: 0.2458, Val Acc: 0.8977, Val AUC: 0.9741\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Epoch [2/10] - Train Loss: 0.3097, Val Loss: 0.2137, Val Acc: 0.9091, Val AUC: 0.9808\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [3/10] - Train Loss: 0.2861, Val Loss: 0.2156, Val Acc: 0.9205, Val AUC: 0.9850\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [4/10] - Train Loss: 0.2362, Val Loss: 0.2070, Val Acc: 0.9205, Val AUC: 0.9850\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Epoch [5/10] - Train Loss: 0.1943, Val Loss: 0.1825, Val Acc: 0.9205, Val AUC: 0.9844\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8723\n",
      "Epoch [6/10] - Train Loss: 0.1935, Val Loss: 0.1935, Val Acc: 0.9205, Val AUC: 0.9850\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8511\n",
      "Epoch [7/10] - Train Loss: 0.1639, Val Loss: 0.2000, Val Acc: 0.9091, Val AUC: 0.9865\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8511\n",
      "Epoch [8/10] - Train Loss: 0.2203, Val Loss: 0.2358, Val Acc: 0.9091, Val AUC: 0.9855\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.8511\n",
      "Epoch [9/10] - Train Loss: 0.1893, Val Loss: 0.2644, Val Acc: 0.9091, Val AUC: 0.9803\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.8723\n",
      "Epoch [10/10] - Train Loss: 0.1378, Val Loss: 0.1937, Val Acc: 0.8977, Val AUC: 0.9803\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.9362\n",
      "Test Loss: 0.1992, Test Accuracy: 0.9091, Test AUC: 0.9782\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_fullbackprop_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "# freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10, lr=bilstm_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:26,478] A new study created in memory with name: no-name-acbae4c0-19d9-4dac-9d54-b8a4a4a888b3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:29,545] Trial 0 finished with value: 0.6917267044385275 and parameters: {'lr': 0.00011184999188973175, 'weight_decay': 0.0038911553822280824, 'dropout': 0.41788763209743973}. Best is trial 0 with value: 0.6917267044385275.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:32,556] Trial 1 finished with value: 0.6911607185999552 and parameters: {'lr': 0.004861479511871579, 'weight_decay': 0.00678242370684255, 'dropout': 0.4708297728301043}. Best is trial 1 with value: 0.6911607185999552.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:35,250] Trial 2 finished with value: 0.6912485361099243 and parameters: {'lr': 0.0005097156284362277, 'weight_decay': 8.450564349631409e-06, 'dropout': 0.255266892392929}. Best is trial 1 with value: 0.6911607185999552.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:37,892] Trial 3 finished with value: 0.6911595463752747 and parameters: {'lr': 0.0009077722990548418, 'weight_decay': 0.0011087842869043228, 'dropout': 0.4810015855433156}. Best is trial 3 with value: 0.6911595463752747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:40,624] Trial 4 finished with value: 0.6913943886756897 and parameters: {'lr': 2.5768483709886867e-05, 'weight_decay': 1.1305101502241685e-05, 'dropout': 0.25203923553392804}. Best is trial 3 with value: 0.6911595463752747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:43,545] Trial 5 finished with value: 0.691409687201182 and parameters: {'lr': 1.28035852107187e-05, 'weight_decay': 7.913649178750308e-05, 'dropout': 0.1651817524035159}. Best is trial 3 with value: 0.6911595463752747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:46,322] Trial 6 finished with value: 0.6912764112154642 and parameters: {'lr': 0.0003934109350103694, 'weight_decay': 0.0007145339857960306, 'dropout': 0.4004477495124966}. Best is trial 3 with value: 0.6911595463752747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:49,061] Trial 7 finished with value: 0.6911998987197876 and parameters: {'lr': 0.0014323506781238485, 'weight_decay': 0.0005324635664177779, 'dropout': 0.48280453302207715}. Best is trial 3 with value: 0.6911595463752747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:51,877] Trial 8 finished with value: 0.691315491994222 and parameters: {'lr': 8.947693305634839e-05, 'weight_decay': 0.0009162438420455992, 'dropout': 0.454694150757465}. Best is trial 3 with value: 0.6911595463752747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:54,525] Trial 9 finished with value: 0.6917888124783834 and parameters: {'lr': 8.417958782901998e-05, 'weight_decay': 0.006587482170545084, 'dropout': 0.3980121437465066}. Best is trial 3 with value: 0.6911595463752747.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:57,185] Trial 10 finished with value: 0.6911254525184631 and parameters: {'lr': 0.006024641684636313, 'weight_decay': 1.6926188281757663e-06, 'dropout': 0.3241591467066894}. Best is trial 10 with value: 0.6911254525184631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:31:59,827] Trial 11 finished with value: 0.6911862889925638 and parameters: {'lr': 0.006072564974544911, 'weight_decay': 1.7969891680398377e-06, 'dropout': 0.34549278972550357}. Best is trial 10 with value: 0.6911254525184631.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.006024641684636313, 'weight_decay': 1.6926188281757663e-06, 'dropout': 0.3241591467066894}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM with Attention\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze LSTM encoder\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-att-trans/LSTMAttTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_att_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Load compatible weights\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=dropout)\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=12)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_att_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.006024641684636313,\n",
       " 'weight_decay': 1.6926188281757663e-06,\n",
       " 'dropout': 0.3241591467066894}"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_att_frozen_best_parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/20] - Train Loss: 0.6888, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6492\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/20] - Train Loss: 0.6907, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6969\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/20] - Train Loss: 0.6918, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.7364\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/20] - Train Loss: 0.6859, Val Loss: 0.6906, Val Acc: 0.5341, Val AUC: 0.7468\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/20] - Train Loss: 0.6915, Val Loss: 0.6906, Val Acc: 0.5341, Val AUC: 0.7551\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/20] - Train Loss: 0.6890, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.7571\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/20] - Train Loss: 0.6906, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.7613\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [8/20] - Train Loss: 0.6904, Val Loss: 0.6901, Val Acc: 0.5341, Val AUC: 0.7675\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [9/20] - Train Loss: 0.6905, Val Loss: 0.6898, Val Acc: 0.5341, Val AUC: 0.7696\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [10/20] - Train Loss: 0.6921, Val Loss: 0.6896, Val Acc: 0.5341, Val AUC: 0.7737\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [11/20] - Train Loss: 0.6890, Val Loss: 0.6895, Val Acc: 0.5341, Val AUC: 0.7706\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [12/20] - Train Loss: 0.6903, Val Loss: 0.6893, Val Acc: 0.5341, Val AUC: 0.7696\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [13/20] - Train Loss: 0.6887, Val Loss: 0.6892, Val Acc: 0.5341, Val AUC: 0.7701\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [14/20] - Train Loss: 0.6868, Val Loss: 0.6890, Val Acc: 0.5341, Val AUC: 0.7706\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [15/20] - Train Loss: 0.6912, Val Loss: 0.6888, Val Acc: 0.5341, Val AUC: 0.7706\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [16/20] - Train Loss: 0.6892, Val Loss: 0.6886, Val Acc: 0.5341, Val AUC: 0.7680\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [17/20] - Train Loss: 0.6890, Val Loss: 0.6885, Val Acc: 0.5341, Val AUC: 0.7686\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [18/20] - Train Loss: 0.6904, Val Loss: 0.6883, Val Acc: 0.5341, Val AUC: 0.7686\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [19/20] - Train Loss: 0.6907, Val Loss: 0.6882, Val Acc: 0.5341, Val AUC: 0.7639\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [20/20] - Train Loss: 0.6871, Val Loss: 0.6881, Val Acc: 0.5341, Val AUC: 0.7665\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Test Loss: 0.6915, Test Accuracy: 0.5341, Test AUC: 0.5895\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=lstm_att_frozen_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_att_frozen_best_parameters['lr'],\n",
    "                      weight_decay=lstm_att_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full back prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:04,513] A new study created in memory with name: no-name-8a0b6ec6-5696-4348-bf32-f03b71f9de24\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:07,818] Trial 0 finished with value: 0.6908225019772848 and parameters: {'lr': 0.000182865466062125, 'weight_decay': 4.974266697193392e-06, 'dropout': 0.34789681227059543}. Best is trial 0 with value: 0.6908225019772848.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:11,071] Trial 1 finished with value: 0.6905583143234253 and parameters: {'lr': 0.0004116223425756722, 'weight_decay': 4.31537898394225e-05, 'dropout': 0.41494789277459365}. Best is trial 1 with value: 0.6905583143234253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:14,006] Trial 2 finished with value: 0.6911760767300924 and parameters: {'lr': 0.0016417446641922148, 'weight_decay': 0.0023633989033395733, 'dropout': 0.19768364263138416}. Best is trial 1 with value: 0.6905583143234253.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:17,260] Trial 3 finished with value: 0.5164255499839783 and parameters: {'lr': 0.0012509049122950595, 'weight_decay': 0.00025609867445839827, 'dropout': 0.11232642280180408}. Best is trial 3 with value: 0.5164255499839783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:20,511] Trial 4 finished with value: 0.6912095546722412 and parameters: {'lr': 7.790695287025103e-05, 'weight_decay': 0.00011807038482819234, 'dropout': 0.2299448149722598}. Best is trial 3 with value: 0.5164255499839783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:23,765] Trial 5 finished with value: 0.6923404137293497 and parameters: {'lr': 1.0658440743020017e-05, 'weight_decay': 3.0303974358827533e-05, 'dropout': 0.3936172313346964}. Best is trial 3 with value: 0.5164255499839783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:27,001] Trial 6 finished with value: 0.6911378304163615 and parameters: {'lr': 0.00013869682975592993, 'weight_decay': 3.2266813534300214e-05, 'dropout': 0.3092900258782341}. Best is trial 3 with value: 0.5164255499839783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:30,241] Trial 7 finished with value: 0.5964996814727783 and parameters: {'lr': 0.0005761586989277616, 'weight_decay': 5.754949211674221e-05, 'dropout': 0.19083676892785928}. Best is trial 3 with value: 0.5164255499839783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:33,489] Trial 8 finished with value: 0.6913819313049316 and parameters: {'lr': 4.129954465206679e-05, 'weight_decay': 9.230017175830743e-06, 'dropout': 0.2949128026454127}. Best is trial 3 with value: 0.5164255499839783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:36,567] Trial 9 finished with value: 0.6924185951550802 and parameters: {'lr': 1.3458689345654378e-05, 'weight_decay': 0.0006373682621351586, 'dropout': 0.3956758558291896}. Best is trial 3 with value: 0.5164255499839783.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:39,821] Trial 10 finished with value: 0.5124015212059021 and parameters: {'lr': 0.005281551130796024, 'weight_decay': 1.3229138318592569e-06, 'dropout': 0.10036831742448268}. Best is trial 10 with value: 0.5124015212059021.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:42,810] Trial 11 finished with value: 0.5272869765758514 and parameters: {'lr': 0.009106614309165786, 'weight_decay': 1.0280392857020997e-06, 'dropout': 0.1067146921386465}. Best is trial 10 with value: 0.5124015212059021.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.005281551130796024, 'weight_decay': 1.3229138318592569e-06, 'dropout': 0.10036831742448268}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM with Attention\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze LSTM encoder\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-att-trans-fullback/LSTMAttTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_att_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Load compatible weights\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=dropout)\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=12)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_att_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [38  3]]\n",
      "Sensitivity: 0.0732, Specificity: 0.9787\n",
      "Epoch [1/20] - Train Loss: 0.6892, Val Loss: 0.6827, Val Acc: 0.5568, Val AUC: 0.7769\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.7234\n",
      "Epoch [2/20] - Train Loss: 0.6358, Val Loss: 0.5863, Val Acc: 0.7614, Val AUC: 0.7784\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.8723\n",
      "Epoch [3/20] - Train Loss: 0.5571, Val Loss: 0.5662, Val Acc: 0.7500, Val AUC: 0.8116\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.8511\n",
      "Epoch [4/20] - Train Loss: 0.5068, Val Loss: 0.5969, Val Acc: 0.7159, Val AUC: 0.8158\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.7021\n",
      "Epoch [5/20] - Train Loss: 0.5182, Val Loss: 0.5017, Val Acc: 0.7614, Val AUC: 0.8588\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.7447\n",
      "Epoch [6/20] - Train Loss: 0.4663, Val Loss: 0.4855, Val Acc: 0.8068, Val AUC: 0.8635\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [14 27]]\n",
      "Sensitivity: 0.6585, Specificity: 0.8511\n",
      "Epoch [7/20] - Train Loss: 0.4913, Val Loss: 0.5406, Val Acc: 0.7614, Val AUC: 0.8298\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.7872\n",
      "Epoch [8/20] - Train Loss: 0.4478, Val Loss: 0.5216, Val Acc: 0.8182, Val AUC: 0.8402\n",
      "\n",
      "Confusion Matrix:\n",
      "[[24 23]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.5106\n",
      "Epoch [9/20] - Train Loss: 0.4396, Val Loss: 0.6969, Val Acc: 0.7045, Val AUC: 0.8780\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [19 22]]\n",
      "Sensitivity: 0.5366, Specificity: 0.9149\n",
      "Epoch [10/20] - Train Loss: 0.5601, Val Loss: 0.5739, Val Acc: 0.7386, Val AUC: 0.8225\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.7872\n",
      "Epoch [11/20] - Train Loss: 0.5078, Val Loss: 0.4945, Val Acc: 0.8068, Val AUC: 0.8480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.7447\n",
      "Epoch [12/20] - Train Loss: 0.4386, Val Loss: 0.5102, Val Acc: 0.8068, Val AUC: 0.8516\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.7234\n",
      "Epoch [13/20] - Train Loss: 0.5908, Val Loss: 0.5013, Val Acc: 0.7955, Val AUC: 0.8609\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [20 21]]\n",
      "Sensitivity: 0.5122, Specificity: 0.8936\n",
      "Epoch [14/20] - Train Loss: 0.5196, Val Loss: 0.5722, Val Acc: 0.7159, Val AUC: 0.8516\n",
      "\n",
      "Confusion Matrix:\n",
      "[[31 16]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.6596\n",
      "Epoch [15/20] - Train Loss: 0.4855, Val Loss: 0.5620, Val Acc: 0.7841, Val AUC: 0.7929\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.7872\n",
      "Epoch [16/20] - Train Loss: 0.5459, Val Loss: 0.4600, Val Acc: 0.8068, Val AUC: 0.8864\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.7872\n",
      "Epoch [17/20] - Train Loss: 0.4699, Val Loss: 0.4918, Val Acc: 0.7955, Val AUC: 0.8583\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8298\n",
      "Epoch [18/20] - Train Loss: 0.4454, Val Loss: 0.4988, Val Acc: 0.7727, Val AUC: 0.8578\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8085\n",
      "Epoch [19/20] - Train Loss: 0.4240, Val Loss: 0.4871, Val Acc: 0.8182, Val AUC: 0.9050\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.8723\n",
      "Epoch [20/20] - Train Loss: 0.3517, Val Loss: 0.5158, Val Acc: 0.8068, Val AUC: 0.8755\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.9149\n",
      "Test Loss: 0.4485, Test Accuracy: 0.8182, Test AUC: 0.8869\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=lstm_att_fullbackprop_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_att_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=lstm_att_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:48,546] A new study created in memory with name: no-name-d22904c6-f4df-420c-8655-1181e2336f37\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:51,715] Trial 0 finished with value: 0.6940928300221761 and parameters: {'lr': 1.4277446542176254e-05, 'weight_decay': 4.9744119282974765e-06, 'dropout': 0.18731979304759094}. Best is trial 0 with value: 0.6940928300221761.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:54,710] Trial 1 finished with value: 0.6912325421969095 and parameters: {'lr': 0.0008821915956710311, 'weight_decay': 5.363204870780238e-05, 'dropout': 0.33330071652379273}. Best is trial 1 with value: 0.6912325421969095.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:32:57,752] Trial 2 finished with value: 0.6936431725819906 and parameters: {'lr': 1.66841372611692e-05, 'weight_decay': 0.00030869325662258377, 'dropout': 0.36127736860542614}. Best is trial 1 with value: 0.6912325421969095.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:00,816] Trial 3 finished with value: 0.6911749839782715 and parameters: {'lr': 0.003800627072322116, 'weight_decay': 1.6997696246153389e-06, 'dropout': 0.48333414177184764}. Best is trial 3 with value: 0.6911749839782715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:03,937] Trial 4 finished with value: 0.6924215157826742 and parameters: {'lr': 0.0002036811176954758, 'weight_decay': 2.474573715730769e-06, 'dropout': 0.27753616989403296}. Best is trial 3 with value: 0.6911749839782715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:07,067] Trial 5 finished with value: 0.693130612373352 and parameters: {'lr': 0.00011287272712205263, 'weight_decay': 0.00010065075261276227, 'dropout': 0.3896568491560132}. Best is trial 3 with value: 0.6911749839782715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:10,153] Trial 6 finished with value: 0.691195527712504 and parameters: {'lr': 0.009703685578513685, 'weight_decay': 0.0040516788379557885, 'dropout': 0.247162515022296}. Best is trial 3 with value: 0.6911749839782715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:13,233] Trial 7 finished with value: 0.6912040710449219 and parameters: {'lr': 0.0021761426138517436, 'weight_decay': 0.004899520771220201, 'dropout': 0.19057829552463593}. Best is trial 3 with value: 0.6911749839782715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:16,126] Trial 8 finished with value: 0.695331335067749 and parameters: {'lr': 1.1338078122168022e-05, 'weight_decay': 0.00031895719359072085, 'dropout': 0.3553803304898283}. Best is trial 3 with value: 0.6911749839782715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:19,014] Trial 9 finished with value: 0.6935229698816935 and parameters: {'lr': 1.3723214937541697e-05, 'weight_decay': 7.20999550829954e-06, 'dropout': 0.4110933786412554}. Best is trial 3 with value: 0.6911749839782715.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:22,237] Trial 10 finished with value: 0.6911672949790955 and parameters: {'lr': 0.005159249447121794, 'weight_decay': 1.0062004845720793e-06, 'dropout': 0.4939029340688291}. Best is trial 10 with value: 0.6911672949790955.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:25,085] Trial 11 finished with value: 0.6911554336547852 and parameters: {'lr': 0.009682362675250931, 'weight_decay': 1.2818899915903608e-06, 'dropout': 0.49883470316437034}. Best is trial 11 with value: 0.6911554336547852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:28,131] Trial 12 finished with value: 0.6912084023157755 and parameters: {'lr': 0.007984447260198422, 'weight_decay': 1.0029463048164443e-06, 'dropout': 0.4942138487922896}. Best is trial 11 with value: 0.6911554336547852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:31,262] Trial 13 finished with value: 0.6911875605583191 and parameters: {'lr': 0.0009260627869796276, 'weight_decay': 1.5170527193122047e-05, 'dropout': 0.44329525345469223}. Best is trial 11 with value: 0.6911554336547852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:34,693] Trial 14 finished with value: 0.6911931236584982 and parameters: {'lr': 0.002225137500781504, 'weight_decay': 1.8568374511256008e-05, 'dropout': 0.10469829525303334}. Best is trial 11 with value: 0.6911554336547852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:38,163] Trial 15 finished with value: 0.6913549701372782 and parameters: {'lr': 0.0008049095679071178, 'weight_decay': 1.0566589613609426e-06, 'dropout': 0.4423159798696523}. Best is trial 11 with value: 0.6911554336547852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:41,584] Trial 16 finished with value: 0.6911525130271912 and parameters: {'lr': 0.004479770632692179, 'weight_decay': 4.82148108042584e-06, 'dropout': 0.46421214766313257}. Best is trial 16 with value: 0.6911525130271912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:44,807] Trial 17 finished with value: 0.6912390192349752 and parameters: {'lr': 0.0023134915410675185, 'weight_decay': 5.6473407929004035e-06, 'dropout': 0.44580475443835277}. Best is trial 16 with value: 0.6911525130271912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:47,986] Trial 18 finished with value: 0.6940745711326599 and parameters: {'lr': 5.093554528679853e-05, 'weight_decay': 1.625427865020598e-05, 'dropout': 0.4169836464860924}. Best is trial 16 with value: 0.6911525130271912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:33:50,938] Trial 19 finished with value: 0.6919856468836466 and parameters: {'lr': 0.0004654158530598282, 'weight_decay': 4.9687566827078845e-05, 'dropout': 0.4572653871658498}. Best is trial 16 with value: 0.6911525130271912.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.004479770632692179, 'weight_decay': 4.82148108042584e-06, 'dropout': 0.46421214766313257}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm_attn-trans-frozen/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_attn_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=dropout)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/50] - Train Loss: 0.6948, Val Loss: 0.6928, Val Acc: 0.5341, Val AUC: 0.3978\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/50] - Train Loss: 0.6922, Val Loss: 0.6924, Val Acc: 0.5341, Val AUC: 0.4030\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/50] - Train Loss: 0.6919, Val Loss: 0.6918, Val Acc: 0.5341, Val AUC: 0.3985\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/50] - Train Loss: 0.6908, Val Loss: 0.6914, Val Acc: 0.5341, Val AUC: 0.4076\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/50] - Train Loss: 0.6911, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4203\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/50] - Train Loss: 0.6928, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4219\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/50] - Train Loss: 0.6917, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [8/50] - Train Loss: 0.6907, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4323\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [9/50] - Train Loss: 0.6904, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4354\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [10/50] - Train Loss: 0.6983, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4421\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [11/50] - Train Loss: 0.6928, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.4577\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [12/50] - Train Loss: 0.6919, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4582\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [13/50] - Train Loss: 0.6903, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4494\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [14/50] - Train Loss: 0.6898, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.4484\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [15/50] - Train Loss: 0.6862, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.4510\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [16/50] - Train Loss: 0.6933, Val Loss: 0.6915, Val Acc: 0.5341, Val AUC: 0.4432\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [17/50] - Train Loss: 0.6905, Val Loss: 0.6914, Val Acc: 0.5341, Val AUC: 0.4567\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [18/50] - Train Loss: 0.6932, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4725\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [19/50] - Train Loss: 0.6905, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.4914\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [20/50] - Train Loss: 0.6922, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5138\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [21/50] - Train Loss: 0.6870, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5332\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [22/50] - Train Loss: 0.6916, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5405\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [23/50] - Train Loss: 0.6935, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5558\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [24/50] - Train Loss: 0.6919, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5771\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [25/50] - Train Loss: 0.6909, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5828\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [26/50] - Train Loss: 0.6950, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5776\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [27/50] - Train Loss: 0.6877, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5828\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [28/50] - Train Loss: 0.6895, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5734\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [29/50] - Train Loss: 0.6891, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.5623\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [30/50] - Train Loss: 0.6923, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.5576\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [31/50] - Train Loss: 0.6906, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.5558\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [32/50] - Train Loss: 0.6975, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5599\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [33/50] - Train Loss: 0.6933, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5760\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [34/50] - Train Loss: 0.6948, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5802\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [35/50] - Train Loss: 0.6914, Val Loss: 0.6916, Val Acc: 0.5341, Val AUC: 0.5906\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [36/50] - Train Loss: 0.6916, Val Loss: 0.6917, Val Acc: 0.5341, Val AUC: 0.5898\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [37/50] - Train Loss: 0.6923, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.5807\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [38/50] - Train Loss: 0.6905, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5734\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [39/50] - Train Loss: 0.6935, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5729\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [40/50] - Train Loss: 0.6869, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5719\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [41/50] - Train Loss: 0.6942, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5656\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [42/50] - Train Loss: 0.6913, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5708\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [43/50] - Train Loss: 0.6917, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5760\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [44/50] - Train Loss: 0.6930, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5807\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [45/50] - Train Loss: 0.6918, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5880\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [46/50] - Train Loss: 0.6914, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.5937\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [47/50] - Train Loss: 0.6856, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5957\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [48/50] - Train Loss: 0.6955, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5859\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [49/50] - Train Loss: 0.6851, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5828\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [50/50] - Train Loss: 0.6900, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5771\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Test Loss: 0.6928, Test Accuracy: 0.5341, Test AUC: 0.6409\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attn-trans-frozen/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=bilstm_attn_frozen_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=50, lr=bilstm_attn_frozen_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_attn_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full back prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:02,680] A new study created in memory with name: no-name-60d70c05-c054-471c-9f48-c6c696484cbf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:05,619] Trial 0 finished with value: 0.6940780480702718 and parameters: {'lr': 0.00011284396596522826, 'weight_decay': 1.2455546982755036e-05, 'dropout': 0.2125378271991731}. Best is trial 0 with value: 0.6940780480702718.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:08,406] Trial 1 finished with value: 0.6911205053329468 and parameters: {'lr': 0.0062239713821380886, 'weight_decay': 5.3783431572485495e-05, 'dropout': 0.2351822104538519}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:11,466] Trial 2 finished with value: 0.6911559104919434 and parameters: {'lr': 0.004518114434036045, 'weight_decay': 0.0005442816772182479, 'dropout': 0.23692919385372196}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:14,607] Trial 3 finished with value: 0.6912071704864502 and parameters: {'lr': 0.0032646402383578734, 'weight_decay': 0.005126341722468404, 'dropout': 0.2401354586515675}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:18,407] Trial 4 finished with value: 0.6935127178827921 and parameters: {'lr': 4.050866321412634e-05, 'weight_decay': 0.00042416849737713784, 'dropout': 0.2728967220803761}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:21,551] Trial 5 finished with value: 0.6939639647801717 and parameters: {'lr': 9.430588914437867e-05, 'weight_decay': 0.0046104532968745315, 'dropout': 0.3778570667816542}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:24,826] Trial 6 finished with value: 0.6911957462628683 and parameters: {'lr': 0.0013800587038979087, 'weight_decay': 0.0003252949522336622, 'dropout': 0.11788565654733257}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:28,003] Trial 7 finished with value: 0.6927539308865865 and parameters: {'lr': 0.00015076051985973152, 'weight_decay': 0.004006601837293045, 'dropout': 0.36568374873703935}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:31,144] Trial 8 finished with value: 0.6911704540252686 and parameters: {'lr': 0.001976046961710006, 'weight_decay': 0.004229348435548887, 'dropout': 0.404980312477643}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:34,275] Trial 9 finished with value: 0.6912237604459127 and parameters: {'lr': 0.005621004961390011, 'weight_decay': 0.007183794377681724, 'dropout': 0.16474841416495184}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:37,493] Trial 10 finished with value: 0.6913552681605021 and parameters: {'lr': 0.000569818114747921, 'weight_decay': 4.3855306201344964e-06, 'dropout': 0.497778962327225}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:40,707] Trial 11 finished with value: 0.6912045478820801 and parameters: {'lr': 0.006232434825911092, 'weight_decay': 5.314451888839868e-05, 'dropout': 0.31407406862112625}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:43,714] Trial 12 finished with value: 0.691211481889089 and parameters: {'lr': 0.000788187985974154, 'weight_decay': 0.00010140999808667817, 'dropout': 0.18323054292485352}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:46,900] Trial 13 finished with value: 0.6911487778027853 and parameters: {'lr': 0.009134571993003108, 'weight_decay': 0.0004673171147845194, 'dropout': 0.29655951296374117}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:50,079] Trial 14 finished with value: 0.6911856929461161 and parameters: {'lr': 0.008915219894828514, 'weight_decay': 3.2172629800784146e-05, 'dropout': 0.3162090523366398}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:53,310] Trial 15 finished with value: 0.6950155893961588 and parameters: {'lr': 1.1794001753626372e-05, 'weight_decay': 2.1711842962107522e-06, 'dropout': 0.2806523922901001}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:56,586] Trial 16 finished with value: 0.69120325644811 and parameters: {'lr': 0.001977244536121798, 'weight_decay': 0.0012063203870229728, 'dropout': 0.44962225698810027}. Best is trial 1 with value: 0.6911205053329468.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:34:59,902] Trial 17 finished with value: 0.6909871697425842 and parameters: {'lr': 0.009450939169366141, 'weight_decay': 0.00012150630488982202, 'dropout': 0.1059706494378321}. Best is trial 17 with value: 0.6909871697425842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:35:03,026] Trial 18 finished with value: 0.6914139787356058 and parameters: {'lr': 0.00041260017504703454, 'weight_decay': 0.00012425055768137112, 'dropout': 0.10036366985682277}. Best is trial 17 with value: 0.6909871697425842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 04:35:06,307] Trial 19 finished with value: 0.6911962429682413 and parameters: {'lr': 0.0009771548059867659, 'weight_decay': 1.517533203174121e-05, 'dropout': 0.14299225133666754}. Best is trial 17 with value: 0.6909871697425842.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.009450939169366141, 'weight_decay': 0.00012150630488982202, 'dropout': 0.1059706494378321}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm_attn-trans-fullbackprop/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_attn_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=dropout)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/120] - Train Loss: 0.6934, Val Loss: 0.6918, Val Acc: 0.5341, Val AUC: 0.6544\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/120] - Train Loss: 0.6887, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6253\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/120] - Train Loss: 0.6907, Val Loss: 0.6915, Val Acc: 0.5341, Val AUC: 0.6134\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/120] - Train Loss: 0.6915, Val Loss: 0.6916, Val Acc: 0.5341, Val AUC: 0.6165\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/120] - Train Loss: 0.6930, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6352\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/120] - Train Loss: 0.6931, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6668\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/120] - Train Loss: 0.6911, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6777\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [8/120] - Train Loss: 0.6889, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6762\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [9/120] - Train Loss: 0.6898, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6694\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [10/120] - Train Loss: 0.6891, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6632\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [11/120] - Train Loss: 0.6895, Val Loss: 0.6914, Val Acc: 0.5341, Val AUC: 0.6710\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [12/120] - Train Loss: 0.6874, Val Loss: 0.6916, Val Acc: 0.5341, Val AUC: 0.6715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [13/120] - Train Loss: 0.6941, Val Loss: 0.6915, Val Acc: 0.5341, Val AUC: 0.6803\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [14/120] - Train Loss: 0.6887, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6975\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [15/120] - Train Loss: 0.6912, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6949\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [16/120] - Train Loss: 0.6948, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7016\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [17/120] - Train Loss: 0.6919, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7099\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [18/120] - Train Loss: 0.6905, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.7156\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [19/120] - Train Loss: 0.6913, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7068\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [20/120] - Train Loss: 0.6895, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6980\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [21/120] - Train Loss: 0.6900, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6959\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [22/120] - Train Loss: 0.6869, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6938\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [23/120] - Train Loss: 0.6880, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.6943\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [24/120] - Train Loss: 0.6854, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6959\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [25/120] - Train Loss: 0.6905, Val Loss: 0.6916, Val Acc: 0.5341, Val AUC: 0.6938\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [26/120] - Train Loss: 0.6917, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6969\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [27/120] - Train Loss: 0.6904, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6972\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [28/120] - Train Loss: 0.6881, Val Loss: 0.6906, Val Acc: 0.5341, Val AUC: 0.7058\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [29/120] - Train Loss: 0.6883, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6969\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [30/120] - Train Loss: 0.6934, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6933\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [31/120] - Train Loss: 0.6911, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.6988\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [32/120] - Train Loss: 0.6896, Val Loss: 0.6906, Val Acc: 0.5341, Val AUC: 0.7011\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [33/120] - Train Loss: 0.6925, Val Loss: 0.6906, Val Acc: 0.5341, Val AUC: 0.7013\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [34/120] - Train Loss: 0.6924, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.7037\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [35/120] - Train Loss: 0.6902, Val Loss: 0.6906, Val Acc: 0.5341, Val AUC: 0.7052\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [36/120] - Train Loss: 0.6873, Val Loss: 0.6906, Val Acc: 0.5341, Val AUC: 0.7011\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [37/120] - Train Loss: 0.6934, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.6923\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [38/120] - Train Loss: 0.6869, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6902\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [39/120] - Train Loss: 0.6923, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6912\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [40/120] - Train Loss: 0.6905, Val Loss: 0.6906, Val Acc: 0.5341, Val AUC: 0.6969\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [41/120] - Train Loss: 0.6927, Val Loss: 0.6905, Val Acc: 0.5341, Val AUC: 0.7037\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [42/120] - Train Loss: 0.6908, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7084\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [43/120] - Train Loss: 0.6909, Val Loss: 0.6905, Val Acc: 0.5341, Val AUC: 0.7052\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [44/120] - Train Loss: 0.6880, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.7068\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [45/120] - Train Loss: 0.6931, Val Loss: 0.6906, Val Acc: 0.5341, Val AUC: 0.7032\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [46/120] - Train Loss: 0.6907, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.7052\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [47/120] - Train Loss: 0.6876, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.7084\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [48/120] - Train Loss: 0.6946, Val Loss: 0.6906, Val Acc: 0.5341, Val AUC: 0.7026\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [49/120] - Train Loss: 0.6933, Val Loss: 0.6905, Val Acc: 0.5341, Val AUC: 0.7037\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [50/120] - Train Loss: 0.6899, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7050\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [51/120] - Train Loss: 0.6889, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.7052\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [52/120] - Train Loss: 0.6881, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.7016\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [53/120] - Train Loss: 0.6836, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6985\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [54/120] - Train Loss: 0.6980, Val Loss: 0.6921, Val Acc: 0.5341, Val AUC: 0.6959\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [55/120] - Train Loss: 0.6926, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.6949\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [56/120] - Train Loss: 0.6893, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.6933\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [57/120] - Train Loss: 0.6908, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6928\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [58/120] - Train Loss: 0.6896, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6933\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [59/120] - Train Loss: 0.6890, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6928\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [60/120] - Train Loss: 0.6904, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6923\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [61/120] - Train Loss: 0.6892, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6902\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [62/120] - Train Loss: 0.6880, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6892\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [63/120] - Train Loss: 0.6886, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6902\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [64/120] - Train Loss: 0.6938, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.6917\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [65/120] - Train Loss: 0.6901, Val Loss: 0.6902, Val Acc: 0.5341, Val AUC: 0.6923\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [66/120] - Train Loss: 0.6936, Val Loss: 0.6902, Val Acc: 0.5341, Val AUC: 0.6928\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [67/120] - Train Loss: 0.6873, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.6933\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [68/120] - Train Loss: 0.6915, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.6907\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [69/120] - Train Loss: 0.6913, Val Loss: 0.6905, Val Acc: 0.5341, Val AUC: 0.6907\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [70/120] - Train Loss: 0.6904, Val Loss: 0.6902, Val Acc: 0.5341, Val AUC: 0.6917\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [71/120] - Train Loss: 0.6932, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.6907\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [72/120] - Train Loss: 0.6890, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.6912\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [73/120] - Train Loss: 0.6907, Val Loss: 0.6902, Val Acc: 0.5341, Val AUC: 0.6907\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [74/120] - Train Loss: 0.6899, Val Loss: 0.6902, Val Acc: 0.5341, Val AUC: 0.6902\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [75/120] - Train Loss: 0.6885, Val Loss: 0.6902, Val Acc: 0.5341, Val AUC: 0.6923\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [76/120] - Train Loss: 0.6929, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6907\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [77/120] - Train Loss: 0.6868, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6886\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [78/120] - Train Loss: 0.6964, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6892\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [79/120] - Train Loss: 0.6909, Val Loss: 0.6902, Val Acc: 0.5341, Val AUC: 0.6928\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [80/120] - Train Loss: 0.6901, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6923\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [81/120] - Train Loss: 0.6902, Val Loss: 0.6905, Val Acc: 0.5341, Val AUC: 0.6902\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [82/120] - Train Loss: 0.6926, Val Loss: 0.6902, Val Acc: 0.5341, Val AUC: 0.6886\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [83/120] - Train Loss: 0.6865, Val Loss: 0.6901, Val Acc: 0.5341, Val AUC: 0.6897\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [84/120] - Train Loss: 0.6895, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6814\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [85/120] - Train Loss: 0.6865, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.6793\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [86/120] - Train Loss: 0.6929, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.6788\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [87/120] - Train Loss: 0.6882, Val Loss: 0.6902, Val Acc: 0.5341, Val AUC: 0.6834\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [88/120] - Train Loss: 0.6892, Val Loss: 0.6901, Val Acc: 0.5341, Val AUC: 0.6866\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [89/120] - Train Loss: 0.6844, Val Loss: 0.6901, Val Acc: 0.5341, Val AUC: 0.6860\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [90/120] - Train Loss: 0.6884, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6834\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [91/120] - Train Loss: 0.6903, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6845\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [92/120] - Train Loss: 0.6884, Val Loss: 0.6905, Val Acc: 0.5341, Val AUC: 0.6840\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [93/120] - Train Loss: 0.6904, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6845\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [94/120] - Train Loss: 0.6931, Val Loss: 0.6901, Val Acc: 0.5341, Val AUC: 0.6866\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [95/120] - Train Loss: 0.6900, Val Loss: 0.6901, Val Acc: 0.5341, Val AUC: 0.6886\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [96/120] - Train Loss: 0.6901, Val Loss: 0.6906, Val Acc: 0.5341, Val AUC: 0.6907\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [97/120] - Train Loss: 0.6900, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6881\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [98/120] - Train Loss: 0.6881, Val Loss: 0.6901, Val Acc: 0.5341, Val AUC: 0.6866\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [99/120] - Train Loss: 0.6832, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6860\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [100/120] - Train Loss: 0.6913, Val Loss: 0.6917, Val Acc: 0.5341, Val AUC: 0.6855\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [101/120] - Train Loss: 0.6927, Val Loss: 0.6905, Val Acc: 0.5341, Val AUC: 0.6855\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [102/120] - Train Loss: 0.6853, Val Loss: 0.6901, Val Acc: 0.5341, Val AUC: 0.6866\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [103/120] - Train Loss: 0.6938, Val Loss: 0.6902, Val Acc: 0.5341, Val AUC: 0.6860\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [104/120] - Train Loss: 0.6888, Val Loss: 0.6900, Val Acc: 0.5341, Val AUC: 0.6881\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [105/120] - Train Loss: 0.6919, Val Loss: 0.6900, Val Acc: 0.5341, Val AUC: 0.6886\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [106/120] - Train Loss: 0.6891, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6881\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [107/120] - Train Loss: 0.6872, Val Loss: 0.6902, Val Acc: 0.5341, Val AUC: 0.6886\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [108/120] - Train Loss: 0.6909, Val Loss: 0.6900, Val Acc: 0.5341, Val AUC: 0.6850\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [109/120] - Train Loss: 0.6891, Val Loss: 0.6900, Val Acc: 0.5341, Val AUC: 0.6855\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [110/120] - Train Loss: 0.6860, Val Loss: 0.6899, Val Acc: 0.5341, Val AUC: 0.6866\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [111/120] - Train Loss: 0.6847, Val Loss: 0.6900, Val Acc: 0.5341, Val AUC: 0.6860\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [112/120] - Train Loss: 0.6883, Val Loss: 0.6905, Val Acc: 0.5341, Val AUC: 0.6855\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [113/120] - Train Loss: 0.6877, Val Loss: 0.6905, Val Acc: 0.5341, Val AUC: 0.6860\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [114/120] - Train Loss: 0.6949, Val Loss: 0.6904, Val Acc: 0.5341, Val AUC: 0.6855\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [115/120] - Train Loss: 0.6910, Val Loss: 0.6899, Val Acc: 0.5341, Val AUC: 0.6881\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [116/120] - Train Loss: 0.6906, Val Loss: 0.6900, Val Acc: 0.5341, Val AUC: 0.6881\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [117/120] - Train Loss: 0.6894, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6892\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [118/120] - Train Loss: 0.6865, Val Loss: 0.6899, Val Acc: 0.5341, Val AUC: 0.6850\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [119/120] - Train Loss: 0.6896, Val Loss: 0.6901, Val Acc: 0.5341, Val AUC: 0.6824\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [120/120] - Train Loss: 0.6841, Val Loss: 0.6903, Val Acc: 0.5341, Val AUC: 0.6814\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Test Loss: 0.6914, Test Accuracy: 0.5341, Test AUC: 0.7374\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attn-trans-fullbackprop/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=bilstm_attn_fullbackprop_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=120, lr=bilstm_attn_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_attn_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
