{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General AMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbbasp = pd.read_csv(\"../models1/database_check/DBAASP_peptides.csv\")\n",
    "dbbasp = dbbasp[[\"ID\", \"SEQUENCE\"]]\n",
    "dbbasp.columns = [\"Peptide ID\", \"Sequence\"]\n",
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "adam_df = pd.concat([adam_df, dbbasp], ignore_index=True)\n",
    "# uniprot_df = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\")\n",
    "# uniprot_df1 = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\")\n",
    "# uniprot_df = pd.concat([uniprot_df, uniprot_df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### raw data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'E', 'L', 'G', 'M', 'W', 'K', 'S', 'Q', 'F', 'H', 'A', 'I', 'R', 'C', 'V', 'T', 'D', 'P', 'N', 'Y'}\n",
      "20\n",
      "Number of sequences after filtering: 3306\n",
      "Dataset sizes: {'Train': 2276, 'Validation': 488, 'Test': 488}\n",
      "Input shape: torch.Size([1959, 20])\n",
      "Target shape: torch.Size([1959, 20])\n",
      "Lengths: tensor([64, 64, 64, 64, 64, 63, 63, 63, 63, 62, 60, 59, 57, 55, 55, 53, 52, 52,\n",
      "        49, 48, 42, 42, 41, 40, 37, 35, 35, 33, 31, 30, 26, 22, 20, 17, 16, 16,\n",
      "        15, 12, 12, 11, 11, 10,  8,  7,  6,  5,  5,  5,  5,  4,  4,  4,  4,  4,\n",
      "         4,  4,  4,  4,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  3,\n",
      "         3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load only positive (AMP) sequences\n",
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "\n",
    "unique_letters = set(''.join(adam_df[\"Sequence\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "print(f\"Number of sequences after filtering: {len(adam_df)}\")\n",
    "adam_df = adam_df.drop_duplicates(subset='Sequence')\n",
    "tb_df = pd.read_csv('../data/all_seq702.csv')\n",
    "adam_df = adam_df[~adam_df['Sequence'].isin(tb_df['Sequences'])]\n",
    "\n",
    "generation_seqs = adam_df[\"Sequence\"].reset_index(drop=True)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "class GenerativeSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        input_seq = seq[:-1]  # all residues except the last\n",
    "        target_seq = seq[1:]  # all residues except the first\n",
    "        length = len(input_seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        input_one_hot = one_hot_torch(input_seq, dtype=self.one_hot_dtype)\n",
    "        target_one_hot = one_hot_torch(target_seq, dtype=self.one_hot_dtype)\n",
    "        # target_indices = torch.tensor([\"ACDEFGHIKLMNPQRSTVWY\".index(res) for res in target_seq], dtype=torch.long)\n",
    "        return input_one_hot, target_one_hot, length\n",
    "\n",
    "def generative_collate_and_pack(batch):\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "\n",
    "    lengths = torch.tensor(lengths)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    targets = [targets[i] for i in sorted_indices]\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    sequences = [seq.T for seq in sequences]  # transpose to [seq_len, features]\n",
    "    targets = [tgt.T for tgt in targets]      # transpose targets as well\n",
    "\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)\n",
    "    padded_targets = pad_sequence(targets, batch_first=False)\n",
    "\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "    packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, packed_target, lengths\n",
    "\n",
    "\n",
    "# Train/val/test split\n",
    "train_seqs, test_seqs = train_test_split(generation_seqs, test_size=0.3, random_state=42)\n",
    "val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = GenerativeSequenceDataset(train_seqs)\n",
    "val_dataset = GenerativeSequenceDataset(val_seqs)\n",
    "test_dataset = GenerativeSequenceDataset(test_seqs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=generative_collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "\n",
    "# Dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\", dataset_sizes)\n",
    "\n",
    "for x, y, l in train_loader:\n",
    "    print(\"Input shape:\", x.data.shape)  # [L, B, 20]\n",
    "    print(\"Target shape:\", y.data.shape)  # [L, B, 20]\n",
    "    print(\"Lengths:\", y.batch_sizes)  # Lengths of sequences in the batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sliding window data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-import required libraries after environment reset\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load only positive (AMP) sequences\n",
    "# adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "\n",
    "# # Clean non-standard amino acids\n",
    "# unique_letters = set(''.join(adam_df[\"Sequence\"]))\n",
    "# amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "# # non_standard_amino_acids = set(unique_letters) - set(amino_acids)\n",
    "# # adam_df = adam_df[~adam_df[\"Sequence\"].str.contains('|'.join(non_standard_amino_acids))]\n",
    "\n",
    "# # Apply sliding window to generate fragments\n",
    "# def generate_fragments(sequences, window_size=15, stride=5):\n",
    "#     fragments = []\n",
    "#     for seq in sequences:\n",
    "#         for start in range(0, len(seq) - window_size + 1, stride):\n",
    "#             fragment = seq[start:start + window_size]\n",
    "#             fragments.append(fragment)\n",
    "#     return fragments\n",
    "\n",
    "# generation_fragments = generate_fragments(adam_df[\"Sequence\"].tolist())\n",
    "\n",
    "# # Define one-hot encoding function\n",
    "# def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "#     amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "#     seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "#     aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "#     arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "#     for i, aa in enumerate(aa_bytes):\n",
    "#         arr[i, seq_bytes == aa] = 1\n",
    "#     return arr\n",
    "\n",
    "# # Dataset using one-hot encoding for generative modeling\n",
    "# class AMPGenerationOneHotDataset(Dataset):\n",
    "#     def __init__(self, sequences):\n",
    "#         self.sequences = sequences\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         seq = self.sequences[idx]\n",
    "#         input_seq = seq[:-1]  # all residues except the last\n",
    "#         target_seq = seq[1:]  # all residues except the first\n",
    "#         input_one_hot = one_hot_torch(input_seq)  # shape: [20, seq_len - 1]\n",
    "#         target_one_hot = one_hot_torch(target_seq)  # shape: [20, seq_len - 1]\n",
    "#         length = input_one_hot.shape[1]\n",
    "#         return input_one_hot, target_one_hot, length\n",
    "\n",
    "# # Collate function for packing one-hot sequences\n",
    "# def collate_and_pack_for_generation(batch):\n",
    "#     sequences, targets, lengths = zip(*batch)\n",
    "#     lengths = torch.tensor(lengths)\n",
    "\n",
    "#     # Sort by length (required for packing)\n",
    "#     sorted_indices = torch.argsort(lengths, descending=True)\n",
    "#     sequences = [sequences[i] for i in sorted_indices]\n",
    "#     targets = [targets[i] for i in sorted_indices]\n",
    "#     lengths = lengths[sorted_indices]\n",
    "\n",
    "#     # Transpose each to [L, 20] and pad\n",
    "#     sequences = [seq.T for seq in sequences]  # from [20, L] to [L, 20]\n",
    "#     targets = [tgt.T for tgt in targets]      # from [20, L] to [L, 20]\n",
    "\n",
    "#     padded_seqs = pad_sequence(sequences, batch_first=False)  # [L, B, 20]\n",
    "#     padded_targets = pad_sequence(targets, batch_first=False)  # [L, B, 20]\n",
    "\n",
    "#     packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "#     packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "\n",
    "#     return packed_input, packed_target, lengths\n",
    "\n",
    "# # Train/val/test split\n",
    "# train_seqs, test_seqs = train_test_split(generation_fragments, test_size=0.3, random_state=42)\n",
    "# val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "\n",
    "# train_dataset = AMPGenerationOneHotDataset(train_seqs)\n",
    "# val_dataset = AMPGenerationOneHotDataset(val_seqs)\n",
    "# test_dataset = AMPGenerationOneHotDataset(test_seqs)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_and_pack_for_generation)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_and_pack_for_generation)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_and_pack_for_generation)\n",
    "\n",
    "# # Dataset sizes for verification\n",
    "# dataset_sizes = {\n",
    "#     \"Train\": len(train_dataset),\n",
    "#     \"Validation\": len(val_dataset),\n",
    "#     \"Test\": len(test_dataset)\n",
    "# }\n",
    "# print(\"Dataset sizes:\", dataset_sizes)\n",
    "\n",
    "# for x, y, l in train_loader:\n",
    "#     print(\"Input shape:\", x.data.shape)  # [L, B, 20]\n",
    "#     print(\"Target shape:\", y.data.shape)  # [L, B, 20]\n",
    "#     print(\"Lengths:\", y.batch_sizes)  # Lengths of sequences in the batch\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=128, num_layers=1, dropout=0.3, output_dim=20):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        dropped = self.dropout(packed_output.data)\n",
    "        logits = self.fc(dropped)\n",
    "        return logits  # shape: [total_timesteps, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x7f5ab3d21ed0>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 775, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "KeyboardInterrupt: \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 2.9105 - Val Loss: 2.8513\n",
      "Epoch 2/10 - Train Loss: 2.8507 - Val Loss: 2.8381\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCannot execute code, session has been disposed. Please try restarting the Kernel. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "model = GenerativeLSTM()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and eval functions\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for packed_input, packed_target, _ in dataloader:\n",
    "        inputs = packed_input.to(device)\n",
    "        targets = torch.argmax(packed_target.data, dim=1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, packed_target, _ in dataloader:\n",
    "            inputs = packed_input.to(device)\n",
    "            targets = torch.argmax(packed_target.data, dim=1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Final test evaluation\n",
    "test_loss = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "class GenerativeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=128, num_layers=1, dropout=0.3):\n",
    "        super(GenerativeLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle packed input\n",
    "        if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n",
    "            packed_output, _ = self.lstm(x)\n",
    "            unpacked_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            return self.fc(unpacked_output)\n",
    "        else:\n",
    "            out, _ = self.lstm(x)\n",
    "            return self.fc(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General AMP - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 09:42:23,365] A new study created in memory with name: no-name-39e4a52e-15b8-43ed-af6c-b28197acf9d7\n",
      "[I 2025-04-24 09:43:19,587] Trial 0 finished with value: 0.29205354303121567 and parameters: {'hidden_dim': 170, 'num_layers': 2, 'dropout': 0.2656485138314876, 'lr': 0.008011877132915504, 'weight_decay': 8.31394692866913e-05}. Best is trial 0 with value: 0.29205354303121567.\n",
      "[I 2025-04-24 09:44:07,686] Trial 1 finished with value: 0.543607734143734 and parameters: {'hidden_dim': 227, 'num_layers': 2, 'dropout': 0.42816344492915914, 'lr': 0.005571700887563167, 'weight_decay': 0.0006397652896710152}. Best is trial 0 with value: 0.29205354303121567.\n",
      "[I 2025-04-24 09:44:48,540] Trial 2 finished with value: 1.1754698157310486 and parameters: {'hidden_dim': 195, 'num_layers': 2, 'dropout': 0.4354436052697497, 'lr': 0.003323673614997853, 'weight_decay': 0.0001282722171795435}. Best is trial 0 with value: 0.29205354303121567.\n",
      "[I 2025-04-24 09:45:22,209] Trial 3 finished with value: 1.0532560050487518 and parameters: {'hidden_dim': 197, 'num_layers': 1, 'dropout': 0.2042772117925995, 'lr': 0.003574128198364371, 'weight_decay': 0.0009262039851615479}. Best is trial 0 with value: 0.29205354303121567.\n",
      "[I 2025-04-24 09:46:00,364] Trial 4 finished with value: 0.34632034599781036 and parameters: {'hidden_dim': 109, 'num_layers': 2, 'dropout': 0.4741970091429819, 'lr': 0.007468592823551809, 'weight_decay': 0.0006086926559304943}. Best is trial 0 with value: 0.29205354303121567.\n",
      "[I 2025-04-24 09:46:42,287] Trial 5 finished with value: 0.2279779426753521 and parameters: {'hidden_dim': 209, 'num_layers': 3, 'dropout': 0.3529933187384503, 'lr': 0.009706604304936833, 'weight_decay': 0.0007854097260239574}. Best is trial 5 with value: 0.2279779426753521.\n",
      "[I 2025-04-24 09:47:26,046] Trial 6 finished with value: 0.2675355114042759 and parameters: {'hidden_dim': 150, 'num_layers': 3, 'dropout': 0.1804979658124184, 'lr': 0.008540414810944632, 'weight_decay': 0.00012205551501000439}. Best is trial 5 with value: 0.2279779426753521.\n",
      "[I 2025-04-24 09:48:02,259] Trial 7 finished with value: 0.25771475955843925 and parameters: {'hidden_dim': 227, 'num_layers': 2, 'dropout': 0.14200485776197094, 'lr': 0.008805481046355253, 'weight_decay': 0.00017906961241427312}. Best is trial 5 with value: 0.2279779426753521.\n",
      "[I 2025-04-24 09:48:35,000] Trial 8 finished with value: 1.8715054392814636 and parameters: {'hidden_dim': 135, 'num_layers': 1, 'dropout': 0.4309601334154186, 'lr': 0.0016777445834470304, 'weight_decay': 0.0004885043111317426}. Best is trial 5 with value: 0.2279779426753521.\n",
      "[I 2025-04-24 09:49:11,230] Trial 9 finished with value: 2.8568696677684784 and parameters: {'hidden_dim': 215, 'num_layers': 2, 'dropout': 0.23842480566120305, 'lr': 0.00024350821783452855, 'weight_decay': 0.0003163236767581361}. Best is trial 5 with value: 0.2279779426753521.\n",
      "[I 2025-04-24 09:49:48,358] Trial 10 finished with value: 0.4810046814382076 and parameters: {'hidden_dim': 66, 'num_layers': 3, 'dropout': 0.33582614686932255, 'lr': 0.006175748399975222, 'weight_decay': 0.0009660017376018117}. Best is trial 5 with value: 0.2279779426753521.\n",
      "[I 2025-04-24 09:50:24,475] Trial 11 finished with value: 0.22196322865784168 and parameters: {'hidden_dim': 247, 'num_layers': 3, 'dropout': 0.12915638774083413, 'lr': 0.009983517577195865, 'weight_decay': 0.0007901836295730717}. Best is trial 11 with value: 0.22196322865784168.\n",
      "[I 2025-04-24 09:51:12,259] Trial 12 finished with value: 0.23469595983624458 and parameters: {'hidden_dim': 251, 'num_layers': 3, 'dropout': 0.3371837221829347, 'lr': 0.009811915016912351, 'weight_decay': 0.0008241479724636152}. Best is trial 11 with value: 0.22196322865784168.\n",
      "[I 2025-04-24 09:52:02,423] Trial 13 finished with value: 0.22426254861056805 and parameters: {'hidden_dim': 253, 'num_layers': 3, 'dropout': 0.10099389039581574, 'lr': 0.009845997280033717, 'weight_decay': 0.0007567280666266063}. Best is trial 11 with value: 0.22196322865784168.\n",
      "[I 2025-04-24 09:52:45,432] Trial 14 finished with value: 0.38339296728372574 and parameters: {'hidden_dim': 256, 'num_layers': 3, 'dropout': 0.11709539813833697, 'lr': 0.007124249040232956, 'weight_decay': 0.0006982376547070575}. Best is trial 11 with value: 0.22196322865784168.\n",
      "[I 2025-04-24 09:53:26,680] Trial 15 finished with value: 0.8063361942768097 and parameters: {'hidden_dim': 240, 'num_layers': 3, 'dropout': 0.10838982890956299, 'lr': 0.004326995962914602, 'weight_decay': 0.00048808323194545405}. Best is trial 11 with value: 0.22196322865784168.\n",
      "[I 2025-04-24 09:54:10,934] Trial 16 finished with value: 0.2334918323904276 and parameters: {'hidden_dim': 168, 'num_layers': 3, 'dropout': 0.1670941151340848, 'lr': 0.009898921422573596, 'weight_decay': 0.0008153289725016642}. Best is trial 11 with value: 0.22196322865784168.\n",
      "[I 2025-04-24 09:54:56,062] Trial 17 finished with value: 0.3979787975549698 and parameters: {'hidden_dim': 185, 'num_layers': 3, 'dropout': 0.21618460109540164, 'lr': 0.006732455162651752, 'weight_decay': 0.0003699944325953699}. Best is trial 11 with value: 0.22196322865784168.\n",
      "[I 2025-04-24 09:55:32,273] Trial 18 finished with value: 0.2812317758798599 and parameters: {'hidden_dim': 239, 'num_layers': 1, 'dropout': 0.10292206679821223, 'lr': 0.00871691601162054, 'weight_decay': 0.0008870674418674352}. Best is trial 11 with value: 0.22196322865784168.\n",
      "[I 2025-04-24 09:56:15,529] Trial 19 finished with value: 0.6064551174640656 and parameters: {'hidden_dim': 105, 'num_layers': 3, 'dropout': 0.28216143533508264, 'lr': 0.005467321450647017, 'weight_decay': 0.0007122248337038007}. Best is trial 11 with value: 0.22196322865784168.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dim': 247, 'num_layers': 3, 'dropout': 0.12915638774083413, 'lr': 0.009983517577195865, 'weight_decay': 0.0007901836295730717}\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary packages after reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Criterion\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss on the last time step of each sequence.\n",
    "    \n",
    "    Args:\n",
    "        output: Tensor of shape [B, L, vocab_size]\n",
    "        target_seq: Tensor of shape [B, L] containing target class indices\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss computed only on the last token of each sequence\n",
    "    \"\"\"\n",
    "    # Get last time step for each sequence\n",
    "    last_token_logits = output[:, -1, :]        # [B, vocab_size]\n",
    "    last_token_targets = target_seq[:, -1, :]      # [B]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "    # print('last_token_logits',last_token_logits.shape)\n",
    "    # print('last_token_targets',last_token_targets.shape)\n",
    "\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen/AMP_LSTM_GEN_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])      # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            # print('target_shape before reshape',target_seq.shape)\n",
    "            # target_seq = target_seq.reshape(-1)\n",
    "            # print(f\"Output shape: {output.shape}, Target shape: {target_seq.shape}\")\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_generator.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])  # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # # target_seq = target_seq.reshape(-1)\n",
    "            # # target_seq = target_seq.reshape(-1, target_seq.shape[-1])\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            # assert output.size(0) == target_seq.size(0), f\"Mismatch: {output.size(0)} vs {target_seq.size(0)}\"\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            # print('loss done')\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            preds = output[:, -1, :]        # shape: [B, vocab_size]\n",
    "            preds = torch.argmax(preds, dim=1)  # shape: [B]\n",
    "\n",
    "            targets = target_seq[:, -1, :]      # shape: [B, vocab_size]\n",
    "            targets = torch.argmax(targets, dim=-1)  # shape: [B]\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Objective for Optuna tuning\n",
    "def objective_generation(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "\n",
    "lstm_gen_best_params = study.best_trial.params\n",
    "print(lstm_gen_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir runs-lstm-gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.6719 | Val Loss = 2.3381 | Acc = 0.9877 | AUC = undefined | Perplexity = 10.3614\n",
      "Epoch 2: Train Loss = 2.0466 | Val Loss = 1.7488 | Acc = 0.9877 | AUC = undefined | Perplexity = 5.7476\n",
      "Epoch 3: Train Loss = 1.5022 | Val Loss = 1.2576 | Acc = 0.9877 | AUC = undefined | Perplexity = 3.5169\n",
      "Epoch 4: Train Loss = 1.0747 | Val Loss = 0.8921 | Acc = 0.9877 | AUC = undefined | Perplexity = 2.4403\n",
      "Epoch 5: Train Loss = 0.7665 | Val Loss = 0.6423 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.9009\n",
      "Epoch 6: Train Loss = 0.5685 | Val Loss = 0.4766 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.6106\n",
      "Epoch 7: Train Loss = 0.4256 | Val Loss = 0.3775 | Acc = 0.9836 | AUC = undefined | Perplexity = 1.4586\n",
      "Epoch 8: Train Loss = 0.3397 | Val Loss = 0.2988 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.3483\n",
      "Epoch 9: Train Loss = 0.2768 | Val Loss = 0.2534 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.2884\n",
      "Epoch 10: Train Loss = 0.2338 | Val Loss = 0.2171 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.2425\n",
      "Epoch 11: Train Loss = 0.2061 | Val Loss = 0.1905 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.2098\n",
      "Epoch 12: Train Loss = 0.1809 | Val Loss = 0.1714 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.1870\n",
      "Epoch 13: Train Loss = 0.1670 | Val Loss = 0.1548 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.1674\n",
      "Epoch 14: Train Loss = 0.1533 | Val Loss = 0.1489 | Acc = 0.9857 | AUC = undefined | Perplexity = 1.1606\n",
      "Epoch 15: Train Loss = 0.1386 | Val Loss = 0.1358 | Acc = 0.9857 | AUC = undefined | Perplexity = 1.1454\n",
      "Epoch 16: Train Loss = 0.1281 | Val Loss = 0.1297 | Acc = 0.9836 | AUC = undefined | Perplexity = 1.1385\n",
      "Epoch 17: Train Loss = 0.1241 | Val Loss = 0.1209 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.1285\n",
      "Epoch 18: Train Loss = 0.1182 | Val Loss = 0.1158 | Acc = 0.9836 | AUC = undefined | Perplexity = 1.1228\n",
      "Epoch 19: Train Loss = 0.1106 | Val Loss = 0.1183 | Acc = 0.9836 | AUC = undefined | Perplexity = 1.1256\n",
      "Epoch 20: Train Loss = 0.1090 | Val Loss = 0.1054 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.1112\n",
      "\n",
      "✅ Final Test Metrics:\n",
      "Loss = 0.1054, Accuracy = 0.9877, AUC = undefined, Perplexity = 1.1112\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_best_params[\"lr\"], weight_decay=lstm_gen_best_params[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "# lstm_gen_best_params = {'hidden_dim': 133, 'num_layers': 2, 'dropout': 0.10063270147175422, 'lr': 0.003237280156212186, 'weight_decay': 2.2594437829479466e-05}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=20)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\"\\n✅ Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n",
    "torch.save(trained_model.state_dict(), 'best_model_lstm_generator_final.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tb amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'E', 'L', 'G', 'M', 'W', 'K', 'S', 'Q', 'F', 'H', 'A', 'I', 'R', 'C', 'V', 'T', 'D', 'P', 'X', 'N', 'Y'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "# df_filtered = df[\n",
    "#     (df['Sequences'].str.len() >= 10) &\n",
    "#     (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "#     (~df['Sequences'].str.contains('X'))\n",
    "# ]\n",
    "df_filtered = df\n",
    "\n",
    "def split_sequence(seq, chunk_size=20):\n",
    "    return [seq[i:i+chunk_size] for i in range(0, len(seq), chunk_size)]\n",
    "\n",
    "new_rows = []\n",
    "for _, row in df_filtered.iterrows():\n",
    "    seq = row['Sequences']\n",
    "    amp_label = row['AMP']\n",
    "    if len(seq) > 40:\n",
    "        for chunk in split_sequence(seq, 20):\n",
    "            new_rows.append({'Sequences': chunk, 'AMP': amp_label})\n",
    "    else:\n",
    "        new_rows.append({'Sequences': seq, 'AMP': amp_label})\n",
    "\n",
    "df_filtered = pd.DataFrame(new_rows)\n",
    "\n",
    "\n",
    "df_filtered = df_filtered[\n",
    "    (df_filtered['Sequences'].str.len() >= 10) &\n",
    "    (df_filtered['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df_filtered['Sequences'].str.contains('X'))\n",
    "]\n",
    "df_filtered = df_filtered[df_filtered['AMP']==1]\n",
    "df_filtered = df_filtered.drop_duplicates(subset='Sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(min([len(x) for x in df_filtered['Sequences']]))\n",
    "print(max([len(x) for x in df_filtered['Sequences']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Input shape': torch.Size([1203, 20]),\n",
       " 'Target shape': torch.Size([1203, 20]),\n",
       " 'Lengths': tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 59, 56, 51, 48, 44, 42, 41, 38, 36,\n",
       "         32, 18, 16, 14, 13, 12, 12, 11, 11, 11, 11,  7,  7,  7,  7,  6,  5,  5,\n",
       "          3,  3,  1])}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import libraries after environment reset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "df = df_filtered\n",
    "\n",
    "# Clean and inspect\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "# non_standard_amino_acids = unique_letters - amino_acids\n",
    "# df = df[~df[\"Sequences\"].str.contains('|'.join(non_standard_amino_acids))]\n",
    "\n",
    "# Extract sequences\n",
    "sequences = df[\"Sequences\"].reset_index(drop=True)\n",
    "\n",
    "# Define one-hot function\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "# Define dataset class\n",
    "class GenerativeSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        input_seq = seq[:-1]\n",
    "        target_seq = seq[1:]\n",
    "        length = len(input_seq.replace(\"X\", \"\"))\n",
    "        input_one_hot = one_hot_torch(input_seq, dtype=self.one_hot_dtype)\n",
    "        target_one_hot = one_hot_torch(target_seq, dtype=self.one_hot_dtype)\n",
    "        return input_one_hot, target_one_hot, length\n",
    "\n",
    "# Define collate function\n",
    "def generative_collate_and_pack(batch):\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    targets = [targets[i] for i in sorted_indices]\n",
    "    lengths = lengths[sorted_indices]\n",
    "    sequences = [seq.T for seq in sequences]\n",
    "    targets = [tgt.T for tgt in targets]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)\n",
    "    padded_targets = pad_sequence(targets, batch_first=False)\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "    packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "    return packed_input, packed_target, lengths\n",
    "\n",
    "# Split and load data\n",
    "train_seqs, test_seqs = train_test_split(sequences, test_size=0.3, random_state=42)\n",
    "val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "train_dataset = GenerativeSequenceDataset(train_seqs)\n",
    "val_dataset = GenerativeSequenceDataset(val_seqs)\n",
    "test_dataset = GenerativeSequenceDataset(test_seqs)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=generative_collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "\n",
    "# Preview batch\n",
    "batch_sample = next(iter(train_loader))\n",
    "batch_sample_shapes = {\n",
    "    \"Input shape\": batch_sample[0].data.shape,\n",
    "    \"Target shape\": batch_sample[1].data.shape,\n",
    "    \"Lengths\": batch_sample[0].batch_sizes\n",
    "}\n",
    "batch_sample_shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 171\n",
      "Validation: 37\n",
      "Test: 37\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train for full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 10:11:14,059] A new study created in memory with name: no-name-e9ca107c-4e23-4873-85ca-c69bf279a146\n",
      "[I 2025-04-24 10:11:19,896] Trial 0 finished with value: 0.18188254535198212 and parameters: {'dropout': 0.24678797353839577, 'lr': 0.008594761299464238, 'weight_decay': 0.0009517778012328149}. Best is trial 0 with value: 0.18188254535198212.\n",
      "[I 2025-04-24 10:11:25,828] Trial 1 finished with value: 0.2015029788017273 and parameters: {'dropout': 0.3909797957559561, 'lr': 0.006216064073374344, 'weight_decay': 0.000784507995534682}. Best is trial 0 with value: 0.18188254535198212.\n",
      "[I 2025-04-24 10:11:31,313] Trial 2 finished with value: 0.20068062841892242 and parameters: {'dropout': 0.12298074582499506, 'lr': 0.00011367954424524167, 'weight_decay': 0.000785148468503358}. Best is trial 0 with value: 0.18188254535198212.\n",
      "[I 2025-04-24 10:11:37,014] Trial 3 finished with value: 0.19988277554512024 and parameters: {'dropout': 0.3417578994521054, 'lr': 0.007401726162839175, 'weight_decay': 0.00039895902665565826}. Best is trial 0 with value: 0.18188254535198212.\n",
      "[I 2025-04-24 10:11:43,104] Trial 4 finished with value: 0.19337965548038483 and parameters: {'dropout': 0.13543412860164114, 'lr': 0.00870650010068621, 'weight_decay': 0.0005377569108902662}. Best is trial 0 with value: 0.18188254535198212.\n",
      "[I 2025-04-24 10:11:49,674] Trial 5 finished with value: 0.19022047519683838 and parameters: {'dropout': 0.31553027267473116, 'lr': 0.0016070918653959992, 'weight_decay': 0.00010845363910998042}. Best is trial 0 with value: 0.18188254535198212.\n",
      "[I 2025-04-24 10:11:56,123] Trial 6 finished with value: 0.20323194563388824 and parameters: {'dropout': 0.1646051369749436, 'lr': 0.0004219951673889182, 'weight_decay': 1.148262228029806e-05}. Best is trial 0 with value: 0.18188254535198212.\n",
      "[I 2025-04-24 10:12:01,941] Trial 7 finished with value: 0.19218620657920837 and parameters: {'dropout': 0.2465718268877665, 'lr': 0.004314296139068641, 'weight_decay': 0.000274891766091489}. Best is trial 0 with value: 0.18188254535198212.\n",
      "[I 2025-04-24 10:12:07,654] Trial 8 finished with value: 0.17938487231731415 and parameters: {'dropout': 0.13882938162931305, 'lr': 0.001464264209101335, 'weight_decay': 0.0002912839503135125}. Best is trial 8 with value: 0.17938487231731415.\n",
      "[I 2025-04-24 10:12:13,443] Trial 9 finished with value: 0.18546098470687866 and parameters: {'dropout': 0.46493511159595424, 'lr': 0.004394173574275916, 'weight_decay': 0.00014441195194423708}. Best is trial 8 with value: 0.17938487231731415.\n",
      "[I 2025-04-24 10:12:20,032] Trial 10 finished with value: 0.19627508521080017 and parameters: {'dropout': 0.21162295561046252, 'lr': 0.0027865186708340836, 'weight_decay': 0.0005219972719958934}. Best is trial 8 with value: 0.17938487231731415.\n",
      "[I 2025-04-24 10:12:25,804] Trial 11 finished with value: 0.1940474957227707 and parameters: {'dropout': 0.23671758746283145, 'lr': 0.00938985756472794, 'weight_decay': 0.0009565514768919674}. Best is trial 8 with value: 0.17938487231731415.\n",
      "[I 2025-04-24 10:12:31,984] Trial 12 finished with value: 0.18796409666538239 and parameters: {'dropout': 0.19434644578225505, 'lr': 0.006279354228765237, 'weight_decay': 0.0006785635152134933}. Best is trial 8 with value: 0.17938487231731415.\n",
      "[I 2025-04-24 10:12:37,444] Trial 13 finished with value: 0.18648087978363037 and parameters: {'dropout': 0.10122307132986233, 'lr': 0.0028994374703497057, 'weight_decay': 0.0009758945897561154}. Best is trial 8 with value: 0.17938487231731415.\n",
      "[I 2025-04-24 10:12:43,151] Trial 14 finished with value: 0.1993638426065445 and parameters: {'dropout': 0.2701552506974553, 'lr': 0.00998868943217055, 'weight_decay': 0.00031878514556602547}. Best is trial 8 with value: 0.17938487231731415.\n",
      "[I 2025-04-24 10:12:50,116] Trial 15 finished with value: 0.1963987648487091 and parameters: {'dropout': 0.18008540710147608, 'lr': 0.0076872753973013155, 'weight_decay': 0.0006043741408327554}. Best is trial 8 with value: 0.17938487231731415.\n",
      "[I 2025-04-24 10:12:56,121] Trial 16 finished with value: 0.20289865136146545 and parameters: {'dropout': 0.364596431073793, 'lr': 0.003024252395860261, 'weight_decay': 0.00036701950648732073}. Best is trial 8 with value: 0.17938487231731415.\n",
      "[I 2025-04-24 10:13:02,307] Trial 17 finished with value: 0.2130189836025238 and parameters: {'dropout': 0.2957548147007488, 'lr': 0.00628525054543146, 'weight_decay': 0.00018971189716750702}. Best is trial 8 with value: 0.17938487231731415.\n",
      "[I 2025-04-24 10:13:09,312] Trial 18 finished with value: 0.1935543417930603 and parameters: {'dropout': 0.4267136023218067, 'lr': 0.0049449651921038, 'weight_decay': 0.0008603341859631956}. Best is trial 8 with value: 0.17938487231731415.\n",
      "[I 2025-04-24 10:13:14,439] Trial 19 finished with value: 0.19519320130348206 and parameters: {'dropout': 0.15292596080357335, 'lr': 0.0015703758252182123, 'weight_decay': 0.0004467263190039511}. Best is trial 8 with value: 0.17938487231731415.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best transfer learning hyperparameters: {'dropout': 0.13882938162931305, 'lr': 0.001464264209101335, 'weight_decay': 0.0002912839503135125}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Criterion\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Transfer Learning Loader\n",
    "def load_pretrained_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    last_token_logits = output[:, -1, :]\n",
    "    last_token_targets = target_seq[:, -1, :]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen-tb/AMP_LSTM_GEN_TRANSFER_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # if train:\n",
    "            #     torch.save(model.state_dict(), 'best_model_lstm_transfer.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "    \n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Optuna objective for fine-tuning\n",
    "def objective_generation(trial):\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(    \n",
    "                hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "                num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "                dropout=dropout\n",
    "                # dropout=lstm_gen_best_params[\"dropout\"]\n",
    "                )\n",
    "    model = load_pretrained_weights(model, 'best_model_lstm_generator_final.pt')  # path to the general AMP model\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=20, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "lstm_gen_best_params_tb = study.best_trial.params\n",
    "print(\"Best transfer learning hyperparameters:\", lstm_gen_best_params_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 3.0329 | Val Loss = 3.0233 | Acc = 0.0270 | AUC = undefined | Perplexity = 20.5580\n",
      "Epoch 2: Train Loss = 3.0231 | Val Loss = 3.0125 | Acc = 0.0270 | AUC = undefined | Perplexity = 20.3385\n",
      "Epoch 3: Train Loss = 3.0123 | Val Loss = 2.9993 | Acc = 0.0270 | AUC = undefined | Perplexity = 20.0712\n",
      "Epoch 4: Train Loss = 2.9970 | Val Loss = 2.9754 | Acc = 0.0270 | AUC = undefined | Perplexity = 19.5973\n",
      "Epoch 5: Train Loss = 2.9844 | Val Loss = 2.9430 | Acc = 0.0270 | AUC = undefined | Perplexity = 18.9727\n",
      "Epoch 6: Train Loss = 2.9729 | Val Loss = 2.9306 | Acc = 0.0270 | AUC = undefined | Perplexity = 18.7394\n",
      "Epoch 7: Train Loss = 2.9773 | Val Loss = 2.9341 | Acc = 0.0000 | AUC = undefined | Perplexity = 18.8044\n",
      "Epoch 8: Train Loss = 2.9475 | Val Loss = 2.9320 | Acc = 0.0000 | AUC = undefined | Perplexity = 18.7655\n",
      "Epoch 9: Train Loss = 2.9300 | Val Loss = 2.9232 | Acc = 0.0000 | AUC = undefined | Perplexity = 18.6015\n",
      "Epoch 10: Train Loss = 2.9664 | Val Loss = 2.9157 | Acc = 0.0270 | AUC = undefined | Perplexity = 18.4618\n",
      "Epoch 11: Train Loss = 2.9150 | Val Loss = 2.9094 | Acc = 0.0000 | AUC = undefined | Perplexity = 18.3464\n",
      "Epoch 12: Train Loss = 2.9407 | Val Loss = 2.9039 | Acc = 0.9730 | AUC = undefined | Perplexity = 18.2457\n",
      "Epoch 13: Train Loss = 2.9096 | Val Loss = 2.8965 | Acc = 0.9730 | AUC = undefined | Perplexity = 18.1101\n",
      "Epoch 14: Train Loss = 2.8943 | Val Loss = 2.8884 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.9647\n",
      "Epoch 15: Train Loss = 2.8991 | Val Loss = 2.8779 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.7772\n",
      "Epoch 16: Train Loss = 2.8787 | Val Loss = 2.8669 | Acc = 1.0000 | AUC = undefined | Perplexity = 17.5828\n",
      "Epoch 17: Train Loss = 2.8916 | Val Loss = 2.8574 | Acc = 1.0000 | AUC = undefined | Perplexity = 17.4154\n",
      "Epoch 18: Train Loss = 2.8709 | Val Loss = 2.8489 | Acc = 1.0000 | AUC = undefined | Perplexity = 17.2691\n",
      "Epoch 19: Train Loss = 2.8739 | Val Loss = 2.8412 | Acc = 1.0000 | AUC = undefined | Perplexity = 17.1361\n",
      "Epoch 20: Train Loss = 2.8437 | Val Loss = 2.8332 | Acc = 1.0000 | AUC = undefined | Perplexity = 16.9997\n",
      "Epoch 21: Train Loss = 2.8694 | Val Loss = 2.8258 | Acc = 1.0000 | AUC = undefined | Perplexity = 16.8745\n",
      "Epoch 22: Train Loss = 2.8295 | Val Loss = 2.8202 | Acc = 1.0000 | AUC = undefined | Perplexity = 16.7802\n",
      "Epoch 23: Train Loss = 2.8198 | Val Loss = 2.8115 | Acc = 1.0000 | AUC = undefined | Perplexity = 16.6352\n",
      "Epoch 24: Train Loss = 2.8112 | Val Loss = 2.8024 | Acc = 1.0000 | AUC = undefined | Perplexity = 16.4836\n",
      "Epoch 25: Train Loss = 2.7892 | Val Loss = 2.7940 | Acc = 1.0000 | AUC = undefined | Perplexity = 16.3470\n",
      "Epoch 26: Train Loss = 2.8184 | Val Loss = 2.7880 | Acc = 0.9730 | AUC = undefined | Perplexity = 16.2478\n",
      "Epoch 27: Train Loss = 2.7896 | Val Loss = 2.7806 | Acc = 0.9730 | AUC = undefined | Perplexity = 16.1281\n",
      "Epoch 28: Train Loss = 2.8166 | Val Loss = 2.7728 | Acc = 0.9730 | AUC = undefined | Perplexity = 16.0028\n",
      "Epoch 29: Train Loss = 2.7786 | Val Loss = 2.7644 | Acc = 0.9730 | AUC = undefined | Perplexity = 15.8697\n",
      "Epoch 30: Train Loss = 2.7913 | Val Loss = 2.7579 | Acc = 0.9730 | AUC = undefined | Perplexity = 15.7670\n",
      "Epoch 31: Train Loss = 2.7971 | Val Loss = 2.7538 | Acc = 0.9730 | AUC = undefined | Perplexity = 15.7029\n",
      "Epoch 32: Train Loss = 2.7560 | Val Loss = 2.7473 | Acc = 0.9730 | AUC = undefined | Perplexity = 15.6010\n",
      "Epoch 33: Train Loss = 2.7487 | Val Loss = 2.7363 | Acc = 0.9730 | AUC = undefined | Perplexity = 15.4304\n",
      "Epoch 34: Train Loss = 2.7513 | Val Loss = 2.7255 | Acc = 0.9730 | AUC = undefined | Perplexity = 15.2634\n",
      "Epoch 35: Train Loss = 2.7272 | Val Loss = 2.7160 | Acc = 1.0000 | AUC = undefined | Perplexity = 15.1194\n",
      "Epoch 36: Train Loss = 2.7208 | Val Loss = 2.7057 | Acc = 1.0000 | AUC = undefined | Perplexity = 14.9648\n",
      "Epoch 37: Train Loss = 2.7156 | Val Loss = 2.6948 | Acc = 1.0000 | AUC = undefined | Perplexity = 14.8028\n",
      "Epoch 38: Train Loss = 2.7121 | Val Loss = 2.6851 | Acc = 1.0000 | AUC = undefined | Perplexity = 14.6599\n",
      "Epoch 39: Train Loss = 2.6942 | Val Loss = 2.6768 | Acc = 1.0000 | AUC = undefined | Perplexity = 14.5385\n",
      "Epoch 40: Train Loss = 2.6921 | Val Loss = 2.6678 | Acc = 1.0000 | AUC = undefined | Perplexity = 14.4080\n",
      "Epoch 41: Train Loss = 2.6816 | Val Loss = 2.6609 | Acc = 1.0000 | AUC = undefined | Perplexity = 14.3093\n",
      "Epoch 42: Train Loss = 2.7127 | Val Loss = 2.6527 | Acc = 1.0000 | AUC = undefined | Perplexity = 14.1918\n",
      "Epoch 43: Train Loss = 2.6754 | Val Loss = 2.6454 | Acc = 1.0000 | AUC = undefined | Perplexity = 14.0895\n",
      "Epoch 44: Train Loss = 2.6553 | Val Loss = 2.6389 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.9979\n",
      "Epoch 45: Train Loss = 2.6660 | Val Loss = 2.6338 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.9264\n",
      "Epoch 46: Train Loss = 2.6426 | Val Loss = 2.6292 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.8622\n",
      "Epoch 47: Train Loss = 2.6396 | Val Loss = 2.6225 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.7706\n",
      "Epoch 48: Train Loss = 2.6376 | Val Loss = 2.6133 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.6435\n",
      "Epoch 49: Train Loss = 2.6245 | Val Loss = 2.6031 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.5060\n",
      "Epoch 50: Train Loss = 2.6115 | Val Loss = 2.5939 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.3821\n",
      "Epoch 51: Train Loss = 2.6084 | Val Loss = 2.5841 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.2514\n",
      "Epoch 52: Train Loss = 2.5963 | Val Loss = 2.5748 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.1293\n",
      "Epoch 53: Train Loss = 2.5914 | Val Loss = 2.5672 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.0292\n",
      "Epoch 54: Train Loss = 2.5798 | Val Loss = 2.5593 | Acc = 1.0000 | AUC = undefined | Perplexity = 12.9274\n",
      "Epoch 55: Train Loss = 2.5784 | Val Loss = 2.5516 | Acc = 1.0000 | AUC = undefined | Perplexity = 12.8277\n",
      "Epoch 56: Train Loss = 2.5950 | Val Loss = 2.5443 | Acc = 1.0000 | AUC = undefined | Perplexity = 12.7345\n",
      "Epoch 57: Train Loss = 2.5503 | Val Loss = 2.5398 | Acc = 1.0000 | AUC = undefined | Perplexity = 12.6770\n",
      "Epoch 58: Train Loss = 2.5446 | Val Loss = 2.5328 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.5890\n",
      "Epoch 59: Train Loss = 2.5563 | Val Loss = 2.5268 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.5139\n",
      "Epoch 60: Train Loss = 2.5294 | Val Loss = 2.5205 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.4349\n",
      "Epoch 61: Train Loss = 2.5298 | Val Loss = 2.5114 | Acc = 1.0000 | AUC = undefined | Perplexity = 12.3216\n",
      "Epoch 62: Train Loss = 2.5139 | Val Loss = 2.5025 | Acc = 1.0000 | AUC = undefined | Perplexity = 12.2133\n",
      "Epoch 63: Train Loss = 2.5067 | Val Loss = 2.4942 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.1119\n",
      "Epoch 64: Train Loss = 2.4973 | Val Loss = 2.4856 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.0087\n",
      "Epoch 65: Train Loss = 2.4888 | Val Loss = 2.4792 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.9320\n",
      "Epoch 66: Train Loss = 2.4819 | Val Loss = 2.4703 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.8259\n",
      "Epoch 67: Train Loss = 2.4812 | Val Loss = 2.4647 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.7594\n",
      "Epoch 68: Train Loss = 2.4728 | Val Loss = 2.4551 | Acc = 1.0000 | AUC = undefined | Perplexity = 11.6474\n",
      "Epoch 69: Train Loss = 2.4583 | Val Loss = 2.4476 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.5611\n",
      "Epoch 70: Train Loss = 2.4495 | Val Loss = 2.4391 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.4627\n",
      "Epoch 71: Train Loss = 2.4416 | Val Loss = 2.4325 | Acc = 1.0000 | AUC = undefined | Perplexity = 11.3870\n",
      "Epoch 72: Train Loss = 2.4308 | Val Loss = 2.4232 | Acc = 1.0000 | AUC = undefined | Perplexity = 11.2821\n",
      "Epoch 73: Train Loss = 2.4636 | Val Loss = 2.4163 | Acc = 1.0000 | AUC = undefined | Perplexity = 11.2047\n",
      "Epoch 74: Train Loss = 2.4474 | Val Loss = 2.4095 | Acc = 1.0000 | AUC = undefined | Perplexity = 11.1282\n",
      "Epoch 75: Train Loss = 2.4127 | Val Loss = 2.4055 | Acc = 1.0000 | AUC = undefined | Perplexity = 11.0841\n",
      "Epoch 76: Train Loss = 2.4208 | Val Loss = 2.3999 | Acc = 1.0000 | AUC = undefined | Perplexity = 11.0218\n",
      "Epoch 77: Train Loss = 2.4019 | Val Loss = 2.3887 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.8997\n",
      "Epoch 78: Train Loss = 2.3959 | Val Loss = 2.3792 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.7961\n",
      "Epoch 79: Train Loss = 2.3876 | Val Loss = 2.3703 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.7004\n",
      "Epoch 80: Train Loss = 2.3825 | Val Loss = 2.3616 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.6080\n",
      "Epoch 81: Train Loss = 2.3663 | Val Loss = 2.3541 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.5287\n",
      "Epoch 82: Train Loss = 2.3896 | Val Loss = 2.3455 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.4386\n",
      "Epoch 83: Train Loss = 2.3580 | Val Loss = 2.3387 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.3675\n",
      "Epoch 84: Train Loss = 2.3499 | Val Loss = 2.3314 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.2924\n",
      "Epoch 85: Train Loss = 2.3719 | Val Loss = 2.3243 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.2193\n",
      "Epoch 86: Train Loss = 2.3494 | Val Loss = 2.3194 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.1694\n",
      "Epoch 87: Train Loss = 2.3235 | Val Loss = 2.3149 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.1239\n",
      "Epoch 88: Train Loss = 2.3271 | Val Loss = 2.3099 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.0735\n",
      "Epoch 89: Train Loss = 2.3145 | Val Loss = 2.3020 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.9942\n",
      "Epoch 90: Train Loss = 2.3311 | Val Loss = 2.2942 | Acc = 1.0000 | AUC = undefined | Perplexity = 9.9165\n",
      "Epoch 91: Train Loss = 2.3094 | Val Loss = 2.2890 | Acc = 1.0000 | AUC = undefined | Perplexity = 9.8650\n",
      "Epoch 92: Train Loss = 2.2921 | Val Loss = 2.2819 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.7951\n",
      "Epoch 93: Train Loss = 2.2894 | Val Loss = 2.2730 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.7083\n",
      "Epoch 94: Train Loss = 2.2982 | Val Loss = 2.2643 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.6244\n",
      "Epoch 95: Train Loss = 2.2685 | Val Loss = 2.2563 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.5476\n",
      "Epoch 96: Train Loss = 2.2710 | Val Loss = 2.2482 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.4704\n",
      "Epoch 97: Train Loss = 2.2558 | Val Loss = 2.2408 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.4007\n",
      "Epoch 98: Train Loss = 2.2447 | Val Loss = 2.2330 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.3280\n",
      "Epoch 99: Train Loss = 2.2356 | Val Loss = 2.2252 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.2558\n",
      "Epoch 100: Train Loss = 2.2349 | Val Loss = 2.2191 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.1993\n",
      "Epoch 101: Train Loss = 2.2256 | Val Loss = 2.2126 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.1393\n",
      "Epoch 102: Train Loss = 2.2140 | Val Loss = 2.2032 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.0540\n",
      "Epoch 103: Train Loss = 2.2750 | Val Loss = 2.1954 | Acc = 0.9730 | AUC = undefined | Perplexity = 8.9840\n",
      "Epoch 104: Train Loss = 2.1984 | Val Loss = 2.1906 | Acc = 0.9730 | AUC = undefined | Perplexity = 8.9408\n",
      "Epoch 105: Train Loss = 2.2011 | Val Loss = 2.1854 | Acc = 0.9730 | AUC = undefined | Perplexity = 8.8945\n",
      "Epoch 106: Train Loss = 2.1993 | Val Loss = 2.1786 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.8340\n",
      "Epoch 107: Train Loss = 2.1818 | Val Loss = 2.1711 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.7675\n",
      "Epoch 108: Train Loss = 2.1827 | Val Loss = 2.1620 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.6887\n",
      "Epoch 109: Train Loss = 2.1634 | Val Loss = 2.1533 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.6136\n",
      "Epoch 110: Train Loss = 2.1652 | Val Loss = 2.1449 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.5410\n",
      "Epoch 111: Train Loss = 2.1524 | Val Loss = 2.1367 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.4716\n",
      "Epoch 112: Train Loss = 2.1420 | Val Loss = 2.1274 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.3931\n",
      "Epoch 113: Train Loss = 2.1333 | Val Loss = 2.1182 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.3161\n",
      "Epoch 114: Train Loss = 2.1292 | Val Loss = 2.1101 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.2489\n",
      "Epoch 115: Train Loss = 2.1189 | Val Loss = 2.1015 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.1785\n",
      "Epoch 116: Train Loss = 2.1122 | Val Loss = 2.0933 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.1114\n",
      "Epoch 117: Train Loss = 2.1236 | Val Loss = 2.0853 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.0474\n",
      "Epoch 118: Train Loss = 2.0946 | Val Loss = 2.0792 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.9984\n",
      "Epoch 119: Train Loss = 2.1021 | Val Loss = 2.0729 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.9477\n",
      "Epoch 120: Train Loss = 2.0832 | Val Loss = 2.0662 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.8949\n",
      "Epoch 121: Train Loss = 2.0759 | Val Loss = 2.0608 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.8526\n",
      "Epoch 122: Train Loss = 2.0660 | Val Loss = 2.0557 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.8122\n",
      "Epoch 123: Train Loss = 2.0782 | Val Loss = 2.0490 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.7602\n",
      "Epoch 124: Train Loss = 2.1082 | Val Loss = 2.0430 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.7137\n",
      "Epoch 125: Train Loss = 2.0483 | Val Loss = 2.0408 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.6970\n",
      "Epoch 126: Train Loss = 2.0425 | Val Loss = 2.0345 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.6485\n",
      "Epoch 127: Train Loss = 2.0434 | Val Loss = 2.0258 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.5823\n",
      "Epoch 128: Train Loss = 2.0385 | Val Loss = 2.0175 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.5194\n",
      "Epoch 129: Train Loss = 2.0277 | Val Loss = 2.0098 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.4622\n",
      "Epoch 130: Train Loss = 2.0130 | Val Loss = 2.0017 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.4014\n",
      "Epoch 131: Train Loss = 2.0067 | Val Loss = 1.9940 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.3445\n",
      "Epoch 132: Train Loss = 2.0008 | Val Loss = 1.9848 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.2772\n",
      "Epoch 133: Train Loss = 2.0082 | Val Loss = 1.9777 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.2261\n",
      "Epoch 134: Train Loss = 1.9836 | Val Loss = 1.9702 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.1718\n",
      "Epoch 135: Train Loss = 1.9767 | Val Loss = 1.9620 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.1137\n",
      "Epoch 136: Train Loss = 1.9702 | Val Loss = 1.9527 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.0480\n",
      "Epoch 137: Train Loss = 1.9637 | Val Loss = 1.9446 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.9906\n",
      "Epoch 138: Train Loss = 1.9978 | Val Loss = 1.9389 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.9511\n",
      "Epoch 139: Train Loss = 1.9570 | Val Loss = 1.9342 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.9183\n",
      "Epoch 140: Train Loss = 1.9409 | Val Loss = 1.9281 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.8762\n",
      "Epoch 141: Train Loss = 1.9348 | Val Loss = 1.9215 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.8313\n",
      "Epoch 142: Train Loss = 1.9276 | Val Loss = 1.9155 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.7907\n",
      "Epoch 143: Train Loss = 1.9230 | Val Loss = 1.9077 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.7375\n",
      "Epoch 144: Train Loss = 1.9150 | Val Loss = 1.9002 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.6870\n",
      "Epoch 145: Train Loss = 1.9063 | Val Loss = 1.8931 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.6398\n",
      "Epoch 146: Train Loss = 1.9122 | Val Loss = 1.8872 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.6008\n",
      "Epoch 147: Train Loss = 1.8927 | Val Loss = 1.8827 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.5709\n",
      "Epoch 148: Train Loss = 1.9031 | Val Loss = 1.8762 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.5285\n",
      "Epoch 149: Train Loss = 1.8764 | Val Loss = 1.8692 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.4828\n",
      "Epoch 150: Train Loss = 1.8709 | Val Loss = 1.8619 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.4357\n",
      "Epoch 151: Train Loss = 1.8648 | Val Loss = 1.8560 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.3979\n",
      "Epoch 152: Train Loss = 1.8601 | Val Loss = 1.8504 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.3621\n",
      "Epoch 153: Train Loss = 1.8561 | Val Loss = 1.8434 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.3180\n",
      "Epoch 154: Train Loss = 1.8456 | Val Loss = 1.8374 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.2801\n",
      "Epoch 155: Train Loss = 1.8567 | Val Loss = 1.8304 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.2364\n",
      "Epoch 156: Train Loss = 1.8307 | Val Loss = 1.8251 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.2036\n",
      "Epoch 157: Train Loss = 1.8239 | Val Loss = 1.8195 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.1691\n",
      "Epoch 158: Train Loss = 1.8193 | Val Loss = 1.8128 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.1274\n",
      "Epoch 159: Train Loss = 1.8113 | Val Loss = 1.8045 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.0771\n",
      "Epoch 160: Train Loss = 1.8035 | Val Loss = 1.7968 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.0304\n",
      "Epoch 161: Train Loss = 1.7982 | Val Loss = 1.7886 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.9808\n",
      "Epoch 162: Train Loss = 1.7900 | Val Loss = 1.7809 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.9352\n",
      "Epoch 163: Train Loss = 1.8297 | Val Loss = 1.7744 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.8969\n",
      "Epoch 164: Train Loss = 1.7981 | Val Loss = 1.7703 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.8728\n",
      "Epoch 165: Train Loss = 1.7721 | Val Loss = 1.7659 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.8469\n",
      "Epoch 166: Train Loss = 1.7659 | Val Loss = 1.7589 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.8059\n",
      "Epoch 167: Train Loss = 1.7612 | Val Loss = 1.7505 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.7574\n",
      "Epoch 168: Train Loss = 1.7607 | Val Loss = 1.7435 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.7172\n",
      "Epoch 169: Train Loss = 1.7443 | Val Loss = 1.7349 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.6682\n",
      "Epoch 170: Train Loss = 1.7472 | Val Loss = 1.7264 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.6204\n",
      "Epoch 171: Train Loss = 1.7341 | Val Loss = 1.7184 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.5757\n",
      "Epoch 172: Train Loss = 1.7259 | Val Loss = 1.7127 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.5439\n",
      "Epoch 173: Train Loss = 1.7288 | Val Loss = 1.7062 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.5082\n",
      "Epoch 174: Train Loss = 1.7127 | Val Loss = 1.6994 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.4709\n",
      "Epoch 175: Train Loss = 1.7045 | Val Loss = 1.6909 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.4241\n",
      "Epoch 176: Train Loss = 1.6983 | Val Loss = 1.6833 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.3831\n",
      "Epoch 177: Train Loss = 1.6939 | Val Loss = 1.6762 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.3453\n",
      "Epoch 178: Train Loss = 1.6844 | Val Loss = 1.6703 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.3135\n",
      "Epoch 179: Train Loss = 1.6820 | Val Loss = 1.6652 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.2866\n",
      "Epoch 180: Train Loss = 1.6740 | Val Loss = 1.6607 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.2629\n",
      "Epoch 181: Train Loss = 1.6683 | Val Loss = 1.6559 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.2379\n",
      "Epoch 182: Train Loss = 1.6611 | Val Loss = 1.6512 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.2132\n",
      "Epoch 183: Train Loss = 1.6528 | Val Loss = 1.6444 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.1781\n",
      "Epoch 184: Train Loss = 1.6737 | Val Loss = 1.6381 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.1451\n",
      "Epoch 185: Train Loss = 1.6432 | Val Loss = 1.6322 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.1151\n",
      "Epoch 186: Train Loss = 1.6352 | Val Loss = 1.6270 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.0885\n",
      "Epoch 187: Train Loss = 1.6282 | Val Loss = 1.6202 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.0543\n",
      "Epoch 188: Train Loss = 1.6267 | Val Loss = 1.6119 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.0124\n",
      "Epoch 189: Train Loss = 1.6178 | Val Loss = 1.6055 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.9803\n",
      "Epoch 190: Train Loss = 1.6104 | Val Loss = 1.6005 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.9554\n",
      "Epoch 191: Train Loss = 1.6173 | Val Loss = 1.5966 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.9360\n",
      "Epoch 192: Train Loss = 1.5995 | Val Loss = 1.5906 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.9067\n",
      "Epoch 193: Train Loss = 1.5906 | Val Loss = 1.5851 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.8798\n",
      "Epoch 194: Train Loss = 1.6059 | Val Loss = 1.5797 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.8534\n",
      "Epoch 195: Train Loss = 1.5800 | Val Loss = 1.5751 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.8311\n",
      "Epoch 196: Train Loss = 1.5721 | Val Loss = 1.5680 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.7971\n",
      "Epoch 197: Train Loss = 1.5688 | Val Loss = 1.5605 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.7611\n",
      "Epoch 198: Train Loss = 1.5586 | Val Loss = 1.5539 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.7299\n",
      "Epoch 199: Train Loss = 1.5566 | Val Loss = 1.5455 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.6903\n",
      "Epoch 200: Train Loss = 1.5451 | Val Loss = 1.5390 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.6597\n",
      " Final Test Metrics:\n",
      "Loss = 1.5390, Accuracy = 1.0000, AUC = undefined, Perplexity = 4.6597\n",
      "Model saved to final_amp_generator_lstm.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_best_params_tb[\"lr\"], weight_decay=lstm_gen_best_params_tb[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "# lstm_gen_best_params = {'hidden_dim': 133, 'num_layers': 2}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params_tb[\"dropout\"],\n",
    "    # weights_decay=lstm_gen_best_params_tb[\"weight_decay\"],\n",
    "    # lr=lstm_gen_best_params_tb[\"lr\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=200)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\" Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n",
    "# Save model weights\n",
    "torch.save(trained_model.state_dict(), \"final_amp_generator_lstm.pt\")\n",
    "print(\"Model saved to final_amp_generator_lstm.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Recreate your amino acid vocab\n",
    "aa_vocab = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aa_to_idx = {aa: i for i, aa in enumerate(aa_vocab)}\n",
    "idx_to_aa = {i: aa for aa, i in aa_to_idx.items()}\n",
    "\n",
    "def one_hot_encode_amino_acid(aa, vocab=aa_vocab):\n",
    "    vec = torch.zeros(len(vocab))\n",
    "    vec[aa_to_idx[aa]] = 1.0\n",
    "    return vec\n",
    "\n",
    "def generate_sequence_from_seed(model, seed, max_length=30, temperature=1.0, device='cpu'):\n",
    "    model.eval()\n",
    "    input_seq = [one_hot_encode_amino_acid(aa).to(device) for aa in seed]\n",
    "    input_tensor = torch.stack(input_seq).unsqueeze(0)  # [1, L, 20]\n",
    "\n",
    "    generated = seed.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(seed)):\n",
    "            output = model(input_tensor)  # [1, L, vocab]\n",
    "            logits = output[0, -1, :]  # Last time step → [vocab]\n",
    "\n",
    "            # Apply temperature and sample\n",
    "            probs = F.softmax(logits / temperature, dim=-1).cpu().numpy()\n",
    "            next_idx = np.random.choice(len(aa_vocab), p=probs)\n",
    "            next_aa = idx_to_aa[next_idx]\n",
    "\n",
    "            # Update sequence\n",
    "            next_aa_vec = one_hot_encode_amino_acid(next_aa).to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, 20]\n",
    "            input_tensor = torch.cat([input_tensor, next_aa_vec], dim=1)\n",
    "            generated.append(next_aa)\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class LengthSampler:\n",
    "    def __init__(self, sequence_lengths):\n",
    "        \"\"\"\n",
    "        Initialize sampler from observed sequence lengths.\n",
    "        \n",
    "        Args:\n",
    "            sequence_lengths (list[int]): List of sequence lengths (e.g., [20, 21, 20, 23, ...])\n",
    "        \"\"\"\n",
    "        self.length_counts = Counter(sequence_lengths)\n",
    "        self.lengths = np.array(sorted(self.length_counts.keys()))\n",
    "        counts = np.array([self.length_counts[l] for l in self.lengths])\n",
    "        self.probs = counts / counts.sum()  # Empirical probabilities\n",
    "\n",
    "    def sample(self, n=1):\n",
    "        \"\"\"\n",
    "        Sample one or more lengths based on the learned distribution.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray of sampled lengths\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.lengths, size=n, p=self.probs)\n",
    "length_sampler = LengthSampler([len(seq) for seq in df.loc[df['AMP'] == 1, :]['Sequences']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EWQMDDAVNPAYRGGG\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    # dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "gen_model.eval()\n",
    "# Define a seed and generate a sequence\n",
    "sampled_length = length_sampler.sample()[0]\n",
    "start_aa = sample_start_amino_acid()\n",
    "seed_sequence = list(start_aa)  # start with valine\n",
    "generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1.0, device=device)\n",
    "\n",
    "print(\"Generated AMP sequence:\", generated_peptide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EWQNDDAVNPAYSGGG\n",
      "Generated AMP sequence: ALIGNDGHKRGKLD\n",
      "Generated AMP sequence: KCAYYSGDPKFKDYHPHKKG\n",
      "Generated AMP sequence: IRWVNWCGAHHHSHHKGPGYPGDPPPPGHGTLHGHHPPY\n",
      "Generated AMP sequence: IAQRMRKKIAECPHKYGHP\n",
      "Generated AMP sequence: FAFDWSPVSGYK\n",
      "Generated AMP sequence: EVGAFISTAKHGGHYHKPHYYGKHHF\n",
      "Generated AMP sequence: VKAFWGDKYGPPGPHPPKGR\n",
      "Generated AMP sequence: DCAMPAKGPGPHYG\n",
      "Generated AMP sequence: WAWVFPSLKGDYYP\n",
      "Generated AMP sequence: QGQVVRPCFYNAGP\n",
      "Generated AMP sequence: CCLPPFQGHQ\n",
      "Generated AMP sequence: ATPMAHGGYHYPRKKKGPHD\n",
      "Generated AMP sequence: DCWYWHAYIYYTHHSHGKYP\n",
      "Generated AMP sequence: HANYCKVQPPHHRRWYKKPP\n",
      "Generated AMP sequence: ISVGHCMAKKHMDDRHGKPG\n",
      "Generated AMP sequence: TAALLPQYKHRHIGDYRPHG\n",
      "Generated AMP sequence: YELQPGYQLPHG\n",
      "Generated AMP sequence: ARAAAATPKDKKGHH\n",
      "Generated AMP sequence: VNAHPKTPFDPCLYKHPHKY\n",
      "Generated AMP sequence: HYVDACADPDHSCRHG\n",
      "Generated AMP sequence: VNVQSGERSYHHQHYTHPPG\n",
      "Generated AMP sequence: QKSGVHAWDHYYKPIHHPPPPGKGKHYHGGPKGG\n",
      "Generated AMP sequence: IASQAAYHHSYYPHGPKHYG\n",
      "Generated AMP sequence: RAKACCPQMYHHWGYDYEYK\n",
      "Generated AMP sequence: WALYLPPKPMYDHYYINHGHHKGYYPKHPPGYPYPKHYVG\n",
      "Generated AMP sequence: KGSYDNHYTS\n",
      "Generated AMP sequence: AHFATSYYLRYTGKGYLGP\n",
      "Generated AMP sequence: GGAPLRKTLLWHGDPPPGGD\n",
      "Generated AMP sequence: QMHIWHKRHPVYGYK\n",
      "Generated AMP sequence: MIYKGPGCEFGGP\n",
      "Generated AMP sequence: KGVKPDFAGGGE\n",
      "Generated AMP sequence: FIDHKPASPDW\n",
      "Generated AMP sequence: HAFSREFHKPHKPDGPYKKGHKGHHDHGHGYKPPK\n",
      "Generated AMP sequence: MLMRIDGHPLH\n",
      "Generated AMP sequence: ENEADGEFGGYDKHYGHYWPHGPYKKHPGHHKGGKHKGK\n",
      "Generated AMP sequence: DAGCAYHSGPQMKHHYRYGP\n",
      "Generated AMP sequence: PCARMTDRGGGRPKHYHQHHHHPKGYHKYLGYNHGP\n",
      "Generated AMP sequence: ELVSDGGQALQWHRGSGHPG\n",
      "Generated AMP sequence: NQQPPLGHGYMH\n",
      "Generated AMP sequence: NWCMKNAVYLPYPGKLHPY\n",
      "Generated AMP sequence: YIAYTDYVKNHDHRAHHKYHHPHGHGGKHYHKPDP\n",
      "Generated AMP sequence: KYCHAYKNCQGY\n",
      "Generated AMP sequence: CCAKMARKKKHK\n",
      "Generated AMP sequence: RCTYHGPHAGPP\n",
      "Generated AMP sequence: VDEPACSFPG\n",
      "Generated AMP sequence: EEQTSHPGHYA\n",
      "Generated AMP sequence: PDADMIWSHG\n",
      "Generated AMP sequence: DMFNHLIGYQGVKYQH\n",
      "Generated AMP sequence: VELPFDSYKG\n",
      "Generated AMP sequence: LAWAMGLPSGAGY\n",
      "Generated AMP sequence: YMNPDWIHKCGPFMGHHHPHKKPYPGDHKGK\n",
      "Generated AMP sequence: NGRAAQKPKGSRPHKHGYGY\n",
      "Generated AMP sequence: WCLVQSPPTGKGYYDPYG\n",
      "Generated AMP sequence: HWAQGYYYKVSHSDLGGGPH\n",
      "Generated AMP sequence: DAGLSGPWPGAWCYKYPYHPHK\n",
      "Generated AMP sequence: CVYRIIQGDHHHGDDYHHPG\n",
      "Generated AMP sequence: ISAIVYKPFYGYPNKGGGGPHGYYYYGHPPGPGGKKK\n",
      "Generated AMP sequence: LVFCVYTSPLD\n",
      "Generated AMP sequence: DGEQKAGPDTKKLRHPK\n",
      "Generated AMP sequence: IVSPTVPSPPPPG\n",
      "Generated AMP sequence: EVASCHQESSKAHPYPHPKPGPKYHPYGYPGH\n",
      "Generated AMP sequence: PAMGNAVYYQGPCDHKPPHHY\n",
      "Generated AMP sequence: KICSPFCPPGYGHYHPHH\n",
      "Generated AMP sequence: RHVEEAPHVKYGWPPSPPGDYKPKGGPHGKGKHPHHDGH\n",
      "Generated AMP sequence: NMFPTRAKDGYGKPPKDHKG\n",
      "Generated AMP sequence: GAAHCMPSGGEPPDY\n",
      "Generated AMP sequence: NLQVQSHSDY\n",
      "Generated AMP sequence: NSIPLQCAGGVGYHGPPKKP\n",
      "Generated AMP sequence: HYYPEPAAGKVP\n",
      "Generated AMP sequence: KHGHYAVMKPKWYHHKYGNPPGP\n",
      "Generated AMP sequence: DCEYDTKGVKKHLGHYHPHPGYHGGGKHPHGKPGG\n",
      "Generated AMP sequence: YQPKTSMVGE\n",
      "Generated AMP sequence: GALWACKYHKCGY\n",
      "Generated AMP sequence: VAYPVKKKPFAHPGPYGPHWGPRYPKKYKDYPPGPHPG\n",
      "Generated AMP sequence: IYAYHVRMQWHHPRPPHQGK\n",
      "Generated AMP sequence: GFHHPHKGHG\n",
      "Generated AMP sequence: RNPNKHVKKA\n",
      "Generated AMP sequence: PHHMLNRSPYAGAP\n",
      "Generated AMP sequence: KEWAYHCKGHHRKHMPHHGGPGKPGKHYKPKGPY\n",
      "Generated AMP sequence: VSVEENHTWHGQHRHPGPHK\n",
      "Generated AMP sequence: IQTEAAPNLGHM\n",
      "Generated AMP sequence: MYAPHIQPYFEPKHGGYGHG\n",
      "Generated AMP sequence: CALASFLVPKHHPYP\n",
      "Generated AMP sequence: IQKAMYEPGKEKPHPPYGHG\n",
      "Generated AMP sequence: CWLTLPCRESRPDHHHGYKHHHKKYKYSGYGPHYWPPHGH\n",
      "Generated AMP sequence: MQGYPSDVYKNQ\n",
      "Generated AMP sequence: PKHCHAADYLYI\n",
      "Generated AMP sequence: KKIAPFPPFDRKDY\n",
      "Generated AMP sequence: DQYEDCGDPD\n",
      "Generated AMP sequence: HYSNGPLIEWKGHHRPPDHK\n",
      "Generated AMP sequence: WERCLFAGYR\n",
      "Generated AMP sequence: MKALKTCKFHPKHHKPYYPHPHHRFYHPPYPKGKGPKP\n",
      "Generated AMP sequence: HAQAAAYQHH\n",
      "Generated AMP sequence: SRPDDCPRGADYHPP\n",
      "Generated AMP sequence: PDANRPLAYRHYMKPQ\n",
      "Generated AMP sequence: RAGWEHIKPYGGGYPHQTSY\n",
      "Generated AMP sequence: FRKTQRPFLYYD\n",
      "Generated AMP sequence: KAQSEKTQLMKH\n",
      "Generated AMP sequence: FPVPRDKAGPYFHRGKPLGH\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    # dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "gen_model.eval()\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1.2, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-24 10:28:25,991] A new study created in memory with name: no-name-b18246ca-b8ce-44e6-8d69-a246afdf7863\n",
      "[I 2025-04-24 10:28:30,732] Trial 0 finished with value: 0.19571612775325775 and parameters: {'dropout': 0.3738698734696153, 'lr': 0.008929602955292291, 'weight_decay': 0.00038939709500130466}. Best is trial 0 with value: 0.19571612775325775.\n",
      "[I 2025-04-24 10:28:35,480] Trial 1 finished with value: 0.19314280152320862 and parameters: {'dropout': 0.41696652333028095, 'lr': 0.003128895078184863, 'weight_decay': 0.0006020832987418363}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:28:40,667] Trial 2 finished with value: 0.20010095834732056 and parameters: {'dropout': 0.3497662374507716, 'lr': 0.0034644974046034883, 'weight_decay': 0.000795963699494901}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:28:45,413] Trial 3 finished with value: 0.1991792917251587 and parameters: {'dropout': 0.18578045936297793, 'lr': 0.007855076852649084, 'weight_decay': 0.0008527308389421976}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:28:50,556] Trial 4 finished with value: 0.20735089480876923 and parameters: {'dropout': 0.472297763270189, 'lr': 0.0005552162369158968, 'weight_decay': 0.0009424710391079003}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:28:55,933] Trial 5 finished with value: 0.1988036185503006 and parameters: {'dropout': 0.3116132751223898, 'lr': 0.0029927531405799126, 'weight_decay': 0.0007066362898829478}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:29:00,846] Trial 6 finished with value: 0.19765393435955048 and parameters: {'dropout': 0.46104617368101997, 'lr': 0.00523573379133449, 'weight_decay': 0.00025990269733392336}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:29:06,121] Trial 7 finished with value: 0.19824309647083282 and parameters: {'dropout': 0.44039124219899173, 'lr': 0.002993593670443116, 'weight_decay': 0.00021353974984615446}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:29:10,732] Trial 8 finished with value: 0.20441488921642303 and parameters: {'dropout': 0.13427775182816687, 'lr': 0.00046516719585996285, 'weight_decay': 0.0007191430093534227}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:29:16,448] Trial 9 finished with value: 0.19956831634044647 and parameters: {'dropout': 0.19823770617408054, 'lr': 0.005571071705944005, 'weight_decay': 0.000948495158061724}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:29:21,045] Trial 10 finished with value: 0.20837371051311493 and parameters: {'dropout': 0.26610596995482433, 'lr': 0.006497115841708902, 'weight_decay': 1.2841546919701565e-05}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:29:25,793] Trial 11 finished with value: 0.19527597725391388 and parameters: {'dropout': 0.39083052069977336, 'lr': 0.009915385028490922, 'weight_decay': 0.0004983304037169666}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:29:30,339] Trial 12 finished with value: 0.19797439873218536 and parameters: {'dropout': 0.3993149954575364, 'lr': 0.009932751534276382, 'weight_decay': 0.0005717642355733969}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:29:34,872] Trial 13 finished with value: 0.20281997323036194 and parameters: {'dropout': 0.49736833674050074, 'lr': 0.002197686535096312, 'weight_decay': 0.00050422148043294}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:29:40,002] Trial 14 finished with value: 0.19503800570964813 and parameters: {'dropout': 0.41391286005940475, 'lr': 0.007077769078336783, 'weight_decay': 0.0006180490301796036}. Best is trial 1 with value: 0.19314280152320862.\n",
      "[I 2025-04-24 10:29:44,912] Trial 15 finished with value: 0.1921045035123825 and parameters: {'dropout': 0.2787933989664577, 'lr': 0.007081024979360626, 'weight_decay': 0.000648528973755982}. Best is trial 15 with value: 0.1921045035123825.\n",
      "[I 2025-04-24 10:29:49,565] Trial 16 finished with value: 0.19863441586494446 and parameters: {'dropout': 0.28216544931831533, 'lr': 0.004286476686593491, 'weight_decay': 0.00039972827000662515}. Best is trial 15 with value: 0.1921045035123825.\n",
      "[I 2025-04-24 10:29:54,446] Trial 17 finished with value: 0.20168893039226532 and parameters: {'dropout': 0.2534598611084732, 'lr': 0.00616092408252056, 'weight_decay': 0.0006642931632829692}. Best is trial 15 with value: 0.1921045035123825.\n",
      "[I 2025-04-24 10:29:59,504] Trial 18 finished with value: 0.19535349309444427 and parameters: {'dropout': 0.33213318593498653, 'lr': 0.00786841081024415, 'weight_decay': 0.0007970781801494686}. Best is trial 15 with value: 0.1921045035123825.\n",
      "[I 2025-04-24 10:30:05,359] Trial 19 finished with value: 0.19913265109062195 and parameters: {'dropout': 0.2253376754675266, 'lr': 0.0018724751077374242, 'weight_decay': 0.00034785176957708016}. Best is trial 15 with value: 0.1921045035123825.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best transfer learning hyperparameters: {'dropout': 0.2787933989664577, 'lr': 0.007081024979360626, 'weight_decay': 0.000648528973755982}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Criterion\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Transfer Learning Loader\n",
    "def load_pretrained_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    last_token_logits = output[:, -1, :]\n",
    "    last_token_targets = target_seq[:, -1, :]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen-tb/AMP_LSTM_GEN_TRANSFER_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # if train:\n",
    "            #     torch.save(model.state_dict(), 'best_model_lstm_transfer.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "    \n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Optuna objective for fine-tuning\n",
    "def objective_generation(trial):\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(    \n",
    "                hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "                num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "                dropout=dropout\n",
    "                # dropout=lstm_gen_best_params[\"dropout\"]\n",
    "                )\n",
    "    model = load_pretrained_weights(model, 'best_model_lstm_generator_final.pt')  # path to the general AMP model\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=20, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "lstm_gen_frozen_best_params_tb = study.best_trial.params\n",
    "print(\"Best transfer learning hyperparameters:\", lstm_gen_frozen_best_params_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.9309 | Val Loss = 2.9048 | Acc = 0.9730 | AUC = undefined | Perplexity = 18.2613\n",
      "Epoch 2: Train Loss = 2.8908 | Val Loss = 2.8630 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.5148\n",
      "Epoch 3: Train Loss = 2.8480 | Val Loss = 2.8220 | Acc = 0.9730 | AUC = undefined | Perplexity = 16.8105\n",
      "Epoch 4: Train Loss = 2.8080 | Val Loss = 2.7816 | Acc = 0.9730 | AUC = undefined | Perplexity = 16.1445\n",
      "Epoch 5: Train Loss = 2.7681 | Val Loss = 2.7414 | Acc = 0.9730 | AUC = undefined | Perplexity = 15.5082\n",
      "Epoch 6: Train Loss = 2.7323 | Val Loss = 2.7016 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.9036\n",
      "Epoch 7: Train Loss = 2.6880 | Val Loss = 2.6622 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.3281\n",
      "Epoch 8: Train Loss = 2.6532 | Val Loss = 2.6232 | Acc = 0.9730 | AUC = undefined | Perplexity = 13.7791\n",
      "Epoch 9: Train Loss = 2.6103 | Val Loss = 2.5844 | Acc = 0.9730 | AUC = undefined | Perplexity = 13.2557\n",
      "Epoch 10: Train Loss = 2.5796 | Val Loss = 2.5460 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.7558\n",
      "Epoch 11: Train Loss = 2.5351 | Val Loss = 2.5078 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.2774\n",
      "Epoch 12: Train Loss = 2.4972 | Val Loss = 2.4701 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.8238\n",
      "Epoch 13: Train Loss = 2.4653 | Val Loss = 2.4322 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.3842\n",
      "Epoch 14: Train Loss = 2.4235 | Val Loss = 2.3949 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.9672\n",
      "Epoch 15: Train Loss = 2.3865 | Val Loss = 2.3575 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.5646\n",
      "Epoch 16: Train Loss = 2.3453 | Val Loss = 2.3203 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.1786\n",
      "Epoch 17: Train Loss = 2.3181 | Val Loss = 2.2833 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.8095\n",
      "Epoch 18: Train Loss = 2.2733 | Val Loss = 2.2467 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.4563\n",
      "Epoch 19: Train Loss = 2.2455 | Val Loss = 2.2105 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.1207\n",
      "Epoch 20: Train Loss = 2.2046 | Val Loss = 2.1748 | Acc = 0.9730 | AUC = undefined | Perplexity = 8.8003\n",
      "Epoch 21: Train Loss = 2.1667 | Val Loss = 2.1394 | Acc = 0.9730 | AUC = undefined | Perplexity = 8.4945\n",
      "Epoch 22: Train Loss = 2.1539 | Val Loss = 2.1046 | Acc = 0.9730 | AUC = undefined | Perplexity = 8.2036\n",
      "Epoch 23: Train Loss = 2.0981 | Val Loss = 2.0698 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.9230\n",
      "Epoch 24: Train Loss = 2.0650 | Val Loss = 2.0352 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.6539\n",
      "Epoch 25: Train Loss = 2.0280 | Val Loss = 2.0011 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.3970\n",
      "Epoch 26: Train Loss = 1.9873 | Val Loss = 1.9673 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.1511\n",
      "Epoch 27: Train Loss = 1.9624 | Val Loss = 1.9340 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.9169\n",
      "Epoch 28: Train Loss = 1.9289 | Val Loss = 1.9005 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.6893\n",
      "Epoch 29: Train Loss = 1.8909 | Val Loss = 1.8674 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.4711\n",
      "Epoch 30: Train Loss = 1.8554 | Val Loss = 1.8344 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.2611\n",
      "Epoch 31: Train Loss = 1.8242 | Val Loss = 1.8017 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.0602\n",
      "Epoch 32: Train Loss = 1.7933 | Val Loss = 1.7696 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.8688\n",
      "Epoch 33: Train Loss = 1.7627 | Val Loss = 1.7380 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.6857\n",
      "Epoch 34: Train Loss = 1.7495 | Val Loss = 1.7061 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.5076\n",
      "Epoch 35: Train Loss = 1.6950 | Val Loss = 1.6750 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.3388\n",
      "Epoch 36: Train Loss = 1.6742 | Val Loss = 1.6442 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.1768\n",
      "Epoch 37: Train Loss = 1.6412 | Val Loss = 1.6139 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.0224\n",
      "Epoch 38: Train Loss = 1.6145 | Val Loss = 1.5842 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.8755\n",
      "Epoch 39: Train Loss = 1.5822 | Val Loss = 1.5549 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.7348\n",
      "Epoch 40: Train Loss = 1.5688 | Val Loss = 1.5261 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.6000\n",
      "Epoch 41: Train Loss = 1.5123 | Val Loss = 1.4974 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.4702\n",
      "Epoch 42: Train Loss = 1.5030 | Val Loss = 1.4693 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.3462\n",
      "Epoch 43: Train Loss = 1.4543 | Val Loss = 1.4416 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.2273\n",
      "Epoch 44: Train Loss = 1.4300 | Val Loss = 1.4144 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.1140\n",
      "Epoch 45: Train Loss = 1.4101 | Val Loss = 1.3876 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.0051\n",
      "Epoch 46: Train Loss = 1.3889 | Val Loss = 1.3608 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.8995\n",
      "Epoch 47: Train Loss = 1.3504 | Val Loss = 1.3347 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.7990\n",
      "Epoch 48: Train Loss = 1.3241 | Val Loss = 1.3086 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.7011\n",
      "Epoch 49: Train Loss = 1.3068 | Val Loss = 1.2831 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.6078\n",
      "Epoch 50: Train Loss = 1.2713 | Val Loss = 1.2576 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.5168\n",
      "Epoch 51: Train Loss = 1.2534 | Val Loss = 1.2328 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.4307\n",
      "Epoch 52: Train Loss = 1.2331 | Val Loss = 1.2086 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.3487\n",
      "Epoch 53: Train Loss = 1.1986 | Val Loss = 1.1846 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.2694\n",
      "Epoch 54: Train Loss = 1.1820 | Val Loss = 1.1609 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.1930\n",
      "Epoch 55: Train Loss = 1.1579 | Val Loss = 1.1374 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.1187\n",
      "Epoch 56: Train Loss = 1.1337 | Val Loss = 1.1146 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.0482\n",
      "Epoch 57: Train Loss = 1.1230 | Val Loss = 1.0923 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.9812\n",
      "Epoch 58: Train Loss = 1.0842 | Val Loss = 1.0708 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.9178\n",
      "Epoch 59: Train Loss = 1.0634 | Val Loss = 1.0496 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.8565\n",
      "Epoch 60: Train Loss = 1.0559 | Val Loss = 1.0284 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.7966\n",
      " Final Test Metrics:\n",
      "Loss = 1.0284, Accuracy = 0.9730, AUC = undefined, Perplexity = 2.7966\n",
      "Model saved to final_amp_frozen_generator_lstm.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_frozen_best_params_tb[\"lr\"], weight_decay=lstm_gen_frozen_best_params_tb[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-frozen-gen-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "# lstm_gen_best_params = {'hidden_dim': 133, 'num_layers': 2, 'dropout': 0.10063270147175422, 'lr': 0.003237280156212186, 'weight_decay': 2.2594437829479466e-05}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_frozen_best_params_tb[\"dropout\"],\n",
    "    # weights_decay=lstm_gen_frozen_best_params_tb[\"weight_decay\"],\n",
    "    # lr=lstm_gen_frozen_best_params_tb[\"lr\"]\n",
    ")\n",
    "freeze_encoder(final_model)\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\" Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n",
    "# Save model weights\n",
    "torch.save(trained_model.state_dict(), \"final_amp_frozen_generator_lstm.pt\")\n",
    "print(\"Model saved to final_amp_frozen_generator_lstm.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EWPLADAVLPDYPGGG\n",
      "Generated AMP sequence: AHGEMCGHIPGKKG\n",
      "Generated AMP sequence: KAAYYRGDPHGKGYGPHKKG\n",
      "Generated AMP sequence: IQWVLYAGCHHHPHHKGPGYPGDPPPPGHGPKHGHHPKR\n",
      "Generated AMP sequence: IAPQKQKKHCGDPHKYGHP\n",
      "Generated AMP sequence: FADAYRPVRGYK\n",
      "Generated AMP sequence: EVDADHRTAKHGGHYHKPHYYGHHHG\n",
      "Generated AMP sequence: VHADWGDKYGPPGPHKKKGP\n",
      "Generated AMP sequence: DAALPAKGPGPHYG\n",
      "Generated AMP sequence: WAWTEPRKKGGYYK\n",
      "Generated AMP sequence: QDPVVRPDGYKCGP\n",
      "Generated AMP sequence: CAKPPFPGHP\n",
      "Generated AMP sequence: ASNKAHGGYHYNPKKHGPHG\n",
      "Generated AMP sequence: DAWYWHAYHYYRHHPHGKYP\n",
      "Generated AMP sequence: HALYAKVPPPHHPPRYKHPK\n",
      "Generated AMP sequence: IRVFGALAKKHKFGPHGKPG\n",
      "Generated AMP sequence: TAAKKNPYKHPHHGGYPPHG\n",
      "Generated AMP sequence: YAKPPGYPKLHG\n",
      "Generated AMP sequence: AQAAAATPKGKIGHH\n",
      "Generated AMP sequence: VLAGMKTPGEPDKYKHKHKY\n",
      "Generated AMP sequence: HYVAAAADPEHRDPHG\n",
      "Generated AMP sequence: VLTPRGFPRYHHPHYPHPPG\n",
      "Generated AMP sequence: QHSFVHAYEHYYKKHHHPPPPGHGKHRHGGPKGG\n",
      "Generated AMP sequence: IASPAAYHHQYYPHGPKHYG\n",
      "Generated AMP sequence: RAHAAAPPKYHHSGYDYGYK\n",
      "Generated AMP sequence: WAKYKNPIPKYGHYYHKHGHHKGYYPKHPPGYPYPKHYPG\n",
      "Generated AMP sequence: KDRYALHYSR\n",
      "Generated AMP sequence: AFCATRYYKPYRGHGYKGP\n",
      "Generated AMP sequence: GDAPKQKSKKYHGGPKPGGG\n",
      "Generated AMP sequence: QKFHWGKQHNTYGYH\n",
      "Generated AMP sequence: MGYIGNGDGGGGP\n",
      "Generated AMP sequence: KDVHPDGAGHGG\n",
      "Generated AMP sequence: FGAGKPARPGY\n",
      "Generated AMP sequence: HACRQDGHKNHHPGGPYKKGHKGHHGHGHGYKPPH\n",
      "Generated AMP sequence: MIKQHCGHPKH\n",
      "Generated AMP sequence: EKAAAGEGHGYGKHYGHYPPGGPYKKHPGHHHGGKHKGH\n",
      "Generated AMP sequence: DAEAAYHRGPPKHHHYPYGP\n",
      "Generated AMP sequence: PAAPKSDRGGGPPKHSHPHHHHPHGYHKYKGYKHGP\n",
      "Generated AMP sequence: EHVRAGGPAKPYHPGPGHPG\n",
      "Generated AMP sequence: NPPMPKGHGYKH\n",
      "Generated AMP sequence: NWALKMAVYKPYPGKKHPY\n",
      "Generated AMP sequence: YGAYSAYVKKHGHPDHHKYHHPHGHGGHHYHKKGP\n",
      "Generated AMP sequence: KYAGAYKLDPGY\n",
      "Generated AMP sequence: CAAHKAQIKHHK\n",
      "Generated AMP sequence: RATYGGPHAGPP\n",
      "Generated AMP sequence: VAAPAARGPG\n",
      "Generated AMP sequence: EAPSSHPGHYA\n",
      "Generated AMP sequence: PAAALHWRHG\n",
      "Generated AMP sequence: DKCMHKHGYPGVKYPH\n",
      "Generated AMP sequence: VAKMECSYKG\n",
      "Generated AMP sequence: LAVALGKPRGAGY\n",
      "Generated AMP sequence: YKLNCYHHKDGPGKGHHHPHKHPYPGGGKGH\n",
      "Generated AMP sequence: NCQAAPKPHGQPPHKHGYGY\n",
      "Generated AMP sequence: WAKTPRPPSGKGYYGPYG\n",
      "Generated AMP sequence: HVAPGYYYKTRHPGKGGGPH\n",
      "Generated AMP sequence: DADKRGNWNGDWDYKYPYHPHH\n",
      "Generated AMP sequence: CTYQHHPGGHHHGGDYHHPG\n",
      "Generated AMP sequence: IQAHVYKNGYGYPKKGGGGPHGYRYYGHPPGPGGKKK\n",
      "Generated AMP sequence: LTCAVYTRPKG\n",
      "Generated AMP sequence: DDCPKAGPDSKKKPHPK\n",
      "Generated AMP sequence: ITRNSTPSPPLPG\n",
      "Generated AMP sequence: ETASAGPGRRKAHKYKGKKPGPKYHKYGYPGH\n",
      "Generated AMP sequence: PAKGMAVYYPGPDDHHPPHHY\n",
      "Generated AMP sequence: KGARPFAPPHYGHYHKHH\n",
      "Generated AMP sequence: REVCDAPHVKYGSPPPPKGGYKPHGGPHGHGKHPHHGGH\n",
      "Generated AMP sequence: NKDPTRAKFGYGKKPKDHKG\n",
      "Generated AMP sequence: GAAGAKPRGGGPPGY\n",
      "Generated AMP sequence: NIPTPRHRDY\n",
      "Generated AMP sequence: NRGNKPACGGWGYHGPPHKP\n",
      "Generated AMP sequence: HYYNDPAAGIVP\n",
      "Generated AMP sequence: KFEGYATKHPKYYHHKYGKKPGP\n",
      "Generated AMP sequence: DAAYCSKGVHKHKGHYGPHPGYHGGGHHPHGKPGG\n",
      "Generated AMP sequence: YPNHSRLVGG\n",
      "Generated AMP sequence: GAIYAAHYHKDGY\n",
      "Generated AMP sequence: VAYMTHKKPGDHPGPYGPHPGPPYPHKPKGYPPGLHKG\n",
      "Generated AMP sequence: IYAYHTRKPYHHPPPPHPGK\n",
      "Generated AMP sequence: GCFGPHHGHG\n",
      "Generated AMP sequence: RKNMHHTKKC\n",
      "Generated AMP sequence: PEGKKMQRPYDGDP\n",
      "Generated AMP sequence: KAWAYHCKGHHPKHKPHHGGPGKPGKGYHPKGKY\n",
      "Generated AMP sequence: VRTCDMHSYHGPHPHPGKHK\n",
      "Generated AMP sequence: IPTCAAPMKGHK\n",
      "Generated AMP sequence: MYAPHHPPYGGPKHGGYGHG\n",
      "Generated AMP sequence: CAIARFKWPKHHPYP\n",
      "Generated AMP sequence: IPHAKYFPGKGHPHPPYGHG\n",
      "Generated AMP sequence: CWKSKNAPGRPPGHHHGYKHHHKHYHYPGYGPHPPPPHGH\n",
      "Generated AMP sequence: MNEYNSDTYKKP\n",
      "Generated AMP sequence: PHGAGAAFYKYH\n",
      "Generated AMP sequence: KHGANFNPGDPHGY\n",
      "Generated AMP sequence: DNYDAAGDPD\n",
      "Generated AMP sequence: HYRMGNKHGYHGHHPPPDHH\n",
      "Generated AMP sequence: WAQAKFAGYP\n",
      "Generated AMP sequence: MGAKHSCKGHPKHHHPYYPHPHGPGYHKPYKKGHGPHP\n",
      "Generated AMP sequence: HAPAAAYPHH\n",
      "Generated AMP sequence: SQNACAPPGAGYHPK\n",
      "Generated AMP sequence: PAAMRPKAYPHYKKPP\n",
      "Generated AMP sequence: RAEWDHHHMYGGGYPHPPPY\n",
      "Generated AMP sequence: FQHSPRPGKYYG\n",
      "Generated AMP sequence: KAPRDKSPKKKH\n",
      "Generated AMP sequence: FNTMQDIAGPYGHPGKPKGH\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=0.7, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TB no transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 17:27:16,689] A new study created in memory with name: no-name-fce0a8bc-2cfe-46cc-b056-da4361ab3bf9\n",
      "[I 2025-04-23 17:27:21,070] Trial 0 finished with value: 2.6234683990478516 and parameters: {'hidden_dim': 227, 'num_layers': 1, 'dropout': 0.46511171984776123, 'lr': 0.007525480141110266, 'weight_decay': 0.0006493437340915416}. Best is trial 0 with value: 2.6234683990478516.\n",
      "[I 2025-04-23 17:27:26,186] Trial 1 finished with value: 2.7615227699279785 and parameters: {'hidden_dim': 87, 'num_layers': 3, 'dropout': 0.39720828640486194, 'lr': 0.004563633109233991, 'weight_decay': 0.0006372563882503678}. Best is trial 0 with value: 2.6234683990478516.\n",
      "[I 2025-04-23 17:27:31,797] Trial 2 finished with value: 2.695784568786621 and parameters: {'hidden_dim': 231, 'num_layers': 2, 'dropout': 0.32349721668470843, 'lr': 0.00699099456729401, 'weight_decay': 5.5818143337677604e-05}. Best is trial 0 with value: 2.6234683990478516.\n",
      "[I 2025-04-23 17:27:36,662] Trial 3 finished with value: 2.583503007888794 and parameters: {'hidden_dim': 145, 'num_layers': 3, 'dropout': 0.413369057661946, 'lr': 0.0078081821724113615, 'weight_decay': 0.00020094910040001642}. Best is trial 3 with value: 2.583503007888794.\n",
      "[I 2025-04-23 17:27:41,260] Trial 4 finished with value: 2.752042770385742 and parameters: {'hidden_dim': 73, 'num_layers': 3, 'dropout': 0.44142322820386737, 'lr': 0.003587890095231379, 'weight_decay': 0.0007829202531600584}. Best is trial 3 with value: 2.583503007888794.\n",
      "[I 2025-04-23 17:27:47,654] Trial 5 finished with value: 2.9863569736480713 and parameters: {'hidden_dim': 241, 'num_layers': 2, 'dropout': 0.4272655263283651, 'lr': 0.00019046506910004563, 'weight_decay': 0.000535320280799172}. Best is trial 3 with value: 2.583503007888794.\n",
      "[I 2025-04-23 17:27:52,681] Trial 6 finished with value: 2.925967216491699 and parameters: {'hidden_dim': 209, 'num_layers': 2, 'dropout': 0.4873745873567189, 'lr': 0.0007810739645071821, 'weight_decay': 0.0001795383512783441}. Best is trial 3 with value: 2.583503007888794.\n",
      "[I 2025-04-23 17:27:57,141] Trial 7 finished with value: 2.9115796089172363 and parameters: {'hidden_dim': 171, 'num_layers': 1, 'dropout': 0.4420233739146304, 'lr': 0.0020085855035006715, 'weight_decay': 0.0006640623354835379}. Best is trial 3 with value: 2.583503007888794.\n",
      "[I 2025-04-23 17:28:02,174] Trial 8 finished with value: 2.9548306465148926 and parameters: {'hidden_dim': 77, 'num_layers': 2, 'dropout': 0.4416821346579247, 'lr': 0.0002669843777550729, 'weight_decay': 4.5696348462040585e-05}. Best is trial 3 with value: 2.583503007888794.\n",
      "[I 2025-04-23 17:28:07,650] Trial 9 finished with value: 2.863022804260254 and parameters: {'hidden_dim': 176, 'num_layers': 2, 'dropout': 0.10627953754264384, 'lr': 0.003003690566749762, 'weight_decay': 0.00035328893722744506}. Best is trial 3 with value: 2.583503007888794.\n",
      "[I 2025-04-23 17:28:12,442] Trial 10 finished with value: 2.498018980026245 and parameters: {'hidden_dim': 124, 'num_layers': 3, 'dropout': 0.19588087043927183, 'lr': 0.009803427934071084, 'weight_decay': 0.0009525168234538826}. Best is trial 10 with value: 2.498018980026245.\n",
      "[I 2025-04-23 17:28:17,969] Trial 11 finished with value: 2.468547821044922 and parameters: {'hidden_dim': 125, 'num_layers': 3, 'dropout': 0.1809741409069741, 'lr': 0.009903431049726066, 'weight_decay': 0.00095634187480499}. Best is trial 11 with value: 2.468547821044922.\n",
      "[I 2025-04-23 17:28:26,446] Trial 12 finished with value: 2.539414405822754 and parameters: {'hidden_dim': 122, 'num_layers': 3, 'dropout': 0.16526568413148107, 'lr': 0.009995791340914812, 'weight_decay': 0.0009526026845681472}. Best is trial 11 with value: 2.468547821044922.\n",
      "[I 2025-04-23 17:28:31,335] Trial 13 finished with value: 2.5286383628845215 and parameters: {'hidden_dim': 116, 'num_layers': 3, 'dropout': 0.21760968004100975, 'lr': 0.009968558408290267, 'weight_decay': 0.0009824964372881857}. Best is trial 11 with value: 2.468547821044922.\n",
      "[I 2025-04-23 17:28:36,212] Trial 14 finished with value: 2.5200817584991455 and parameters: {'hidden_dim': 116, 'num_layers': 3, 'dropout': 0.2473448403560071, 'lr': 0.008192460442393823, 'weight_decay': 0.0008304681771048723}. Best is trial 11 with value: 2.468547821044922.\n",
      "[I 2025-04-23 17:28:42,346] Trial 15 finished with value: 2.715729236602783 and parameters: {'hidden_dim': 138, 'num_layers': 3, 'dropout': 0.16619598943722622, 'lr': 0.0061269135665971055, 'weight_decay': 0.0008468494342264737}. Best is trial 11 with value: 2.468547821044922.\n",
      "[I 2025-04-23 17:28:47,555] Trial 16 finished with value: 2.5553781986236572 and parameters: {'hidden_dim': 191, 'num_layers': 3, 'dropout': 0.29753201311416694, 'lr': 0.00890880413340856, 'weight_decay': 0.0009948757340968956}. Best is trial 11 with value: 2.468547821044922.\n",
      "[I 2025-04-23 17:28:58,490] Trial 17 finished with value: 2.696479082107544 and parameters: {'hidden_dim': 100, 'num_layers': 1, 'dropout': 0.11299603745244942, 'lr': 0.006016026882374946, 'weight_decay': 0.00043081416322794577}. Best is trial 11 with value: 2.468547821044922.\n",
      "[I 2025-04-23 17:29:08,667] Trial 18 finished with value: 2.536205291748047 and parameters: {'hidden_dim': 152, 'num_layers': 3, 'dropout': 0.2173715233307566, 'lr': 0.008902487848542345, 'weight_decay': 0.0008088442099612532}. Best is trial 11 with value: 2.468547821044922.\n",
      "[I 2025-04-23 17:29:22,937] Trial 19 finished with value: 2.5462191104888916 and parameters: {'hidden_dim': 99, 'num_layers': 2, 'dropout': 0.15927059705599803, 'lr': 0.00905296990796893, 'weight_decay': 0.0009014732113789122}. Best is trial 11 with value: 2.468547821044922.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dim': 125, 'num_layers': 3, 'dropout': 0.1809741409069741, 'lr': 0.009903431049726066, 'weight_decay': 0.00095634187480499}\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary packages after reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Criterion\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss on the last time step of each sequence.\n",
    "    \n",
    "    Args:\n",
    "        output: Tensor of shape [B, L, vocab_size]\n",
    "        target_seq: Tensor of shape [B, L] containing target class indices\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss computed only on the last token of each sequence\n",
    "    \"\"\"\n",
    "    # Get last time step for each sequence\n",
    "    last_token_logits = output[:, -1, :]        # [B, vocab_size]\n",
    "    last_token_targets = target_seq[:, -1, :]      # [B]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "    # print('last_token_logits',last_token_logits.shape)\n",
    "    # print('last_token_targets',last_token_targets.shape)\n",
    "\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen-notrans-tb/AMP_LSTM_GEN_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])      # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            # print('target_shape before reshape',target_seq.shape)\n",
    "            # target_seq = target_seq.reshape(-1)\n",
    "            # print(f\"Output shape: {output.shape}, Target shape: {target_seq.shape}\")\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_generator-notrans-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])  # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # # target_seq = target_seq.reshape(-1)\n",
    "            # # target_seq = target_seq.reshape(-1, target_seq.shape[-1])\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            # assert output.size(0) == target_seq.size(0), f\"Mismatch: {output.size(0)} vs {target_seq.size(0)}\"\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            # print('loss done')\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            preds = output[:, -1, :]        # shape: [B, vocab_size]\n",
    "            preds = torch.argmax(preds, dim=1)  # shape: [B]\n",
    "\n",
    "            targets = target_seq[:, -1, :]      # shape: [B, vocab_size]\n",
    "            targets = torch.argmax(targets, dim=-1)  # shape: [B]\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Objective for Optuna tuning\n",
    "def objective_generation(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "\n",
    "lstm_gen_notrans_tb_best_params = study.best_trial.params\n",
    "print(lstm_gen_notrans_tb_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.9141 | Val Loss = 2.8729 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.6879\n",
      "Epoch 2: Train Loss = 2.8598 | Val Loss = 2.8129 | Acc = 0.9730 | AUC = undefined | Perplexity = 16.6579\n",
      "Epoch 3: Train Loss = 2.7946 | Val Loss = 2.7297 | Acc = 1.0000 | AUC = undefined | Perplexity = 15.3288\n",
      "Epoch 4: Train Loss = 2.7253 | Val Loss = 2.6674 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.4023\n",
      "Epoch 5: Train Loss = 2.6725 | Val Loss = 2.6059 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.5428\n",
      "Epoch 6: Train Loss = 2.6100 | Val Loss = 2.5701 | Acc = 0.9730 | AUC = undefined | Perplexity = 13.0667\n",
      "Epoch 7: Train Loss = 2.5629 | Val Loss = 2.5236 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.4729\n",
      "Epoch 8: Train Loss = 2.5047 | Val Loss = 2.4722 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.8488\n",
      "Epoch 9: Train Loss = 2.4530 | Val Loss = 2.4157 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.1979\n",
      "Epoch 10: Train Loss = 2.4267 | Val Loss = 2.3613 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.6051\n",
      "Epoch 11: Train Loss = 2.3482 | Val Loss = 2.3122 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.0969\n",
      "Epoch 12: Train Loss = 2.2971 | Val Loss = 2.2598 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.5815\n",
      "Epoch 13: Train Loss = 2.2640 | Val Loss = 2.2092 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.1087\n",
      "Epoch 14: Train Loss = 2.2024 | Val Loss = 2.1578 | Acc = 0.9730 | AUC = undefined | Perplexity = 8.6520\n",
      "Epoch 15: Train Loss = 2.1532 | Val Loss = 2.1106 | Acc = 0.9730 | AUC = undefined | Perplexity = 8.2535\n",
      "Epoch 16: Train Loss = 2.1045 | Val Loss = 2.0617 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.8589\n",
      "Epoch 17: Train Loss = 2.0513 | Val Loss = 2.0058 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.4321\n",
      "Epoch 18: Train Loss = 2.0162 | Val Loss = 1.9563 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.0734\n",
      "Epoch 19: Train Loss = 1.9552 | Val Loss = 1.9117 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.7645\n",
      "Epoch 20: Train Loss = 1.9067 | Val Loss = 1.8561 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.3988\n",
      "Epoch 21: Train Loss = 1.8608 | Val Loss = 1.8060 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.0859\n",
      "Epoch 22: Train Loss = 1.8137 | Val Loss = 1.7661 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.8479\n",
      "Epoch 23: Train Loss = 1.7637 | Val Loss = 1.7238 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.6056\n",
      "Epoch 24: Train Loss = 1.7161 | Val Loss = 1.6747 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.3371\n",
      "Epoch 25: Train Loss = 1.6892 | Val Loss = 1.6342 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.1255\n",
      "Epoch 26: Train Loss = 1.6288 | Val Loss = 1.5935 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.9211\n",
      "Epoch 27: Train Loss = 1.5905 | Val Loss = 1.5512 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.7173\n",
      "Epoch 28: Train Loss = 1.5508 | Val Loss = 1.5133 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.5415\n",
      "Epoch 29: Train Loss = 1.5020 | Val Loss = 1.4745 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.3689\n",
      "Epoch 30: Train Loss = 1.4656 | Val Loss = 1.4309 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.1823\n",
      "Epoch 31: Train Loss = 1.4176 | Val Loss = 1.3914 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.0203\n",
      "Epoch 32: Train Loss = 1.3815 | Val Loss = 1.3538 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.8722\n",
      "Epoch 33: Train Loss = 1.3416 | Val Loss = 1.3178 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.7350\n",
      "Epoch 34: Train Loss = 1.3183 | Val Loss = 1.2821 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.6042\n",
      "Epoch 35: Train Loss = 1.2809 | Val Loss = 1.2397 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.4544\n",
      "Epoch 36: Train Loss = 1.2292 | Val Loss = 1.2063 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.3412\n",
      "Epoch 37: Train Loss = 1.2220 | Val Loss = 1.1730 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.2317\n",
      "Epoch 38: Train Loss = 1.1676 | Val Loss = 1.1442 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.1398\n",
      "Epoch 39: Train Loss = 1.1307 | Val Loss = 1.1123 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.0414\n",
      "Epoch 40: Train Loss = 1.1068 | Val Loss = 1.0783 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.9396\n",
      "Epoch 41: Train Loss = 1.0750 | Val Loss = 1.0431 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.8379\n",
      "Epoch 42: Train Loss = 1.0322 | Val Loss = 1.0143 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.7574\n",
      "Epoch 43: Train Loss = 1.0067 | Val Loss = 0.9808 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.6665\n",
      "Epoch 44: Train Loss = 0.9799 | Val Loss = 0.9527 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.5926\n",
      "Epoch 45: Train Loss = 0.9566 | Val Loss = 0.9322 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.5400\n",
      "Epoch 46: Train Loss = 0.9603 | Val Loss = 0.9317 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.5389\n",
      "Epoch 47: Train Loss = 0.9537 | Val Loss = 0.8940 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.4450\n",
      "Epoch 48: Train Loss = 0.8795 | Val Loss = 0.8586 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.3600\n",
      "Epoch 49: Train Loss = 0.8531 | Val Loss = 0.8319 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.2978\n",
      "Epoch 50: Train Loss = 0.8351 | Val Loss = 0.8058 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.2385\n",
      "Epoch 51: Train Loss = 0.8037 | Val Loss = 0.7835 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.1892\n",
      "Epoch 52: Train Loss = 0.7844 | Val Loss = 0.7615 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.1415\n",
      "Epoch 53: Train Loss = 0.8305 | Val Loss = 0.7459 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.1083\n",
      "Epoch 54: Train Loss = 0.7467 | Val Loss = 0.7319 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0790\n",
      "Epoch 55: Train Loss = 0.7304 | Val Loss = 0.7134 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0408\n",
      "Epoch 56: Train Loss = 0.6971 | Val Loss = 0.6910 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9956\n",
      "Epoch 57: Train Loss = 0.6836 | Val Loss = 0.6683 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9509\n",
      "Epoch 58: Train Loss = 0.6617 | Val Loss = 0.6498 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9152\n",
      "Epoch 59: Train Loss = 0.6529 | Val Loss = 0.6348 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.8867\n",
      "Epoch 60: Train Loss = 0.6355 | Val Loss = 0.6199 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.8587\n",
      "\n",
      "✅ Final Test Metrics:\n",
      "Loss = 0.6199, Accuracy = 0.9730, AUC = undefined, Perplexity = 1.8587\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_notrans_tb_best_params[\"lr\"], weight_decay=lstm_gen_notrans_tb_best_params[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen-notrans-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_notrans_tb_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_notrans_tb_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_notrans_tb_best_params[\"dropout\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\"\\n✅ Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EVKKDGEPKKDYKHGG\n",
      "Generated AMP sequence: AAADKGHKKKGKKG\n",
      "Generated AMP sequence: KAAYYPHGKKGKGPHKHKKG\n",
      "Generated AMP sequence: IHWTKYGGGHKHKKHKGKGYKGDKKKKGKGPKKGKKKKP\n",
      "Generated AMP sequence: IAKPKPKKKEGGKHKPHKK\n",
      "Generated AMP sequence: FAAAYPKPMGPK\n",
      "Generated AMP sequence: EQAAGKPPAKKHGKYKKKKYYHKHHG\n",
      "Generated AMP sequence: VAADYGGKYHKKHKKKKKGK\n",
      "Generated AMP sequence: DAAKKAKGKGKKYG\n",
      "Generated AMP sequence: WAVSGKPKKHGPPK\n",
      "Generated AMP sequence: QAKVVPKGGPKDGK\n",
      "Generated AMP sequence: CAGLKGKHHK\n",
      "Generated AMP sequence: AMIKAHHHYKPKKKKKGKHG\n",
      "Generated AMP sequence: DAWYYHAYKYYPHKPKGKYK\n",
      "Generated AMP sequence: HAHYDKRKKKKHKKPPKKKK\n",
      "Generated AMP sequence: IKSFHEKDKKHKGGKKGKKH\n",
      "Generated AMP sequence: TAAIKKKYKHKHKGGYKKKG\n",
      "Generated AMP sequence: YAGMKHYKKKKH\n",
      "Generated AMP sequence: AGAAAAPKKGKKGKK\n",
      "Generated AMP sequence: VAAGKKPKGGKGKYKKKKKY\n",
      "Generated AMP sequence: HVTAAFAGKGHPGKHG\n",
      "Generated AMP sequence: VARPPHGKPYKKKKYPKKKG\n",
      "Generated AMP sequence: QAPEVHATGHYYKKKHKKKKKGKGKKPKGGKKGG\n",
      "Generated AMP sequence: IAPLADYKKLYYKKGKKKPG\n",
      "Generated AMP sequence: RAAAAGKKKYKHPHYDYGPK\n",
      "Generated AMP sequence: WAGYKKKKKKPGHYPKKHGKKKGYYKKIKKGPKYKKKYPG\n",
      "Generated AMP sequence: KAPYDKKYPP\n",
      "Generated AMP sequence: AAAASPYYKKYPHKGYKHK\n",
      "Generated AMP sequence: GAAKKPKPKKPKGGKKKHGD\n",
      "Generated AMP sequence: QAAGWHKMKKPYGTK\n",
      "Generated AMP sequence: MAYHGKGGGGGGK\n",
      "Generated AMP sequence: KASHKGGEGHGG\n",
      "Generated AMP sequence: FAAGKKDPKGP\n",
      "Generated AMP sequence: HAAPPGGKKKKKKGHKPKKGKKHHKGKHKGPKKKK\n",
      "Generated AMP sequence: MAGPHGHKKKK\n",
      "Generated AMP sequence: EAAADGGGHGPGKKYGKYPKHGKSKKHKGKKKHGKHKGK\n",
      "Generated AMP sequence: DAAAAYHPHKKKKKKVKYGK\n",
      "Generated AMP sequence: PAAPKPGPGGGKKKKPKKKKKHKKHPKKPKGYKKGK\n",
      "Generated AMP sequence: EASPDHHKGKKPKKGLGKKG\n",
      "Generated AMP sequence: NEKKLKHHGRKK\n",
      "Generated AMP sequence: NTAKKKAPYKKSKGKKKKY\n",
      "Generated AMP sequence: YAAYQGYPKKKGKKDKKKPKKKKHKGGKKPKKKDK\n",
      "Generated AMP sequence: KVAGAYKKGKGP\n",
      "Generated AMP sequence: CAAHKDPKKKKK\n",
      "Generated AMP sequence: RAQYHGKKDGKK\n",
      "Generated AMP sequence: VAAKAGPGKH\n",
      "Generated AMP sequence: EAKRQIKGHPD\n",
      "Generated AMP sequence: PAAAKKSPHH\n",
      "Generated AMP sequence: DAAKHKKHYKGPKPKK\n",
      "Generated AMP sequence: VAFKGGPYKG\n",
      "Generated AMP sequence: LATAKHKKPGDGP\n",
      "Generated AMP sequence: YAGKEYKKKGGKGKHKHKKHKKKYKHGHKGK\n",
      "Generated AMP sequence: NAMAAMKKKHKKKHKKGPGY\n",
      "Generated AMP sequence: WAFSPPKKPHKHYYGKRG\n",
      "Generated AMP sequence: HRALGYYYKPPHKGKHGGKK\n",
      "Generated AMP sequence: DAAKPHKQKHGPGPKYKYKKKK\n",
      "Generated AMP sequence: CPYPHKKHGKHHHGFYKKKH\n",
      "Generated AMP sequence: IIAGTYKKGYHYKKKHGHGKKGYPSYGKKKGKHHKKK\n",
      "Generated AMP sequence: LPAAVYPPKKG\n",
      "Generated AMP sequence: DAANKDGKGPKKKKKKK\n",
      "Generated AMP sequence: IPPKRRKPKKKKG\n",
      "Generated AMP sequence: EPAQCHKGPPKDHKYKHKKKGKKYKKWGYKGH\n",
      "Generated AMP sequence: PAGGKAQYYKGKGGKKKKKHY\n",
      "Generated AMP sequence: KAAPLGGKKHYGKYKKKH\n",
      "Generated AMP sequence: RASAGAKKPKYGPKKLKKGGPKKKGGKKGKGKHKKKDGK\n",
      "Generated AMP sequence: NAALRPDKGHYGKKKKDKKG\n",
      "Generated AMP sequence: GAAGCKKPGGGKKGY\n",
      "Generated AMP sequence: NAKSNPHPGP\n",
      "Generated AMP sequence: NKAKKNGGHGPHYKGKKKKK\n",
      "Generated AMP sequence: HYYKGKGDHKPK\n",
      "Generated AMP sequence: KAAGYAQKKKKPPKHKPHKKKGK\n",
      "Generated AMP sequence: DAAYFQKHPKKKKGKYHKKKGYKHGGKKKKHKKGG\n",
      "Generated AMP sequence: YGIHRPKPGG\n",
      "Generated AMP sequence: GAFYAGKYHKGGY\n",
      "Generated AMP sequence: VAYKSKKKKGGHKGKYGKKPGKKYKKKPKGVKKHKKKG\n",
      "Generated AMP sequence: IYAYHRPKKPKKKKKKKKGK\n",
      "Generated AMP sequence: GAAGLHKHKH\n",
      "Generated AMP sequence: RAKKKHPKKE\n",
      "Generated AMP sequence: PAAKKKPPKYDGDK\n",
      "Generated AMP sequence: KAVAYHGKHHHKKHKKHKGGKGKKGKHYKKKGKY\n",
      "Generated AMP sequence: VKRAGKKPRKHKHKKKGKKK\n",
      "Generated AMP sequence: IGQAAAKKKHKK\n",
      "Generated AMP sequence: MYALHKKKYGGKKHGGPGKH\n",
      "Generated AMP sequence: CAEAPGKPKKHKKYK\n",
      "Generated AMP sequence: IFAAKYGKGKGKKKKKSHKG\n",
      "Generated AMP sequence: CSFRKKGKGPKKGHHKGYKHKKKKPKYLHTGKKPPKKHGK\n",
      "Generated AMP sequence: MDAYKPGPYKKK\n",
      "Generated AMP sequence: PAAAHDCGYKYK\n",
      "Generated AMP sequence: KAAAKGKKGGKKGY\n",
      "Generated AMP sequence: DDYADGHGKG\n",
      "Generated AMP sequence: HYPKGKKKGPKGKKKKKDKK\n",
      "Generated AMP sequence: WAMAKGAHYK\n",
      "Generated AMP sequence: MAAKKQGKGHKKKKKKPYKKKKHKGPKKKPKKGKGKKK\n",
      "Generated AMP sequence: HAKAAAYKKH\n",
      "Generated AMP sequence: SHIAEFKKHDGYHKK\n",
      "Generated AMP sequence: PAAKPKKFYKHYKKKK\n",
      "Generated AMP sequence: RAAYGHKKKYHGGPKHKPKP\n",
      "Generated AMP sequence: FGARPPKGKYYG\n",
      "Generated AMP sequence: KAKQGKPKKKKH\n",
      "Generated AMP sequence: FCRKPGKAHKYGKKGKKKGK\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = trained_model\n",
    "gen_model.to(device)\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=0.7, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides-notrans.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
