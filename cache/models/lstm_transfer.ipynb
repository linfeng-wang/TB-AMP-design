{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual FASTA parsing (without Biopython)\n",
    "# fasta_path = \"../data//naturalAMPs_APD2024a-ADAM.fasta.txt\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\", index=False)\n",
    "\n",
    "\n",
    "# Manual FASTA parsing (without Biopython)\n",
    "# fasta_path = \"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# fasta_path = \"../data/uniprotkb_length_5_TO_30_NOT_antimicrob_2025_04_14.fasta (1)\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "uniprot_df = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\")\n",
    "uniprot_df1 = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\")\n",
    "uniprot_df = pd.concat([uniprot_df, uniprot_df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 2\n",
      "Range of sequence lengths: 181\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sequence length: 34.4038112522686\n",
      "Median sequence length: 29.0\n",
      "Standard deviation of sequence lengths: 23.719451486351453\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate sequence lengths\n",
    "sequence_lengths = adam_df['Sequence'].apply(len)\n",
    "\n",
    "# Calculate the range of sequence lengths\n",
    "length_range = sequence_lengths.max() - sequence_lengths.min()\n",
    "print(sequence_lengths.max(),sequence_lengths.min())\n",
    "print(f\"Range of sequence lengths: {length_range}\")\n",
    "\n",
    "# Draw a box plot\n",
    "plt.boxplot(sequence_lengths)\n",
    "plt.title(\"Box Plot of Sequence Lengths\")\n",
    "plt.ylabel(\"Sequence Length\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display distribution statistics\n",
    "mean_length = sequence_lengths.mean()\n",
    "median_length = sequence_lengths.median()\n",
    "std_dev_length = sequence_lengths.std()\n",
    "\n",
    "print(f\"Mean sequence length: {mean_length}\")\n",
    "print(f\"Median sequence length: {median_length}\")\n",
    "print(f\"Standard deviation of sequence lengths: {std_dev_length}\")\n",
    "\n",
    "# adam_df = adam_df.drop(columns=['Sequence Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_57360/3724575810.py:8: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_uniprot_df = uniprot_df.groupby('Sequence Length', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Peptide ID  \\\n",
      "81192  sp|P12997|BIOB_CITFR Biotin synthase (Fragment...   \n",
      "81715  sp|P85089|GTF2_LEUME Dextransucrase 2 (Fragmen...   \n",
      "81193  sp|P13071|BIOA_CITFR Adenosylmethionine-8-amin...   \n",
      "81176  sp|P0DKJ0|P160B_ARATH Peptide encoded by miPEP...   \n",
      "81362  sp|P41853|FARP_ARTTR FMRFamide-like neuropepti...   \n",
      "...                                                  ...   \n",
      "52463  tr|F5P1Z5|F5P1Z5_SHIFL ATP synthase subunit c ...   \n",
      "55078  sp|O28338|PURS_ARCFU Phosphoribosylformylglyci...   \n",
      "26968  tr|A0A4D5YML7|A0A4D5YML7_9ROSI ATP synthase su...   \n",
      "39606  tr|A0A7L3GVD8|A0A7L3GVD8_9PASS Serine palmitoy...   \n",
      "30647  tr|A0A5C1DC31|A0A5C1DC31_9ANNE Cytochrome c ox...   \n",
      "\n",
      "                                                Sequence  \n",
      "81192                                              MAHSS  \n",
      "81715                                              DSTNY  \n",
      "81193                                              MTTDD  \n",
      "81176                                              MFSPQ  \n",
      "81362                                              RYIRF  \n",
      "...                                                  ...  \n",
      "52463  MENLNMDLLYMAAAVMMGLAAIGAAIGIGILGGKFLEGAARQPDLI...  \n",
      "55078  MIADVYIELKEGVADPEGEATLKALRLLGFKRVKKVSTVKVFRIDI...  \n",
      "26968  NPLISAASVIAAGLAVGLASIGPGIGQGTAAGQAVEGIARQPEAEG...  \n",
      "39606  MDVRSTLSYLYWLFCQFELITCSYLMEPWEKVLFYSFNLAMLGLLL...  \n",
      "30647  GFGNWLVPLMLGAPDMAFPRINNLGFWLIPPAVILLVMSAFIEKGA...  \n",
      "\n",
      "[3739 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate sequence lengths for uniprot_df\n",
    "uniprot_df['Sequence Length'] = uniprot_df['Sequence'].apply(len)\n",
    "\n",
    "# Calculate sequence lengths for adam_df\n",
    "adam_df['Sequence Length'] = adam_df['Sequence'].apply(len)\n",
    "\n",
    "# Perform stratified sampling to select more samples\n",
    "sampled_uniprot_df = uniprot_df.groupby('Sequence Length', group_keys=False).apply(\n",
    "    lambda x: x.sample(\n",
    "        n=min(len(x), int(1.5 * adam_df['Sequence Length'].value_counts().get(x.name, 0))),  # Increase sample size\n",
    "        random_state=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# Drop the 'Sequence Length' column after sampling\n",
    "sampled_uniprot_df = sampled_uniprot_df.drop(columns=['Sequence Length'])\n",
    "adam_df = adam_df.drop(columns=['Sequence Length'])\n",
    "\n",
    "# Draw a box plot to visualize the distribution\n",
    "plt.boxplot(sampled_uniprot_df['Sequence'].apply(len))\n",
    "plt.title(\"Box Plot of Sequence Lengths (Sampled Negatives)\")\n",
    "plt.ylabel(\"Sequence Length\")\n",
    "plt.show()\n",
    "\n",
    "print(sampled_uniprot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183\n",
      "{'G', 'R', 'M', 'Q', 'B', 'N', 'E', 'S', 'V', 'T', 'I', 'Z', 'L', 'Y', 'W', 'D', 'H', 'F', 'C', 'K', 'A', 'P', 'X'}\n",
      "23\n",
      "{'Z', 'B'}\n",
      "Number of 'B' values: 2\n",
      "Number of sequences after filtering: 7042\n"
     ]
    }
   ],
   "source": [
    "\n",
    "adam_df['label'] = 1\n",
    "sampled_uniprot_df['label'] = 0\n",
    "adam_df.columns = [\"Peptide ID\", \"Sequences\", 'label']\n",
    "sampled_uniprot_df.columns = [\"Peptide ID\", \"Sequences\" , 'label']\n",
    "df = pd.concat([adam_df, sampled_uniprot_df], ignore_index=True)\n",
    "\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWYX\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "\n",
    "# Filter out sequences containing non-standard amino acids\n",
    "df = df[~df['Sequences'].str.contains('|'.join(non_standard_amino_acids))]\n",
    "print(f\"Number of sequences after filtering: {len(df)}\")\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define One-Hot Encoding Function for DNA Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        length = len(seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        return one_hot_torch(seq, dtype=self.one_hot_dtype), torch.tensor(label, dtype=torch.float32), length\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def collate_and_pack(batch):\n",
    "    # batch = list of (tensor_seq, label, length)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "\n",
    "    # lengths as tensor\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # Sort by descending length (required by pack_padded_sequence)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    labels = torch.tensor([labels[i] for i in sorted_indices])\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    # Stack to shape: (batch_size, 20, seq_len) and transpose for LSTM input\n",
    "    # LSTM expects input of shape (seq_len, batch_size, features)\n",
    "    sequences = [seq.T for seq in sequences]  # Transpose each [20, L] to [L, 20]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)  # shape: [max_len, batch, features]\n",
    "\n",
    "    # Pack the sequence\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 4067\n",
      "Validation: 872\n",
      "Test: 872\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        # packed_input: PackedSequence\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # hn: [num_layers, batch_size, hidden_dim]\n",
    "        # We'll use the **last layer's** hidden state as feature\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(last_hidden)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.6597, Val Loss: 0.5910, Val Acc: 0.6250, Val AUC: 0.8321\n",
      "Epoch [2/10] - Train Loss: 0.5181, Val Loss: 0.4545, Val Acc: 0.8200, Val AUC: 0.8879\n",
      "Epoch [3/10] - Train Loss: 0.4452, Val Loss: 0.4306, Val Acc: 0.8406, Val AUC: 0.9012\n",
      "Epoch [4/10] - Train Loss: 0.4989, Val Loss: 0.5526, Val Acc: 0.6869, Val AUC: 0.8404\n",
      "Epoch [5/10] - Train Loss: 0.4587, Val Loss: 0.4103, Val Acc: 0.8372, Val AUC: 0.9125\n",
      "Epoch [6/10] - Train Loss: 0.4230, Val Loss: 0.4271, Val Acc: 0.8268, Val AUC: 0.9173\n",
      "Epoch [7/10] - Train Loss: 0.4311, Val Loss: 0.3782, Val Acc: 0.8509, Val AUC: 0.9236\n",
      "Epoch [8/10] - Train Loss: 0.3921, Val Loss: 0.3834, Val Acc: 0.8452, Val AUC: 0.9213\n",
      "Epoch [9/10] - Train Loss: 0.3825, Val Loss: 0.3533, Val Acc: 0.8624, Val AUC: 0.9285\n",
      "Epoch [10/10] - Train Loss: 0.3809, Val Loss: 0.3461, Val Acc: 0.8647, Val AUC: 0.9308\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "    # Set up TensorBoard writer\n",
    "    log_dir = f\"runs/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Run evaluation\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return history\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # handle corner cases\n",
    "\n",
    "    # Sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        # Print metrics\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "model = LSTMClassifier(hidden_dim=64)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding in regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[401 160]\n",
      " [ 89 407]]\n",
      "Sensitivity (Recall for Positive Class): 0.8206\n",
      "Specificity (Recall for Negative Class): 0.7148\n",
      "Epoch [1/10] - Train Loss: 0.6630, Val Loss: 0.5471, Val Acc: 0.7644, Val AUC: 0.8299\n",
      "\n",
      "Confusion Matrix:\n",
      "[[460 101]\n",
      " [101 395]]\n",
      "Sensitivity (Recall for Positive Class): 0.7964\n",
      "Specificity (Recall for Negative Class): 0.8200\n",
      "Epoch [2/10] - Train Loss: 0.5161, Val Loss: 0.4849, Val Acc: 0.8089, Val AUC: 0.8625\n",
      "\n",
      "Confusion Matrix:\n",
      "[[192 369]\n",
      " [ 29 467]]\n",
      "Sensitivity (Recall for Positive Class): 0.9415\n",
      "Specificity (Recall for Negative Class): 0.3422\n",
      "Epoch [3/10] - Train Loss: 0.6564, Val Loss: 0.6466, Val Acc: 0.6235, Val AUC: 0.8047\n",
      "\n",
      "Confusion Matrix:\n",
      "[[424 137]\n",
      " [119 377]]\n",
      "Sensitivity (Recall for Positive Class): 0.7601\n",
      "Specificity (Recall for Negative Class): 0.7558\n",
      "Epoch [4/10] - Train Loss: 0.5960, Val Loss: 0.5372, Val Acc: 0.7578, Val AUC: 0.8364\n",
      "\n",
      "Confusion Matrix:\n",
      "[[458 103]\n",
      " [ 77 419]]\n",
      "Sensitivity (Recall for Positive Class): 0.8448\n",
      "Specificity (Recall for Negative Class): 0.8164\n",
      "Epoch [5/10] - Train Loss: 0.4724, Val Loss: 0.3893, Val Acc: 0.8297, Val AUC: 0.9082\n",
      "\n",
      "Confusion Matrix:\n",
      "[[515  46]\n",
      " [121 375]]\n",
      "Sensitivity (Recall for Positive Class): 0.7560\n",
      "Specificity (Recall for Negative Class): 0.9180\n",
      "Epoch [6/10] - Train Loss: 0.3992, Val Loss: 0.3821, Val Acc: 0.8420, Val AUC: 0.9220\n",
      "\n",
      "Confusion Matrix:\n",
      "[[476  85]\n",
      " [ 67 429]]\n",
      "Sensitivity (Recall for Positive Class): 0.8649\n",
      "Specificity (Recall for Negative Class): 0.8485\n",
      "Epoch [7/10] - Train Loss: 0.3674, Val Loss: 0.3593, Val Acc: 0.8562, Val AUC: 0.9305\n",
      "\n",
      "Confusion Matrix:\n",
      "[[512  49]\n",
      " [ 91 405]]\n",
      "Sensitivity (Recall for Positive Class): 0.8165\n",
      "Specificity (Recall for Negative Class): 0.9127\n",
      "Epoch [8/10] - Train Loss: 0.3570, Val Loss: 0.3330, Val Acc: 0.8675, Val AUC: 0.9375\n",
      "\n",
      "Confusion Matrix:\n",
      "[[451 110]\n",
      " [ 55 441]]\n",
      "Sensitivity (Recall for Positive Class): 0.8891\n",
      "Specificity (Recall for Negative Class): 0.8039\n",
      "Epoch [9/10] - Train Loss: 0.4099, Val Loss: 0.3379, Val Acc: 0.8439, Val AUC: 0.9359\n",
      "\n",
      "Confusion Matrix:\n",
      "[[464  97]\n",
      " [ 43 453]]\n",
      "Sensitivity (Recall for Positive Class): 0.9133\n",
      "Specificity (Recall for Negative Class): 0.8271\n",
      "Epoch [10/10] - Train Loss: 0.3665, Val Loss: 0.3215, Val Acc: 0.8675, Val AUC: 0.9421\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # L2 regularization\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "    # Set up TensorBoard writer\n",
    "    log_dir = f\"runs/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)   \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Run evaluation\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.4f}, \"\n",
    "              f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # handle corner cases\n",
    "\n",
    "    # Sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        # Print metrics\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=64, dropout=0.5)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3,\n",
    "                      weight_decay=1e-4, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling on general AMP data (bayesian optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 13:40:09,815] A new study created in memory with name: no-name-72e22362-9ee0-4827-95bd-29b1edf0f97d\n",
      "[W 2025-04-23 13:40:24,797] Trial 0 failed with parameters: {'hidden_dim': 55, 'num_layers': 3, 'dropout': 0.34473061470148847, 'lr': 0.008950764073931026, 'weight_decay': 0.0019483074746614874} because of the following error: KeyboardInterrupt().\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_57360/14054317.py\", line 128, in objective\n",
      "    val_auc = train_model(model, train_loader, val_loader, num_epochs=20, lr=lr,\n",
      "  File \"/tmp/ipykernel_57360/14054317.py\", line 56, in train_model\n",
      "    loss.backward()\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/_tensor.py\", line 626, in backward\n",
      "    torch.autograd.backward(\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/autograd/__init__.py\", line 347, in backward\n",
      "    _engine_run_backward(\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n",
      "    return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
      "KeyboardInterrupt\n",
      "[W 2025-04-23 13:40:24,817] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 134\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m val_auc\n\u001b[1;32m    133\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 134\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    137\u001b[0m lstm_best_param \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[12], line 128\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    125\u001b[0m weight_decay \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;241m1e-2\u001b[39m)\n\u001b[1;32m    127\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMClassifier(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39mhidden_dim, num_layers\u001b[38;5;241m=\u001b[39mnum_layers, dropout\u001b[38;5;241m=\u001b[39mdropout)\n\u001b[0;32m--> 128\u001b[0m val_auc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    129\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mweight_decay\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val_auc\n",
      "Cell \u001b[0;32mIn[12], line 56\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr, weight_decay, device, verbose, train)\u001b[0m\n\u001b[1;32m     54\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(packed_input)\n\u001b[1;32m     55\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m---> 56\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     57\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m     58\u001b[0m epoch_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/_tensor.py:626\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    616\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    617\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    618\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    619\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    624\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    625\u001b[0m     )\n\u001b[0;32m--> 626\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/autograd/__init__.py:347\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    342\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    346\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 347\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    349\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    350\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    351\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    353\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/autograd/graph.py:823\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    821\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 823\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    825\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    827\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_auc = train_model(model, train_loader, val_loader, num_epochs=20, lr=lr,\n",
    "                          weight_decay=weight_decay, verbose=False)\n",
    "    return val_auc\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_dim': 74,\n",
       " 'num_layers': 3,\n",
       " 'dropout': 0.3037059572844035,\n",
       " 'lr': 0.00774103421243492,\n",
       " 'weight_decay': 2.4221276513292614e-05}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/20] - Train Loss: 0.6918, Val Loss: 0.6884, Val Acc: 0.5444, Val AUC: 0.6068\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/20] - Train Loss: 0.6898, Val Loss: 0.6890, Val Acc: 0.5444, Val AUC: 0.6575\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/20] - Train Loss: 0.6892, Val Loss: 0.6877, Val Acc: 0.5444, Val AUC: 0.6705\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/20] - Train Loss: 0.6859, Val Loss: 0.6871, Val Acc: 0.5444, Val AUC: 0.6869\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/20] - Train Loss: 0.6841, Val Loss: 0.6841, Val Acc: 0.5444, Val AUC: 0.7063\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[145], line 6\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m#                       weight_decay=1.1052415577383506e-05, verbose=True)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMClassifier(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39mlstm_best_param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], num_layers\u001b[38;5;241m=\u001b[39mlstm_best_param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], dropout\u001b[38;5;241m=\u001b[39m lstm_best_param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm_best_param\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      7\u001b[0m \u001b[43m                      \u001b[49m\u001b[43mweight_decay\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlstm_best_param\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mweight_decay\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mBCELoss()\n\u001b[1;32m      9\u001b[0m val_loss, val_acc, val_auc \u001b[38;5;241m=\u001b[39m evaluate_model(model, test_loader, criterion, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "Cell \u001b[0;32mIn[142], line 49\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr, weight_decay, device, verbose, train)\u001b[0m\n\u001b[1;32m     46\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     47\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m packed_input, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     50\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     51\u001b[0m     packed_input \u001b[38;5;241m=\u001b[39m packed_input\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[141], line 25\u001b[0m, in \u001b[0;36mSequenceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[1;32m     24\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(seq\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# unpadded length\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mone_hot_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot_dtype\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), length\n",
      "Cell \u001b[0;32mIn[141], line 8\u001b[0m, in \u001b[0;36mone_hot_torch\u001b[0;34m(seq, dtype)\u001b[0m\n\u001b[1;32m      6\u001b[0m arr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(amino_acids), \u001b[38;5;28mlen\u001b[39m(seq_bytes), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, aa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(aa_bytes):\n\u001b[0;32m----> 8\u001b[0m     arr[i, seq_bytes \u001b[38;5;241m==\u001b[39m aa] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout= lstm_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_best_param['lr'],\n",
    "                      weight_decay=lstm_best_param['weight_decay'], verbose=True, train=False)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-19 17:02:11,381] A new study created in memory with name: no-name-b26d41e5-5b22-487a-927f-8e71f08cae60\n",
      "[I 2025-04-19 17:04:26,631] Trial 0 finished with value: 0.2602984458208084 and parameters: {'hidden_dim': 44, 'num_layers': 2, 'dropout': 0.21839908879243236, 'lr': 0.00076605000375419, 'weight_decay': 0.003205826180974212}. Best is trial 0 with value: 0.2602984458208084.\n",
      "[I 2025-04-19 17:07:50,919] Trial 1 finished with value: 0.2747341035043492 and parameters: {'hidden_dim': 85, 'num_layers': 2, 'dropout': 0.32812227261038773, 'lr': 0.0009471445496245934, 'weight_decay': 0.005203843007063293}. Best is trial 0 with value: 0.2602984458208084.\n",
      "[I 2025-04-19 17:09:00,636] Trial 2 finished with value: 0.2049465661539751 and parameters: {'hidden_dim': 51, 'num_layers': 1, 'dropout': 0.416685029891974, 'lr': 0.009983247660693553, 'weight_decay': 0.0026246839387646355}. Best is trial 2 with value: 0.2049465661539751.\n",
      "[I 2025-04-19 17:10:26,562] Trial 3 finished with value: 0.20678091224502115 and parameters: {'hidden_dim': 72, 'num_layers': 1, 'dropout': 0.23084293961659183, 'lr': 0.007621093080982238, 'weight_decay': 0.002535372490209248}. Best is trial 2 with value: 0.2049465661539751.\n",
      "[I 2025-04-19 17:12:15,930] Trial 4 finished with value: 0.21192381750134862 and parameters: {'hidden_dim': 102, 'num_layers': 1, 'dropout': 0.49582432537803245, 'lr': 0.006010654150160104, 'weight_decay': 0.0012970062791400959}. Best is trial 2 with value: 0.2049465661539751.\n",
      "[I 2025-04-19 17:15:10,461] Trial 5 finished with value: 0.6914038693203646 and parameters: {'hidden_dim': 83, 'num_layers': 2, 'dropout': 0.45807152887499825, 'lr': 0.0022414426023452893, 'weight_decay': 0.008832389303833413}. Best is trial 2 with value: 0.2049465661539751.\n",
      "[I 2025-04-19 17:16:01,624] Trial 6 finished with value: 0.3927049689433154 and parameters: {'hidden_dim': 34, 'num_layers': 1, 'dropout': 0.2178005196216105, 'lr': 0.00015102514080921054, 'weight_decay': 0.006112392173571112}. Best is trial 2 with value: 0.2049465661539751.\n",
      "[I 2025-04-19 17:17:58,028] Trial 7 finished with value: 0.1961194544154055 and parameters: {'hidden_dim': 128, 'num_layers': 1, 'dropout': 0.30204345734302485, 'lr': 0.008663957594980074, 'weight_decay': 0.0005646431306718692}. Best is trial 7 with value: 0.1961194544154055.\n",
      "[I 2025-04-19 17:19:47,477] Trial 8 finished with value: 0.19623432089300716 and parameters: {'hidden_dim': 120, 'num_layers': 1, 'dropout': 0.3396650577152682, 'lr': 0.009720203241685186, 'weight_decay': 0.00029536232953259123}. Best is trial 7 with value: 0.1961194544154055.\n",
      "[I 2025-04-19 17:22:35,093] Trial 9 finished with value: 0.20753061683738933 and parameters: {'hidden_dim': 81, 'num_layers': 2, 'dropout': 0.1758425842541762, 'lr': 0.009479079917279783, 'weight_decay': 0.0008352958766044223}. Best is trial 7 with value: 0.1961194544154055.\n",
      "[I 2025-04-19 17:27:57,015] Trial 10 finished with value: 0.6915899935890647 and parameters: {'hidden_dim': 126, 'num_layers': 3, 'dropout': 0.12667048187776594, 'lr': 0.0041335894773909376, 'weight_decay': 0.007385811901899476}. Best is trial 7 with value: 0.1961194544154055.\n",
      "[I 2025-04-19 17:29:46,938] Trial 11 finished with value: 0.20057790156672983 and parameters: {'hidden_dim': 123, 'num_layers': 1, 'dropout': 0.33803876277601663, 'lr': 0.00794391869017482, 'weight_decay': 4.048491973601974e-05}. Best is trial 7 with value: 0.1961194544154055.\n",
      "[I 2025-04-19 17:34:27,640] Trial 12 finished with value: 0.27438269117299247 and parameters: {'hidden_dim': 108, 'num_layers': 3, 'dropout': 0.3838235317917308, 'lr': 0.008189938374696632, 'weight_decay': 0.0034820661341568356}. Best is trial 7 with value: 0.1961194544154055.\n",
      "[I 2025-04-19 17:36:14,212] Trial 13 finished with value: 0.20335913318044999 and parameters: {'hidden_dim': 110, 'num_layers': 1, 'dropout': 0.28413194039637335, 'lr': 0.006303444711516389, 'weight_decay': 0.0016375959756822775}. Best is trial 7 with value: 0.1961194544154055.\n",
      "[I 2025-04-19 17:38:07,589] Trial 14 finished with value: 0.20497303675202763 and parameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.2845722833786236, 'lr': 0.008781418221947699, 'weight_decay': 0.0002988451576596952}. Best is trial 7 with value: 0.1961194544154055.\n",
      "[I 2025-04-19 17:41:11,793] Trial 15 finished with value: 0.24833798057892742 and parameters: {'hidden_dim': 102, 'num_layers': 2, 'dropout': 0.3563019841831678, 'lr': 0.004168087516423322, 'weight_decay': 0.004224295230737381}. Best is trial 7 with value: 0.1961194544154055.\n",
      "[I 2025-04-19 17:42:54,903] Trial 16 finished with value: 0.2231347043724621 and parameters: {'hidden_dim': 111, 'num_layers': 1, 'dropout': 0.4048698232091995, 'lr': 0.007069041256604472, 'weight_decay': 0.0019525037106003697}. Best is trial 7 with value: 0.1961194544154055.\n",
      "[I 2025-04-19 17:47:25,969] Trial 17 finished with value: 0.691507009898915 and parameters: {'hidden_dim': 93, 'num_layers': 3, 'dropout': 0.2741129968812849, 'lr': 0.009398676355930682, 'weight_decay': 0.009763459813285842}. Best is trial 7 with value: 0.1961194544154055.\n",
      "[I 2025-04-19 17:49:32,450] Trial 18 finished with value: 0.27566445312079263 and parameters: {'hidden_dim': 63, 'num_layers': 2, 'dropout': 0.31980865443824796, 'lr': 0.005616300881948385, 'weight_decay': 0.00521306756230825}. Best is trial 7 with value: 0.1961194544154055.\n",
      "[I 2025-04-19 17:51:20,175] Trial 19 finished with value: 0.18758816491155064 and parameters: {'hidden_dim': 118, 'num_layers': 1, 'dropout': 0.3737648901755298, 'lr': 0.0069931464580274445, 'weight_decay': 0.0009766140966463652}. Best is trial 19 with value: 0.18758816491155064.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 118, 'num_layers': 1, 'dropout': 0.3737648901755298, 'lr': 0.0069931464580274445, 'weight_decay': 0.0009766140966463652}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# Updated BiLSTM with flatten layer as previously defined\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_flatten/BiLSTM_Flatten_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "    max_seq_len = 100  # fixed for now; match your padding/truncation\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage (uncomment and run in your local environment):\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_dim': 118,\n",
       " 'num_layers': 1,\n",
       " 'dropout': 0.3737648901755298,\n",
       " 'lr': 0.0069931464580274445,\n",
       " 'weight_decay': 0.0009766140966463652}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[539  22]\n",
      " [284 212]]\n",
      "Sensitivity: 0.4274, Specificity: 0.9608\n",
      "Epoch [1/30] - Train Loss: 0.6032, Val Loss: 0.5549, Val Acc: 0.7105, Val AUC: 0.8802\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[559   2]\n",
      " [486  10]]\n",
      "Sensitivity: 0.0202, Specificity: 0.9964\n",
      "Epoch [2/30] - Train Loss: 0.4529, Val Loss: 0.7654, Val Acc: 0.5383, Val AUC: 0.7775\n",
      "\n",
      "Confusion Matrix:\n",
      "[[371 190]\n",
      " [ 32 464]]\n",
      "Sensitivity: 0.9355, Specificity: 0.6613\n",
      "Epoch [3/30] - Train Loss: 0.4436, Val Loss: 0.4312, Val Acc: 0.7900, Val AUC: 0.9062\n",
      "\n",
      "Confusion Matrix:\n",
      "[[396 165]\n",
      " [ 50 446]]\n",
      "Sensitivity: 0.8992, Specificity: 0.7059\n",
      "Epoch [4/30] - Train Loss: 0.3906, Val Loss: 0.4328, Val Acc: 0.7966, Val AUC: 0.8918\n",
      "\n",
      "Confusion Matrix:\n",
      "[[501  60]\n",
      " [113 383]]\n",
      "Sensitivity: 0.7722, Specificity: 0.8930\n",
      "Epoch [5/30] - Train Loss: 0.3813, Val Loss: 0.3893, Val Acc: 0.8363, Val AUC: 0.9182\n",
      "\n",
      "Confusion Matrix:\n",
      "[[368 193]\n",
      " [129 367]]\n",
      "Sensitivity: 0.7399, Specificity: 0.6560\n",
      "Epoch [6/30] - Train Loss: 0.3810, Val Loss: 0.5782, Val Acc: 0.6954, Val AUC: 0.7758\n",
      "\n",
      "Confusion Matrix:\n",
      "[[395 166]\n",
      " [106 390]]\n",
      "Sensitivity: 0.7863, Specificity: 0.7041\n",
      "Epoch [7/30] - Train Loss: 0.5214, Val Loss: 0.5057, Val Acc: 0.7427, Val AUC: 0.8257\n",
      "\n",
      "Confusion Matrix:\n",
      "[[521  40]\n",
      " [122 374]]\n",
      "Sensitivity: 0.7540, Specificity: 0.9287\n",
      "Epoch [8/30] - Train Loss: 0.4155, Val Loss: 0.3728, Val Acc: 0.8467, Val AUC: 0.9266\n",
      "\n",
      "Confusion Matrix:\n",
      "[[477  84]\n",
      " [ 58 438]]\n",
      "Sensitivity: 0.8831, Specificity: 0.8503\n",
      "Epoch [9/30] - Train Loss: 0.3328, Val Loss: 0.3405, Val Acc: 0.8657, Val AUC: 0.9407\n",
      "\n",
      "Confusion Matrix:\n",
      "[[520  41]\n",
      " [ 93 403]]\n",
      "Sensitivity: 0.8125, Specificity: 0.9269\n",
      "Epoch [10/30] - Train Loss: 0.3642, Val Loss: 0.3235, Val Acc: 0.8732, Val AUC: 0.9373\n",
      "\n",
      "Confusion Matrix:\n",
      "[[499  62]\n",
      " [ 61 435]]\n",
      "Sensitivity: 0.8770, Specificity: 0.8895\n",
      "Epoch [11/30] - Train Loss: 0.3780, Val Loss: 0.3028, Val Acc: 0.8836, Val AUC: 0.9457\n",
      "\n",
      "Confusion Matrix:\n",
      "[[508  53]\n",
      " [ 60 436]]\n",
      "Sensitivity: 0.8790, Specificity: 0.9055\n",
      "Epoch [12/30] - Train Loss: 0.3240, Val Loss: 0.2922, Val Acc: 0.8931, Val AUC: 0.9494\n",
      "\n",
      "Confusion Matrix:\n",
      "[[441 120]\n",
      " [ 54 442]]\n",
      "Sensitivity: 0.8911, Specificity: 0.7861\n",
      "Epoch [13/30] - Train Loss: 0.3319, Val Loss: 0.3729, Val Acc: 0.8354, Val AUC: 0.9196\n",
      "\n",
      "Confusion Matrix:\n",
      "[[472  89]\n",
      " [ 49 447]]\n",
      "Sensitivity: 0.9012, Specificity: 0.8414\n",
      "Epoch [14/30] - Train Loss: 0.3580, Val Loss: 0.3184, Val Acc: 0.8694, Val AUC: 0.9421\n",
      "\n",
      "Confusion Matrix:\n",
      "[[507  54]\n",
      " [ 61 435]]\n",
      "Sensitivity: 0.8770, Specificity: 0.9037\n",
      "Epoch [15/30] - Train Loss: 0.3100, Val Loss: 0.2966, Val Acc: 0.8912, Val AUC: 0.9484\n",
      "\n",
      "Confusion Matrix:\n",
      "[[468  93]\n",
      " [ 64 432]]\n",
      "Sensitivity: 0.8710, Specificity: 0.8342\n",
      "Epoch [16/30] - Train Loss: 0.3251, Val Loss: 0.3591, Val Acc: 0.8515, Val AUC: 0.9231\n",
      "\n",
      "Confusion Matrix:\n",
      "[[475  86]\n",
      " [ 76 420]]\n",
      "Sensitivity: 0.8468, Specificity: 0.8467\n",
      "Epoch [17/30] - Train Loss: 0.3577, Val Loss: 0.3386, Val Acc: 0.8467, Val AUC: 0.9311\n",
      "\n",
      "Confusion Matrix:\n",
      "[[505  56]\n",
      " [ 64 432]]\n",
      "Sensitivity: 0.8710, Specificity: 0.9002\n",
      "Epoch [18/30] - Train Loss: 0.3202, Val Loss: 0.2966, Val Acc: 0.8865, Val AUC: 0.9488\n",
      "\n",
      "Confusion Matrix:\n",
      "[[488  73]\n",
      " [ 51 445]]\n",
      "Sensitivity: 0.8972, Specificity: 0.8699\n",
      "Epoch [19/30] - Train Loss: 0.2814, Val Loss: 0.2942, Val Acc: 0.8827, Val AUC: 0.9503\n",
      "\n",
      "Confusion Matrix:\n",
      "[[536  25]\n",
      " [148 348]]\n",
      "Sensitivity: 0.7016, Specificity: 0.9554\n",
      "Epoch [20/30] - Train Loss: 0.3049, Val Loss: 0.3719, Val Acc: 0.8363, Val AUC: 0.9265\n",
      "\n",
      "Confusion Matrix:\n",
      "[[484  77]\n",
      " [ 41 455]]\n",
      "Sensitivity: 0.9173, Specificity: 0.8627\n",
      "Epoch [21/30] - Train Loss: 0.2815, Val Loss: 0.2858, Val Acc: 0.8884, Val AUC: 0.9544\n",
      "\n",
      "Confusion Matrix:\n",
      "[[440 121]\n",
      " [ 76 420]]\n",
      "Sensitivity: 0.8468, Specificity: 0.7843\n",
      "Epoch [22/30] - Train Loss: 0.3891, Val Loss: 0.3878, Val Acc: 0.8136, Val AUC: 0.9063\n",
      "\n",
      "Confusion Matrix:\n",
      "[[434 127]\n",
      " [ 53 443]]\n",
      "Sensitivity: 0.8931, Specificity: 0.7736\n",
      "Epoch [23/30] - Train Loss: 0.4256, Val Loss: 0.3837, Val Acc: 0.8297, Val AUC: 0.9279\n",
      "\n",
      "Confusion Matrix:\n",
      "[[392 169]\n",
      " [ 17 479]]\n",
      "Sensitivity: 0.9657, Specificity: 0.6988\n",
      "Epoch [24/30] - Train Loss: 0.3517, Val Loss: 0.3906, Val Acc: 0.8240, Val AUC: 0.9269\n",
      "\n",
      "Confusion Matrix:\n",
      "[[520  41]\n",
      " [ 76 420]]\n",
      "Sensitivity: 0.8468, Specificity: 0.9269\n",
      "Epoch [25/30] - Train Loss: 0.3002, Val Loss: 0.2750, Val Acc: 0.8893, Val AUC: 0.9564\n",
      "\n",
      "Confusion Matrix:\n",
      "[[530  31]\n",
      " [ 84 412]]\n",
      "Sensitivity: 0.8306, Specificity: 0.9447\n",
      "Epoch [26/30] - Train Loss: 0.2917, Val Loss: 0.2822, Val Acc: 0.8912, Val AUC: 0.9562\n",
      "\n",
      "Confusion Matrix:\n",
      "[[479  82]\n",
      " [ 40 456]]\n",
      "Sensitivity: 0.9194, Specificity: 0.8538\n",
      "Epoch [27/30] - Train Loss: 0.2992, Val Loss: 0.2794, Val Acc: 0.8846, Val AUC: 0.9575\n",
      "\n",
      "Confusion Matrix:\n",
      "[[540  21]\n",
      " [166 330]]\n",
      "Sensitivity: 0.6653, Specificity: 0.9626\n",
      "Epoch [28/30] - Train Loss: 0.3194, Val Loss: 0.3568, Val Acc: 0.8231, Val AUC: 0.9498\n",
      "\n",
      "Confusion Matrix:\n",
      "[[477  84]\n",
      " [ 32 464]]\n",
      "Sensitivity: 0.9355, Specificity: 0.8503\n",
      "Epoch [29/30] - Train Loss: 0.2863, Val Loss: 0.2870, Val Acc: 0.8903, Val AUC: 0.9618\n",
      "\n",
      "Confusion Matrix:\n",
      "[[525  36]\n",
      " [ 68 428]]\n",
      "Sensitivity: 0.8629, Specificity: 0.9358\n",
      "Epoch [30/30] - Train Loss: 0.2723, Val Loss: 0.2414, Val Acc: 0.9016, Val AUC: 0.9670\n",
      "\n",
      "Confusion Matrix:\n",
      "[[519  41]\n",
      " [ 61 435]]\n",
      "Sensitivity: 0.8770, Specificity: 0.9268\n",
      "Test Loss: 0.2475, Test Accuracy: 0.9034, Test AUC: 0.9641\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout= bilstm_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=30, lr=bilstm_best_param['lr'],\n",
    "                      weight_decay=bilstm_best_param['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 10:19:32,293] A new study created in memory with name: no-name-9557a733-2448-499b-ab77-2986317364e6\n",
      "[I 2025-04-20 10:20:44,597] Trial 0 finished with value: 0.3844235960175009 and parameters: {'hidden_dim': 119, 'num_layers': 1, 'dropout': 0.20035346233192072, 'lr': 0.0014547294700081501, 'weight_decay': 8.003959581389578e-05}. Best is trial 0 with value: 0.3844235960175009.\n",
      "[I 2025-04-20 10:21:46,744] Trial 1 finished with value: 0.41313882610377145 and parameters: {'hidden_dim': 90, 'num_layers': 1, 'dropout': 0.490678241469906, 'lr': 0.0023404280158707885, 'weight_decay': 7.446558203851261e-05}. Best is trial 0 with value: 0.3844235960175009.\n",
      "[I 2025-04-20 10:22:51,117] Trial 2 finished with value: 0.4422876817338607 and parameters: {'hidden_dim': 56, 'num_layers': 2, 'dropout': 0.3616827091878919, 'lr': 0.000116908600437973, 'weight_decay': 4.196451358948849e-06}. Best is trial 0 with value: 0.3844235960175009.\n",
      "[I 2025-04-20 10:23:50,312] Trial 3 finished with value: 0.32991501338341656 and parameters: {'hidden_dim': 88, 'num_layers': 1, 'dropout': 0.4008882478146155, 'lr': 0.00478178191785242, 'weight_decay': 0.00028306106324820906}. Best is trial 3 with value: 0.32991501338341656.\n",
      "[I 2025-04-20 10:24:38,537] Trial 4 finished with value: 0.4322758877978605 and parameters: {'hidden_dim': 68, 'num_layers': 1, 'dropout': 0.27245001416099157, 'lr': 0.00031169687140373225, 'weight_decay': 0.00020584277592946846}. Best is trial 3 with value: 0.32991501338341656.\n",
      "[I 2025-04-20 10:27:04,313] Trial 5 finished with value: 0.40727872883572297 and parameters: {'hidden_dim': 110, 'num_layers': 3, 'dropout': 0.1993246938856833, 'lr': 0.0004692776253812438, 'weight_decay': 0.00026557233359506715}. Best is trial 3 with value: 0.32991501338341656.\n",
      "[I 2025-04-20 10:29:18,905] Trial 6 finished with value: 0.36633385279599356 and parameters: {'hidden_dim': 97, 'num_layers': 3, 'dropout': 0.19212848312488068, 'lr': 0.0006079597486083831, 'weight_decay': 5.194783797306756e-05}. Best is trial 3 with value: 0.32991501338341656.\n",
      "[I 2025-04-20 10:30:25,003] Trial 7 finished with value: 0.4157338615726022 and parameters: {'hidden_dim': 128, 'num_layers': 1, 'dropout': 0.1098983644523544, 'lr': 0.0002542084006384803, 'weight_decay': 9.560379286328327e-05}. Best is trial 3 with value: 0.32991501338341656.\n",
      "[I 2025-04-20 10:31:23,310] Trial 8 finished with value: 0.41179058832280774 and parameters: {'hidden_dim': 86, 'num_layers': 1, 'dropout': 0.10365463746939439, 'lr': 0.00021168408218142547, 'weight_decay': 2.1117390294497004e-06}. Best is trial 3 with value: 0.32991501338341656.\n",
      "[I 2025-04-20 10:33:44,218] Trial 9 finished with value: 0.411264601875754 and parameters: {'hidden_dim': 103, 'num_layers': 3, 'dropout': 0.4484253234606769, 'lr': 0.0010877890031992755, 'weight_decay': 8.075803979191198e-05}. Best is trial 3 with value: 0.32991501338341656.\n",
      "[I 2025-04-20 10:34:36,345] Trial 10 finished with value: 0.6916100102312425 and parameters: {'hidden_dim': 48, 'num_layers': 2, 'dropout': 0.36874554163269285, 'lr': 0.008024257085952714, 'weight_decay': 0.009393000211830909}. Best is trial 3 with value: 0.32991501338341656.\n",
      "[I 2025-04-20 10:36:23,315] Trial 11 finished with value: 0.6916124294785893 and parameters: {'hidden_dim': 74, 'num_layers': 3, 'dropout': 0.38611580436353093, 'lr': 0.004893142172565384, 'weight_decay': 0.0014696851466520701}. Best is trial 3 with value: 0.32991501338341656.\n",
      "[I 2025-04-20 10:38:01,550] Trial 12 finished with value: 0.3695571597884683 and parameters: {'hidden_dim': 97, 'num_layers': 2, 'dropout': 0.2652391641246786, 'lr': 0.0006005903533084007, 'weight_decay': 1.1278903442690517e-05}. Best is trial 3 with value: 0.32991501338341656.\n",
      "[I 2025-04-20 10:39:03,301] Trial 13 finished with value: 0.69161496442907 and parameters: {'hidden_dim': 35, 'num_layers': 3, 'dropout': 0.20372068174516944, 'lr': 0.002096901854312056, 'weight_decay': 0.0009559288523163627}. Best is trial 3 with value: 0.32991501338341656.\n",
      "[I 2025-04-20 10:40:25,811] Trial 14 finished with value: 0.3520637820748722 and parameters: {'hidden_dim': 81, 'num_layers': 2, 'dropout': 0.42999697247977425, 'lr': 0.003548579423929614, 'weight_decay': 1.7393376191843906e-05}. Best is trial 3 with value: 0.32991501338341656.\n",
      "[I 2025-04-20 10:41:39,734] Trial 15 finished with value: 0.3181809292120092 and parameters: {'hidden_dim': 69, 'num_layers': 2, 'dropout': 0.4276008810103424, 'lr': 0.004552444223564782, 'weight_decay': 1.3361425038153041e-05}. Best is trial 15 with value: 0.3181809292120092.\n",
      "[I 2025-04-20 10:42:49,246] Trial 16 finished with value: 0.3730347875286551 and parameters: {'hidden_dim': 64, 'num_layers': 2, 'dropout': 0.3303400479647959, 'lr': 0.009374196490015455, 'weight_decay': 1.734843937844558e-05}. Best is trial 15 with value: 0.3181809292120092.\n",
      "[I 2025-04-20 10:43:33,934] Trial 17 finished with value: 0.41868018227465015 and parameters: {'hidden_dim': 47, 'num_layers': 1, 'dropout': 0.42629011991337173, 'lr': 0.004800249884509056, 'weight_decay': 0.000912171130763565}. Best is trial 15 with value: 0.3181809292120092.\n",
      "[I 2025-04-20 10:44:56,836] Trial 18 finished with value: 0.6916114091873169 and parameters: {'hidden_dim': 75, 'num_layers': 2, 'dropout': 0.4979920162903372, 'lr': 0.003017202239083473, 'weight_decay': 0.004479935029937108}. Best is trial 15 with value: 0.3181809292120092.\n",
      "[I 2025-04-20 10:46:07,447] Trial 19 finished with value: 0.33391933230792775 and parameters: {'hidden_dim': 61, 'num_layers': 2, 'dropout': 0.3106432269621113, 'lr': 0.005479716265075894, 'weight_decay': 1.0390628475033573e-06}. Best is trial 15 with value: 0.3181809292120092.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 69, 'num_layers': 2, 'dropout': 0.4276008810103424, 'lr': 0.004552444223564782, 'weight_decay': 1.3361425038153041e-05}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# LSTM with Attention classifier\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)  # shape: [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)  # shape: [batch, seq_len]\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)  # normalize\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # shape: [batch, hidden_dim]\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-attn/LSTM_Attn_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-lstm_attention.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_attn_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[406 155]\n",
      " [ 42 454]]\n",
      "Sensitivity: 0.9153, Specificity: 0.7237\n",
      "Epoch [1/20] - Train Loss: 0.5294, Val Loss: 0.4672, Val Acc: 0.8136, Val AUC: 0.8776\n",
      "\n",
      "Confusion Matrix:\n",
      "[[443 118]\n",
      " [ 58 438]]\n",
      "Sensitivity: 0.8831, Specificity: 0.7897\n",
      "Epoch [2/20] - Train Loss: 0.4590, Val Loss: 0.3855, Val Acc: 0.8335, Val AUC: 0.9233\n",
      "\n",
      "Confusion Matrix:\n",
      "[[418 143]\n",
      " [ 44 452]]\n",
      "Sensitivity: 0.9113, Specificity: 0.7451\n",
      "Epoch [3/20] - Train Loss: 0.4101, Val Loss: 0.3751, Val Acc: 0.8231, Val AUC: 0.9206\n",
      "\n",
      "Confusion Matrix:\n",
      "[[397 164]\n",
      " [ 44 452]]\n",
      "Sensitivity: 0.9113, Specificity: 0.7077\n",
      "Epoch [4/20] - Train Loss: 0.5205, Val Loss: 0.4424, Val Acc: 0.8032, Val AUC: 0.9061\n",
      "\n",
      "Confusion Matrix:\n",
      "[[500  61]\n",
      " [100 396]]\n",
      "Sensitivity: 0.7984, Specificity: 0.8913\n",
      "Epoch [5/20] - Train Loss: 0.4908, Val Loss: 0.3589, Val Acc: 0.8477, Val AUC: 0.9304\n",
      "\n",
      "Confusion Matrix:\n",
      "[[492  69]\n",
      " [ 62 434]]\n",
      "Sensitivity: 0.8750, Specificity: 0.8770\n",
      "Epoch [6/20] - Train Loss: 0.3654, Val Loss: 0.3135, Val Acc: 0.8761, Val AUC: 0.9453\n",
      "\n",
      "Confusion Matrix:\n",
      "[[474  87]\n",
      " [ 42 454]]\n",
      "Sensitivity: 0.9153, Specificity: 0.8449\n",
      "Epoch [7/20] - Train Loss: 0.3200, Val Loss: 0.2776, Val Acc: 0.8780, Val AUC: 0.9544\n",
      "\n",
      "Confusion Matrix:\n",
      "[[519  42]\n",
      " [ 73 423]]\n",
      "Sensitivity: 0.8528, Specificity: 0.9251\n",
      "Epoch [8/20] - Train Loss: 0.2896, Val Loss: 0.2604, Val Acc: 0.8912, Val AUC: 0.9645\n",
      "\n",
      "Confusion Matrix:\n",
      "[[526  35]\n",
      " [ 82 414]]\n",
      "Sensitivity: 0.8347, Specificity: 0.9376\n",
      "Epoch [9/20] - Train Loss: 0.2926, Val Loss: 0.2568, Val Acc: 0.8893, Val AUC: 0.9670\n",
      "\n",
      "Confusion Matrix:\n",
      "[[539  22]\n",
      " [108 388]]\n",
      "Sensitivity: 0.7823, Specificity: 0.9608\n",
      "Epoch [10/20] - Train Loss: 0.2530, Val Loss: 0.2778, Val Acc: 0.8770, Val AUC: 0.9658\n",
      "\n",
      "Confusion Matrix:\n",
      "[[507  54]\n",
      " [ 42 454]]\n",
      "Sensitivity: 0.9153, Specificity: 0.9037\n",
      "Epoch [11/20] - Train Loss: 0.2350, Val Loss: 0.2145, Val Acc: 0.9092, Val AUC: 0.9715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[524  37]\n",
      " [ 63 433]]\n",
      "Sensitivity: 0.8730, Specificity: 0.9340\n",
      "Epoch [12/20] - Train Loss: 0.2171, Val Loss: 0.2152, Val Acc: 0.9054, Val AUC: 0.9726\n",
      "\n",
      "Confusion Matrix:\n",
      "[[548  13]\n",
      " [138 358]]\n",
      "Sensitivity: 0.7218, Specificity: 0.9768\n",
      "Epoch [13/20] - Train Loss: 0.2274, Val Loss: 0.3093, Val Acc: 0.8571, Val AUC: 0.9698\n",
      "\n",
      "Confusion Matrix:\n",
      "[[523  38]\n",
      " [ 66 430]]\n",
      "Sensitivity: 0.8669, Specificity: 0.9323\n",
      "Epoch [14/20] - Train Loss: 0.2745, Val Loss: 0.2331, Val Acc: 0.9016, Val AUC: 0.9693\n",
      "\n",
      "Confusion Matrix:\n",
      "[[531  30]\n",
      " [ 60 436]]\n",
      "Sensitivity: 0.8790, Specificity: 0.9465\n",
      "Epoch [15/20] - Train Loss: 0.2192, Val Loss: 0.2148, Val Acc: 0.9149, Val AUC: 0.9754\n",
      "\n",
      "Confusion Matrix:\n",
      "[[539  22]\n",
      " [ 91 405]]\n",
      "Sensitivity: 0.8165, Specificity: 0.9608\n",
      "Epoch [16/20] - Train Loss: 0.1945, Val Loss: 0.2593, Val Acc: 0.8931, Val AUC: 0.9703\n",
      "\n",
      "Confusion Matrix:\n",
      "[[533  28]\n",
      " [ 66 430]]\n",
      "Sensitivity: 0.8669, Specificity: 0.9501\n",
      "Epoch [17/20] - Train Loss: 0.1979, Val Loss: 0.2130, Val Acc: 0.9111, Val AUC: 0.9771\n",
      "\n",
      "Confusion Matrix:\n",
      "[[545  16]\n",
      " [133 363]]\n",
      "Sensitivity: 0.7319, Specificity: 0.9715\n",
      "Epoch [18/20] - Train Loss: 0.2089, Val Loss: 0.3720, Val Acc: 0.8590, Val AUC: 0.9629\n",
      "\n",
      "Confusion Matrix:\n",
      "[[500  61]\n",
      " [ 86 410]]\n",
      "Sensitivity: 0.8266, Specificity: 0.8913\n",
      "Epoch [19/20] - Train Loss: 0.4073, Val Loss: 0.2913, Val Acc: 0.8609, Val AUC: 0.9495\n",
      "\n",
      "Confusion Matrix:\n",
      "[[519  42]\n",
      " [ 54 442]]\n",
      "Sensitivity: 0.8911, Specificity: 0.9251\n",
      "Epoch [20/20] - Train Loss: 0.2708, Val Loss: 0.2270, Val Acc: 0.9092, Val AUC: 0.9708\n",
      "\n",
      "Confusion Matrix:\n",
      "[[517  43]\n",
      " [ 54 442]]\n",
      "Sensitivity: 0.8911, Specificity: 0.9232\n",
      "Test Loss: 0.2303, Test Accuracy: 0.9081, Test AUC: 0.9682\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_attn_best_param['hidden_dim'], num_layers=lstm_attn_best_param['num_layers'], dropout= lstm_attn_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_attn_best_param['lr'],\n",
    "                      weight_decay=lstm_attn_best_param['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm + attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 12:07:43,124] A new study created in memory with name: no-name-78722334-0bf2-4ef1-962f-21ac3920c3b5\n",
      "[I 2025-04-22 12:10:47,744] Trial 0 finished with value: 0.6916118544690749 and parameters: {'hidden_dim': 89, 'num_layers': 2, 'dropout': 0.46985894169850995, 'lr': 0.007592770320322348, 'weight_decay': 0.006834172891656189}. Best is trial 0 with value: 0.6916118544690749.\n",
      "[I 2025-04-22 12:13:50,621] Trial 1 finished with value: 0.6916124785647673 and parameters: {'hidden_dim': 91, 'num_layers': 2, 'dropout': 0.47575276064995253, 'lr': 0.006665871184168655, 'weight_decay': 0.008171773782241416}. Best is trial 0 with value: 0.6916118544690749.\n",
      "[I 2025-04-22 12:15:43,927] Trial 2 finished with value: 0.6907618817161111 and parameters: {'hidden_dim': 126, 'num_layers': 1, 'dropout': 0.2553051435046747, 'lr': 0.0010624858533356837, 'weight_decay': 0.0034487203446180734}. Best is trial 2 with value: 0.6907618817161111.\n",
      "[I 2025-04-22 12:17:17,382] Trial 3 finished with value: 0.6909791476586286 and parameters: {'hidden_dim': 83, 'num_layers': 1, 'dropout': 0.38456535172158113, 'lr': 0.001719734637616873, 'weight_decay': 0.003938484129069794}. Best is trial 2 with value: 0.6907618817161111.\n",
      "[I 2025-04-22 12:18:40,166] Trial 4 finished with value: 0.6916120087399202 and parameters: {'hidden_dim': 33, 'num_layers': 2, 'dropout': 0.24856762006680683, 'lr': 0.007210875292659104, 'weight_decay': 0.00864622245038993}. Best is trial 2 with value: 0.6907618817161111.\n",
      "[I 2025-04-22 12:20:36,144] Trial 5 finished with value: 0.6916129238465253 and parameters: {'hidden_dim': 33, 'num_layers': 3, 'dropout': 0.3508283512598839, 'lr': 0.009174301576953975, 'weight_decay': 0.009245155069126872}. Best is trial 2 with value: 0.6907618817161111.\n",
      "[I 2025-04-22 12:22:30,139] Trial 6 finished with value: 0.4077439851620618 and parameters: {'hidden_dim': 118, 'num_layers': 1, 'dropout': 0.16261796272846213, 'lr': 0.003559893155709648, 'weight_decay': 0.0018746942921094951}. Best is trial 6 with value: 0.4077439851620618.\n",
      "[I 2025-04-22 12:26:08,179] Trial 7 finished with value: 0.6916103924022001 and parameters: {'hidden_dim': 71, 'num_layers': 3, 'dropout': 0.10386886849248894, 'lr': 0.007257374041048862, 'weight_decay': 0.0006589748730255864}. Best is trial 6 with value: 0.4077439851620618.\n",
      "[I 2025-04-22 12:28:51,142] Trial 8 finished with value: 0.6916105431668899 and parameters: {'hidden_dim': 78, 'num_layers': 2, 'dropout': 0.10945028247159923, 'lr': 0.004411097363598733, 'weight_decay': 0.004844864752865788}. Best is trial 6 with value: 0.4077439851620618.\n",
      "[I 2025-04-22 12:30:33,196] Trial 9 finished with value: 0.26899924961959615 and parameters: {'hidden_dim': 98, 'num_layers': 1, 'dropout': 0.37211454472869254, 'lr': 0.004382784249768086, 'weight_decay': 0.0006912203412648963}. Best is trial 9 with value: 0.26899924961959615.\n",
      "[I 2025-04-22 12:32:22,661] Trial 10 finished with value: 0.2411293212105246 and parameters: {'hidden_dim': 106, 'num_layers': 1, 'dropout': 0.3825953800165834, 'lr': 0.002909196801035554, 'weight_decay': 8.572321075391888e-05}. Best is trial 10 with value: 0.2411293212105246.\n",
      "[I 2025-04-22 12:34:10,336] Trial 11 finished with value: 0.2977152571958654 and parameters: {'hidden_dim': 107, 'num_layers': 1, 'dropout': 0.39302391042228185, 'lr': 0.0029022975531817876, 'weight_decay': 6.634137295098643e-05}. Best is trial 10 with value: 0.2411293212105246.\n",
      "[I 2025-04-22 12:35:59,739] Trial 12 finished with value: 0.4395643560325398 and parameters: {'hidden_dim': 107, 'num_layers': 1, 'dropout': 0.33435234222583277, 'lr': 0.005378002954158989, 'weight_decay': 0.0021979778821346082}. Best is trial 10 with value: 0.2411293212105246.\n",
      "[I 2025-04-22 12:37:14,190] Trial 13 finished with value: 0.2824825968812494 and parameters: {'hidden_dim': 63, 'num_layers': 1, 'dropout': 0.42636719081479324, 'lr': 0.002584271343692759, 'weight_decay': 0.0016312374564546941}. Best is trial 10 with value: 0.2411293212105246.\n",
      "[I 2025-04-22 12:39:02,565] Trial 14 finished with value: 0.6904346346855164 and parameters: {'hidden_dim': 101, 'num_layers': 1, 'dropout': 0.2786336803249695, 'lr': 0.005133788960336391, 'weight_decay': 0.0029992920581275936}. Best is trial 10 with value: 0.2411293212105246.\n",
      "[I 2025-04-22 12:41:10,372] Trial 15 finished with value: 0.28213078923085155 and parameters: {'hidden_dim': 57, 'num_layers': 2, 'dropout': 0.3254716149317893, 'lr': 0.0006095360562897029, 'weight_decay': 0.0007575343388278675}. Best is trial 10 with value: 0.2411293212105246.\n",
      "[I 2025-04-22 12:42:53,863] Trial 16 finished with value: 0.691280417582568 and parameters: {'hidden_dim': 98, 'num_layers': 1, 'dropout': 0.42671382102483507, 'lr': 0.0038725637717222207, 'weight_decay': 0.00612322642233558}. Best is trial 10 with value: 0.2411293212105246.\n",
      "[I 2025-04-22 12:48:09,564] Trial 17 finished with value: 0.37967726588249207 and parameters: {'hidden_dim': 120, 'num_layers': 3, 'dropout': 0.2094501449446693, 'lr': 0.005906075859644491, 'weight_decay': 4.047942664225663e-05}. Best is trial 10 with value: 0.2411293212105246.\n",
      "[I 2025-04-22 12:50:05,145] Trial 18 finished with value: 0.3526723472511067 and parameters: {'hidden_dim': 114, 'num_layers': 1, 'dropout': 0.37701791274070157, 'lr': 0.002213275070908699, 'weight_decay': 0.0013260027706150446}. Best is trial 10 with value: 0.2411293212105246.\n",
      "[I 2025-04-22 12:52:04,634] Trial 19 finished with value: 0.6915502653402441 and parameters: {'hidden_dim': 51, 'num_layers': 2, 'dropout': 0.4977231147203755, 'lr': 0.00013811140372800134, 'weight_decay': 0.002672989429541931}. Best is trial 10 with value: 0.2411293212105246.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 106, 'num_layers': 1, 'dropout': 0.3825953800165834, 'lr': 0.002909196801035554, 'weight_decay': 8.572321075391888e-05}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# BiLSTM with Attention Classifier\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[559   2]\n",
      " [491   5]]\n",
      "Sensitivity: 0.0101, Specificity: 0.9964\n",
      "Epoch [1/30] - Train Loss: 0.6520, Val Loss: 0.6894, Val Acc: 0.5336, Val AUC: 0.5453\n",
      "\n",
      "Confusion Matrix:\n",
      "[[360 201]\n",
      " [ 94 402]]\n",
      "Sensitivity: 0.8105, Specificity: 0.6417\n",
      "Epoch [2/30] - Train Loss: 0.6757, Val Loss: 0.6741, Val Acc: 0.7209, Val AUC: 0.7831\n",
      "\n",
      "Confusion Matrix:\n",
      "[[402 159]\n",
      " [ 51 445]]\n",
      "Sensitivity: 0.8972, Specificity: 0.7166\n",
      "Epoch [3/30] - Train Loss: 0.5892, Val Loss: 0.4816, Val Acc: 0.8013, Val AUC: 0.8653\n",
      "\n",
      "Confusion Matrix:\n",
      "[[452 109]\n",
      " [ 40 456]]\n",
      "Sensitivity: 0.9194, Specificity: 0.8057\n",
      "Epoch [4/30] - Train Loss: 0.4536, Val Loss: 0.3412, Val Acc: 0.8590, Val AUC: 0.9308\n",
      "\n",
      "Confusion Matrix:\n",
      "[[478  83]\n",
      " [ 43 453]]\n",
      "Sensitivity: 0.9133, Specificity: 0.8520\n",
      "Epoch [5/30] - Train Loss: 0.3595, Val Loss: 0.2979, Val Acc: 0.8808, Val AUC: 0.9483\n",
      "\n",
      "Confusion Matrix:\n",
      "[[448 113]\n",
      " [ 23 473]]\n",
      "Sensitivity: 0.9536, Specificity: 0.7986\n",
      "Epoch [6/30] - Train Loss: 0.3697, Val Loss: 0.3700, Val Acc: 0.8713, Val AUC: 0.9422\n",
      "\n",
      "Confusion Matrix:\n",
      "[[476  85]\n",
      " [ 67 429]]\n",
      "Sensitivity: 0.8649, Specificity: 0.8485\n",
      "Epoch [7/30] - Train Loss: 0.5165, Val Loss: 0.3626, Val Acc: 0.8562, Val AUC: 0.9271\n",
      "\n",
      "Confusion Matrix:\n",
      "[[444 117]\n",
      " [ 38 458]]\n",
      "Sensitivity: 0.9234, Specificity: 0.7914\n",
      "Epoch [8/30] - Train Loss: 0.3903, Val Loss: 0.3166, Val Acc: 0.8534, Val AUC: 0.9470\n",
      "\n",
      "Confusion Matrix:\n",
      "[[533  28]\n",
      " [ 90 406]]\n",
      "Sensitivity: 0.8185, Specificity: 0.9501\n",
      "Epoch [9/30] - Train Loss: 0.3292, Val Loss: 0.2965, Val Acc: 0.8884, Val AUC: 0.9581\n",
      "\n",
      "Confusion Matrix:\n",
      "[[524  37]\n",
      " [ 86 410]]\n",
      "Sensitivity: 0.8266, Specificity: 0.9340\n",
      "Epoch [10/30] - Train Loss: 0.3218, Val Loss: 0.2818, Val Acc: 0.8836, Val AUC: 0.9535\n",
      "\n",
      "Confusion Matrix:\n",
      "[[423 138]\n",
      " [ 34 462]]\n",
      "Sensitivity: 0.9315, Specificity: 0.7540\n",
      "Epoch [11/30] - Train Loss: 0.3529, Val Loss: 0.3662, Val Acc: 0.8373, Val AUC: 0.9380\n",
      "\n",
      "Confusion Matrix:\n",
      "[[501  60]\n",
      " [ 55 441]]\n",
      "Sensitivity: 0.8891, Specificity: 0.8930\n",
      "Epoch [12/30] - Train Loss: 0.4600, Val Loss: 0.2894, Val Acc: 0.8912, Val AUC: 0.9521\n",
      "\n",
      "Confusion Matrix:\n",
      "[[375 186]\n",
      " [ 14 482]]\n",
      "Sensitivity: 0.9718, Specificity: 0.6684\n",
      "Epoch [13/30] - Train Loss: 0.3185, Val Loss: 0.5775, Val Acc: 0.8108, Val AUC: 0.9384\n",
      "\n",
      "Confusion Matrix:\n",
      "[[500  61]\n",
      " [ 52 444]]\n",
      "Sensitivity: 0.8952, Specificity: 0.8913\n",
      "Epoch [14/30] - Train Loss: 0.3417, Val Loss: 0.2816, Val Acc: 0.8931, Val AUC: 0.9528\n",
      "\n",
      "Confusion Matrix:\n",
      "[[497  64]\n",
      " [ 43 453]]\n",
      "Sensitivity: 0.9133, Specificity: 0.8859\n",
      "Epoch [15/30] - Train Loss: 0.2893, Val Loss: 0.2657, Val Acc: 0.8988, Val AUC: 0.9619\n",
      "\n",
      "Confusion Matrix:\n",
      "[[530  31]\n",
      " [ 78 418]]\n",
      "Sensitivity: 0.8427, Specificity: 0.9447\n",
      "Epoch [16/30] - Train Loss: 0.3104, Val Loss: 0.2891, Val Acc: 0.8969, Val AUC: 0.9581\n",
      "\n",
      "Confusion Matrix:\n",
      "[[520  41]\n",
      " [ 90 406]]\n",
      "Sensitivity: 0.8185, Specificity: 0.9269\n",
      "Epoch [17/30] - Train Loss: 0.3494, Val Loss: 0.3126, Val Acc: 0.8761, Val AUC: 0.9473\n",
      "\n",
      "Confusion Matrix:\n",
      "[[481  80]\n",
      " [122 374]]\n",
      "Sensitivity: 0.7540, Specificity: 0.8574\n",
      "Epoch [18/30] - Train Loss: 0.3953, Val Loss: 0.3786, Val Acc: 0.8089, Val AUC: 0.9076\n",
      "\n",
      "Confusion Matrix:\n",
      "[[490  71]\n",
      " [102 394]]\n",
      "Sensitivity: 0.7944, Specificity: 0.8734\n",
      "Epoch [19/30] - Train Loss: 0.3907, Val Loss: 0.3473, Val Acc: 0.8363, Val AUC: 0.9257\n",
      "\n",
      "Confusion Matrix:\n",
      "[[477  84]\n",
      " [ 65 431]]\n",
      "Sensitivity: 0.8690, Specificity: 0.8503\n",
      "Epoch [20/30] - Train Loss: 0.3733, Val Loss: 0.3395, Val Acc: 0.8590, Val AUC: 0.9266\n",
      "\n",
      "Confusion Matrix:\n",
      "[[435 126]\n",
      " [ 46 450]]\n",
      "Sensitivity: 0.9073, Specificity: 0.7754\n",
      "Epoch [21/30] - Train Loss: 0.4321, Val Loss: 0.3954, Val Acc: 0.8373, Val AUC: 0.9028\n",
      "\n",
      "Confusion Matrix:\n",
      "[[415 146]\n",
      " [ 25 471]]\n",
      "Sensitivity: 0.9496, Specificity: 0.7398\n",
      "Epoch [22/30] - Train Loss: 0.4402, Val Loss: 0.3461, Val Acc: 0.8382, Val AUC: 0.9316\n",
      "\n",
      "Confusion Matrix:\n",
      "[[380 181]\n",
      " [ 14 482]]\n",
      "Sensitivity: 0.9718, Specificity: 0.6774\n",
      "Epoch [23/30] - Train Loss: 0.3564, Val Loss: 0.4651, Val Acc: 0.8155, Val AUC: 0.9357\n",
      "\n",
      "Confusion Matrix:\n",
      "[[488  73]\n",
      " [145 351]]\n",
      "Sensitivity: 0.7077, Specificity: 0.8699\n",
      "Epoch [24/30] - Train Loss: 0.3896, Val Loss: 0.3799, Val Acc: 0.7938, Val AUC: 0.9014\n",
      "\n",
      "Confusion Matrix:\n",
      "[[466  95]\n",
      " [ 80 416]]\n",
      "Sensitivity: 0.8387, Specificity: 0.8307\n",
      "Epoch [25/30] - Train Loss: 0.4433, Val Loss: 0.3973, Val Acc: 0.8344, Val AUC: 0.9183\n",
      "\n",
      "Confusion Matrix:\n",
      "[[448 113]\n",
      " [ 45 451]]\n",
      "Sensitivity: 0.9093, Specificity: 0.7986\n",
      "Epoch [26/30] - Train Loss: 0.4146, Val Loss: 0.3308, Val Acc: 0.8505, Val AUC: 0.9389\n",
      "\n",
      "Confusion Matrix:\n",
      "[[500  61]\n",
      " [ 61 435]]\n",
      "Sensitivity: 0.8770, Specificity: 0.8913\n",
      "Epoch [27/30] - Train Loss: 0.3740, Val Loss: 0.2903, Val Acc: 0.8846, Val AUC: 0.9516\n",
      "\n",
      "Confusion Matrix:\n",
      "[[501  60]\n",
      " [ 80 416]]\n",
      "Sensitivity: 0.8387, Specificity: 0.8930\n",
      "Epoch [28/30] - Train Loss: 0.3598, Val Loss: 0.3148, Val Acc: 0.8675, Val AUC: 0.9423\n",
      "\n",
      "Confusion Matrix:\n",
      "[[441 120]\n",
      " [ 28 468]]\n",
      "Sensitivity: 0.9435, Specificity: 0.7861\n",
      "Epoch [29/30] - Train Loss: 0.3452, Val Loss: 0.3139, Val Acc: 0.8600, Val AUC: 0.9484\n",
      "\n",
      "Confusion Matrix:\n",
      "[[553   8]\n",
      " [207 289]]\n",
      "Sensitivity: 0.5827, Specificity: 0.9857\n",
      "Epoch [30/30] - Train Loss: 0.3246, Val Loss: 0.4402, Val Acc: 0.7966, Val AUC: 0.9457\n",
      "\n",
      "Confusion Matrix:\n",
      "[[542  18]\n",
      " [203 293]]\n",
      "Sensitivity: 0.5907, Specificity: 0.9679\n",
      "Test Loss: 0.4428, Test Accuracy: 0.7907, Test AUC: 0.9293\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model-bilstm_attention.pt')\n",
    "\n",
    "\n",
    "model = BiLSTMWithAttentionClassifier(\n",
    "    input_dim=20,\n",
    "    hidden_dim=bilstm_attn_best_param['hidden_dim'],\n",
    "    num_layers=bilstm_attn_best_param['num_layers'],\n",
    "    dropout=bilstm_attn_best_param['dropout']\n",
    ")\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=30, lr=bilstm_attn_best_param['lr'],\n",
    "                      weight_decay=bilstm_attn_best_param['weight_decay'], verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning to TB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'K', 'P', 'X', 'M', 'N', 'G', 'I', 'T', 'D', 'Y', 'Q', 'C', 'H', 'S', 'V', 'E', 'L', 'A', 'F', 'R', 'W'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 269\n",
      "Validation: 90\n",
      "Test: 90\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 14:46:37,616] A new study created in memory with name: no-name-0d2c8d70-c041-4e89-aef8-09c8c764a29d\n",
      "[I 2025-04-22 14:51:38,298] Trial 0 finished with value: 0.4906810224056244 and parameters: {'lr': 0.00029459295919321096, 'weight_decay': 0.00041123032554754574, 'dropout': 0.4026469476037292}. Best is trial 0 with value: 0.4906810224056244.\n",
      "[I 2025-04-22 14:56:41,650] Trial 1 finished with value: 0.47201791405677795 and parameters: {'lr': 0.0004503565705254801, 'weight_decay': 1.8480134425459972e-06, 'dropout': 0.17743764969613363}. Best is trial 1 with value: 0.47201791405677795.\n",
      "[I 2025-04-22 15:01:03,140] Trial 2 finished with value: 0.5174084007740021 and parameters: {'lr': 4.586658137880564e-05, 'weight_decay': 0.0061186361421569175, 'dropout': 0.29505108036462874}. Best is trial 1 with value: 0.47201791405677795.\n",
      "[I 2025-04-22 15:02:40,551] Trial 3 finished with value: 0.5120506882667542 and parameters: {'lr': 8.153778131102356e-05, 'weight_decay': 0.0037325168035185242, 'dropout': 0.16144998886122297}. Best is trial 1 with value: 0.47201791405677795.\n",
      "[I 2025-04-22 15:02:55,871] Trial 4 finished with value: 0.5180416504542033 and parameters: {'lr': 4.38813909372333e-05, 'weight_decay': 0.00037311454007050364, 'dropout': 0.2989046233515198}. Best is trial 1 with value: 0.47201791405677795.\n",
      "[I 2025-04-22 15:03:09,515] Trial 5 finished with value: 0.517969568570455 and parameters: {'lr': 5.104627658732078e-05, 'weight_decay': 5.383987827379832e-06, 'dropout': 0.4882132362760795}. Best is trial 1 with value: 0.47201791405677795.\n",
      "[I 2025-04-22 15:03:23,172] Trial 6 finished with value: 0.4208306272824605 and parameters: {'lr': 0.005804685974806929, 'weight_decay': 1.9101936388592966e-05, 'dropout': 0.43746580657121437}. Best is trial 6 with value: 0.4208306272824605.\n",
      "[I 2025-04-22 15:03:37,242] Trial 7 finished with value: 0.5083250105381012 and parameters: {'lr': 0.00011499542870932807, 'weight_decay': 0.0025910091677922267, 'dropout': 0.2710461767907648}. Best is trial 6 with value: 0.4208306272824605.\n",
      "[I 2025-04-22 15:03:49,174] Trial 8 finished with value: 0.5036520759264628 and parameters: {'lr': 0.00015138153832690846, 'weight_decay': 0.0016932643547168465, 'dropout': 0.3169549320185956}. Best is trial 6 with value: 0.4208306272824605.\n",
      "[I 2025-04-22 15:03:57,671] Trial 9 finished with value: 0.42310335238774616 and parameters: {'lr': 0.004484698582306077, 'weight_decay': 0.0006580388867593674, 'dropout': 0.48476073481843496}. Best is trial 6 with value: 0.4208306272824605.\n",
      "[I 2025-04-22 15:04:06,790] Trial 10 finished with value: 0.4161060651143392 and parameters: {'lr': 0.00821742224858213, 'weight_decay': 2.4699688423297757e-05, 'dropout': 0.39097522814970825}. Best is trial 10 with value: 0.4161060651143392.\n",
      "[I 2025-04-22 15:04:16,211] Trial 11 finished with value: 0.4149552782376607 and parameters: {'lr': 0.007631181032847596, 'weight_decay': 2.6922911501658407e-05, 'dropout': 0.40237939197963885}. Best is trial 11 with value: 0.4149552782376607.\n",
      "[I 2025-04-22 15:04:26,045] Trial 12 finished with value: 0.43905303875605267 and parameters: {'lr': 0.0016895079480286133, 'weight_decay': 6.091287426963216e-05, 'dropout': 0.37692635733145335}. Best is trial 11 with value: 0.4149552782376607.\n",
      "[I 2025-04-22 15:04:34,540] Trial 13 finished with value: 0.41491926709810895 and parameters: {'lr': 0.009551633347012976, 'weight_decay': 3.398187179038976e-05, 'dropout': 0.3700572272856807}. Best is trial 13 with value: 0.41491926709810895.\n",
      "[I 2025-04-22 15:04:43,144] Trial 14 finished with value: 0.5223344365755717 and parameters: {'lr': 1.2150197591280398e-05, 'weight_decay': 9.952990920013989e-05, 'dropout': 0.35385319465215914}. Best is trial 13 with value: 0.41491926709810895.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.009551633347012976, 'weight_decay': 3.398187179038976e-05, 'dropout': 0.3700572272856807}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation function with detailed output\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    # print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    # print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    # print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function for frozen encoder\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-lstm-trans-frozen/FrozenEncoder_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        # if val_auc > best_val_auc:\n",
    "        #     best_val_auc = val_auc\n",
    "        #     torch.save(model.state_dict(), 'best_model_frozen.pt')\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "lstm_best_param = {'hidden_dim': 74,\n",
    " 'num_layers': 3,\n",
    " 'dropout': 0.3037059572844035,\n",
    " 'lr': 0.00774103421243492,\n",
    " 'weight_decay': 2.4221276513292614e-05}\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=74, num_layers=3, dropout=dropout)\n",
    "    model.load_state_dict(torch.load('best_model_lstm.pt'))\n",
    "    \n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=15)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_frozen_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.009551633347012976,\n",
       " 'weight_decay': 3.398187179038976e-05,\n",
       " 'dropout': 0.3700572272856807}"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_frozen_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8000, AUC: 0.8691\n",
      "Sensitivity: 0.7073, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [12 29]]\n",
      "Epoch [1/20] - Train Loss: 0.4568, Val Loss: 0.4598, Val Acc: 0.8000, Val AUC: 0.8691\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8701\n",
      "Sensitivity: 0.7073, Specificity: 0.8571\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [12 29]]\n",
      "Epoch [2/20] - Train Loss: 0.4511, Val Loss: 0.4396, Val Acc: 0.7889, Val AUC: 0.8701\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8711\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [3/20] - Train Loss: 0.4131, Val Loss: 0.4278, Val Acc: 0.7889, Val AUC: 0.8711\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8706\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [4/20] - Train Loss: 0.4231, Val Loss: 0.4221, Val Acc: 0.7889, Val AUC: 0.8706\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8706\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [5/20] - Train Loss: 0.4001, Val Loss: 0.4188, Val Acc: 0.7889, Val AUC: 0.8706\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8721\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [6/20] - Train Loss: 0.4205, Val Loss: 0.4180, Val Acc: 0.7889, Val AUC: 0.8721\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8731\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [7/20] - Train Loss: 0.4136, Val Loss: 0.4178, Val Acc: 0.7889, Val AUC: 0.8731\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8736\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [8/20] - Train Loss: 0.3781, Val Loss: 0.4183, Val Acc: 0.7889, Val AUC: 0.8736\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7778, AUC: 0.8746\n",
      "Sensitivity: 0.6829, Specificity: 0.8571\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [13 28]]\n",
      "Epoch [9/20] - Train Loss: 0.4086, Val Loss: 0.4192, Val Acc: 0.7778, Val AUC: 0.8746\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8741\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [10/20] - Train Loss: 0.4007, Val Loss: 0.4169, Val Acc: 0.7889, Val AUC: 0.8741\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8741\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [11/20] - Train Loss: 0.3921, Val Loss: 0.4162, Val Acc: 0.7889, Val AUC: 0.8741\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7778, AUC: 0.8746\n",
      "Sensitivity: 0.6829, Specificity: 0.8571\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [13 28]]\n",
      "Epoch [12/20] - Train Loss: 0.4159, Val Loss: 0.4164, Val Acc: 0.7778, Val AUC: 0.8746\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8746\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [13/20] - Train Loss: 0.3588, Val Loss: 0.4162, Val Acc: 0.7889, Val AUC: 0.8746\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8736\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [14/20] - Train Loss: 0.3980, Val Loss: 0.4193, Val Acc: 0.7889, Val AUC: 0.8736\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8751\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [15/20] - Train Loss: 0.3947, Val Loss: 0.4189, Val Acc: 0.7889, Val AUC: 0.8751\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7778, AUC: 0.8756\n",
      "Sensitivity: 0.6829, Specificity: 0.8571\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [13 28]]\n",
      "Epoch [16/20] - Train Loss: 0.3909, Val Loss: 0.4190, Val Acc: 0.7778, Val AUC: 0.8756\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8756\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [17/20] - Train Loss: 0.3872, Val Loss: 0.4219, Val Acc: 0.7889, Val AUC: 0.8756\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7778, AUC: 0.8771\n",
      "Sensitivity: 0.6829, Specificity: 0.8571\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [13 28]]\n",
      "Epoch [18/20] - Train Loss: 0.4000, Val Loss: 0.4206, Val Acc: 0.7778, Val AUC: 0.8771\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8766\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [19/20] - Train Loss: 0.3913, Val Loss: 0.4198, Val Acc: 0.7889, Val AUC: 0.8766\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8761\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Epoch [20/20] - Train Loss: 0.4034, Val Loss: 0.4202, Val Acc: 0.7889, Val AUC: 0.8761\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8646\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Test Loss: 0.4590, Test Accuracy: 0.7889, Test AUC: 0.8646\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-trans-frozen/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Best hyperparameters: {'lr': 0.009940295438316211, 'weight_decay': 1.4383289881186473e-05, 'dropout': 0.22563027249521914}\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=74, num_layers=3, dropout=lstm_frozen_best_param['dropout'])\n",
    "model.load_state_dict(torch.load('best_model_lstm.pt'))\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_frozen_best_param['lr'],\n",
    "                      weight_decay=lstm_frozen_best_param['weight_decay'], verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 15:57:18,855] A new study created in memory with name: no-name-2fd2bde9-3990-4aca-a51b-5eb44af1b6b6\n",
      "[I 2025-04-22 16:00:36,232] Trial 0 finished with value: 0.3121185749769211 and parameters: {'lr': 0.0011600925306825354, 'weight_decay': 6.284163255894181e-05, 'dropout': 0.1465439280224056}. Best is trial 0 with value: 0.3121185749769211.\n",
      "[I 2025-04-22 16:01:06,311] Trial 1 finished with value: 0.30792737503846485 and parameters: {'lr': 0.0015427872690244417, 'weight_decay': 0.00024790716935044476, 'dropout': 0.25441143365716756}. Best is trial 1 with value: 0.30792737503846485.\n",
      "[I 2025-04-22 16:01:32,564] Trial 2 finished with value: 0.4422498345375061 and parameters: {'lr': 8.949802097157093e-05, 'weight_decay': 1.4047374349727724e-06, 'dropout': 0.1522961829242273}. Best is trial 1 with value: 0.30792737503846485.\n",
      "[I 2025-04-22 16:01:59,584] Trial 3 finished with value: 0.40053096413612366 and parameters: {'lr': 0.00020101316432051788, 'weight_decay': 0.00023912084034580908, 'dropout': 0.1104490612635948}. Best is trial 1 with value: 0.30792737503846485.\n",
      "[I 2025-04-22 16:02:29,785] Trial 4 finished with value: 0.3444167623917262 and parameters: {'lr': 0.0004839143788454453, 'weight_decay': 9.781946940820719e-06, 'dropout': 0.16211662307811303}. Best is trial 1 with value: 0.30792737503846485.\n",
      "[I 2025-04-22 16:02:57,249] Trial 5 finished with value: 0.5042774379253387 and parameters: {'lr': 1.8000183457714708e-05, 'weight_decay': 0.00010346702247489456, 'dropout': 0.27423462078668415}. Best is trial 1 with value: 0.30792737503846485.\n",
      "[I 2025-04-22 16:03:25,867] Trial 6 finished with value: 0.3928045431772868 and parameters: {'lr': 0.00022749046333290707, 'weight_decay': 0.0002653379907553406, 'dropout': 0.16828403365321393}. Best is trial 1 with value: 0.30792737503846485.\n",
      "[I 2025-04-22 16:03:53,901] Trial 7 finished with value: 0.337284579873085 and parameters: {'lr': 0.0009866994347768306, 'weight_decay': 0.0024394734490485116, 'dropout': 0.4651065581174999}. Best is trial 1 with value: 0.30792737503846485.\n",
      "[I 2025-04-22 16:04:18,576] Trial 8 finished with value: 0.3133152921994527 and parameters: {'lr': 0.005740047398499189, 'weight_decay': 4.481662491861123e-05, 'dropout': 0.4784251740436334}. Best is trial 1 with value: 0.30792737503846485.\n",
      "[I 2025-04-22 16:04:42,776] Trial 9 finished with value: 0.3912864526112874 and parameters: {'lr': 0.0002750848797345188, 'weight_decay': 0.00013899311746314619, 'dropout': 0.3002769359379146}. Best is trial 1 with value: 0.30792737503846485.\n",
      "[I 2025-04-22 16:05:07,711] Trial 10 finished with value: 0.3183436046044032 and parameters: {'lr': 0.008259499873767943, 'weight_decay': 0.004934721451762671, 'dropout': 0.3164172371932987}. Best is trial 1 with value: 0.30792737503846485.\n",
      "[I 2025-04-22 16:05:32,555] Trial 11 finished with value: 0.2839040706555049 and parameters: {'lr': 0.0019261139149500178, 'weight_decay': 1.7415984703472294e-05, 'dropout': 0.22874596765773414}. Best is trial 11 with value: 0.2839040706555049.\n",
      "[I 2025-04-22 16:05:56,005] Trial 12 finished with value: 0.2941100100676219 and parameters: {'lr': 0.002467110126567054, 'weight_decay': 7.551412363167152e-06, 'dropout': 0.23870629525516004}. Best is trial 11 with value: 0.2839040706555049.\n",
      "[I 2025-04-22 16:06:21,589] Trial 13 finished with value: 0.3035551259915034 and parameters: {'lr': 0.003980354438866671, 'weight_decay': 5.914759636302295e-06, 'dropout': 0.375419997955585}. Best is trial 11 with value: 0.2839040706555049.\n",
      "[I 2025-04-22 16:06:47,081] Trial 14 finished with value: 0.28745319445927936 and parameters: {'lr': 0.002411787345792507, 'weight_decay': 1.2013490357298706e-05, 'dropout': 0.2187604546608801}. Best is trial 11 with value: 0.2839040706555049.\n",
      "[I 2025-04-22 16:07:10,908] Trial 15 finished with value: 0.2922557046016057 and parameters: {'lr': 0.002578818515211023, 'weight_decay': 2.625989176776471e-05, 'dropout': 0.21637542235801993}. Best is trial 11 with value: 0.2839040706555049.\n",
      "[I 2025-04-22 16:07:35,609] Trial 16 finished with value: 0.34579835335413617 and parameters: {'lr': 0.0006742733302397078, 'weight_decay': 1.4893214878994893e-06, 'dropout': 0.3539135014223867}. Best is trial 11 with value: 0.2839040706555049.\n",
      "[I 2025-04-22 16:07:58,920] Trial 17 finished with value: 0.4563244084517161 and parameters: {'lr': 7.289211669038069e-05, 'weight_decay': 0.0008306635686342875, 'dropout': 0.20971744271937395}. Best is trial 11 with value: 0.2839040706555049.\n",
      "[I 2025-04-22 16:08:22,200] Trial 18 finished with value: 0.3126682639122009 and parameters: {'lr': 0.0026696477346198613, 'weight_decay': 1.655107414759998e-05, 'dropout': 0.40500920134731644}. Best is trial 11 with value: 0.2839040706555049.\n",
      "[I 2025-04-22 16:08:45,356] Trial 19 finished with value: 0.2765665501356125 and parameters: {'lr': 0.009963308232008547, 'weight_decay': 3.1586184284955368e-06, 'dropout': 0.19744567581033662}. Best is trial 19 with value: 0.2765665501356125.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.009963308232008547, 'weight_decay': 3.1586184284955368e-06, 'dropout': 0.19744567581033662}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation function with detailed output\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    # print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    # print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    # print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function for frozen encoder\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    log_dir = f\"runs-lstm-transfer-fullbackprop/fullbackprop_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        # if val_auc > best_val_auc:\n",
    "        #     best_val_auc = val_auc\n",
    "        #     torch.save(model.state_dict(), 'best_model_frozen.pt')\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_full_backprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# # Load the best pretrained model and fine-tune\n",
    "# def finetune_with_frozen_encoder(pretrained_model_path, train_loader, val_loader, hidden_dim, num_layers, dropout):\n",
    "#     model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "#     model.load_state_dict(torch.load(pretrained_model_path))\n",
    "#     # freeze_encoder(model)\n",
    "\n",
    "#     best_auc = train_finetune_model(\n",
    "#         model=model,\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         num_epochs=10,\n",
    "#         lr=1e-3,\n",
    "#         weight_decay=1e-4\n",
    "#     )\n",
    "\n",
    "#     model.load_state_dict(torch.load('best_model_frozen.pt'))\n",
    "#     evaluate_model(model, val_loader, nn.BCELoss())\n",
    "\n",
    "#     return model, best_auc\n",
    "\n",
    "# model, best_auc = finetune_with_frozen_encoder(\n",
    "#     pretrained_model_path='best_model-lstm.pt',\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     hidden_dim=47,  # or from Optuna\n",
    "#     num_layers=2,\n",
    "#     dropout=0.3\n",
    "# )\n",
    "\n",
    "\n",
    "# lstm_best_param = {'hidden_dim': 74,\n",
    "#  'num_layers': 3,\n",
    "#  'dropout': 0.3037059572844035,\n",
    "#  'lr': 0.00774103421243492,\n",
    "#  'weight_decay': 2.4221276513292614e-05}\n",
    "\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=74, num_layers=3, dropout=dropout)\n",
    "    model.load_state_dict(torch.load('best_model_lstm.pt'))\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_fullbackprop_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5444, AUC: 0.7023\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Epoch [1/19] - Train Loss: 0.6916, Val Loss: 334.0142, Val Acc: 0.5444, Val AUC: 0.7023\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7000, AUC: 0.7641\n",
      "Sensitivity: 0.4634, Specificity: 0.8980\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [22 19]]\n",
      "Epoch [2/19] - Train Loss: 0.6750, Val Loss: 333.9610, Val Acc: 0.7000, Val AUC: 0.7641\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7222, AUC: 0.7591\n",
      "Sensitivity: 0.4634, Specificity: 0.9388\n",
      "Confusion Matrix:\n",
      "[[46  3]\n",
      " [22 19]]\n",
      "Epoch [3/19] - Train Loss: 0.5751, Val Loss: 333.9765, Val Acc: 0.7222, Val AUC: 0.7591\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7667, AUC: 0.8104\n",
      "Sensitivity: 0.8293, Specificity: 0.7143\n",
      "Confusion Matrix:\n",
      "[[35 14]\n",
      " [ 7 34]]\n",
      "Epoch [4/19] - Train Loss: 0.5033, Val Loss: 333.8800, Val Acc: 0.7667, Val AUC: 0.8104\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8333, AUC: 0.8636\n",
      "Sensitivity: 0.7805, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [ 9 32]]\n",
      "Epoch [5/19] - Train Loss: 0.5313, Val Loss: 333.7919, Val Acc: 0.8333, Val AUC: 0.8636\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7889, AUC: 0.8716\n",
      "Sensitivity: 0.6098, Specificity: 0.9388\n",
      "Confusion Matrix:\n",
      "[[46  3]\n",
      " [16 25]]\n",
      "Epoch [6/19] - Train Loss: 0.4987, Val Loss: 333.8124, Val Acc: 0.7889, Val AUC: 0.8716\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8222, AUC: 0.8691\n",
      "Sensitivity: 0.7073, Specificity: 0.9184\n",
      "Confusion Matrix:\n",
      "[[45  4]\n",
      " [12 29]]\n",
      "Epoch [7/19] - Train Loss: 0.4359, Val Loss: 333.7724, Val Acc: 0.8222, Val AUC: 0.8691\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6667, AUC: 0.7108\n",
      "Sensitivity: 0.2683, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [30 11]]\n",
      "Epoch [8/19] - Train Loss: 0.6679, Val Loss: 333.9483, Val Acc: 0.6667, Val AUC: 0.7108\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6556, AUC: 0.6964\n",
      "Sensitivity: 0.2683, Specificity: 0.9796\n",
      "Confusion Matrix:\n",
      "[[48  1]\n",
      " [30 11]]\n",
      "Epoch [9/19] - Train Loss: 0.5793, Val Loss: 333.9554, Val Acc: 0.6556, Val AUC: 0.6964\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6556, AUC: 0.7103\n",
      "Sensitivity: 0.2683, Specificity: 0.9796\n",
      "Confusion Matrix:\n",
      "[[48  1]\n",
      " [30 11]]\n",
      "Epoch [10/19] - Train Loss: 0.5479, Val Loss: 333.9402, Val Acc: 0.6556, Val AUC: 0.7103\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5556, AUC: 0.5241\n",
      "Sensitivity: 0.1463, Specificity: 0.8980\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [35  6]]\n",
      "Epoch [11/19] - Train Loss: 0.6891, Val Loss: 334.0228, Val Acc: 0.5556, Val AUC: 0.5241\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5889, AUC: 0.6720\n",
      "Sensitivity: 0.1951, Specificity: 0.9184\n",
      "Confusion Matrix:\n",
      "[[45  4]\n",
      " [33  8]]\n",
      "Epoch [12/19] - Train Loss: 0.6555, Val Loss: 333.9906, Val Acc: 0.5889, Val AUC: 0.6720\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6556, AUC: 0.6730\n",
      "Sensitivity: 0.4634, Specificity: 0.8163\n",
      "Confusion Matrix:\n",
      "[[40  9]\n",
      " [22 19]]\n",
      "Epoch [13/19] - Train Loss: 0.5947, Val Loss: 333.9667, Val Acc: 0.6556, Val AUC: 0.6730\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7222, AUC: 0.7287\n",
      "Sensitivity: 0.5366, Specificity: 0.8776\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [19 22]]\n",
      "Epoch [14/19] - Train Loss: 0.5258, Val Loss: 333.9348, Val Acc: 0.7222, Val AUC: 0.7287\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7556, AUC: 0.7889\n",
      "Sensitivity: 0.5854, Specificity: 0.8980\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [17 24]]\n",
      "Epoch [15/19] - Train Loss: 0.4743, Val Loss: 333.8828, Val Acc: 0.7556, Val AUC: 0.7889\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7667, AUC: 0.8188\n",
      "Sensitivity: 0.7073, Specificity: 0.8163\n",
      "Confusion Matrix:\n",
      "[[40  9]\n",
      " [12 29]]\n",
      "Epoch [16/19] - Train Loss: 0.4614, Val Loss: 333.8396, Val Acc: 0.7667, Val AUC: 0.8188\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7778, AUC: 0.8477\n",
      "Sensitivity: 0.5610, Specificity: 0.9592\n",
      "Confusion Matrix:\n",
      "[[47  2]\n",
      " [18 23]]\n",
      "Epoch [17/19] - Train Loss: 0.3735, Val Loss: 333.8803, Val Acc: 0.7778, Val AUC: 0.8477\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7778, AUC: 0.8228\n",
      "Sensitivity: 0.7561, Specificity: 0.7959\n",
      "Confusion Matrix:\n",
      "[[39 10]\n",
      " [10 31]]\n",
      "Epoch [18/19] - Train Loss: 0.3483, Val Loss: 333.8416, Val Acc: 0.7778, Val AUC: 0.8228\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7556, AUC: 0.8293\n",
      "Sensitivity: 0.7317, Specificity: 0.7755\n",
      "Confusion Matrix:\n",
      "[[38 11]\n",
      " [11 30]]\n",
      "Epoch [19/19] - Train Loss: 0.2805, Val Loss: 333.9099, Val Acc: 0.7556, Val AUC: 0.8293\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7222, AUC: 0.7815\n",
      "Sensitivity: 0.7073, Specificity: 0.7347\n",
      "Confusion Matrix:\n",
      "[[36 13]\n",
      " [12 29]]\n",
      "Test Loss: 334.0237, Test Accuracy: 0.7222, Test AUC: 0.7815\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 1000.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Best hyperparameters: {'lr': 0.008986542560528932, 'weight_decay': 2.3033044758439348e-06, 'dropout': 0.17164705350229123}\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.17164705350229123)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.008986542560528932,\n",
    "                      weight_decay=2.3033044758439348e-06, verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:43:10,269] A new study created in memory with name: no-name-eb8b7852-4f2e-40d6-90a6-2f071c8e5e2f\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:43:20,876] Trial 0 finished with value: 0.5023543735345205 and parameters: {'lr': 2.3335848243162707e-05, 'weight_decay': 0.0011180295014566086, 'dropout': 0.43516711418523524}. Best is trial 0 with value: 0.5023543735345205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:43:29,581] Trial 1 finished with value: 0.331018144885699 and parameters: {'lr': 0.009455223580062516, 'weight_decay': 1.603761337597022e-05, 'dropout': 0.2760462120282645}. Best is trial 1 with value: 0.331018144885699.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:43:38,114] Trial 2 finished with value: 0.37372104823589325 and parameters: {'lr': 0.000489723751148945, 'weight_decay': 2.215389888605569e-05, 'dropout': 0.43949948589959564}. Best is trial 1 with value: 0.331018144885699.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:43:46,303] Trial 3 finished with value: 0.4848433832327525 and parameters: {'lr': 2.3983568443166548e-05, 'weight_decay': 0.000265167634018538, 'dropout': 0.15556905136785232}. Best is trial 1 with value: 0.331018144885699.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:43:54,306] Trial 4 finished with value: 0.4088914891084035 and parameters: {'lr': 7.356885508742978e-05, 'weight_decay': 0.0007351939668497223, 'dropout': 0.10699933898505064}. Best is trial 1 with value: 0.331018144885699.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:44:01,683] Trial 5 finished with value: 0.36174143354098004 and parameters: {'lr': 0.005994850616092257, 'weight_decay': 0.005658747374782232, 'dropout': 0.22357938975589553}. Best is trial 1 with value: 0.331018144885699.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:44:10,262] Trial 6 finished with value: 0.39163445432980853 and parameters: {'lr': 0.00019743127577145366, 'weight_decay': 0.0010901950822587532, 'dropout': 0.35141680610585757}. Best is trial 1 with value: 0.331018144885699.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:44:21,979] Trial 7 finished with value: 0.38255469501018524 and parameters: {'lr': 0.0003752879335802349, 'weight_decay': 0.00018809128075216353, 'dropout': 0.3282069194013414}. Best is trial 1 with value: 0.331018144885699.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:44:28,048] Trial 8 finished with value: 0.4495243926843007 and parameters: {'lr': 3.888204069354846e-05, 'weight_decay': 0.005091092539773318, 'dropout': 0.4450606513295222}. Best is trial 1 with value: 0.331018144885699.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:44:33,768] Trial 9 finished with value: 0.3259330987930298 and parameters: {'lr': 0.009277756892612359, 'weight_decay': 0.007087210931281583, 'dropout': 0.17885881665986722}. Best is trial 9 with value: 0.3259330987930298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:44:39,968] Trial 10 finished with value: 0.3499974012374878 and parameters: {'lr': 0.0020179873381084767, 'weight_decay': 1.5052396676671534e-06, 'dropout': 0.21902686469155025}. Best is trial 9 with value: 0.3259330987930298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:44:45,455] Trial 11 finished with value: 0.3602117995421092 and parameters: {'lr': 0.007627007645969603, 'weight_decay': 8.915380170831348e-06, 'dropout': 0.24077269804172263}. Best is trial 9 with value: 0.3259330987930298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:44:50,902] Trial 12 finished with value: 0.3433910508950551 and parameters: {'lr': 0.0019109949993984926, 'weight_decay': 3.0071145459543625e-05, 'dropout': 0.277045291878584}. Best is trial 9 with value: 0.3259330987930298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:44:56,170] Trial 13 finished with value: 0.33839601775010425 and parameters: {'lr': 0.0021812193262524247, 'weight_decay': 3.3839617768394016e-06, 'dropout': 0.17610732942576818}. Best is trial 9 with value: 0.3259330987930298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:45:01,223] Trial 14 finished with value: 0.42248696088790894 and parameters: {'lr': 0.009870579089352682, 'weight_decay': 5.261390518294477e-05, 'dropout': 0.3715250699500538}. Best is trial 9 with value: 0.3259330987930298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:45:06,573] Trial 15 finished with value: 0.3664361983537674 and parameters: {'lr': 0.0010052197000623747, 'weight_decay': 8.867903453931933e-06, 'dropout': 0.2807919593623092}. Best is trial 9 with value: 0.3259330987930298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:45:12,122] Trial 16 finished with value: 0.3553706109523773 and parameters: {'lr': 0.0037887152226696564, 'weight_decay': 0.0001020804595259073, 'dropout': 0.17330231591889628}. Best is trial 9 with value: 0.3259330987930298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:45:17,617] Trial 17 finished with value: 0.3619195520877838 and parameters: {'lr': 0.0008985051828990493, 'weight_decay': 9.504293523437055e-06, 'dropout': 0.11760455486232924}. Best is trial 9 with value: 0.3259330987930298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:45:22,942] Trial 18 finished with value: 0.400753657023112 and parameters: {'lr': 0.0001249568513454652, 'weight_decay': 0.0024551273063148256, 'dropout': 0.3853650693030935}. Best is trial 9 with value: 0.3259330987930298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:45:28,346] Trial 19 finished with value: 0.5729557275772095 and parameters: {'lr': 1.0798840195495096e-05, 'weight_decay': 0.00033091489004911327, 'dropout': 0.49186396358429557}. Best is trial 9 with value: 0.3259330987930298.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.009277756892612359, 'weight_decay': 0.007087210931281583, 'dropout': 0.17885881665986722}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm-trans-frozen/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=118, num_layers=1, dropout=dropout, max_seq_len=100)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.009277756892612359,\n",
       " 'weight_decay': 0.007087210931281583,\n",
       " 'dropout': 0.17885881665986722}"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_frozen_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8367\n",
      "Epoch [1/15] - Train Loss: 0.4531, Val Loss: 0.4280, Val Acc: 0.7778, Val AUC: 0.8766\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8367\n",
      "Epoch [2/15] - Train Loss: 0.4264, Val Loss: 0.4235, Val Acc: 0.7778, Val AUC: 0.8800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8367\n",
      "Epoch [3/15] - Train Loss: 0.4329, Val Loss: 0.4192, Val Acc: 0.7778, Val AUC: 0.8835\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8367\n",
      "Epoch [4/15] - Train Loss: 0.4132, Val Loss: 0.4174, Val Acc: 0.7778, Val AUC: 0.8875\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.8367\n",
      "Epoch [5/15] - Train Loss: 0.3841, Val Loss: 0.4164, Val Acc: 0.7889, Val AUC: 0.8885\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8367\n",
      "Epoch [6/15] - Train Loss: 0.3917, Val Loss: 0.4139, Val Acc: 0.7778, Val AUC: 0.8920\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8367\n",
      "Epoch [7/15] - Train Loss: 0.3825, Val Loss: 0.4129, Val Acc: 0.7778, Val AUC: 0.8905\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8367\n",
      "Epoch [8/15] - Train Loss: 0.3734, Val Loss: 0.4122, Val Acc: 0.7778, Val AUC: 0.8905\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  9]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8163\n",
      "Epoch [9/15] - Train Loss: 0.3626, Val Loss: 0.4124, Val Acc: 0.7667, Val AUC: 0.8895\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.8367\n",
      "Epoch [10/15] - Train Loss: 0.3553, Val Loss: 0.4128, Val Acc: 0.7667, Val AUC: 0.8880\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.8367\n",
      "Epoch [11/15] - Train Loss: 0.3514, Val Loss: 0.4121, Val Acc: 0.7667, Val AUC: 0.8885\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  9]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8163\n",
      "Epoch [12/15] - Train Loss: 0.3593, Val Loss: 0.4109, Val Acc: 0.7667, Val AUC: 0.8875\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  9]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8163\n",
      "Epoch [13/15] - Train Loss: 0.3536, Val Loss: 0.4109, Val Acc: 0.7667, Val AUC: 0.8870\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  9]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.8163\n",
      "Epoch [14/15] - Train Loss: 0.3575, Val Loss: 0.4115, Val Acc: 0.7556, Val AUC: 0.8860\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  9]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.8163\n",
      "Epoch [15/15] - Train Loss: 0.3503, Val Loss: 0.4114, Val Acc: 0.7556, Val AUC: 0.8860\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.8571\n",
      "Test Loss: 0.4273, Test Accuracy: 0.8000, Test AUC: 0.8810\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=118, num_layers=1, dropout=bilstm_frozen_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=15, lr=bilstm_frozen_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:50:01,797] A new study created in memory with name: no-name-9d29e846-9104-463c-8c0c-7afe2ff0bb73\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:50:10,907] Trial 0 finished with value: 0.39799461762110394 and parameters: {'lr': 0.00010892703304944279, 'weight_decay': 1.0736633504676355e-06, 'dropout': 0.39179214892239445}. Best is trial 0 with value: 0.39799461762110394.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:50:19,933] Trial 1 finished with value: 0.31180902818838757 and parameters: {'lr': 0.0010554477184172171, 'weight_decay': 0.0004573469392683518, 'dropout': 0.1599433822436323}. Best is trial 1 with value: 0.31180902818838757.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:50:28,596] Trial 2 finished with value: 0.283524622519811 and parameters: {'lr': 0.0031504138985299843, 'weight_decay': 1.2705221614052957e-06, 'dropout': 0.21728580468140055}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:50:37,056] Trial 3 finished with value: 0.40231017271677655 and parameters: {'lr': 0.0001278443526372003, 'weight_decay': 2.7013943122661382e-05, 'dropout': 0.2707261019040521}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:50:45,711] Trial 4 finished with value: 0.40434269110361737 and parameters: {'lr': 6.813271587896625e-05, 'weight_decay': 2.437478099116392e-06, 'dropout': 0.15700908406217487}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:50:54,199] Trial 5 finished with value: 0.454436331987381 and parameters: {'lr': 2.3080908048973473e-05, 'weight_decay': 3.892311766266586e-05, 'dropout': 0.18943617344247807}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:51:02,650] Trial 6 finished with value: 0.407478928565979 and parameters: {'lr': 6.3270846047434e-05, 'weight_decay': 5.237031616080392e-06, 'dropout': 0.36159503759512124}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:51:10,921] Trial 7 finished with value: 0.34220366179943085 and parameters: {'lr': 0.0002722872648125804, 'weight_decay': 5.7486949493079035e-05, 'dropout': 0.29548019201811415}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:51:19,818] Trial 8 finished with value: 0.31618215640385944 and parameters: {'lr': 0.0016476153136882829, 'weight_decay': 0.0032011505261727003, 'dropout': 0.2426760873488093}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:51:28,849] Trial 9 finished with value: 0.41342538595199585 and parameters: {'lr': 4.8190791667294685e-05, 'weight_decay': 0.00010934714205268503, 'dropout': 0.3123430803969145}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:51:37,086] Trial 10 finished with value: 0.3467424710591634 and parameters: {'lr': 0.008826557870842284, 'weight_decay': 6.673856241597427e-06, 'dropout': 0.11423798213420487}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:51:45,787] Trial 11 finished with value: 0.33246560394763947 and parameters: {'lr': 0.0015226247693474115, 'weight_decay': 0.0007561204845874783, 'dropout': 0.19507621876644604}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:51:54,154] Trial 12 finished with value: 0.34617944558461505 and parameters: {'lr': 0.0016010943684002473, 'weight_decay': 0.00037173429289720603, 'dropout': 0.10047232761441292}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:52:02,712] Trial 13 finished with value: 0.31942644715309143 and parameters: {'lr': 0.005843072858861475, 'weight_decay': 0.00607689327345922, 'dropout': 0.21875879475922827}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:52:11,058] Trial 14 finished with value: 0.334758182366689 and parameters: {'lr': 0.0006028303176408281, 'weight_decay': 0.000990577506537292, 'dropout': 0.4417545164426193}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:52:19,739] Trial 15 finished with value: 0.32708630959192914 and parameters: {'lr': 0.003612502522139149, 'weight_decay': 0.00018810330286980805, 'dropout': 0.14839601796083726}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:52:27,982] Trial 16 finished with value: 0.35577507813771564 and parameters: {'lr': 0.0006115331874981567, 'weight_decay': 1.3961023707811966e-05, 'dropout': 0.15686038256721394}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:52:36,285] Trial 17 finished with value: 0.3125676115353902 and parameters: {'lr': 0.0030627706535609296, 'weight_decay': 0.0011981253460950233, 'dropout': 0.4817111360735101}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:52:44,656] Trial 18 finished with value: 0.35160864392916363 and parameters: {'lr': 0.0007801860537965197, 'weight_decay': 0.00023106326694115721, 'dropout': 0.22738898583843278}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 5 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:52:56,211] Trial 19 finished with value: 0.3696010857820511 and parameters: {'lr': 0.00025422466344742, 'weight_decay': 1.6009562419116798e-05, 'dropout': 0.34039429181168346}. Best is trial 2 with value: 0.283524622519811.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.0031504138985299843, 'weight_decay': 1.2705221614052957e-06, 'dropout': 0.21728580468140055}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm-trans-fullbackprop/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=118, num_layers=1, dropout=dropout, max_seq_len=100)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.0031504138985299843,\n",
       " 'weight_decay': 1.2705221614052957e-06,\n",
       " 'dropout': 0.21728580468140055}"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_fullbackprop_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.8571\n",
      "Epoch [1/15] - Train Loss: 0.4403, Val Loss: 0.3801, Val Acc: 0.8111, Val AUC: 0.9034\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8367\n",
      "Epoch [2/15] - Train Loss: 0.3069, Val Loss: 0.3833, Val Acc: 0.8222, Val AUC: 0.9089\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8571\n",
      "Epoch [3/15] - Train Loss: 0.2854, Val Loss: 0.4057, Val Acc: 0.8444, Val AUC: 0.9074\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8571\n",
      "Epoch [4/15] - Train Loss: 0.2330, Val Loss: 0.3785, Val Acc: 0.8222, Val AUC: 0.9139\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8571\n",
      "Epoch [5/15] - Train Loss: 0.1884, Val Loss: 0.3996, Val Acc: 0.8333, Val AUC: 0.9288\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8776\n",
      "Epoch [6/15] - Train Loss: 0.1741, Val Loss: 0.4377, Val Acc: 0.8333, Val AUC: 0.9248\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  3]\n",
      " [14 27]]\n",
      "Sensitivity: 0.6585, Specificity: 0.9388\n",
      "Epoch [7/15] - Train Loss: 0.1880, Val Loss: 0.4794, Val Acc: 0.8111, Val AUC: 0.8910\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 12]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.7551\n",
      "Epoch [8/15] - Train Loss: 0.2204, Val Loss: 0.4453, Val Acc: 0.8000, Val AUC: 0.9109\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  4]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.9184\n",
      "Epoch [9/15] - Train Loss: 0.1945, Val Loss: 0.4338, Val Acc: 0.8444, Val AUC: 0.9034\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8776\n",
      "Epoch [10/15] - Train Loss: 0.1363, Val Loss: 0.4465, Val Acc: 0.8444, Val AUC: 0.9109\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8776\n",
      "Epoch [11/15] - Train Loss: 0.1021, Val Loss: 0.4393, Val Acc: 0.8556, Val AUC: 0.9199\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8776\n",
      "Epoch [12/15] - Train Loss: 0.0880, Val Loss: 0.4703, Val Acc: 0.8444, Val AUC: 0.9223\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8980\n",
      "Epoch [13/15] - Train Loss: 0.0899, Val Loss: 0.4226, Val Acc: 0.8667, Val AUC: 0.9313\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8776\n",
      "Epoch [14/15] - Train Loss: 0.0802, Val Loss: 0.4465, Val Acc: 0.8667, Val AUC: 0.9418\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8980\n",
      "Epoch [15/15] - Train Loss: 0.0441, Val Loss: 0.4756, Val Acc: 0.8667, Val AUC: 0.9323\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  2]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.9592\n",
      "Test Loss: 0.3610, Test Accuracy: 0.9222, Test AUC: 0.9627\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    else:\n",
    "        model.train()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=118, num_layers=1, dropout=bilstm_fullbackprop_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=15, lr=bilstm_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:53:10,487] A new study created in memory with name: no-name-7867df44-39d0-4a11-b580-7f181bc62d75\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:53:16,338] Trial 0 finished with value: 0.6887914737065634 and parameters: {'lr': 0.0034685534635919306, 'weight_decay': 0.003959810593141905, 'dropout': 0.12402416540595765}. Best is trial 0 with value: 0.6887914737065634.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:53:21,077] Trial 1 finished with value: 0.688648521900177 and parameters: {'lr': 0.00428124843906636, 'weight_decay': 7.464915501203422e-05, 'dropout': 0.4097649018019477}. Best is trial 1 with value: 0.688648521900177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:53:26,235] Trial 2 finished with value: 0.690078337987264 and parameters: {'lr': 0.0002662987691231249, 'weight_decay': 3.0059924218799216e-06, 'dropout': 0.3819187651423831}. Best is trial 1 with value: 0.688648521900177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:53:31,594] Trial 3 finished with value: 0.6929955085118612 and parameters: {'lr': 2.0467835578757365e-05, 'weight_decay': 8.24205840022524e-05, 'dropout': 0.37741698944876056}. Best is trial 1 with value: 0.688648521900177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:53:36,315] Trial 4 finished with value: 0.6901043653488159 and parameters: {'lr': 0.00041952754845406076, 'weight_decay': 0.0039137880384353734, 'dropout': 0.31760151173647067}. Best is trial 1 with value: 0.688648521900177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:53:41,094] Trial 5 finished with value: 0.6909016370773315 and parameters: {'lr': 3.8354369713635926e-05, 'weight_decay': 4.336404643126644e-05, 'dropout': 0.2061025075040968}. Best is trial 1 with value: 0.688648521900177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:53:45,694] Trial 6 finished with value: 0.6888973116874695 and parameters: {'lr': 0.0009543010405433803, 'weight_decay': 4.9954083821399565e-05, 'dropout': 0.1306676566633252}. Best is trial 1 with value: 0.688648521900177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:53:50,841] Trial 7 finished with value: 0.6887438495953878 and parameters: {'lr': 0.004005331590079127, 'weight_decay': 0.005509395341923934, 'dropout': 0.409072872189043}. Best is trial 1 with value: 0.688648521900177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:53:55,604] Trial 8 finished with value: 0.6899891297022501 and parameters: {'lr': 0.0002624541151617177, 'weight_decay': 9.17253903482945e-06, 'dropout': 0.3585368274643702}. Best is trial 1 with value: 0.688648521900177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:54:00,334] Trial 9 finished with value: 0.6903899113337199 and parameters: {'lr': 0.0004785027938878931, 'weight_decay': 0.009857399202546297, 'dropout': 0.4724050660193001}. Best is trial 1 with value: 0.688648521900177.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:54:05,010] Trial 10 finished with value: 0.6886415282885233 and parameters: {'lr': 0.009094750128329793, 'weight_decay': 0.00034968274879836736, 'dropout': 0.47997202215515783}. Best is trial 10 with value: 0.6886415282885233.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:54:09,617] Trial 11 finished with value: 0.6885474920272827 and parameters: {'lr': 0.00810126296687975, 'weight_decay': 0.0004883045455184517, 'dropout': 0.48419922377224167}. Best is trial 11 with value: 0.6885474920272827.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.00810126296687975, 'weight_decay': 0.0004883045455184517, 'dropout': 0.48419922377224167}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM with Attention\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze LSTM encoder\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-att-trans/LSTMAttTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_att_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Load compatible weights\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=dropout)\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=12)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_att_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.00810126296687975,\n",
       " 'weight_decay': 0.0004883045455184517,\n",
       " 'dropout': 0.48419922377224167}"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_att_frozen_best_parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/20] - Train Loss: 0.6897, Val Loss: 0.6895, Val Acc: 0.5444, Val AUC: 0.5963\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/20] - Train Loss: 0.6903, Val Loss: 0.6885, Val Acc: 0.5444, Val AUC: 0.5859\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/20] - Train Loss: 0.6869, Val Loss: 0.6884, Val Acc: 0.5444, Val AUC: 0.6147\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/20] - Train Loss: 0.6894, Val Loss: 0.6882, Val Acc: 0.5444, Val AUC: 0.6327\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/20] - Train Loss: 0.6913, Val Loss: 0.6882, Val Acc: 0.5444, Val AUC: 0.6217\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/20] - Train Loss: 0.6863, Val Loss: 0.6882, Val Acc: 0.5444, Val AUC: 0.6346\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/20] - Train Loss: 0.6904, Val Loss: 0.6879, Val Acc: 0.5444, Val AUC: 0.6411\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [8/20] - Train Loss: 0.6850, Val Loss: 0.6880, Val Acc: 0.5444, Val AUC: 0.6421\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [9/20] - Train Loss: 0.6858, Val Loss: 0.6877, Val Acc: 0.5444, Val AUC: 0.6486\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [10/20] - Train Loss: 0.6864, Val Loss: 0.6875, Val Acc: 0.5444, Val AUC: 0.6570\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [11/20] - Train Loss: 0.6884, Val Loss: 0.6874, Val Acc: 0.5444, Val AUC: 0.6605\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [12/20] - Train Loss: 0.6897, Val Loss: 0.6875, Val Acc: 0.5444, Val AUC: 0.6705\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [13/20] - Train Loss: 0.6873, Val Loss: 0.6873, Val Acc: 0.5444, Val AUC: 0.6670\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [14/20] - Train Loss: 0.6848, Val Loss: 0.6872, Val Acc: 0.5444, Val AUC: 0.6675\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [15/20] - Train Loss: 0.6864, Val Loss: 0.6871, Val Acc: 0.5444, Val AUC: 0.6655\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [16/20] - Train Loss: 0.6855, Val Loss: 0.6870, Val Acc: 0.5444, Val AUC: 0.6635\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [17/20] - Train Loss: 0.6832, Val Loss: 0.6870, Val Acc: 0.5444, Val AUC: 0.6585\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [18/20] - Train Loss: 0.6853, Val Loss: 0.6868, Val Acc: 0.5444, Val AUC: 0.6600\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [19/20] - Train Loss: 0.6866, Val Loss: 0.6867, Val Acc: 0.5444, Val AUC: 0.6655\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [20/20] - Train Loss: 0.6855, Val Loss: 0.6865, Val Acc: 0.5444, Val AUC: 0.6665\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Test Loss: 0.6885, Test Accuracy: 0.5444, Test AUC: 0.6809\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=lstm_att_frozen_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_att_frozen_best_parameters['lr'],\n",
    "                      weight_decay=lstm_att_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full back prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:54:18,879] A new study created in memory with name: no-name-30bff861-f4fa-4e11-a9dc-10fe63dc0733\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:54:25,827] Trial 0 finished with value: 0.6888269186019897 and parameters: {'lr': 0.00023161725737326302, 'weight_decay': 0.0006789287526768497, 'dropout': 0.254073788491035}. Best is trial 0 with value: 0.6888269186019897.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:54:32,385] Trial 1 finished with value: 0.6884546677271525 and parameters: {'lr': 0.001676532254929715, 'weight_decay': 0.0011926561048898304, 'dropout': 0.2691144068977741}. Best is trial 1 with value: 0.6884546677271525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:54:38,995] Trial 2 finished with value: 0.6899946729342142 and parameters: {'lr': 1.6745532264619806e-05, 'weight_decay': 3.8150861006281116e-05, 'dropout': 0.26207036116241844}. Best is trial 1 with value: 0.6884546677271525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:54:45,715] Trial 3 finished with value: 0.6891067028045654 and parameters: {'lr': 0.00012389518926457044, 'weight_decay': 0.0003780249098522084, 'dropout': 0.14090679322945845}. Best is trial 1 with value: 0.6884546677271525.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:54:52,515] Trial 4 finished with value: 0.6029607951641083 and parameters: {'lr': 0.000614962419054572, 'weight_decay': 1.9709499579082144e-05, 'dropout': 0.3839274381526012}. Best is trial 4 with value: 0.6029607951641083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:54:58,915] Trial 5 finished with value: 0.6874303817749023 and parameters: {'lr': 0.00044028953969325787, 'weight_decay': 0.0003209856057163104, 'dropout': 0.19356180645955867}. Best is trial 4 with value: 0.6029607951641083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:55:05,665] Trial 6 finished with value: 0.6906452775001526 and parameters: {'lr': 1.3176462632845961e-05, 'weight_decay': 0.000519188262352177, 'dropout': 0.41583375855580285}. Best is trial 4 with value: 0.6029607951641083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:55:12,332] Trial 7 finished with value: 0.691280464331309 and parameters: {'lr': 1.446356759525051e-05, 'weight_decay': 0.0008128841509980762, 'dropout': 0.4294070014332477}. Best is trial 4 with value: 0.6029607951641083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:55:18,926] Trial 8 finished with value: 0.6891558766365051 and parameters: {'lr': 8.308276659716993e-05, 'weight_decay': 0.00027734740771269103, 'dropout': 0.4626972977907605}. Best is trial 4 with value: 0.6029607951641083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:55:25,891] Trial 9 finished with value: 0.6880476474761963 and parameters: {'lr': 0.00018706655165140362, 'weight_decay': 5.605605344935749e-06, 'dropout': 0.3946119486120706}. Best is trial 4 with value: 0.6029607951641083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:55:32,548] Trial 10 finished with value: 0.6887740890185038 and parameters: {'lr': 0.004666611114871186, 'weight_decay': 0.009223667198282845, 'dropout': 0.3412771133026684}. Best is trial 4 with value: 0.6029607951641083.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:55:39,424] Trial 11 finished with value: 0.5274957120418549 and parameters: {'lr': 0.0009506172620007108, 'weight_decay': 2.201309387443407e-05, 'dropout': 0.14925815091782568}. Best is trial 11 with value: 0.5274957120418549.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.0009506172620007108, 'weight_decay': 2.201309387443407e-05, 'dropout': 0.14925815091782568}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM with Attention\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze LSTM encoder\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-att-trans-fullback/LSTMAttTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_att_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Load compatible weights\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=dropout)\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=12)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_att_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/20] - Train Loss: 0.6891, Val Loss: 0.6885, Val Acc: 0.5444, Val AUC: 0.6909\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/20] - Train Loss: 0.6844, Val Loss: 0.6859, Val Acc: 0.5444, Val AUC: 0.7342\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/20] - Train Loss: 0.6838, Val Loss: 0.6818, Val Acc: 0.5444, Val AUC: 0.7576\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/20] - Train Loss: 0.6770, Val Loss: 0.6697, Val Acc: 0.5444, Val AUC: 0.7770\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48  1]\n",
      " [30 11]]\n",
      "Sensitivity: 0.2683, Specificity: 0.9796\n",
      "Epoch [5/20] - Train Loss: 0.6594, Val Loss: 0.6232, Val Acc: 0.6556, Val AUC: 0.8248\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39 10]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.7959\n",
      "Epoch [6/20] - Train Loss: 0.5977, Val Loss: 0.5350, Val Acc: 0.7889, Val AUC: 0.8507\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32 17]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.6531\n",
      "Epoch [7/20] - Train Loss: 0.5731, Val Loss: 0.5644, Val Acc: 0.7333, Val AUC: 0.8233\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8571\n",
      "Epoch [8/20] - Train Loss: 0.5104, Val Loss: 0.4925, Val Acc: 0.8222, Val AUC: 0.8522\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8367\n",
      "Epoch [9/20] - Train Loss: 0.4879, Val Loss: 0.4828, Val Acc: 0.8111, Val AUC: 0.8447\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  3]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.9388\n",
      "Epoch [10/20] - Train Loss: 0.4629, Val Loss: 0.4399, Val Acc: 0.8556, Val AUC: 0.8810\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  9]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8163\n",
      "Epoch [11/20] - Train Loss: 0.4915, Val Loss: 0.4646, Val Acc: 0.8222, Val AUC: 0.8512\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8571\n",
      "Epoch [12/20] - Train Loss: 0.4312, Val Loss: 0.4511, Val Acc: 0.8222, Val AUC: 0.8621\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  4]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.9184\n",
      "Epoch [13/20] - Train Loss: 0.4306, Val Loss: 0.4211, Val Acc: 0.8556, Val AUC: 0.8805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 13]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.7347\n",
      "Epoch [14/20] - Train Loss: 0.4421, Val Loss: 0.4841, Val Acc: 0.7889, Val AUC: 0.8477\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  3]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.9388\n",
      "Epoch [15/20] - Train Loss: 0.4265, Val Loss: 0.4228, Val Acc: 0.8889, Val AUC: 0.8875\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8571\n",
      "Epoch [16/20] - Train Loss: 0.4156, Val Loss: 0.4306, Val Acc: 0.8444, Val AUC: 0.8805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8980\n",
      "Epoch [17/20] - Train Loss: 0.4058, Val Loss: 0.4232, Val Acc: 0.8556, Val AUC: 0.8830\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39 10]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.7959\n",
      "Epoch [18/20] - Train Loss: 0.3853, Val Loss: 0.4394, Val Acc: 0.8111, Val AUC: 0.8671\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  2]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.9592\n",
      "Epoch [19/20] - Train Loss: 0.5668, Val Loss: 0.5835, Val Acc: 0.8333, Val AUC: 0.8153\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25 24]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.5102\n",
      "Epoch [20/20] - Train Loss: 0.5369, Val Loss: 0.6641, Val Acc: 0.6889, Val AUC: 0.8158\n",
      "\n",
      "Confusion Matrix:\n",
      "[[24 25]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.4898\n",
      "Test Loss: 0.6660, Test Accuracy: 0.6889, Test AUC: 0.7964\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=lstm_att_fullbackprop_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_att_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=lstm_att_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
