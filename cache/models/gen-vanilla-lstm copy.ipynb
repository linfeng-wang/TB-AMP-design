{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General AMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "# uniprot_df = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\")\n",
    "# uniprot_df1 = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\")\n",
    "# uniprot_df = pd.concat([uniprot_df, uniprot_df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### raw data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes: {'Train': 2314, 'Validation': 496, 'Test': 496}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load only positive (AMP) sequences\n",
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "generation_seqs = adam_df[\"Sequence\"].reset_index(drop=True)\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "class GenerativeSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        input_seq = seq[:-1]  # all residues except the last\n",
    "        target_seq = seq[1:]  # all residues except the first\n",
    "        length = len(input_seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        input_one_hot = one_hot_torch(input_seq, dtype=self.one_hot_dtype)\n",
    "        target_one_hot = one_hot_torch(target_seq, dtype=self.one_hot_dtype)\n",
    "        # target_indices = torch.tensor([\"ACDEFGHIKLMNPQRSTVWY\".index(res) for res in target_seq], dtype=torch.long)\n",
    "        return input_one_hot, target_one_hot, length\n",
    "\n",
    "def generative_collate_and_pack(batch):\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "\n",
    "    lengths = torch.tensor(lengths)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    targets = [targets[i] for i in sorted_indices]\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    sequences = [seq.T for seq in sequences]  # transpose to [seq_len, features]\n",
    "    targets = [tgt.T for tgt in targets]      # transpose targets as well\n",
    "\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)\n",
    "    padded_targets = pad_sequence(targets, batch_first=False)\n",
    "\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "    packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, packed_target, lengths\n",
    "\n",
    "\n",
    "# Train/val/test split\n",
    "train_seqs, test_seqs = train_test_split(generation_seqs, test_size=0.3, random_state=42)\n",
    "val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = GenerativeSequenceDataset(train_seqs)\n",
    "val_dataset = GenerativeSequenceDataset(val_seqs)\n",
    "test_dataset = GenerativeSequenceDataset(test_seqs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=generative_collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "\n",
    "# Dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\", dataset_sizes)\n",
    "\n",
    "for x, y, l in train_loader:\n",
    "    print(\"Input shape:\", x.data.shape)  # [L, B, 20]\n",
    "    print(\"Target shape:\", y.data.shape)  # [L, B, 20]\n",
    "    print(\"Lengths:\", y.batch_sizes)  # Lengths of sequences in the batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sliding window data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes: {'Train': 10392, 'Validation': 2227, 'Test': 2227}\n"
     ]
    }
   ],
   "source": [
    "# # Re-import required libraries after environment reset\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load only positive (AMP) sequences\n",
    "# adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "\n",
    "# # Clean non-standard amino acids\n",
    "# unique_letters = set(''.join(adam_df[\"Sequence\"]))\n",
    "# amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "# # non_standard_amino_acids = set(unique_letters) - set(amino_acids)\n",
    "# # adam_df = adam_df[~adam_df[\"Sequence\"].str.contains('|'.join(non_standard_amino_acids))]\n",
    "\n",
    "# # Apply sliding window to generate fragments\n",
    "# def generate_fragments(sequences, window_size=15, stride=5):\n",
    "#     fragments = []\n",
    "#     for seq in sequences:\n",
    "#         for start in range(0, len(seq) - window_size + 1, stride):\n",
    "#             fragment = seq[start:start + window_size]\n",
    "#             fragments.append(fragment)\n",
    "#     return fragments\n",
    "\n",
    "# generation_fragments = generate_fragments(adam_df[\"Sequence\"].tolist())\n",
    "\n",
    "# # Define one-hot encoding function\n",
    "# def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "#     amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "#     seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "#     aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "#     arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "#     for i, aa in enumerate(aa_bytes):\n",
    "#         arr[i, seq_bytes == aa] = 1\n",
    "#     return arr\n",
    "\n",
    "# # Dataset using one-hot encoding for generative modeling\n",
    "# class AMPGenerationOneHotDataset(Dataset):\n",
    "#     def __init__(self, sequences):\n",
    "#         self.sequences = sequences\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         seq = self.sequences[idx]\n",
    "#         input_seq = seq[:-1]  # all residues except the last\n",
    "#         target_seq = seq[1:]  # all residues except the first\n",
    "#         input_one_hot = one_hot_torch(input_seq)  # shape: [20, seq_len - 1]\n",
    "#         target_one_hot = one_hot_torch(target_seq)  # shape: [20, seq_len - 1]\n",
    "#         length = input_one_hot.shape[1]\n",
    "#         return input_one_hot, target_one_hot, length\n",
    "\n",
    "# # Collate function for packing one-hot sequences\n",
    "# def collate_and_pack_for_generation(batch):\n",
    "#     sequences, targets, lengths = zip(*batch)\n",
    "#     lengths = torch.tensor(lengths)\n",
    "\n",
    "#     # Sort by length (required for packing)\n",
    "#     sorted_indices = torch.argsort(lengths, descending=True)\n",
    "#     sequences = [sequences[i] for i in sorted_indices]\n",
    "#     targets = [targets[i] for i in sorted_indices]\n",
    "#     lengths = lengths[sorted_indices]\n",
    "\n",
    "#     # Transpose each to [L, 20] and pad\n",
    "#     sequences = [seq.T for seq in sequences]  # from [20, L] to [L, 20]\n",
    "#     targets = [tgt.T for tgt in targets]      # from [20, L] to [L, 20]\n",
    "\n",
    "#     padded_seqs = pad_sequence(sequences, batch_first=False)  # [L, B, 20]\n",
    "#     padded_targets = pad_sequence(targets, batch_first=False)  # [L, B, 20]\n",
    "\n",
    "#     packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "#     packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "\n",
    "#     return packed_input, packed_target, lengths\n",
    "\n",
    "# # Train/val/test split\n",
    "# train_seqs, test_seqs = train_test_split(generation_fragments, test_size=0.3, random_state=42)\n",
    "# val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "\n",
    "# train_dataset = AMPGenerationOneHotDataset(train_seqs)\n",
    "# val_dataset = AMPGenerationOneHotDataset(val_seqs)\n",
    "# test_dataset = AMPGenerationOneHotDataset(test_seqs)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_and_pack_for_generation)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_and_pack_for_generation)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_and_pack_for_generation)\n",
    "\n",
    "# # Dataset sizes for verification\n",
    "# dataset_sizes = {\n",
    "#     \"Train\": len(train_dataset),\n",
    "#     \"Validation\": len(val_dataset),\n",
    "#     \"Test\": len(test_dataset)\n",
    "# }\n",
    "# print(\"Dataset sizes:\", dataset_sizes)\n",
    "\n",
    "# for x, y, l in train_loader:\n",
    "#     print(\"Input shape:\", x.data.shape)  # [L, B, 20]\n",
    "#     print(\"Target shape:\", y.data.shape)  # [L, B, 20]\n",
    "#     print(\"Lengths:\", y.batch_sizes)  # Lengths of sequences in the batch\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=128, num_layers=1, dropout=0.3, output_dim=20):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        dropped = self.dropout(packed_output.data)\n",
    "        logits = self.fc(dropped)\n",
    "        return logits  # shape: [total_timesteps, 20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 2.8637 - Val Loss: 2.8260\n",
      "Epoch 2/10 - Train Loss: 2.8100 - Val Loss: 2.8008\n",
      "Epoch 3/10 - Train Loss: 2.7908 - Val Loss: 2.7863\n",
      "Epoch 4/10 - Train Loss: 2.7726 - Val Loss: 2.7596\n",
      "Epoch 5/10 - Train Loss: 2.7420 - Val Loss: 2.7366\n",
      "Epoch 6/10 - Train Loss: 2.7251 - Val Loss: 2.7261\n",
      "Epoch 7/10 - Train Loss: 2.7148 - Val Loss: 2.7164\n",
      "Epoch 8/10 - Train Loss: 2.7054 - Val Loss: 2.7068\n",
      "Epoch 9/10 - Train Loss: 2.6965 - Val Loss: 2.6990\n",
      "Epoch 10/10 - Train Loss: 2.6893 - Val Loss: 2.6951\n",
      "Test Loss: 2.6848\n"
     ]
    }
   ],
   "source": [
    "model = GenerativeLSTM()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and eval functions\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for packed_input, packed_target, _ in dataloader:\n",
    "        inputs = packed_input.to(device)\n",
    "        targets = torch.argmax(packed_target.data, dim=1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, packed_target, _ in dataloader:\n",
    "            inputs = packed_input.to(device)\n",
    "            targets = torch.argmax(packed_target.data, dim=1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Final test evaluation\n",
    "test_loss = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "class GenerativeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=128, num_layers=1, dropout=0.3):\n",
    "        super(GenerativeLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle packed input\n",
    "        if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n",
    "            packed_output, _ = self.lstm(x)\n",
    "            unpacked_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            return self.fc(unpacked_output)\n",
    "        else:\n",
    "            out, _ = self.lstm(x)\n",
    "            return self.fc(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General AMP - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-21 00:07:02,076] A new study created in memory with name: no-name-8c2f498f-e0ce-40b2-bf7a-81b25e572e07\n",
      "[I 2025-04-21 00:09:19,442] Trial 0 finished with value: 2.4995902606419156 and parameters: {'hidden_dim': 67, 'num_layers': 2, 'dropout': 0.12570484394409054, 'lr': 0.004677222493926821, 'weight_decay': 7.968246024192056e-05}. Best is trial 0 with value: 2.4995902606419156.\n",
      "[I 2025-04-21 00:13:10,984] Trial 1 finished with value: 2.840637166159494 and parameters: {'hidden_dim': 183, 'num_layers': 3, 'dropout': 0.36419621124213974, 'lr': 0.007874219127353997, 'weight_decay': 0.0009583112681146043}. Best is trial 0 with value: 2.4995902606419156.\n",
      "[I 2025-04-21 00:15:00,348] Trial 2 finished with value: 2.5277307306017196 and parameters: {'hidden_dim': 67, 'num_layers': 1, 'dropout': 0.46238019937650876, 'lr': 0.006522478679353828, 'weight_decay': 0.0003194038900462528}. Best is trial 0 with value: 2.4995902606419156.\n",
      "[I 2025-04-21 00:17:11,381] Trial 3 finished with value: 2.6217634882245746 and parameters: {'hidden_dim': 169, 'num_layers': 1, 'dropout': 0.33837780809622153, 'lr': 0.006804685322109014, 'weight_decay': 0.0005902664167467399}. Best is trial 0 with value: 2.4995902606419156.\n",
      "[I 2025-04-21 00:19:02,092] Trial 4 finished with value: 2.6137746470315117 and parameters: {'hidden_dim': 76, 'num_layers': 1, 'dropout': 0.22150964845372859, 'lr': 0.007624310606642987, 'weight_decay': 0.0007656142398248185}. Best is trial 0 with value: 2.4995902606419156.\n",
      "[I 2025-04-21 00:21:00,129] Trial 5 finished with value: 2.595499052320208 and parameters: {'hidden_dim': 99, 'num_layers': 1, 'dropout': 0.2446450864361963, 'lr': 0.009594526391852749, 'weight_decay': 0.0004530541546147005}. Best is trial 0 with value: 2.4995902606419156.\n",
      "[I 2025-04-21 00:23:13,623] Trial 6 finished with value: 2.5560446671077184 and parameters: {'hidden_dim': 67, 'num_layers': 2, 'dropout': 0.35603708519348165, 'lr': 0.00726769652325526, 'weight_decay': 0.00017506166476081108}. Best is trial 0 with value: 2.4995902606419156.\n",
      "[I 2025-04-21 00:26:50,271] Trial 7 finished with value: 2.841160249710083 and parameters: {'hidden_dim': 157, 'num_layers': 3, 'dropout': 0.19409928421461364, 'lr': 0.005496290127540872, 'weight_decay': 0.0004857002374849964}. Best is trial 0 with value: 2.4995902606419156.\n",
      "[I 2025-04-21 00:31:09,007] Trial 8 finished with value: 2.774833951677595 and parameters: {'hidden_dim': 241, 'num_layers': 3, 'dropout': 0.3621405189445087, 'lr': 0.0016692825949303194, 'weight_decay': 0.00045627206140015007}. Best is trial 0 with value: 2.4995902606419156.\n",
      "[I 2025-04-21 00:33:37,417] Trial 9 finished with value: 2.77964186668396 and parameters: {'hidden_dim': 112, 'num_layers': 2, 'dropout': 0.4244401016634317, 'lr': 0.0003578241019949689, 'weight_decay': 0.0007614390588041875}. Best is trial 0 with value: 2.4995902606419156.\n",
      "[I 2025-04-21 00:36:21,576] Trial 10 finished with value: 2.4199170453207834 and parameters: {'hidden_dim': 133, 'num_layers': 2, 'dropout': 0.10063270147175422, 'lr': 0.003237280156212186, 'weight_decay': 2.2594437829479466e-05}. Best is trial 10 with value: 2.4199170453207834.\n",
      "[I 2025-04-21 00:38:53,557] Trial 11 finished with value: 2.4471864019121443 and parameters: {'hidden_dim': 116, 'num_layers': 2, 'dropout': 0.1005799567838892, 'lr': 0.0035506776226956293, 'weight_decay': 6.204520870455081e-06}. Best is trial 10 with value: 2.4199170453207834.\n",
      "[I 2025-04-21 00:41:36,526] Trial 12 finished with value: 2.4417949063437328 and parameters: {'hidden_dim': 133, 'num_layers': 2, 'dropout': 0.1002537547849301, 'lr': 0.0029242266502103103, 'weight_decay': 3.999021590963365e-05}. Best is trial 10 with value: 2.4199170453207834.\n",
      "[I 2025-04-21 00:44:16,544] Trial 13 finished with value: 2.601674311501639 and parameters: {'hidden_dim': 140, 'num_layers': 2, 'dropout': 0.15226548131669665, 'lr': 0.002427388638918197, 'weight_decay': 0.000201756497884289}. Best is trial 10 with value: 2.4199170453207834.\n",
      "[I 2025-04-21 00:47:22,875] Trial 14 finished with value: 2.6402940273284914 and parameters: {'hidden_dim': 193, 'num_layers': 2, 'dropout': 0.2713134800923613, 'lr': 0.003398001528095318, 'weight_decay': 0.0002658140897099838}. Best is trial 10 with value: 2.4199170453207834.\n",
      "[I 2025-04-21 00:50:45,293] Trial 15 finished with value: 2.6717288494110107 and parameters: {'hidden_dim': 137, 'num_layers': 3, 'dropout': 0.17895388924079797, 'lr': 0.0011529037496038185, 'weight_decay': 3.154851060777372e-05}. Best is trial 10 with value: 2.4199170453207834.\n",
      "[I 2025-04-21 00:54:01,407] Trial 16 finished with value: 2.5341594082968575 and parameters: {'hidden_dim': 214, 'num_layers': 2, 'dropout': 0.1473978288318288, 'lr': 0.0034372980121948685, 'weight_decay': 0.00013571972196181446}. Best is trial 10 with value: 2.4199170453207834.\n",
      "[I 2025-04-21 00:57:20,352] Trial 17 finished with value: 2.8381152084895542 and parameters: {'hidden_dim': 138, 'num_layers': 3, 'dropout': 0.19555034114727304, 'lr': 0.004709700083499294, 'weight_decay': 0.00033102763366557304}. Best is trial 10 with value: 2.4199170453207834.\n",
      "[I 2025-04-21 00:59:47,422] Trial 18 finished with value: 2.555536011287144 and parameters: {'hidden_dim': 98, 'num_layers': 2, 'dropout': 0.10620228510096816, 'lr': 0.0022251613612672373, 'weight_decay': 0.00010551041589497678}. Best is trial 10 with value: 2.4199170453207834.\n",
      "[I 2025-04-21 01:01:51,900] Trial 19 finished with value: 2.754141276223319 and parameters: {'hidden_dim': 156, 'num_layers': 1, 'dropout': 0.29001122566384374, 'lr': 0.0003159671302510286, 'weight_decay': 0.0006182728990079011}. Best is trial 10 with value: 2.4199170453207834.\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary packages after reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Criterion\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss on the last time step of each sequence.\n",
    "    \n",
    "    Args:\n",
    "        output: Tensor of shape [B, L, vocab_size]\n",
    "        target_seq: Tensor of shape [B, L] containing target class indices\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss computed only on the last token of each sequence\n",
    "    \"\"\"\n",
    "    # Get last time step for each sequence\n",
    "    last_token_logits = output[:, -1, :]        # [B, vocab_size]\n",
    "    last_token_targets = target_seq[:, -1, :]      # [B]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "    # print('last_token_logits',last_token_logits.shape)\n",
    "    # print('last_token_targets',last_token_targets.shape)\n",
    "\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen/AMP_LSTM_GEN_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])      # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            # print('target_shape before reshape',target_seq.shape)\n",
    "            # target_seq = target_seq.reshape(-1)\n",
    "            # print(f\"Output shape: {output.shape}, Target shape: {target_seq.shape}\")\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_generator.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])  # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # # target_seq = target_seq.reshape(-1)\n",
    "            # # target_seq = target_seq.reshape(-1, target_seq.shape[-1])\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            # assert output.size(0) == target_seq.size(0), f\"Mismatch: {output.size(0)} vs {target_seq.size(0)}\"\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            # print('loss done')\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            preds = output[:, -1, :]        # shape: [B, vocab_size]\n",
    "            preds = torch.argmax(preds, dim=1)  # shape: [B]\n",
    "\n",
    "            targets = target_seq[:, -1, :]      # shape: [B, vocab_size]\n",
    "            targets = torch.argmax(targets, dim=-1)  # shape: [B]\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    try:\n",
    "        auc = roc_auc_score(\n",
    "            torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "            torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "            multi_class='ovr', average='macro'\n",
    "        )\n",
    "    except:\n",
    "        auc = \"undefined\"\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Objective for Optuna tuning\n",
    "def objective_generation(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "\n",
    "lstm_gen_best_params = study.best_trial.params\n",
    "print(lstm_gen_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dim': 133, 'num_layers': 2, 'dropout': 0.10063270147175422, 'lr': 0.003237280156212186, 'weight_decay': 2.2594437829479466e-05}\n"
     ]
    }
   ],
   "source": [
    "print(lstm_gen_best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow installation not found - running with reduced feature set.\n",
      "Serving TensorBoard on localhost; to expose to the network, use a proxy or pass --bind_all\n",
      "TensorBoard 2.19.0 at http://localhost:6006/ (Press CTRL+C to quit)\n",
      "W0421 20:52:55.743830 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-224236'\n",
      "W0421 20:52:55.744151 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-112545'\n",
      "W0421 20:52:55.744218 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250421-000631'\n",
      "W0421 20:52:55.744282 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-230002'\n",
      "W0421 20:52:55.744336 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-224503'\n",
      "W0421 20:52:55.744387 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-234240'\n",
      "W0421 20:52:55.744437 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-234434'\n",
      "W0421 20:52:55.744486 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-233023'\n",
      "W0421 20:52:55.744535 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-112627'\n",
      "W0421 20:52:55.744601 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-224125'\n",
      "W0421 20:52:55.744654 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-220810'\n",
      "W0421 20:52:55.744702 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-235333'\n",
      "W0421 20:52:55.744749 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-225901'\n",
      "W0421 20:52:55.744802 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-231103'\n",
      "W0421 20:52:55.744863 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-112304'\n",
      "W0421 20:52:55.744911 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-112635'\n",
      "W0421 20:52:55.744964 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-231155'\n",
      "W0421 20:52:55.745014 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250421-000220'\n",
      "W0421 20:52:55.745060 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-224011'\n",
      "W0421 20:52:55.745107 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-224352'\n",
      "W0421 20:52:55.745163 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-232908'\n",
      "W0421 20:52:55.745218 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-234019'\n",
      "W0421 20:52:55.745266 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-220852'\n",
      "W0421 20:52:55.745315 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-234404'\n",
      "W0421 20:52:55.745369 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-235031'\n",
      "W0421 20:52:55.745416 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-224533'\n",
      "W0421 20:52:55.745463 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-223914'\n",
      "W0421 20:52:55.745509 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMP_LSTM_GEN_20250420-234453'\n",
      "W0421 20:53:00.754842 139625411036928 plugin_event_multiplexer.py:267] Deleting accumulator 'AMPGen_LSTM_optuna'\n",
      "^C\n"
     ]
    }
   ],
   "source": [
    "!tensorboard --logdir runs-lstm-gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.9928 | Val Loss = 2.9623 | Acc = 0.0000 | AUC = nan | Perplexity = 19.3423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2: Train Loss = 2.9530 | Val Loss = 2.8945 | Acc = 0.0000 | AUC = nan | Perplexity = 18.0740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: Train Loss = 2.9026 | Val Loss = 2.8560 | Acc = 0.9890 | AUC = nan | Perplexity = 17.3913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4: Train Loss = 2.8563 | Val Loss = 2.8168 | Acc = 0.9890 | AUC = nan | Perplexity = 16.7230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5: Train Loss = 2.8069 | Val Loss = 2.7790 | Acc = 0.9890 | AUC = nan | Perplexity = 16.1036\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6: Train Loss = 2.7684 | Val Loss = 2.7419 | Acc = 0.9890 | AUC = nan | Perplexity = 15.5162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7: Train Loss = 2.7463 | Val Loss = 2.7021 | Acc = 0.9890 | AUC = nan | Perplexity = 14.9116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8: Train Loss = 2.6924 | Val Loss = 2.6615 | Acc = 0.9890 | AUC = nan | Perplexity = 14.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9: Train Loss = 2.6546 | Val Loss = 2.6174 | Acc = 0.9890 | AUC = nan | Perplexity = 13.6999\n",
      "Epoch 10: Train Loss = 2.6081 | Val Loss = 2.5749 | Acc = 0.9890 | AUC = nan | Perplexity = 13.1305\n",
      "\n",
      "âœ… Final Test Metrics:\n",
      "Loss = 2.5749, Accuracy = 0.9890, AUC = nan, Perplexity = 13.1305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n",
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/sklearn/metrics/_ranking.py:379: UndefinedMetricWarning: Only one class is present in y_true. ROC AUC score is not defined in that case.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_best_params[\"lr\"], weight_decay=lstm_gen_best_params[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "lstm_gen_best_params = {'hidden_dim': 133, 'num_layers': 2, 'dropout': 0.10063270147175422, 'lr': 0.003237280156212186, 'weight_decay': 2.2594437829479466e-05}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=10)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\"\\nâœ… Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tb amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'D', 'E', 'H', 'R', 'N', 'V', 'L', 'S', 'M', 'Y', 'G', 'Q', 'I', 'K', 'P', 'C', 'W', 'F', 'T', 'X', 'A'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sequences</th>\n",
       "      <th>AMP</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ILRWKWRWWRWRR</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ILPWKWRWWKWRR</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>FIKWKFRWWKWRK</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>HQFRFRFRVRRK</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>RRWKIVVIRWRR</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>937</th>\n",
       "      <td>CGYSSSCCGYRPLCYRRCYS</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>939</th>\n",
       "      <td>MFLSLPTLTVLIPLVSLAGL</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>940</th>\n",
       "      <td>FYSASVEENFPQGCTSTASL</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>941</th>\n",
       "      <td>CFYSLLLPITIPVYVFFHLW</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>942</th>\n",
       "      <td>TWMGIKLFRHN</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>644 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Sequences  AMP\n",
       "1           ILRWKWRWWRWRR  1.0\n",
       "2           ILPWKWRWWKWRR  1.0\n",
       "18          FIKWKFRWWKWRK  1.0\n",
       "21           HQFRFRFRVRRK  1.0\n",
       "37           RRWKIVVIRWRR  1.0\n",
       "..                    ...  ...\n",
       "937  CGYSSSCCGYRPLCYRRCYS  0.0\n",
       "939  MFLSLPTLTVLIPLVSLAGL  0.0\n",
       "940  FYSASVEENFPQGCTSTASL  0.0\n",
       "941  CFYSLLLPITIPVYVFFHLW  0.0\n",
       "942           TWMGIKLFRHN  0.0\n",
       "\n",
       "[644 rows x 2 columns]"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "# df_filtered = df[\n",
    "#     (df['Sequences'].str.len() >= 10) &\n",
    "#     (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "#     (~df['Sequences'].str.contains('X'))\n",
    "# ]\n",
    "df_filtered = df\n",
    "\n",
    "def split_sequence(seq, chunk_size=20):\n",
    "    return [seq[i:i+chunk_size] for i in range(0, len(seq), chunk_size)]\n",
    "\n",
    "new_rows = []\n",
    "for _, row in df_filtered.iterrows():\n",
    "    seq = row['Sequences']\n",
    "    amp_label = row['AMP']\n",
    "    if len(seq) > 40:\n",
    "        for chunk in split_sequence(seq, 20):\n",
    "            new_rows.append({'Sequences': chunk, 'AMP': amp_label})\n",
    "    else:\n",
    "        new_rows.append({'Sequences': seq, 'AMP': amp_label})\n",
    "\n",
    "df_filtered = pd.DataFrame(new_rows)\n",
    "\n",
    "\n",
    "df_filtered = df_filtered[\n",
    "    (df_filtered['Sequences'].str.len() >= 10) &\n",
    "    (df_filtered['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df_filtered['Sequences'].str.contains('X'))\n",
    "]\n",
    "df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(min([len(x) for x in df_filtered['Sequences']]))\n",
    "print(max([len(x) for x in df_filtered['Sequences']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Input shape': torch.Size([1200, 20]),\n",
       " 'Target shape': torch.Size([1200, 20]),\n",
       " 'Lengths': tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 63, 60, 57, 54, 51, 47, 45, 43, 40,\n",
       "         39, 12, 12, 11,  9,  9,  7,  6,  6,  6,  6,  6,  5,  5,  5,  5,  5,  5,\n",
       "          2,  2,  1])}"
      ]
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import libraries after environment reset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "df = df_filtered\n",
    "\n",
    "# Clean and inspect\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "# non_standard_amino_acids = unique_letters - amino_acids\n",
    "# df = df[~df[\"Sequences\"].str.contains('|'.join(non_standard_amino_acids))]\n",
    "\n",
    "# Extract sequences\n",
    "sequences = df[\"Sequences\"].reset_index(drop=True)\n",
    "\n",
    "# Define one-hot function\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "# Define dataset class\n",
    "class GenerativeSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        input_seq = seq[:-1]\n",
    "        target_seq = seq[1:]\n",
    "        length = len(input_seq.replace(\"X\", \"\"))\n",
    "        input_one_hot = one_hot_torch(input_seq, dtype=self.one_hot_dtype)\n",
    "        target_one_hot = one_hot_torch(target_seq, dtype=self.one_hot_dtype)\n",
    "        return input_one_hot, target_one_hot, length\n",
    "\n",
    "# Define collate function\n",
    "def generative_collate_and_pack(batch):\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    targets = [targets[i] for i in sorted_indices]\n",
    "    lengths = lengths[sorted_indices]\n",
    "    sequences = [seq.T for seq in sequences]\n",
    "    targets = [tgt.T for tgt in targets]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)\n",
    "    padded_targets = pad_sequence(targets, batch_first=False)\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "    packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "    return packed_input, packed_target, lengths\n",
    "\n",
    "# Split and load data\n",
    "train_seqs, test_seqs = train_test_split(sequences, test_size=0.3, random_state=42)\n",
    "val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "train_dataset = GenerativeSequenceDataset(train_seqs)\n",
    "val_dataset = GenerativeSequenceDataset(val_seqs)\n",
    "test_dataset = GenerativeSequenceDataset(test_seqs)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=generative_collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "\n",
    "# Preview batch\n",
    "batch_sample = next(iter(train_loader))\n",
    "batch_sample_shapes = {\n",
    "    \"Input shape\": batch_sample[0].data.shape,\n",
    "    \"Target shape\": batch_sample[1].data.shape,\n",
    "    \"Lengths\": batch_sample[0].batch_sizes\n",
    "}\n",
    "batch_sample_shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 450\n",
      "Validation: 97\n",
      "Test: 97\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train for full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 17:44:18,643] A new study created in memory with name: no-name-a4039145-7a7f-4499-ad6c-cff15e85780f\n",
      "[W 2025-04-22 17:44:18,651] Trial 0 failed with parameters: {'dropout': 0.43744503390158795, 'lr': 0.007268487527455038, 'weight_decay': 0.0007802403187660769} because of the following error: RuntimeError('Error(s) in loading state_dict for GenerativeLSTM:\\n\\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([624, 20]) from checkpoint, the shape in current model is torch.Size([532, 20]).\\n\\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([624, 156]) from checkpoint, the shape in current model is torch.Size([532, 133]).\\n\\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([624]) from checkpoint, the shape in current model is torch.Size([532]).\\n\\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([624]) from checkpoint, the shape in current model is torch.Size([532]).\\n\\tsize mismatch for fc.weight: copying a param with shape torch.Size([20, 156]) from checkpoint, the shape in current model is torch.Size([20, 133]).').\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_38502/107883548.py\", line 118, in objective_generation\n",
      "    model = load_pretrained_weights(model, 'best_model_lstm_generator.pt')  # path to the general AMP model\n",
      "  File \"/tmp/ipykernel_38502/107883548.py\", line 8, in load_pretrained_weights\n",
      "    model.load_state_dict(pretrained_dict, strict=False)\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2581, in load_state_dict\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Error(s) in loading state_dict for GenerativeLSTM:\n",
      "\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([624, 20]) from checkpoint, the shape in current model is torch.Size([532, 20]).\n",
      "\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([624, 156]) from checkpoint, the shape in current model is torch.Size([532, 133]).\n",
      "\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([624]) from checkpoint, the shape in current model is torch.Size([532]).\n",
      "\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([624]) from checkpoint, the shape in current model is torch.Size([532]).\n",
      "\tsize mismatch for fc.weight: copying a param with shape torch.Size([20, 156]) from checkpoint, the shape in current model is torch.Size([20, 133]).\n",
      "[W 2025-04-22 17:44:18,652] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for GenerativeLSTM:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([624, 20]) from checkpoint, the shape in current model is torch.Size([532, 20]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([624, 156]) from checkpoint, the shape in current model is torch.Size([532, 133]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([624]) from checkpoint, the shape in current model is torch.Size([532]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([624]) from checkpoint, the shape in current model is torch.Size([532]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([20, 156]) from checkpoint, the shape in current model is torch.Size([20, 133]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[277], line 124\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Run Optuna\u001b[39;00m\n\u001b[1;32m    123\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 124\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective_generation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    125\u001b[0m lstm_gen_best_params_tb \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest transfer learning hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lstm_gen_best_params_tb)\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[277], line 118\u001b[0m, in \u001b[0;36mobjective_generation\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    110\u001b[0m weight_decay \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweight_decay\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m1e-6\u001b[39m, \u001b[38;5;241m1e-3\u001b[39m)\n\u001b[1;32m    112\u001b[0m model \u001b[38;5;241m=\u001b[39m GenerativeLSTM(    \n\u001b[1;32m    113\u001b[0m             hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m133\u001b[39m,\n\u001b[1;32m    114\u001b[0m             num_layers\u001b[38;5;241m=\u001b[39mlstm_gen_best_params[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m    115\u001b[0m             dropout\u001b[38;5;241m=\u001b[39mdropout\n\u001b[1;32m    116\u001b[0m             \u001b[38;5;66;03m# dropout=lstm_gen_best_params[\"dropout\"]\u001b[39;00m\n\u001b[1;32m    117\u001b[0m             )\n\u001b[0;32m--> 118\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_pretrained_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model_lstm_generator.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# path to the general AMP model\u001b[39;00m\n\u001b[1;32m    119\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m train_model_generation(model, train_loader, val_loader, num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    120\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m val_loss\n",
      "Cell \u001b[0;32mIn[277], line 8\u001b[0m, in \u001b[0;36mload_pretrained_weights\u001b[0;34m(model, checkpoint_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_pretrained_weights\u001b[39m(model, checkpoint_path):\n\u001b[1;32m      7\u001b[0m     pretrained_dict \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mload(checkpoint_path, map_location\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m     \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_dict\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstrict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for GenerativeLSTM:\n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([624, 20]) from checkpoint, the shape in current model is torch.Size([532, 20]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([624, 156]) from checkpoint, the shape in current model is torch.Size([532, 133]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([624]) from checkpoint, the shape in current model is torch.Size([532]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([624]) from checkpoint, the shape in current model is torch.Size([532]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([20, 156]) from checkpoint, the shape in current model is torch.Size([20, 133])."
     ]
    }
   ],
   "source": [
    "\n",
    "# Criterion\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Transfer Learning Loader\n",
    "def load_pretrained_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    last_token_logits = output[:, -1, :]\n",
    "    last_token_targets = target_seq[:, -1, :]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen-tb/AMP_LSTM_GEN_TRANSFER_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # if train:\n",
    "            #     torch.save(model.state_dict(), 'best_model_lstm_transfer.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "    \n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Optuna objective for fine-tuning\n",
    "def objective_generation(trial):\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(    \n",
    "                hidden_dim=156,\n",
    "                num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "                dropout=dropout\n",
    "                # dropout=lstm_gen_best_params[\"dropout\"]\n",
    "                )\n",
    "    model = load_pretrained_weights(model, 'best_model_lstm_generator.pt')  # path to the general AMP model\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=10)\n",
    "lstm_gen_best_params_tb = study.best_trial.params\n",
    "print(\"Best transfer learning hyperparameters:\", lstm_gen_best_params_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout': 0.47197063616986024,\n",
       " 'lr': 0.009876677395572514,\n",
       " 'weight_decay': 0.00044857179145431814}"
      ]
     },
     "execution_count": 214,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gen_best_params_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.9988 | Val Loss = 2.9104 | Acc = 0.9904 | AUC = undefined | Perplexity = 18.3641\n",
      "Epoch 2: Train Loss = 2.8414 | Val Loss = 2.7544 | Acc = 0.9808 | AUC = undefined | Perplexity = 15.7113\n",
      "Epoch 3: Train Loss = 2.7147 | Val Loss = 2.6259 | Acc = 0.9808 | AUC = undefined | Perplexity = 13.8165\n",
      "Epoch 4: Train Loss = 2.5707 | Val Loss = 2.4882 | Acc = 0.9808 | AUC = undefined | Perplexity = 12.0395\n",
      "Epoch 5: Train Loss = 2.4338 | Val Loss = 2.3602 | Acc = 0.9808 | AUC = undefined | Perplexity = 10.5929\n",
      "Epoch 6: Train Loss = 2.2982 | Val Loss = 2.2243 | Acc = 0.9808 | AUC = undefined | Perplexity = 9.2473\n",
      "Epoch 7: Train Loss = 2.1728 | Val Loss = 2.0993 | Acc = 0.9808 | AUC = undefined | Perplexity = 8.1607\n",
      "Epoch 8: Train Loss = 2.0404 | Val Loss = 1.9780 | Acc = 0.9808 | AUC = undefined | Perplexity = 7.2280\n",
      "Epoch 9: Train Loss = 1.9316 | Val Loss = 1.8528 | Acc = 0.9904 | AUC = undefined | Perplexity = 6.3778\n",
      "Epoch 10: Train Loss = 1.8042 | Val Loss = 1.7354 | Acc = 0.9904 | AUC = undefined | Perplexity = 5.6712\n",
      "Epoch 11: Train Loss = 1.6906 | Val Loss = 1.6174 | Acc = 0.9904 | AUC = undefined | Perplexity = 5.0399\n",
      "Epoch 12: Train Loss = 1.5833 | Val Loss = 1.5201 | Acc = 0.9808 | AUC = undefined | Perplexity = 4.5726\n",
      "Epoch 13: Train Loss = 1.4961 | Val Loss = 1.4191 | Acc = 0.9904 | AUC = undefined | Perplexity = 4.1334\n",
      "Epoch 14: Train Loss = 1.3784 | Val Loss = 1.3245 | Acc = 0.9808 | AUC = undefined | Perplexity = 3.7601\n",
      "Epoch 15: Train Loss = 1.3518 | Val Loss = 1.2302 | Acc = 0.9904 | AUC = undefined | Perplexity = 3.4220\n",
      "Epoch 16: Train Loss = 1.2015 | Val Loss = 1.1507 | Acc = 0.9808 | AUC = undefined | Perplexity = 3.1605\n",
      "Epoch 17: Train Loss = 1.1242 | Val Loss = 1.0618 | Acc = 0.9904 | AUC = undefined | Perplexity = 2.8917\n",
      "Epoch 18: Train Loss = 1.0346 | Val Loss = 0.9812 | Acc = 0.9904 | AUC = undefined | Perplexity = 2.6677\n",
      "Epoch 19: Train Loss = 0.9605 | Val Loss = 0.9214 | Acc = 0.9808 | AUC = undefined | Perplexity = 2.5128\n",
      "Epoch 20: Train Loss = 1.1197 | Val Loss = 0.8626 | Acc = 0.9808 | AUC = undefined | Perplexity = 2.3693\n",
      "Epoch 21: Train Loss = 0.8514 | Val Loss = 0.8054 | Acc = 0.9808 | AUC = undefined | Perplexity = 2.2376\n",
      "Epoch 22: Train Loss = 0.7895 | Val Loss = 0.7369 | Acc = 0.9904 | AUC = undefined | Perplexity = 2.0895\n",
      "Epoch 23: Train Loss = 0.7170 | Val Loss = 0.6990 | Acc = 0.9808 | AUC = undefined | Perplexity = 2.0117\n",
      "Epoch 24: Train Loss = 0.6647 | Val Loss = 0.6486 | Acc = 0.9904 | AUC = undefined | Perplexity = 1.9128\n",
      "Epoch 25: Train Loss = 0.6449 | Val Loss = 0.6109 | Acc = 0.9904 | AUC = undefined | Perplexity = 1.8420\n",
      "Epoch 26: Train Loss = 0.6112 | Val Loss = 0.5772 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.7810\n",
      "Epoch 27: Train Loss = 0.5705 | Val Loss = 0.5401 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.7162\n",
      "Epoch 28: Train Loss = 0.7323 | Val Loss = 0.5100 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.6654\n",
      "Epoch 29: Train Loss = 0.5023 | Val Loss = 0.4882 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.6294\n",
      "Epoch 30: Train Loss = 0.6512 | Val Loss = 0.4520 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.5715\n",
      "Epoch 31: Train Loss = 0.4454 | Val Loss = 0.4362 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.5468\n",
      "Epoch 32: Train Loss = 0.4361 | Val Loss = 0.4047 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.4988\n",
      "Epoch 33: Train Loss = 0.3937 | Val Loss = 0.3785 | Acc = 0.9904 | AUC = undefined | Perplexity = 1.4601\n",
      "Epoch 34: Train Loss = 0.3692 | Val Loss = 0.3640 | Acc = 0.9904 | AUC = undefined | Perplexity = 1.4391\n",
      "Epoch 35: Train Loss = 0.3781 | Val Loss = 0.3453 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.4124\n",
      "Epoch 36: Train Loss = 0.3388 | Val Loss = 0.3311 | Acc = 0.9904 | AUC = undefined | Perplexity = 1.3924\n",
      "Epoch 37: Train Loss = 0.3269 | Val Loss = 0.3128 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.3673\n",
      "Epoch 38: Train Loss = 0.3114 | Val Loss = 0.2964 | Acc = 0.9904 | AUC = undefined | Perplexity = 1.3450\n",
      "Epoch 39: Train Loss = 0.3162 | Val Loss = 0.2897 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.3360\n",
      "Epoch 40: Train Loss = 0.2812 | Val Loss = 0.2725 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.3133\n",
      "Epoch 41: Train Loss = 0.2730 | Val Loss = 0.2631 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.3010\n",
      "Epoch 42: Train Loss = 0.2551 | Val Loss = 0.2538 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.2889\n",
      "Epoch 43: Train Loss = 0.2390 | Val Loss = 0.2491 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.2829\n",
      "Epoch 44: Train Loss = 0.4801 | Val Loss = 0.2421 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.2740\n",
      "Epoch 45: Train Loss = 0.2306 | Val Loss = 0.2337 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.2633\n",
      "Epoch 46: Train Loss = 0.2251 | Val Loss = 0.2170 | Acc = 0.9904 | AUC = undefined | Perplexity = 1.2424\n",
      "Epoch 47: Train Loss = 0.2249 | Val Loss = 0.2138 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.2384\n",
      "Epoch 48: Train Loss = 0.2105 | Val Loss = 0.2016 | Acc = 0.9904 | AUC = undefined | Perplexity = 1.2234\n",
      "Epoch 49: Train Loss = 0.2102 | Val Loss = 0.1974 | Acc = 0.9904 | AUC = undefined | Perplexity = 1.2183\n",
      "Epoch 50: Train Loss = 0.2171 | Val Loss = 0.1993 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.2206\n",
      "Epoch 51: Train Loss = 0.1901 | Val Loss = 0.1893 | Acc = 0.9904 | AUC = undefined | Perplexity = 1.2084\n",
      "Epoch 52: Train Loss = 0.1827 | Val Loss = 0.1840 | Acc = 0.9904 | AUC = undefined | Perplexity = 1.2020\n",
      "Epoch 53: Train Loss = 0.1847 | Val Loss = 0.1788 | Acc = 0.9904 | AUC = undefined | Perplexity = 1.1958\n",
      "Epoch 54: Train Loss = 0.1667 | Val Loss = 0.1810 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.1985\n",
      "Epoch 55: Train Loss = 0.1882 | Val Loss = 0.1791 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.1962\n",
      "Epoch 56: Train Loss = 0.1736 | Val Loss = 0.1732 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.1891\n",
      "Epoch 57: Train Loss = 0.1656 | Val Loss = 0.1701 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.1855\n",
      "Epoch 58: Train Loss = 0.1687 | Val Loss = 0.1696 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.1849\n",
      "Epoch 59: Train Loss = 0.4105 | Val Loss = 0.1631 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.1771\n",
      "Epoch 60: Train Loss = 0.3238 | Val Loss = 0.1593 | Acc = 0.9808 | AUC = undefined | Perplexity = 1.1727\n",
      " Final Test Metrics:\n",
      "Loss = 0.1593, Accuracy = 0.9808, AUC = undefined, Perplexity = 1.1727\n",
      "Model saved to final_amp_generator_lstm.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_best_params_tb[\"lr\"], weight_decay=lstm_gen_best_params_tb[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "lstm_gen_best_params = {'hidden_dim': 133, 'num_layers': 2}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params_tb[\"dropout\"],\n",
    "    # weights_decay=lstm_gen_best_params_tb[\"weight_decay\"],\n",
    "    # lr=lstm_gen_best_params_tb[\"lr\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\" Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n",
    "# Save model weights\n",
    "torch.save(trained_model.state_dict(), \"final_amp_generator_lstm.pt\")\n",
    "print(\"Model saved to final_amp_generator_lstm.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Recreate your amino acid vocab\n",
    "aa_vocab = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aa_to_idx = {aa: i for i, aa in enumerate(aa_vocab)}\n",
    "idx_to_aa = {i: aa for aa, i in aa_to_idx.items()}\n",
    "\n",
    "def one_hot_encode_amino_acid(aa, vocab=aa_vocab):\n",
    "    vec = torch.zeros(len(vocab))\n",
    "    vec[aa_to_idx[aa]] = 1.0\n",
    "    return vec\n",
    "\n",
    "def generate_sequence_from_seed(model, seed, max_length=30, temperature=1.0, device='cpu'):\n",
    "    model.eval()\n",
    "    input_seq = [one_hot_encode_amino_acid(aa).to(device) for aa in seed]\n",
    "    input_tensor = torch.stack(input_seq).unsqueeze(0)  # [1, L, 20]\n",
    "\n",
    "    generated = seed.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(seed)):\n",
    "            output = model(input_tensor)  # [1, L, vocab]\n",
    "            logits = output[0, -1, :]  # Last time step â†’ [vocab]\n",
    "\n",
    "            # Apply temperature and sample\n",
    "            probs = F.softmax(logits / temperature, dim=-1).cpu().numpy()\n",
    "            next_idx = np.random.choice(len(aa_vocab), p=probs)\n",
    "            next_aa = idx_to_aa[next_idx]\n",
    "\n",
    "            # Update sequence\n",
    "            next_aa_vec = one_hot_encode_amino_acid(next_aa).to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, 20]\n",
    "            input_tensor = torch.cat([input_tensor, next_aa_vec], dim=1)\n",
    "            generated.append(next_aa)\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class LengthSampler:\n",
    "    def __init__(self, sequence_lengths):\n",
    "        \"\"\"\n",
    "        Initialize sampler from observed sequence lengths.\n",
    "        \n",
    "        Args:\n",
    "            sequence_lengths (list[int]): List of sequence lengths (e.g., [20, 21, 20, 23, ...])\n",
    "        \"\"\"\n",
    "        self.length_counts = Counter(sequence_lengths)\n",
    "        self.lengths = np.array(sorted(self.length_counts.keys()))\n",
    "        counts = np.array([self.length_counts[l] for l in self.lengths])\n",
    "        self.probs = counts / counts.sum()  # Empirical probabilities\n",
    "\n",
    "    def sample(self, n=1):\n",
    "        \"\"\"\n",
    "        Sample one or more lengths based on the learned distribution.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray of sampled lengths\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.lengths, size=n, p=self.probs)\n",
    "length_sampler = LengthSampler([len(seq) for seq in df.loc[df['AMP'] == 1, :]['Sequences']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: ERDDAAASKPAVRAAA\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    # dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "gen_model.eval()\n",
    "# Define a seed and generate a sequence\n",
    "sampled_length = length_sampler.sample()[0]\n",
    "start_aa = sample_start_amino_acid()\n",
    "seed_sequence = list(start_aa)  # start with valine\n",
    "generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1.0, device=device)\n",
    "\n",
    "print(\"Generated AMP sequence:\", generated_peptide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EAAAAAARKLASRAAA\n",
      "Generated AMP sequence: AAAAAAAADRAGKA\n",
      "Generated AMP sequence: KAARSMAAKDAGASAKAHIA\n",
      "Generated AMP sequence: IAGKASAAAADARDAIARAVRAARNQ\n",
      "Generated AMP sequence: IAAAMDAAAAPKSFAPQKRG\n",
      "Generated AMP sequence: FAAAAFAFSADQAAAASRKS\n",
      "Generated AMP sequence: EAAAKRAAADRSAGEAACSC\n",
      "Generated AMP sequence: VAASSADAAAKGAASAAGVA\n",
      "Generated AMP sequence: DAAAAEHFARAAAKLAGAKA\n",
      "Generated AMP sequence: WAGAAASSAKRIIAASSKDD\n",
      "Generated AMP sequence: QAAAAAASKAAKAAILKAPA\n",
      "Generated AMP sequence: CAAEACAAAATDSK\n",
      "Generated AMP sequence: AAAAAIAAKASSSDASFSSS\n",
      "Generated AMP sequence: DAAAAASKIAKVAG\n",
      "Generated AMP sequence: HAAAAAQRSSGGRKMRSCDAKA\n",
      "Generated AMP sequence: IAAAAAQAAGQAKAAIIK\n",
      "Generated AMP sequence: THAAIAAAASRLDAAAIPKA\n",
      "Generated AMP sequence: YAAAAAAPAAAARMGAGGAFDKKADK\n",
      "Generated AMP sequence: AAAAAFAGSIDKFISDSSA\n",
      "Generated AMP sequence: VAAAAAARAR\n",
      "Generated AMP sequence: HAAAPKQAAQRVDD\n",
      "Generated AMP sequence: VAEFAKLASGRASDASACSS\n",
      "Generated AMP sequence: QAAAAGLQQAGAIFSDAAQK\n",
      "Generated AMP sequence: IAAAKIAAVCD\n",
      "Generated AMP sequence: RASAAANGDSAGAFAAAKQK\n",
      "Generated AMP sequence: WAAGASASASGVAISIKLFKKSAASS\n",
      "Generated AMP sequence: KAAAAAFATVLIARLASR\n",
      "Generated AMP sequence: AAAARRAAARVAKDSRRGEAASRYWK\n",
      "Generated AMP sequence: GAAAAASIAKKDALGRHRIK\n",
      "Generated AMP sequence: QAAAFDKAAACKDFSDGRDKSS\n",
      "Generated AMP sequence: MAAAAVDAKAAA\n",
      "Generated AMP sequence: KAAAAASDKAAA\n",
      "Generated AMP sequence: FAAAAAAAFLAR\n",
      "Generated AMP sequence: HAAMAAQMAADGKDFQAAPS\n",
      "Generated AMP sequence: MAAAAAAAAAAAASKLRGA\n",
      "Generated AMP sequence: EAAAAAAKICVKAAAAAAAA\n",
      "Generated AMP sequence: DAAATAASRRAAKSKKARACFGA\n",
      "Generated AMP sequence: PAAAAADAAAA\n",
      "Generated AMP sequence: EAAAAKGDDASRSAQSAAQKRARAAARLID\n",
      "Generated AMP sequence: NAAAAAAMFASDISKASKCARK\n",
      "Generated AMP sequence: NAAAAALAIQSCRARADRAA\n",
      "Generated AMP sequence: YAAAAAAASKDFSAKGKASS\n",
      "Generated AMP sequence: KADAACGCNSSFAVRASSHK\n",
      "Generated AMP sequence: CAAAAAAFSACQFAFAA\n",
      "Generated AMP sequence: RAAAAEAKASADAWGKAQA\n",
      "Generated AMP sequence: VAAAACAQDGEDIAASSDAKEAAP\n",
      "Generated AMP sequence: EAAAAAAQAKAAAPSRDLAA\n",
      "Generated AMP sequence: PAAAAAFCSRCADKAKEIFASQA\n",
      "Generated AMP sequence: DAAAAAAFKAARVHAAASAKAK\n",
      "Generated AMP sequence: VAAAARRIKKASEDGAAQAK\n",
      "Generated AMP sequence: LAAADAFDKSPAA\n",
      "Generated AMP sequence: YAAAAAMAAPGLF\n",
      "Generated AMP sequence: NAAAADAASASFA\n",
      "Generated AMP sequence: WAAAAGRAFAVSAMSAKSAL\n",
      "Generated AMP sequence: HAQQARRARAKAAA\n",
      "Generated AMP sequence: DAAAAAPAKSKAASASISRW\n",
      "Generated AMP sequence: CAAAARVPDDPAADA\n",
      "Generated AMP sequence: IAAAVAAKASRAES\n",
      "Generated AMP sequence: LAAAVASKKGAAAARDASSSVADQLA\n",
      "Generated AMP sequence: DAAAACASAASSSRKIAEDA\n",
      "Generated AMP sequence: IAAAAAREFKRCLKASRKRS\n",
      "Generated AMP sequence: EAAAAKASSARACQARRGAA\n",
      "Generated AMP sequence: PKAAAAPAQGSCKSASLAAN\n",
      "Generated AMP sequence: KAAAARTSPA\n",
      "Generated AMP sequence: RAAAAKKDAWDFARLAALKA\n",
      "Generated AMP sequence: NAAQAFAAVDSAAAKDSGSASRRRQK\n",
      "Generated AMP sequence: GACAIAAAKDAG\n",
      "Generated AMP sequence: NAAAAAAACF\n",
      "Generated AMP sequence: NAAFGACAAVAGKMKACGAD\n",
      "Generated AMP sequence: HAAAAGQAAA\n",
      "Generated AMP sequence: KAAQAAKRLRARASIRFKIQ\n",
      "Generated AMP sequence: DAAANATAARK\n",
      "Generated AMP sequence: YAAAWTIAKAAAFSPQFDD\n",
      "Generated AMP sequence: GAAAAKCRSDAKSAKKQAPSAATASGASFG\n",
      "Generated AMP sequence: VAAASAIAQASFAAA\n",
      "Generated AMP sequence: IAAAADKAAAPKFRRKSAAA\n",
      "Generated AMP sequence: GAGAAASAFA\n",
      "Generated AMP sequence: RPNASFRDFFKA\n",
      "Generated AMP sequence: PAAAASAKDS\n",
      "Generated AMP sequence: KAAQAAGRIASL\n",
      "Generated AMP sequence: VAAAAAIYAVDSRKQSDCLR\n",
      "Generated AMP sequence: IAAAAAAAADLDFADAAKLK\n",
      "Generated AMP sequence: MAAAAAAACIGKQRPSAA\n",
      "Generated AMP sequence: CAAARASAAF\n",
      "Generated AMP sequence: IAAAAAGLADAAL\n",
      "Generated AMP sequence: CAAAAASDRIAK\n",
      "Generated AMP sequence: MAAIAAGARSCARARFLAKFKAQSAA\n",
      "Generated AMP sequence: PAAAAAGEVA\n",
      "Generated AMP sequence: KAAACVAALIAAASACADAH\n",
      "Generated AMP sequence: DAAAPFDAAK\n",
      "Generated AMP sequence: HAAAAAFSALAIAFQDKRSADAVSIRIKAQ\n",
      "Generated AMP sequence: WAAAAAAAASIA\n",
      "Generated AMP sequence: MAAARAVRASARDSSRR\n",
      "Generated AMP sequence: HAAACATKRASSGK\n",
      "Generated AMP sequence: SAAAAAAAASISFAGFAKAK\n",
      "Generated AMP sequence: PAAAAAWALVAAAAALAGWR\n",
      "Generated AMP sequence: RAAAAARDACDRQQAEGAAQ\n",
      "Generated AMP sequence: FAAAATQSFAIF\n",
      "Generated AMP sequence: KAAAAKCACFRSSRALFARA\n",
      "Generated AMP sequence: FAAAQFGAGAPGLAAPAAASQDAD\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    # dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "gen_model.eval()\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=0.7, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 15:11:24,898] A new study created in memory with name: no-name-cf68f5aa-42bb-4b5b-8b48-a267cfc12f6f\n",
      "[I 2025-04-22 15:11:31,811] Trial 0 finished with value: 2.1874172687530518 and parameters: {'dropout': 0.26927805101609836, 'lr': 0.006607107825528057, 'weight_decay': 0.0007828517656279971}. Best is trial 0 with value: 2.1874172687530518.\n",
      "[I 2025-04-22 15:11:37,863] Trial 1 finished with value: 2.287254810333252 and parameters: {'dropout': 0.44973959975333355, 'lr': 0.005780681311997045, 'weight_decay': 0.0007892633251980037}. Best is trial 0 with value: 2.1874172687530518.\n",
      "[I 2025-04-22 15:11:43,597] Trial 2 finished with value: 2.50271737575531 and parameters: {'dropout': 0.4481068863127341, 'lr': 0.0038894908204979723, 'weight_decay': 0.0007341287032190064}. Best is trial 0 with value: 2.1874172687530518.\n",
      "[I 2025-04-22 15:11:49,360] Trial 3 finished with value: 2.6780993938446045 and parameters: {'dropout': 0.22541801700207792, 'lr': 0.002533643023326825, 'weight_decay': 0.0006279418723843857}. Best is trial 0 with value: 2.1874172687530518.\n",
      "[I 2025-04-22 15:11:55,575] Trial 4 finished with value: 2.5472041368484497 and parameters: {'dropout': 0.30960227576436417, 'lr': 0.0036488610536654096, 'weight_decay': 0.0007208093051585932}. Best is trial 0 with value: 2.1874172687530518.\n",
      "[I 2025-04-22 15:12:01,477] Trial 5 finished with value: 2.1212894916534424 and parameters: {'dropout': 0.3402136218943675, 'lr': 0.007180611606871151, 'weight_decay': 0.0008128745410642789}. Best is trial 5 with value: 2.1212894916534424.\n",
      "[I 2025-04-22 15:12:07,365] Trial 6 finished with value: 2.2637096643447876 and parameters: {'dropout': 0.3773133121621458, 'lr': 0.005894116143663885, 'weight_decay': 0.00046294983078255374}. Best is trial 5 with value: 2.1212894916534424.\n",
      "[I 2025-04-22 15:12:13,194] Trial 7 finished with value: 2.1938241720199585 and parameters: {'dropout': 0.30035032368207376, 'lr': 0.006556541608108955, 'weight_decay': 0.0006525479708065609}. Best is trial 5 with value: 2.1212894916534424.\n",
      "[I 2025-04-22 15:12:19,116] Trial 8 finished with value: 2.3647643327713013 and parameters: {'dropout': 0.46496364621100184, 'lr': 0.005114409334560946, 'weight_decay': 0.000739264077142385}. Best is trial 5 with value: 2.1212894916534424.\n",
      "[I 2025-04-22 15:12:25,622] Trial 9 finished with value: 2.943788170814514 and parameters: {'dropout': 0.17749310644435556, 'lr': 0.000420868738424753, 'weight_decay': 0.00017001176542300563}. Best is trial 5 with value: 2.1212894916534424.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best transfer learning hyperparameters: {'dropout': 0.3402136218943675, 'lr': 0.007180611606871151, 'weight_decay': 0.0008128745410642789}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Criterion\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Transfer Learning Loader\n",
    "def load_pretrained_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    last_token_logits = output[:, -1, :]\n",
    "    last_token_targets = target_seq[:, -1, :]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-frozen-gen-tb/AMP_LSTM_GEN_TRANSFER_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # if train:\n",
    "            #     torch.save(model.state_dict(), 'best_model_lstm_transfer.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "    \n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Optuna objective for fine-tuning\n",
    "def objective_generation(trial):\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(    \n",
    "                hidden_dim=156,\n",
    "                num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "                dropout=lstm_gen_best_params[\"dropout\"])\n",
    "    model = load_pretrained_weights(model, 'best_model_lstm_generator.pt')  # path to the general AMP model\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=10)\n",
    "lstm_gen_frozen_best_params_tb = study.best_trial.params\n",
    "print(\"Best transfer learning hyperparameters:\", lstm_gen_frozen_best_params_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout': 0.3402136218943675,\n",
       " 'lr': 0.007180611606871151,\n",
       " 'weight_decay': 0.0008128745410642789}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gen_frozen_best_params_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.9842 | Val Loss = 2.9296 | Acc = 0.9688 | AUC = undefined | Perplexity = 18.7204\n",
      "Epoch 2: Train Loss = 2.8892 | Val Loss = 2.8361 | Acc = 0.9688 | AUC = undefined | Perplexity = 17.0489\n",
      "Epoch 3: Train Loss = 2.7940 | Val Loss = 2.7438 | Acc = 0.9688 | AUC = undefined | Perplexity = 15.5452\n",
      "Epoch 4: Train Loss = 2.7025 | Val Loss = 2.6530 | Acc = 0.9688 | AUC = undefined | Perplexity = 14.1966\n",
      "Epoch 5: Train Loss = 2.6142 | Val Loss = 2.5641 | Acc = 0.9688 | AUC = undefined | Perplexity = 12.9887\n",
      "Epoch 6: Train Loss = 2.5228 | Val Loss = 2.4753 | Acc = 0.9688 | AUC = undefined | Perplexity = 11.8852\n",
      "Epoch 7: Train Loss = 2.4312 | Val Loss = 2.3884 | Acc = 0.9688 | AUC = undefined | Perplexity = 10.8957\n",
      "Epoch 8: Train Loss = 2.3420 | Val Loss = 2.3028 | Acc = 0.9688 | AUC = undefined | Perplexity = 10.0026\n",
      "Epoch 9: Train Loss = 2.2545 | Val Loss = 2.2179 | Acc = 0.9688 | AUC = undefined | Perplexity = 9.1885\n",
      "Epoch 10: Train Loss = 2.1691 | Val Loss = 2.1347 | Acc = 0.9688 | AUC = undefined | Perplexity = 8.4541\n",
      "Epoch 11: Train Loss = 2.0845 | Val Loss = 2.0533 | Acc = 0.9688 | AUC = undefined | Perplexity = 7.7938\n",
      "Epoch 12: Train Loss = 2.0048 | Val Loss = 1.9719 | Acc = 0.9688 | AUC = undefined | Perplexity = 7.1841\n",
      "Epoch 13: Train Loss = 1.9203 | Val Loss = 1.8933 | Acc = 0.9688 | AUC = undefined | Perplexity = 6.6412\n",
      "Epoch 14: Train Loss = 1.8445 | Val Loss = 1.8161 | Acc = 0.9688 | AUC = undefined | Perplexity = 6.1476\n",
      "Epoch 15: Train Loss = 1.7647 | Val Loss = 1.7421 | Acc = 0.9688 | AUC = undefined | Perplexity = 5.7096\n",
      "Epoch 16: Train Loss = 1.7006 | Val Loss = 1.6682 | Acc = 0.9688 | AUC = undefined | Perplexity = 5.3024\n",
      "Epoch 17: Train Loss = 1.6203 | Val Loss = 1.5970 | Acc = 0.9688 | AUC = undefined | Perplexity = 4.9383\n",
      "Epoch 18: Train Loss = 1.5499 | Val Loss = 1.5290 | Acc = 0.9688 | AUC = undefined | Perplexity = 4.6137\n",
      "Epoch 19: Train Loss = 1.4770 | Val Loss = 1.4635 | Acc = 0.9688 | AUC = undefined | Perplexity = 4.3212\n",
      "Epoch 20: Train Loss = 1.4074 | Val Loss = 1.4014 | Acc = 0.9688 | AUC = undefined | Perplexity = 4.0609\n",
      "Epoch 21: Train Loss = 1.3507 | Val Loss = 1.3406 | Acc = 0.9688 | AUC = undefined | Perplexity = 3.8215\n",
      "Epoch 22: Train Loss = 1.2839 | Val Loss = 1.2810 | Acc = 0.9688 | AUC = undefined | Perplexity = 3.6002\n",
      "Epoch 23: Train Loss = 1.2403 | Val Loss = 1.2237 | Acc = 0.9688 | AUC = undefined | Perplexity = 3.3998\n",
      "Epoch 24: Train Loss = 1.1674 | Val Loss = 1.1688 | Acc = 0.9688 | AUC = undefined | Perplexity = 3.2181\n",
      "Epoch 25: Train Loss = 1.1466 | Val Loss = 1.1161 | Acc = 0.9688 | AUC = undefined | Perplexity = 3.0528\n",
      "Epoch 26: Train Loss = 1.0897 | Val Loss = 1.0649 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.9004\n",
      "Epoch 27: Train Loss = 1.0195 | Val Loss = 1.0184 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.7688\n",
      "Epoch 28: Train Loss = 0.9740 | Val Loss = 0.9750 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.6511\n",
      "Epoch 29: Train Loss = 0.9307 | Val Loss = 0.9321 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.5399\n",
      "Epoch 30: Train Loss = 0.8885 | Val Loss = 0.8918 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.4394\n",
      "Epoch 31: Train Loss = 0.8509 | Val Loss = 0.8534 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.3477\n",
      "Epoch 32: Train Loss = 0.8085 | Val Loss = 0.8178 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.2656\n",
      "Epoch 33: Train Loss = 0.7865 | Val Loss = 0.7847 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.1918\n",
      "Epoch 34: Train Loss = 0.7388 | Val Loss = 0.7527 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.1228\n",
      "Epoch 35: Train Loss = 0.7034 | Val Loss = 0.7234 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.0614\n",
      "Epoch 36: Train Loss = 0.6729 | Val Loss = 0.6961 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.0060\n",
      "Epoch 37: Train Loss = 0.6497 | Val Loss = 0.6694 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.9531\n",
      "Epoch 38: Train Loss = 0.6301 | Val Loss = 0.6439 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.9038\n",
      "Epoch 39: Train Loss = 0.5941 | Val Loss = 0.6196 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.8581\n",
      "Epoch 40: Train Loss = 0.5859 | Val Loss = 0.5966 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.8160\n",
      "Epoch 41: Train Loss = 0.5534 | Val Loss = 0.5750 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.7771\n",
      "Epoch 42: Train Loss = 0.5332 | Val Loss = 0.5538 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.7398\n",
      "Epoch 43: Train Loss = 0.5071 | Val Loss = 0.5352 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.7078\n",
      "Epoch 44: Train Loss = 0.5008 | Val Loss = 0.5183 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.6792\n",
      "Epoch 45: Train Loss = 0.4740 | Val Loss = 0.5013 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.6508\n",
      "Epoch 46: Train Loss = 0.4480 | Val Loss = 0.4857 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.6253\n",
      "Epoch 47: Train Loss = 0.4528 | Val Loss = 0.4711 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.6018\n",
      "Epoch 48: Train Loss = 0.4243 | Val Loss = 0.4542 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5749\n",
      "Epoch 49: Train Loss = 0.4084 | Val Loss = 0.4401 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5529\n",
      "Epoch 50: Train Loss = 0.3924 | Val Loss = 0.4289 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5355\n",
      "Epoch 51: Train Loss = 0.4049 | Val Loss = 0.4167 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5169\n",
      "Epoch 52: Train Loss = 0.3711 | Val Loss = 0.4065 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5016\n",
      "Epoch 53: Train Loss = 0.3764 | Val Loss = 0.3948 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4840\n",
      "Epoch 54: Train Loss = 0.3440 | Val Loss = 0.3851 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4698\n",
      "Epoch 55: Train Loss = 0.3338 | Val Loss = 0.3776 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4588\n",
      "Epoch 56: Train Loss = 0.3242 | Val Loss = 0.3703 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4482\n",
      "Epoch 57: Train Loss = 0.3150 | Val Loss = 0.3624 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4368\n",
      "Epoch 58: Train Loss = 0.3123 | Val Loss = 0.3543 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4252\n",
      "Epoch 59: Train Loss = 0.3390 | Val Loss = 0.3439 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4105\n",
      "Epoch 60: Train Loss = 0.2983 | Val Loss = 0.3366 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4001\n",
      " Final Test Metrics:\n",
      "Loss = 0.3366, Accuracy = 0.9688, AUC = undefined, Perplexity = 1.4001\n",
      "Model saved to final_amp_generator_lstm.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_frozen_best_params_tb[\"lr\"], weight_decay=lstm_gen_frozen_best_params_tb[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-frozen-gen-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "# lstm_gen_best_params = {'hidden_dim': 133, 'num_layers': 2, 'dropout': 0.10063270147175422, 'lr': 0.003237280156212186, 'weight_decay': 2.2594437829479466e-05}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_frozen_best_params_tb[\"dropout\"],\n",
    "    # weights_decay=lstm_gen_frozen_best_params_tb[\"weight_decay\"],\n",
    "    # lr=lstm_gen_frozen_best_params_tb[\"lr\"]\n",
    ")\n",
    "freeze_encoder(final_model)\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\" Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n",
    "# Save model weights\n",
    "torch.save(trained_model.state_dict(), \"final_amp_frozen_generator_lstm.pt\")\n",
    "print(\"Model saved to final_amp_generator_lstm.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EAAAAAAYRRDYYKGG\n",
      "Generated AMP sequence: AAAANAKNNYINRG\n",
      "Generated AMP sequence: KAAWYSKGRNGNGYLRNNPG\n",
      "Generated AMP sequence: IAPRNYDGFNNNYNNPGYGYYKDYRRYGNGYRNGNNRRY\n",
      "Generated AMP sequence: IAAKKRNNNDGFRNNYKNSK\n",
      "Generated AMP sequence: FAASRPYYGY\n",
      "Generated AMP sequence: EAHAAGNYYANNKGNYNNRN\n",
      "Generated AMP sequence: VGAAAGARNGNYKGNYKRYKRNRRPGYNGGRRDNKRGRN\n",
      "Generated AMP sequence: DAAAYYHRYPNKGYYRNNRYYYRGGYRDGRDGPRRK\n",
      "Generated AMP sequence: WAAINYRPGNKKYNYRYNPN\n",
      "Generated AMP sequence: QAAANDYYYNDY\n",
      "Generated AMP sequence: CIRPAKYNGPYRPGRYGN\n",
      "Generated AMP sequence: AAAGAGYYYYNNYRRYYNNGPGNPNRGGYNG\n",
      "Generated AMP sequence: DAAAAANNRRYNNYNNGFYY\n",
      "Generated AMP sequence: HAAAANRRKYRPRNKNVDGG\n",
      "Generated AMP sequence: IAAAANNGNN\n",
      "Generated AMP sequence: TAAANNYRGGRFRYPNRNPY\n",
      "Generated AMP sequence: YGKAAAAGRGNYFYNGR\n",
      "Generated AMP sequence: AAANADRYYNNYNYYNSSGY\n",
      "Generated AMP sequence: VAARDAYGNYYPRNNNRSYY\n",
      "Generated AMP sequence: HAAAGYMGGSR\n",
      "Generated AMP sequence: VAAARRDGYNN\n",
      "Generated AMP sequence: QAYKCASPNYGNDNGGGRSRYNNYKY\n",
      "Generated AMP sequence: IKARKYDPYN\n",
      "Generated AMP sequence: RAAALYAKYYNRNGNNRGYY\n",
      "Generated AMP sequence: WAANNAYYYRRNYYGFNYYG\n",
      "Generated AMP sequence: KASNRNNKGYYYYPYYYKNG\n",
      "Generated AMP sequence: AAADNKFRNWNYPPYNGGSRRKGDNRNNYNNYNRYYG\n",
      "Generated AMP sequence: GAAAYNKRKGGGGGRGNYNRGKGGNGGGNKNNRGY\n",
      "Generated AMP sequence: QAESAGWRGKNNRNNSGKRY\n",
      "Generated AMP sequence: MAAAKGKNDNKNGYRRYNGP\n",
      "Generated AMP sequence: KAAAAKRPNYRKGGKGGNGY\n",
      "Generated AMP sequence: FAAYAKYYYK\n",
      "Generated AMP sequence: HANAKGRGNNNK\n",
      "Generated AMP sequence: MAAAANNDNGG\n",
      "Generated AMP sequence: EAAANRPNNNYYYGRYGGSPYGYKGGYRNNYNYNNNNSNK\n",
      "Generated AMP sequence: DAARNAYRNGYRPYYGNKSGPYYNYGYGNYGKRR\n",
      "Generated AMP sequence: PAAAADYPNNYGRNRDYYPR\n",
      "Generated AMP sequence: EAAANLRYYNGYYGYYNRNGNYDNNPYNNRNKNGG\n",
      "Generated AMP sequence: NALALPARGYGNGYNRGSKY\n",
      "Generated AMP sequence: NAAAKASNNNNP\n",
      "Generated AMP sequence: YACWCGRNDGRR\n",
      "Generated AMP sequence: KAADAATGRK\n",
      "Generated AMP sequence: CAAPRKRGNYD\n",
      "Generated AMP sequence: RAAALLYYNKN\n",
      "Generated AMP sequence: VAAAKNKYSGYNYYNFNPRL\n",
      "Generated AMP sequence: EAYAAGAYGPNP\n",
      "Generated AMP sequence: PAAAAYYPRRGYNNNGGRGR\n",
      "Generated AMP sequence: DAAAPGPNRYRK\n",
      "Generated AMP sequence: VAAAGNKSGG\n",
      "Generated AMP sequence: LAAAASTRKQNGYGYNGPYRYR\n",
      "Generated AMP sequence: YAAAAYYDRYGPYGRNYYYN\n",
      "Generated AMP sequence: NAANANGGGRNRGNPYNRYRKFYFYPYYYN\n",
      "Generated AMP sequence: WAAASYRNNRKGNNNKGEYNNRKY\n",
      "Generated AMP sequence: HAARYNPGYKYRRNKGKGYNGYYYY\n",
      "Generated AMP sequence: DAAGASGHPRPG\n",
      "Generated AMP sequence: CAARYYYRPGNNKRNGKRGYNNRYNRPNYYR\n",
      "Generated AMP sequence: IAANNRPRGYYFYGNSGYYNDNRYRNRPY\n",
      "Generated AMP sequence: LAAYAPYGYRG\n",
      "Generated AMP sequence: DAAAAPAYYYSGSF\n",
      "Generated AMP sequence: IAAKNNKYNN\n",
      "Generated AMP sequence: EAAAARRKYGNY\n",
      "Generated AMP sequence: PAAAYKYIIFRNYNYGYY\n",
      "Generated AMP sequence: KAAAAAYRYNGGRNKNGRNYNNDG\n",
      "Generated AMP sequence: RAAANYSDNGKYGNRRP\n",
      "Generated AMP sequence: NAAAAADNGP\n",
      "Generated AMP sequence: GAAAAPRCYGPRYRYNYGYP\n",
      "Generated AMP sequence: NAAAPAAKGYKYNGYRNPRKYYRKRG\n",
      "Generated AMP sequence: NAAPPRNNNY\n",
      "Generated AMP sequence: HAAAPNYYNL\n",
      "Generated AMP sequence: KAAANRGRYGKYGYNKYNNN\n",
      "Generated AMP sequence: DAAYAPKSGYNKGGNNYNKR\n",
      "Generated AMP sequence: YAAAPPNYYPYKGNGPYGGN\n",
      "Generated AMP sequence: GAAAAYYAYRYNNNRGGNRKRYGRNYGRYYRNRYPG\n",
      "Generated AMP sequence: VAAANKRGRYGYNYYPRYNNRYYYNYGPDNNNRNN\n",
      "Generated AMP sequence: IAAANQPNNYNN\n",
      "Generated AMP sequence: GAAALNPSYR\n",
      "Generated AMP sequence: RAAANYGYGYNGNKNNYPNRRNNGGRGNRGPLYNYPG\n",
      "Generated AMP sequence: PAANVGGRNYYNKYNYNRGR\n",
      "Generated AMP sequence: KAAHSFEARRPKNRNYGRN\n",
      "Generated AMP sequence: VAAYAARPKGGYHNKNGN\n",
      "Generated AMP sequence: IAAATPNNNR\n",
      "Generated AMP sequence: MAAGGANYGRKPGNSNRYYKNGYYPYNRGTGYYRGNNNGY\n",
      "Generated AMP sequence: CAAANNYNYYKYGYNYYYYN\n",
      "Generated AMP sequence: IAAGAYRYGY\n",
      "Generated AMP sequence: CAAKANNGNGFGYPYNNNNGRKRRGGYNGYGRYKG\n",
      "Generated AMP sequence: MAAFANYYRNR\n",
      "Generated AMP sequence: PAARGENNYRYDNNELYGPK\n",
      "Generated AMP sequence: KATNYNFPNY\n",
      "Generated AMP sequence: DAAAPNNNNYY\n",
      "Generated AMP sequence: HAAFGGYDYNRRYRRGNGRNRGGRGGDYRNNNYRGGG\n",
      "Generated AMP sequence: WAAAAYKSRNKGRYRPGYYN\n",
      "Generated AMP sequence: MAAIRPGNYKNNNRYKGKYRNYYYYKSNYRYRGPYYGGG\n",
      "Generated AMP sequence: HAAARRNPNNPRYRYGNDKRYG\n",
      "Generated AMP sequence: SAAAPPGNYPASGSKYN\n",
      "Generated AMP sequence: PAARYKNRYRNNRA\n",
      "Generated AMP sequence: RAAAMNRFPPPRRYNYNGGN\n",
      "Generated AMP sequence: FAAAGAKNGKRRYYRNRYGNYNPRG\n",
      "Generated AMP sequence: KAAARGNKYKNYYGYN\n",
      "Generated AMP sequence: FAAGGNRSGRGSRYGGNNKY\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=0.7, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TB no transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 17:01:58,515] A new study created in memory with name: no-name-c654134e-18ef-4e7c-b6a8-0c3c9c0a9776\n",
      "[I 2025-04-22 17:02:10,163] Trial 0 finished with value: 2.150532364845276 and parameters: {'hidden_dim': 147, 'num_layers': 2, 'dropout': 0.21524128990002822, 'lr': 0.006837006425657788, 'weight_decay': 0.00012461526895376682}. Best is trial 0 with value: 2.150532364845276.\n",
      "[I 2025-04-22 17:02:25,104] Trial 1 finished with value: 2.836009979248047 and parameters: {'hidden_dim': 182, 'num_layers': 3, 'dropout': 0.11722142159873612, 'lr': 0.0011926690024992002, 'weight_decay': 0.0008115649896051376}. Best is trial 0 with value: 2.150532364845276.\n",
      "[I 2025-04-22 17:02:34,528] Trial 2 finished with value: 2.6635775566101074 and parameters: {'hidden_dim': 76, 'num_layers': 2, 'dropout': 0.3452801947193345, 'lr': 0.0033961443134127556, 'weight_decay': 0.0006403105212609755}. Best is trial 0 with value: 2.150532364845276.\n",
      "[I 2025-04-22 17:02:42,528] Trial 3 finished with value: 2.7118335962295532 and parameters: {'hidden_dim': 136, 'num_layers': 1, 'dropout': 0.4902645520488613, 'lr': 0.003005010036083719, 'weight_decay': 0.0009399754239411217}. Best is trial 0 with value: 2.150532364845276.\n",
      "[I 2025-04-22 17:02:50,872] Trial 4 finished with value: 2.226178765296936 and parameters: {'hidden_dim': 70, 'num_layers': 2, 'dropout': 0.46937972453725296, 'lr': 0.0057698156348564966, 'weight_decay': 0.0008940326593060807}. Best is trial 0 with value: 2.150532364845276.\n",
      "[I 2025-04-22 17:03:07,233] Trial 5 finished with value: 1.8923547267913818 and parameters: {'hidden_dim': 200, 'num_layers': 3, 'dropout': 0.45585196069207756, 'lr': 0.00913881809640831, 'weight_decay': 5.9934218359007e-05}. Best is trial 5 with value: 1.8923547267913818.\n",
      "[I 2025-04-22 17:03:17,302] Trial 6 finished with value: 2.4211589097976685 and parameters: {'hidden_dim': 140, 'num_layers': 2, 'dropout': 0.35094158391454, 'lr': 0.005105835002587207, 'weight_decay': 0.0005708358873861433}. Best is trial 5 with value: 1.8923547267913818.\n",
      "[I 2025-04-22 17:03:24,534] Trial 7 finished with value: 2.291481137275696 and parameters: {'hidden_dim': 128, 'num_layers': 1, 'dropout': 0.11193669292450772, 'lr': 0.006051637458907547, 'weight_decay': 0.0008445225245187799}. Best is trial 5 with value: 1.8923547267913818.\n",
      "[I 2025-04-22 17:03:33,137] Trial 8 finished with value: 2.5850695371627808 and parameters: {'hidden_dim': 220, 'num_layers': 1, 'dropout': 0.30049694603827337, 'lr': 0.0037118846320070985, 'weight_decay': 0.00011994058004589086}. Best is trial 5 with value: 1.8923547267913818.\n",
      "[I 2025-04-22 17:03:51,699] Trial 9 finished with value: 2.4962730407714844 and parameters: {'hidden_dim': 254, 'num_layers': 3, 'dropout': 0.3389936565611339, 'lr': 0.004002583844665486, 'weight_decay': 0.0006896566740516282}. Best is trial 5 with value: 1.8923547267913818.\n",
      "[I 2025-04-22 17:04:06,561] Trial 10 finished with value: 1.8765982389450073 and parameters: {'hidden_dim': 192, 'num_layers': 3, 'dropout': 0.417570219511599, 'lr': 0.009511620037651899, 'weight_decay': 0.0003435479375030387}. Best is trial 10 with value: 1.8765982389450073.\n",
      "[I 2025-04-22 17:04:22,171] Trial 11 finished with value: 1.826145887374878 and parameters: {'hidden_dim': 191, 'num_layers': 3, 'dropout': 0.4117163238758399, 'lr': 0.009948875782679164, 'weight_decay': 0.0003082585042224158}. Best is trial 11 with value: 1.826145887374878.\n",
      "[I 2025-04-22 17:04:36,469] Trial 12 finished with value: 1.8911916613578796 and parameters: {'hidden_dim': 180, 'num_layers': 3, 'dropout': 0.4059491190211576, 'lr': 0.009926291786845126, 'weight_decay': 0.00036790950460890027}. Best is trial 11 with value: 1.826145887374878.\n",
      "[I 2025-04-22 17:04:53,886] Trial 13 finished with value: 1.9838644862174988 and parameters: {'hidden_dim': 225, 'num_layers': 3, 'dropout': 0.40403309468361837, 'lr': 0.008119027526847624, 'weight_decay': 0.0003707564646812554}. Best is trial 11 with value: 1.826145887374878.\n",
      "[I 2025-04-22 17:05:08,872] Trial 14 finished with value: 1.983890175819397 and parameters: {'hidden_dim': 167, 'num_layers': 3, 'dropout': 0.41161500239731047, 'lr': 0.008085757907003957, 'weight_decay': 0.00030568087204818465}. Best is trial 11 with value: 1.826145887374878.\n",
      "[I 2025-04-22 17:05:26,677] Trial 15 finished with value: 1.7450235486030579 and parameters: {'hidden_dim': 207, 'num_layers': 3, 'dropout': 0.26885937025233814, 'lr': 0.009996887855520243, 'weight_decay': 0.00021226522607988078}. Best is trial 15 with value: 1.7450235486030579.\n",
      "[I 2025-04-22 17:05:40,474] Trial 16 finished with value: 2.0980767011642456 and parameters: {'hidden_dim': 237, 'num_layers': 2, 'dropout': 0.22938508493035384, 'lr': 0.007660955684130211, 'weight_decay': 0.00021607446968280618}. Best is trial 15 with value: 1.7450235486030579.\n",
      "[I 2025-04-22 17:05:58,880] Trial 17 finished with value: 1.9734016060829163 and parameters: {'hidden_dim': 212, 'num_layers': 3, 'dropout': 0.2327057371987534, 'lr': 0.008807068330183198, 'weight_decay': 0.0004955676903308923}. Best is trial 15 with value: 1.7450235486030579.\n",
      "[I 2025-04-22 17:06:10,433] Trial 18 finished with value: 3.009200930595398 and parameters: {'hidden_dim': 108, 'num_layers': 3, 'dropout': 0.27552490800982243, 'lr': 0.000201553277345147, 'weight_decay': 0.0002238246509120697}. Best is trial 15 with value: 1.7450235486030579.\n",
      "[I 2025-04-22 17:06:21,564] Trial 19 finished with value: 2.072967290878296 and parameters: {'hidden_dim': 164, 'num_layers': 2, 'dropout': 0.16692873850180975, 'lr': 0.0071477244640677715, 'weight_decay': 6.462027509171475e-06}. Best is trial 15 with value: 1.7450235486030579.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dim': 207, 'num_layers': 3, 'dropout': 0.26885937025233814, 'lr': 0.009996887855520243, 'weight_decay': 0.00021226522607988078}\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary packages after reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Criterion\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss on the last time step of each sequence.\n",
    "    \n",
    "    Args:\n",
    "        output: Tensor of shape [B, L, vocab_size]\n",
    "        target_seq: Tensor of shape [B, L] containing target class indices\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss computed only on the last token of each sequence\n",
    "    \"\"\"\n",
    "    # Get last time step for each sequence\n",
    "    last_token_logits = output[:, -1, :]        # [B, vocab_size]\n",
    "    last_token_targets = target_seq[:, -1, :]      # [B]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "    # print('last_token_logits',last_token_logits.shape)\n",
    "    # print('last_token_targets',last_token_targets.shape)\n",
    "\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen-notrans-tb/AMP_LSTM_GEN_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])      # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            # print('target_shape before reshape',target_seq.shape)\n",
    "            # target_seq = target_seq.reshape(-1)\n",
    "            # print(f\"Output shape: {output.shape}, Target shape: {target_seq.shape}\")\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_generator-notrans-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])  # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # # target_seq = target_seq.reshape(-1)\n",
    "            # # target_seq = target_seq.reshape(-1, target_seq.shape[-1])\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            # assert output.size(0) == target_seq.size(0), f\"Mismatch: {output.size(0)} vs {target_seq.size(0)}\"\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            # print('loss done')\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            preds = output[:, -1, :]        # shape: [B, vocab_size]\n",
    "            preds = torch.argmax(preds, dim=1)  # shape: [B]\n",
    "\n",
    "            targets = target_seq[:, -1, :]      # shape: [B, vocab_size]\n",
    "            targets = torch.argmax(targets, dim=-1)  # shape: [B]\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Objective for Optuna tuning\n",
    "def objective_generation(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "\n",
    "lstm_gen_notrans_tb_best_params = study.best_trial.params\n",
    "print(lstm_gen_notrans_tb_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.8760 | Val Loss = 2.8038 | Acc = 0.9688 | AUC = undefined | Perplexity = 16.5075\n",
      "Epoch 2: Train Loss = 2.7450 | Val Loss = 2.6764 | Acc = 0.9688 | AUC = undefined | Perplexity = 14.5320\n",
      "Epoch 3: Train Loss = 2.6098 | Val Loss = 2.5469 | Acc = 0.9688 | AUC = undefined | Perplexity = 12.7680\n",
      "Epoch 4: Train Loss = 2.4978 | Val Loss = 2.4312 | Acc = 0.9688 | AUC = undefined | Perplexity = 11.3720\n",
      "Epoch 5: Train Loss = 2.3622 | Val Loss = 2.3036 | Acc = 0.9688 | AUC = undefined | Perplexity = 10.0103\n",
      "Epoch 6: Train Loss = 2.2445 | Val Loss = 2.1812 | Acc = 0.9688 | AUC = undefined | Perplexity = 8.8569\n",
      "Epoch 7: Train Loss = 2.1183 | Val Loss = 2.0608 | Acc = 0.9688 | AUC = undefined | Perplexity = 7.8520\n",
      "Epoch 8: Train Loss = 1.9954 | Val Loss = 1.9424 | Acc = 0.9688 | AUC = undefined | Perplexity = 6.9752\n",
      "Epoch 9: Train Loss = 1.8961 | Val Loss = 1.8349 | Acc = 0.9688 | AUC = undefined | Perplexity = 6.2648\n",
      "Epoch 10: Train Loss = 1.7744 | Val Loss = 1.7299 | Acc = 0.9688 | AUC = undefined | Perplexity = 5.6398\n",
      "Epoch 11: Train Loss = 1.6739 | Val Loss = 1.6248 | Acc = 0.9688 | AUC = undefined | Perplexity = 5.0775\n",
      "Epoch 12: Train Loss = 1.5710 | Val Loss = 1.5273 | Acc = 0.9688 | AUC = undefined | Perplexity = 4.6057\n",
      "Epoch 13: Train Loss = 1.4856 | Val Loss = 1.4277 | Acc = 0.9688 | AUC = undefined | Perplexity = 4.1691\n",
      "Epoch 14: Train Loss = 1.3778 | Val Loss = 1.3357 | Acc = 0.9792 | AUC = undefined | Perplexity = 3.8025\n",
      "Epoch 15: Train Loss = 1.3207 | Val Loss = 1.2648 | Acc = 0.9688 | AUC = undefined | Perplexity = 3.5423\n",
      "Epoch 16: Train Loss = 1.2115 | Val Loss = 1.1759 | Acc = 0.9688 | AUC = undefined | Perplexity = 3.2411\n",
      "Epoch 17: Train Loss = 1.1327 | Val Loss = 1.0984 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.9993\n",
      "Epoch 18: Train Loss = 1.0581 | Val Loss = 1.0419 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.8347\n",
      "Epoch 19: Train Loss = 0.9805 | Val Loss = 0.9780 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.6590\n",
      "Epoch 20: Train Loss = 0.9186 | Val Loss = 0.9120 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.4892\n",
      "Epoch 21: Train Loss = 0.8670 | Val Loss = 0.8549 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.3512\n",
      "Epoch 22: Train Loss = 0.8251 | Val Loss = 0.8016 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.2291\n",
      "Epoch 23: Train Loss = 0.7476 | Val Loss = 0.7487 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.1142\n",
      "Epoch 24: Train Loss = 0.6979 | Val Loss = 0.7048 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.0235\n",
      "Epoch 25: Train Loss = 0.6581 | Val Loss = 0.6690 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.9522\n",
      "Epoch 26: Train Loss = 0.6346 | Val Loss = 0.6289 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.8755\n",
      "Epoch 27: Train Loss = 0.5854 | Val Loss = 0.5925 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.8086\n",
      "Epoch 28: Train Loss = 0.5470 | Val Loss = 0.5606 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.7518\n",
      "Epoch 29: Train Loss = 0.5110 | Val Loss = 0.5269 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.6937\n",
      "Epoch 30: Train Loss = 0.4766 | Val Loss = 0.5078 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.6616\n",
      "Epoch 31: Train Loss = 0.4751 | Val Loss = 0.4759 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.6094\n",
      "Epoch 32: Train Loss = 0.4358 | Val Loss = 0.4470 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5635\n",
      "Epoch 33: Train Loss = 0.4216 | Val Loss = 0.4294 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5363\n",
      "Epoch 34: Train Loss = 0.3925 | Val Loss = 0.4077 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5033\n",
      "Epoch 35: Train Loss = 0.3676 | Val Loss = 0.3946 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4837\n",
      "Epoch 36: Train Loss = 0.3814 | Val Loss = 0.3713 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4496\n",
      "Epoch 37: Train Loss = 0.3418 | Val Loss = 0.3667 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4430\n",
      "Epoch 38: Train Loss = 0.3279 | Val Loss = 0.3404 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4055\n",
      "Epoch 39: Train Loss = 0.3190 | Val Loss = 0.3230 | Acc = 0.9792 | AUC = undefined | Perplexity = 1.3812\n",
      "Epoch 40: Train Loss = 0.3223 | Val Loss = 0.3135 | Acc = 0.9792 | AUC = undefined | Perplexity = 1.3682\n",
      "Epoch 41: Train Loss = 0.2935 | Val Loss = 0.3026 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.3533\n",
      "Epoch 42: Train Loss = 0.2819 | Val Loss = 0.3115 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.3655\n",
      "Epoch 43: Train Loss = 0.2743 | Val Loss = 0.2888 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.3348\n",
      "Epoch 44: Train Loss = 0.2520 | Val Loss = 0.2737 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.3149\n",
      "Epoch 45: Train Loss = 0.2469 | Val Loss = 0.2720 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.3126\n",
      "Epoch 46: Train Loss = 0.2393 | Val Loss = 0.2660 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.3048\n",
      "Epoch 47: Train Loss = 0.2314 | Val Loss = 0.2644 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.3026\n",
      "Epoch 48: Train Loss = 0.2426 | Val Loss = 0.2579 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2942\n",
      "Epoch 49: Train Loss = 0.2065 | Val Loss = 0.2434 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2755\n",
      "Epoch 50: Train Loss = 0.2076 | Val Loss = 0.2387 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2696\n",
      "Epoch 51: Train Loss = 0.2215 | Val Loss = 0.2318 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2608\n",
      "Epoch 52: Train Loss = 0.1922 | Val Loss = 0.2218 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2484\n",
      "Epoch 53: Train Loss = 0.1835 | Val Loss = 0.2174 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2428\n",
      "Epoch 54: Train Loss = 0.1954 | Val Loss = 0.2175 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2429\n",
      "Epoch 55: Train Loss = 0.1902 | Val Loss = 0.2143 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2390\n",
      "Epoch 56: Train Loss = 0.1954 | Val Loss = 0.2112 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2352\n",
      "Epoch 57: Train Loss = 0.1773 | Val Loss = 0.2011 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2227\n",
      "Epoch 58: Train Loss = 0.1601 | Val Loss = 0.2020 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2238\n",
      "Epoch 59: Train Loss = 0.1591 | Val Loss = 0.2030 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2251\n",
      "Epoch 60: Train Loss = 0.1594 | Val Loss = 0.2012 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.2229\n",
      "\n",
      "âœ… Final Test Metrics:\n",
      "Loss = 0.2012, Accuracy = 0.9688, AUC = undefined, Perplexity = 1.2229\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_notrans_tb_best_params[\"lr\"], weight_decay=lstm_gen_notrans_tb_best_params[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen-notrans-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_notrans_tb_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_notrans_tb_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_notrans_tb_best_params[\"dropout\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\"\\nâœ… Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EAAAAFDYPRDYYKKK\n",
      "Generated AMP sequence: AAAANDKNNYKNNG\n",
      "Generated AMP sequence: KAAVYSLGRNGNGYNPNNNK\n",
      "Generated AMP sequence: IANRNYGGGNNNYNNNGYGYVKDYRSTGNGYPNGNNSPY\n",
      "Generated AMP sequence: IAAKMSNNNDGGPNNYNNSK\n",
      "Generated AMP sequence: FAASRPYYGY\n",
      "Generated AMP sequence: EAGAAGNYYDNNKGNYNNRN\n",
      "Generated AMP sequence: VEAAAGCPNGNYNGNYNPSNRNPPNGYNKGNQDNKPIRN\n",
      "Generated AMP sequence: DAAAYYKPYNNNGYYPNNRYYYPGGYNDGPDGNRPK\n",
      "Generated AMP sequence: WAAINYPNGNNNYNYPYNNN\n",
      "Generated AMP sequence: QAAANGYYYNDY\n",
      "Generated AMP sequence: CGRPALYNGNYRNGPYGN\n",
      "Generated AMP sequence: AAAGDHYYYYNNYPRYYNNGNGNNNNGGYNG\n",
      "Generated AMP sequence: DAAAAANNPRYNNYNNGGYY\n",
      "Generated AMP sequence: HAAAANRPNYSNPNNNSDGG\n",
      "Generated AMP sequence: IAAAANNGNN\n",
      "Generated AMP sequence: TAAANNYPGGPGNYNNPNNY\n",
      "Generated AMP sequence: YDGAAAAGRGNYFYNGR\n",
      "Generated AMP sequence: AAANAGSYYNNYNYYNSSGY\n",
      "Generated AMP sequence: VAARGAYGNYYNPNNNPSYY\n",
      "Generated AMP sequence: HAAAGYNGGSP\n",
      "Generated AMP sequence: VAAARRGGYNN\n",
      "Generated AMP sequence: QAWKFASNNYGNDNGGGPSNYNNYKY\n",
      "Generated AMP sequence: IHARKYFNYN\n",
      "Generated AMP sequence: RAAANYDMYYNPNKNNNGYY\n",
      "Generated AMP sequence: WAANNGYYYRPNYYGGNYYG\n",
      "Generated AMP sequence: KASNSNNLGYYYYNSYYNNG\n",
      "Generated AMP sequence: AAADNKGQNSNYNNYNGGSPRKGDNNNNYNNYNPYYG\n",
      "Generated AMP sequence: GAAAYNNPKGGGGGPKNYNPHKGGNKGGNKNNRGY\n",
      "Generated AMP sequence: QAASAGYSGKNNPNNSGNRY\n",
      "Generated AMP sequence: MAAAKGKNDNKNGYNRYNGN\n",
      "Generated AMP sequence: KAAAAKPNNYPLGGNGKNHY\n",
      "Generated AMP sequence: FAAYANYYYN\n",
      "Generated AMP sequence: HAMANGSGNNNN\n",
      "Generated AMP sequence: MAAAANNGNGG\n",
      "Generated AMP sequence: EAAANRPNNNYYYGSYKGSNYGYKGGYPNNYNYNNNNSNL\n",
      "Generated AMP sequence: DAARNCYPNGYPNYYGNNSGNSYNYGYGNYGLRR\n",
      "Generated AMP sequence: PAAADGYNNNYGNNPDYYNR\n",
      "Generated AMP sequence: EAAANNRYYNGYYGYYNNNGNYDNNNYNNSNKNGK\n",
      "Generated AMP sequence: NAKANPAPGYGNGYNNGSKY\n",
      "Generated AMP sequence: NAAAMASNNNNN\n",
      "Generated AMP sequence: YAAVFGPNDGRP\n",
      "Generated AMP sequence: KAADAATGPK\n",
      "Generated AMP sequence: CAAPSNPINYD\n",
      "Generated AMP sequence: RAAANNYYNNN\n",
      "Generated AMP sequence: VAAALNKYSGYNYYNFNNPN\n",
      "Generated AMP sequence: EAWAAGAYGNNN\n",
      "Generated AMP sequence: PAAAAYYPPPGYNNNGGSGN\n",
      "Generated AMP sequence: DAAAPHNNPYSK\n",
      "Generated AMP sequence: VAAAKNNSGG\n",
      "Generated AMP sequence: LAAAASTRNNNGYGYNKNYSYP\n",
      "Generated AMP sequence: YAAAAYYDRYHNYGRNYYYN\n",
      "Generated AMP sequence: NAANANHGGRNRGNNYNPYPMGYFYNYYYN\n",
      "Generated AMP sequence: WAAATYSNNSKGNNNLGDYNNRKY\n",
      "Generated AMP sequence: HAAPYNPGYKYPNNKKKKYNGYYYY\n",
      "Generated AMP sequence: DAAGASHKNNNG\n",
      "Generated AMP sequence: CAARYYYPNGNNNRNGKRGYNNNYNQNNYYP\n",
      "Generated AMP sequence: IAANNPPSGYYGYGNSGYYNDNPYPNPNY\n",
      "Generated AMP sequence: LAAYDPYGYRG\n",
      "Generated AMP sequence: DAAADPDYYYSGSG\n",
      "Generated AMP sequence: IAAKNNKYNN\n",
      "Generated AMP sequence: EAAAAPPLYGNY\n",
      "Generated AMP sequence: PAAAYNYKKGPNYNYKYY\n",
      "Generated AMP sequence: KAAAAAYPYNGGRNKNGNNYNNDG\n",
      "Generated AMP sequence: RAAANYSDNGLYGNPRN\n",
      "Generated AMP sequence: NAAADAGNGN\n",
      "Generated AMP sequence: GAAAAPRDYGNRYRYNYGYN\n",
      "Generated AMP sequence: NAAAPADKGYKYNKYPNNRKYYPKRG\n",
      "Generated AMP sequence: NAAPPRNNNY\n",
      "Generated AMP sequence: HAAAPNYYNN\n",
      "Generated AMP sequence: KAAANRGRYGKYKYNNYNNN\n",
      "Generated AMP sequence: DAAYAPNSGYNLGGNNYNNP\n",
      "Generated AMP sequence: YAAAPPNYYNYKGNGNYGGN\n",
      "Generated AMP sequence: GAAAAYYAYPYNNNPGGNRKPYGPNYKRYYRNPYNG\n",
      "Generated AMP sequence: VAAANMPGPYGYNYYNSYNNPYSYNYGNDNNNRNN\n",
      "Generated AMP sequence: IAAANPPNNYNN\n",
      "Generated AMP sequence: GAAANNPSYR\n",
      "Generated AMP sequence: RAAANYKYGYNGNNNNYNNNRNNGKRKNRGNNYNYNG\n",
      "Generated AMP sequence: PAANVGGPNYYNLYNYNPGP\n",
      "Generated AMP sequence: KAAHSGGDPPNKNNNYGRN\n",
      "Generated AMP sequence: VAAYAARNNGGYKNKNGN\n",
      "Generated AMP sequence: IAAAVPNNNP\n",
      "Generated AMP sequence: MAAGGANYGRKNGNSNPYYNNGYYNYNPGSGYYRGNNNGY\n",
      "Generated AMP sequence: CAAANNYNYYKYGYNYYYYN\n",
      "Generated AMP sequence: IAAGDYPYGY\n",
      "Generated AMP sequence: CAAKANNGNGFGYNYNNNNGPKPPGGYNGYGRYNG\n",
      "Generated AMP sequence: MAAFANYYPNP\n",
      "Generated AMP sequence: PAAQGGNNYSSDNNDNSGNK\n",
      "Generated AMP sequence: KASNYNGNNY\n",
      "Generated AMP sequence: DAAAPNNNNYY\n",
      "Generated AMP sequence: HAAEGGYDYNPPYPPGNGSNRGGRGGDYSNNNYPKIG\n",
      "Generated AMP sequence: WAAAAYKSPNKGPYPNGYYN\n",
      "Generated AMP sequence: MAAIRPGNYKNNNPYNGKYPNYYYYNSNYRYPKNYYGGG\n",
      "Generated AMP sequence: HAAASRNNNNNRYPSGNDNRYG\n",
      "Generated AMP sequence: SAAAPNGNYNDSGSKYN\n",
      "Generated AMP sequence: PAAPYKNRYNNNRD\n",
      "Generated AMP sequence: RAAANNPGNNNNQYNYNGGN\n",
      "Generated AMP sequence: FAAAGDNNGKNSYYPNPYGNYNNNK\n",
      "Generated AMP sequence: KAAASGNNYNNYYKYN\n",
      "Generated AMP sequence: FAAGKNRSGPGSPYGGNNNY\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = trained_model\n",
    "gen_model.to(device)\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=0.7, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
