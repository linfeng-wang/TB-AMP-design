{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('all_seq722.csv')\n",
    "df = pd.read_csv('all_seq702.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ngram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/xgboost/core.py:158: UserWarning: [12:41:44] WARNING: /home/conda/feedstock_root/build_artifacts/xgboost-split_1738880503067/work/src/learner.cc:740: \n",
      "Parameters: { \"use_label_encoder\" } are not used.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary libraries after execution state reset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "# Step 1: Alternative Embedding - k-mer Frequency (simple n-gram approach)\n",
    "def get_kmer_features(sequences, k=3):\n",
    "    \"\"\"Convert sequences into k-mer frequency vectors\"\"\"\n",
    "    vectorizer = CountVectorizer(analyzer='char', ngram_range=(k, k))\n",
    "    X_kmers = vectorizer.fit_transform(sequences).toarray()\n",
    "    return X_kmers\n",
    "\n",
    "X = get_kmer_features(df[\"Sequences\"], k=3)\n",
    "y = df[\"AMP\"].values\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 3: Baseline Models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"SVM\": SVC(kernel=\"linear\", probability=True),\n",
    "    \"XGBoost\": XGBClassifier(use_label_encoder=False, eval_metric=\"logloss\")\n",
    "}\n",
    "\n",
    "# Step 4: Model Evaluation\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0  # Sensitivity (Recall)\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity\n",
    "\n",
    "    results.append([model_name, acc, auc, sens, spec])\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"AUC\", \"Sensitivity\", \"Specificity\"])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.808511</td>\n",
       "      <td>0.889135</td>\n",
       "      <td>0.814286</td>\n",
       "      <td>0.802817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.878169</td>\n",
       "      <td>0.957143</td>\n",
       "      <td>0.492958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>0.874648</td>\n",
       "      <td>0.885714</td>\n",
       "      <td>0.563380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>0.702128</td>\n",
       "      <td>0.766197</td>\n",
       "      <td>0.471429</td>\n",
       "      <td>0.929577</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy       AUC  Sensitivity  Specificity\n",
       "0  Logistic Regression  0.808511  0.889135     0.814286     0.802817\n",
       "1        Random Forest  0.723404  0.878169     0.957143     0.492958\n",
       "2                  SVM  0.723404  0.874648     0.885714     0.563380\n",
       "3              XGBoost  0.702128  0.766197     0.471429     0.929577"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fixing the OneHotEncoder issue by using the correct parameter (`sparse=False` for older versions)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "def get_onehot_features(sequences):\n",
    "    \"\"\"Convert sequences into one-hot encoded vectors.\"\"\"\n",
    "    # Flatten all sequences into a set of unique characters\n",
    "    unique_chars = sorted(set(\"\".join(sequences)))\n",
    "    \n",
    "    # Create a mapping from characters to indices\n",
    "    char_to_index = {char: idx for idx, char in enumerate(unique_chars)}\n",
    "    \n",
    "    # Encode each sequence as a list of indices\n",
    "    indexed_sequences = [[char_to_index[char] for char in seq] for seq in sequences]\n",
    "\n",
    "    # Pad sequences to the same length (max sequence length)\n",
    "    max_length = max(len(seq) for seq in sequences)\n",
    "    padded_sequences = [seq + [len(unique_chars)] * (max_length - len(seq)) for seq in indexed_sequences]  # Padding index\n",
    "    \n",
    "    # Convert to NumPy array\n",
    "    padded_sequences = np.array(padded_sequences)\n",
    "\n",
    "    # One-Hot Encode with corrected parameter name\n",
    "    encoder = OneHotEncoder( categories=[list(range(len(unique_chars) + 1))] * max_length)\n",
    "    X_onehot = encoder.fit_transform(padded_sequences)\n",
    "    \n",
    "    return X_onehot\n",
    "\n",
    "# # Sample DataFrame\n",
    "# data = {\n",
    "#     \"Sequences\": [\"WKWLKKWIK\", \"ILRWKWRWWRWRR\", \"ILPWKWRWWKWRR\", \"RWRRKWWWW\", \"WRKFWKYLK\"],\n",
    "#     \"AMP\": [1, 1, 1, 1, 1]  # Example data, need to add negatives for proper training\n",
    "# }\n",
    "# df = pd.DataFrame(data)\n",
    "\n",
    "# # Simulate some negative examples (for realistic binary classification)\n",
    "# negative_sequences = [\"MVLSPADKT\", \"SGRGKQGGKV\", \"ADEMKRYGQ\", \"TSLYNRFST\", \"MGDVEKGKK\"]\n",
    "# negative_labels = [0] * len(negative_sequences)\n",
    "\n",
    "# df_neg = pd.DataFrame({\"Sequences\": negative_sequences, \"AMP\": negative_labels})\n",
    "# df = pd.concat([df, df_neg], ignore_index=True)\n",
    "\n",
    "# Apply One-Hot Encoding to Sequences\n",
    "X = get_onehot_features(df[\"Sequences\"])\n",
    "y = df[\"AMP\"].values\n",
    "\n",
    "# Step 2: Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 3: Baseline Models\n",
    "models = {\n",
    "    \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "    \"Random Forest\": RandomForestClassifier(n_estimators=100),\n",
    "    \"SVM\": SVC(kernel=\"linear\", probability=True)\n",
    "}\n",
    "\n",
    "# Step 4: Model Evaluation\n",
    "results = []\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, \"predict_proba\") else None\n",
    "\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    auc = roc_auc_score(y_test, y_prob) if y_prob is not None else None\n",
    "    tn, fp, fn, tp = confusion_matrix(y_test, y_pred).ravel()\n",
    "    sens = tp / (tp + fn) if (tp + fn) > 0 else 0  # Sensitivity (Recall)\n",
    "    spec = tn / (tn + fp) if (tn + fp) > 0 else 0  # Specificity\n",
    "\n",
    "    results.append([model_name, acc, auc, sens, spec])\n",
    "\n",
    "# Convert results to DataFrame and display\n",
    "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"AUC\", \"Sensitivity\", \"Specificity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>AUC</th>\n",
       "      <th>Sensitivity</th>\n",
       "      <th>Specificity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.865248</td>\n",
       "      <td>0.932998</td>\n",
       "      <td>0.914286</td>\n",
       "      <td>0.816901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.945875</td>\n",
       "      <td>0.900000</td>\n",
       "      <td>0.802817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.858156</td>\n",
       "      <td>0.910060</td>\n",
       "      <td>0.928571</td>\n",
       "      <td>0.788732</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Model  Accuracy       AUC  Sensitivity  Specificity\n",
       "0  Logistic Regression  0.865248  0.932998     0.914286     0.816901\n",
       "1        Random Forest  0.851064  0.945875     0.900000     0.802817\n",
       "2                  SVM  0.858156  0.910060     0.928571     0.788732"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
