{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11010). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at /pytorch/c10/cuda/CUDAFunctions.cpp:109.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "\n",
    "def set_seed(seed_value=42):\n",
    "    \"\"\"Set seed for reproducibility across numpy, torch, and random.\"\"\"\n",
    "    random.seed(seed_value)  # Python's built-in random module\n",
    "    np.random.seed(seed_value)  # NumPy random seed\n",
    "\n",
    "    torch.manual_seed(seed_value)  # PyTorch random seed for CPU\n",
    "    torch.cuda.manual_seed(seed_value)  # PyTorch random seed for CUDA\n",
    "    torch.cuda.manual_seed_all(seed_value)  # PyTorch seed for all GPUs\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True  # Ensures deterministic behavior in cuDNN\n",
    "    torch.backends.cudnn.benchmark = False  # Disables cuDNN auto-optimization\n",
    "\n",
    "# Usage\n",
    "set_seed(42)  # Call this function before running models or data splits to ensure reproducibility\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'N', 'M', 'T', 'F', 'I', 'C', 'K', 'G', 'V', 'W', 'D', 'E', 'X', 'S', 'H', 'Y', 'A', 'P', 'L', 'R', 'Q'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "# df = pd.read_csv('all_seq722.csv')\n",
    "\n",
    "# df = df[~df[\"Sequences\"].str.contains('-')]\n",
    "# df['Sequences'] = df['Sequences'].str.upper()\n",
    "# max_length = df['Sequences'].str.len().max()\n",
    "# print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X')\n",
    "\n",
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-import necessary libraries after execution state reset\n",
    "# Define One-Hot Encoding Function for DNA Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "# Define custom dataset class with transformation\n",
    "\n",
    "# Updating the Dataset class with the OneHotEncoder function at the end\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences  # Raw sequences\n",
    "        self.labels = labels  # Labels\n",
    "        self.one_hot_dtype = one_hot_dtype  # Data type for one-hot encoding\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seqs_comb = self.sequences.iloc[idx]  # Get sequence\n",
    "        amp_label = self.labels.iloc[idx]    # Get corresponding label\n",
    "        # Apply one-hot encoding transformation at the end\n",
    "        return one_hot_torch(seqs_comb, dtype=self.one_hot_dtype), torch.tensor(amp_label, dtype=torch.float32)\n",
    "    \n",
    "# Convert dataset into PyTorch Dataset\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.15, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.15, random_state=42, stratify=y_train_val\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Convert back to PyTorch datasets\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "# for x,y in train_loader:\n",
    "#     print(x)\n",
    "#     print(y)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27     IRMRIRVLLXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...\n",
       "194    WALVNXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...\n",
       "201    WLLVXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...\n",
       "41     RRYHWRIYIXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...\n",
       "302    INLKALAALAKKILXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...\n",
       "                             ...                        \n",
       "652    GPQQQHRLXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...\n",
       "182    RQIKIWFQNRRMKWKKXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...\n",
       "670    VHWSAEEKQLXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...\n",
       "21     HQFRFRFRVRRKXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...\n",
       "601    WREMSVWXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX...\n",
       "Name: Sequences, Length: 508, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate specificity\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "# Training and Evaluation Function\n",
    "def train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_,num_epochs=100):\n",
    "    writer = SummaryWriter(tensorboard_)\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        for sequences, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            ic(sequences.shape)\n",
    "            outputs = model(sequences)\n",
    "            loss = criterion(outputs, labels.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "        \n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "\n",
    "        # Evaluation\n",
    "        model.eval()\n",
    "        all_labels = []\n",
    "        all_preds = []\n",
    "        val_losses = []\n",
    "        all_probs = []\n",
    "        with torch.no_grad():\n",
    "            for sequences, labels in val_loader:\n",
    "                outputs = model(sequences)\n",
    "                loss = criterion(outputs, labels.unsqueeze(1))\n",
    "                val_losses.append(loss.item())\n",
    "                preds = (outputs > 0.5).float()\n",
    "                all_probs.extend(outputs.numpy())\n",
    "                all_labels.extend(labels.numpy())\n",
    "                all_preds.extend(preds.numpy())\n",
    "                \n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            writer.add_scalar('Loss/Val', avg_val_loss, epoch)\n",
    "\n",
    "        all_labels = np.array(all_labels)\n",
    "        all_preds = np.array(all_preds)\n",
    "        accuracy = accuracy_score(all_labels, all_preds)\n",
    "        auc = roc_auc_score(all_labels, all_probs)\n",
    "        sensitivity = recall_score(all_labels, all_preds)\n",
    "        specificity = specificity_score(all_labels, all_preds)\n",
    "\n",
    "        writer.add_scalar('Metrics/Accuracy', accuracy, epoch)\n",
    "        writer.add_scalar('Metrics/AUC', auc, epoch)\n",
    "        writer.add_scalar('Metrics/Sensitivity', sensitivity, epoch)\n",
    "        writer.add_scalar('Metrics/Specificity', specificity, epoch)\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, TrainLoss: {avg_train_loss:.4f}, ValLoss: {avg_val_loss:.4f}, Accuracy: {accuracy:.4f}, AUC: {auc:.4f}, Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}')\n",
    "\n",
    "    writer.close()\n",
    "    \n",
    "\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient computation\n",
    "        for sequences, labels in test_loader:\n",
    "            outputs = model(sequences)\n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_probs.extend(outputs.numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "            all_preds.extend(preds.numpy())\n",
    "    \n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    \n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    auc = roc_auc_score(all_labels, all_probs)\n",
    "    sensitivity = recall_score(all_labels, all_preds)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_labels, all_preds).ravel()\n",
    "    specificity = tn / (tn + fp)\n",
    "    \n",
    "    print(accuracy)\n",
    "    print(auc)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}')\n",
    "    print(f'Test AUC: {auc:.4f}')\n",
    "    print(f'Test Sensitivity (Recall): {sensitivity:.4f}')\n",
    "    print(f'Test Specificity: {specificity:.4f}')\n",
    "    \n",
    "    return accuracy, auc, sensitivity, specificity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21, TrainLoss: 0.6911, ValLoss: 0.6871, Accuracy: 0.5000, AUC: 0.6602, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch 6/21, TrainLoss: 0.5061, ValLoss: 0.5599, Accuracy: 0.7222, AUC: 0.7995, Sensitivity: 0.6667, Specificity: 0.7778\n",
      "Epoch 11/21, TrainLoss: 0.3502, ValLoss: 0.5525, Accuracy: 0.7444, AUC: 0.8311, Sensitivity: 0.6444, Specificity: 0.8444\n",
      "Epoch 16/21, TrainLoss: 0.2752, ValLoss: 0.7030, Accuracy: 0.7778, AUC: 0.8049, Sensitivity: 0.7333, Specificity: 0.8222\n",
      "Epoch 21/21, TrainLoss: 0.2119, ValLoss: 0.7577, Accuracy: 0.7222, AUC: 0.8188, Sensitivity: 0.7556, Specificity: 0.6889\n",
      "0.6981132075471698\n",
      "0.7985048059807761\n",
      "Test Accuracy: 0.6981\n",
      "Test AUC: 0.7985\n",
      "Test Sensitivity (Recall): 0.7925\n",
      "Test Specificity: 0.6038\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6981132075471698,\n",
       " 0.7985048059807761,\n",
       " 0.7924528301886793,\n",
       " 0.6037735849056604)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "ic.disable()\n",
    "# Define the Basic LSTM Model\n",
    "class BasicLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BasicLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)\n",
    "        ic(h_n.shape)\n",
    "        ic(h_n[-1].shape)\n",
    "        ic(h_n[-1])\n",
    "        out = self.fc(h_n[-1])\n",
    "        return self.sigmoid(out)\n",
    "\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 128       \n",
    "hidden_size = 128    # Number of features in the hidden state\n",
    "num_layers = 2       # Number of stacked LSTM layers\n",
    "output_size = 1      # Binary classification (AMP or not)\n",
    "batch_size = 16\n",
    "num_epochs = 21\n",
    "learning_rate = 0.001\n",
    "tensorboard_ = f\"runs/basic_lstm/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = BasicLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_,num_epochs)\n",
    "\n",
    "evaluate_model(model, test_loader)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## lstm with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale_factor = torch.sqrt(torch.tensor(hidden_size, dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        attention_scores = torch.bmm(query, key.transpose(1, 2))  # [batch, seq_len, seq_len]\n",
    "        attention_scores = attention_scores / self.scale_factor\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # Normalize scores\n",
    "        context_vector = torch.bmm(attention_weights, value)  # Weighted sum\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class LSTM_ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(LSTM_ScaledDotAttention, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.attention = ScaledDotProductAttention(hidden_size)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # LSTM output shape: [batch_size, seq_len, hidden_size]\n",
    "        \n",
    "        # Using last hidden state as Query\n",
    "        query = lstm_out[:, -1, :].unsqueeze(1)  # [batch_size, 1, hidden_size]\n",
    "\n",
    "        # Key and Value are the full sequence outputs\n",
    "        key = lstm_out  # [batch_size, seq_len, hidden_size]\n",
    "        value = lstm_out\n",
    "\n",
    "        context_vector, attention_weights = self.attention(query, key, value)\n",
    "        context_vector = context_vector.squeeze(1)  # [batch_size, hidden_size]\n",
    "\n",
    "        out = self.fc(context_vector)  # [batch_size, output_size]\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12, TrainLoss: 0.6937, ValLoss: 0.6900, Accuracy: 0.5000, AUC: 0.6568, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch 6/12, TrainLoss: 0.5912, ValLoss: 0.5631, Accuracy: 0.7222, AUC: 0.7921, Sensitivity: 0.5556, Specificity: 0.8889\n",
      "Epoch 11/12, TrainLoss: 0.5316, ValLoss: 0.5962, Accuracy: 0.7222, AUC: 0.7960, Sensitivity: 0.5111, Specificity: 0.9333\n",
      "0.6320754716981132\n",
      "0.7080811676753294\n",
      "Test Accuracy: 0.6321\n",
      "Test AUC: 0.7081\n",
      "Test Sensitivity (Recall): 0.4151\n",
      "Test Specificity: 0.8491\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.6320754716981132,\n",
       " 0.7080811676753294,\n",
       " 0.41509433962264153,\n",
       " 0.8490566037735849)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 128  # One-hot encoding size or embedding dimension\n",
    "hidden_size = 128  # LSTM hidden state size\n",
    "num_layers = 2  # Stacked LSTM layers\n",
    "output_size = 1  # Binary classification\n",
    "batch_size = 16\n",
    "num_epochs = 12\n",
    "learning_rate = 0.001\n",
    "tensorboard_log_dir = f\"runs/lstm_attention/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = LSTM_ScaledDotAttention(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs)\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7830188679245284\n",
      "0.8305446778212888\n",
      "Test Accuracy: 0.7830\n",
      "Test AUC: 0.8305\n",
      "Test Sensitivity (Recall): 0.8302\n",
      "Test Specificity: 0.7358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7830188679245284,\n",
       " 0.8305446778212888,\n",
       " 0.8301886792452831,\n",
       " 0.7358490566037735)"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN-LSTM with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "ic.disable()\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attention_weights = nn.Linear(hidden_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        # lstm_output: [batch_size, seq_len, hidden_dim]\n",
    "        attention_scores = self.attention_weights(lstm_output)  # [batch_size, seq_len, 1]\n",
    "        attention_scores = attention_scores.squeeze(-1)  # [batch_size, seq_len]\n",
    "        attention_weights = F.softmax(attention_scores, dim=1)  # [batch_size, seq_len]\n",
    "        context_vector = torch.bmm(attention_weights.unsqueeze(1), lstm_output)  # [batch_size, 1, hidden_dim]\n",
    "        context_vector = context_vector.squeeze(1)  # [batch_size, hidden_dim]\n",
    "        return context_vector, attention_weights\n",
    "\n",
    "class CNN_LSTM_Attention(nn.Module):\n",
    "    def __init__(self, input_channels, cnn_output_dim, lstm_hidden_dim, lstm_layers, output_dim):\n",
    "        super(CNN_LSTM_Attention, self).__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv1d(input_channels, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Conv1d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(kernel_size=2)\n",
    "        )\n",
    "        self.lstm = nn.LSTM(cnn_output_dim, lstm_hidden_dim, lstm_layers, batch_first=True)\n",
    "        self.attention = Attention(lstm_hidden_dim)\n",
    "        self.fc = nn.Linear(lstm_hidden_dim, output_dim)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: [batch_size, seq_len, input_channels]\n",
    "        batch_size, seq_len, _ = x.size()\n",
    "        ic(x.shape)\n",
    "        x = x.view(batch_size * seq_len, -1, x.size(2))  # [batch_size * seq_len, input_channels, feature_dim]\n",
    "        ic(x.shape)\n",
    "        cnn_out = self.cnn(x)  # [batch_size * seq_len, 128, feature_dim/4]\n",
    "        cnn_out = cnn_out.view(batch_size, seq_len, -1)  # [batch_size, seq_len, cnn_output_dim]\n",
    "        lstm_out, _ = self.lstm(cnn_out)  # [batch_size, seq_len, lstm_hidden_dim]\n",
    "        context_vector, attention_weights = self.attention(lstm_out)  # [batch_size, lstm_hidden_dim]\n",
    "        out = self.fc(context_vector)  # [batch_size, output_dim]\n",
    "        return self.sigmoid(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/12, TrainLoss: 0.6967, ValLoss: 0.6915, Accuracy: 0.5000, AUC: 0.6731, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch 6/12, TrainLoss: 0.5147, ValLoss: 0.4816, Accuracy: 0.7222, AUC: 0.8583, Sensitivity: 0.5778, Specificity: 0.8667\n",
      "Epoch 11/12, TrainLoss: 0.3625, ValLoss: 0.4362, Accuracy: 0.8111, AUC: 0.8765, Sensitivity: 0.7333, Specificity: 0.8889\n",
      "0.7452830188679245\n",
      "0.8227127091491635\n",
      "Test Accuracy: 0.7453\n",
      "Test AUC: 0.8227\n",
      "Test Sensitivity (Recall): 0.7358\n",
      "Test Specificity: 0.7547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7452830188679245,\n",
       " 0.8227127091491635,\n",
       " 0.7358490566037735,\n",
       " 0.7547169811320755)"
      ]
     },
     "execution_count": 296,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Hyperparameters\n",
    "input_channels = 1  # Number of input channels\n",
    "cnn_output_dim = 128 * (input_size // 4)  # Adjust based on CNN architecture\n",
    "lstm_hidden_dim = 128\n",
    "lstm_layers = 2\n",
    "output_dim = 1  # Binary classification\n",
    "batch_size = 16\n",
    "num_epochs = 12\n",
    "learning_rate = 0.001\n",
    "tensorboard_log_dir = f\"runs/cnn_lstm_attention/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = CNN_LSTM_Attention(input_channels, cnn_output_dim, lstm_hidden_dim, lstm_layers, output_dim)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs)\n",
    "\n",
    "# Evaluate on Test Set\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## bidirectional LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BiLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)  # Multiply hidden_size by 2 for bidirectional\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        _, (h_n, _) = self.lstm(x)  # LSTM output: (batch_size, seq_len, hidden_size * 2)\n",
    "        # Concatenate the last hidden states from both directions\n",
    "        out = self.fc(torch.cat((h_n[-2], h_n[-1]), dim=1))  \n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/17, TrainLoss: 0.6942, ValLoss: 0.6882, Accuracy: 0.5333, AUC: 0.7600, Sensitivity: 0.9778, Specificity: 0.0889\n",
      "Epoch 6/17, TrainLoss: 0.3690, ValLoss: 0.6131, Accuracy: 0.7667, AUC: 0.8173, Sensitivity: 0.7556, Specificity: 0.7778\n",
      "Epoch 11/17, TrainLoss: 0.2692, ValLoss: 0.6020, Accuracy: 0.7778, AUC: 0.8514, Sensitivity: 0.7111, Specificity: 0.8444\n",
      "Epoch 16/17, TrainLoss: 0.2030, ValLoss: 0.7112, Accuracy: 0.7889, AUC: 0.8400, Sensitivity: 0.7778, Specificity: 0.8000\n",
      "0.7830188679245284\n",
      "0.850124599501602\n",
      "Test Accuracy: 0.7830\n",
      "Test AUC: 0.8501\n",
      "Test Sensitivity (Recall): 0.8113\n",
      "Test Specificity: 0.7547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7830188679245284, 0.850124599501602, 0.8113207547169812, 0.7547169811320755)"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 128  # Number of features per time step (e.g., One-hot encoding size)\n",
    "hidden_size = 128  # Number of LSTM units\n",
    "num_layers = 2  # Stacked LSTM layers\n",
    "output_size = 1  # Binary classification (AMP or not)\n",
    "batch_size = 16\n",
    "num_epochs = 17\n",
    "learning_rate = 0.001\n",
    "tensorboard_log_dir = f\"runs/bilstm/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = BiLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs)\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## biLSTM with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(ScaledDotProductAttention, self).__init__()\n",
    "        self.scale_factor = torch.sqrt(torch.tensor(hidden_size, dtype=torch.float32))\n",
    "    \n",
    "    def forward(self, query, key, value):\n",
    "        attention_scores = torch.bmm(query, key.transpose(1, 2))  # [batch, seq_len, seq_len]\n",
    "        attention_scores = attention_scores / self.scale_factor\n",
    "        attention_weights = F.softmax(attention_scores, dim=-1)  # Normalize scores\n",
    "        context_vector = torch.bmm(attention_weights, value)  # Weighted sum\n",
    "        return context_vector, attention_weights\n",
    "    \n",
    "class BiLSTM_ScaledDotAttention(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(BiLSTM_ScaledDotAttention, self).__init__()\n",
    "        self.bilstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, bidirectional=True)\n",
    "        self.attention = ScaledDotProductAttention(hidden_size * 2)  # Bidirectional -> 2x hidden size\n",
    "        self.fc = nn.Linear(hidden_size * 2, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.bilstm(x)  # LSTM output shape: [batch_size, seq_len, hidden_size * 2]\n",
    "        \n",
    "        # Using last hidden state as Query\n",
    "        query = lstm_out[:, -1, :].unsqueeze(1)  # [batch_size, 1, hidden_size * 2]\n",
    "\n",
    "        # Key and Value are the full sequence outputs\n",
    "        key = lstm_out  # [batch_size, seq_len, hidden_size * 2]\n",
    "        value = lstm_out\n",
    "\n",
    "        context_vector, attention_weights = self.attention(query, key, value)\n",
    "        context_vector = context_vector.squeeze(1)  # [batch_size, hidden_size * 2]\n",
    "\n",
    "        out = self.fc(context_vector)  # [batch_size, output_size]\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/22, TrainLoss: 0.6938, ValLoss: 0.6931, Accuracy: 0.5000, AUC: 0.6563, Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch 6/22, TrainLoss: 0.6840, ValLoss: 0.6767, Accuracy: 0.6889, AUC: 0.6454, Sensitivity: 0.4889, Specificity: 0.8889\n",
      "Epoch 11/22, TrainLoss: 0.6453, ValLoss: 0.6568, Accuracy: 0.5444, AUC: 0.6741, Sensitivity: 0.6667, Specificity: 0.4222\n",
      "Epoch 16/22, TrainLoss: 0.5764, ValLoss: 0.5407, Accuracy: 0.7556, AUC: 0.8188, Sensitivity: 0.6444, Specificity: 0.8667\n",
      "Epoch 21/22, TrainLoss: 0.5198, ValLoss: 0.4896, Accuracy: 0.7556, AUC: 0.8326, Sensitivity: 0.7111, Specificity: 0.8000\n",
      "0.7169811320754716\n",
      "0.769312922748309\n",
      "Test Accuracy: 0.7170\n",
      "Test AUC: 0.7693\n",
      "Test Sensitivity (Recall): 0.8113\n",
      "Test Specificity: 0.6226\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7169811320754716, 0.769312922748309, 0.8113207547169812, 0.6226415094339622)"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 128  # One-hot encoding size or embedding dimension\n",
    "hidden_size = 128  # LSTM hidden state size\n",
    "num_layers = 2  # Stacked BiLSTM layers\n",
    "output_size = 1  # Binary classification\n",
    "batch_size = 16\n",
    "num_epochs = 22\n",
    "learning_rate = 0.0001\n",
    "tensorboard_log_dir = f\"runs/bilstm_attention/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = BiLSTM_ScaledDotAttention(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs)\n",
    "\n",
    "\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacked LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class StackedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super(StackedLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)  # LSTM output: (batch_size, seq_len, hidden_size)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Take the last time step's output\n",
    "        return self.sigmoid(out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/26, TrainLoss: 0.6947, ValLoss: 0.6935, Accuracy: 0.5000, AUC: 0.6588, Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch 6/26, TrainLoss: 0.6905, ValLoss: 0.6899, Accuracy: 0.5667, AUC: 0.6607, Sensitivity: 0.7111, Specificity: 0.4222\n",
      "Epoch 11/26, TrainLoss: 0.6534, ValLoss: 0.6659, Accuracy: 0.5000, AUC: 0.6553, Sensitivity: 0.5556, Specificity: 0.4444\n",
      "Epoch 16/26, TrainLoss: 0.6289, ValLoss: 0.6382, Accuracy: 0.6000, AUC: 0.6879, Sensitivity: 0.5556, Specificity: 0.6444\n",
      "Epoch 21/26, TrainLoss: 0.6009, ValLoss: 0.5974, Accuracy: 0.7444, AUC: 0.7546, Sensitivity: 0.6667, Specificity: 0.8222\n",
      "Epoch 26/26, TrainLoss: 0.5393, ValLoss: 0.5843, Accuracy: 0.7222, AUC: 0.7664, Sensitivity: 0.6222, Specificity: 0.8222\n",
      "0.7264150943396226\n",
      "0.7689569241723033\n",
      "Test Accuracy: 0.7264\n",
      "Test AUC: 0.7690\n",
      "Test Sensitivity (Recall): 0.7170\n",
      "Test Specificity: 0.7358\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.7264150943396226,\n",
       " 0.7689569241723033,\n",
       " 0.7169811320754716,\n",
       " 0.7358490566037735)"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "# Function to calculate specificity\n",
    "def specificity_score(y_true, y_pred):\n",
    "    tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "    return tn / (tn + fp)\n",
    "\n",
    "import torch.optim as optim\n",
    "import time\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 128  # Feature size (e.g., one-hot encoding or embedding)\n",
    "hidden_size = 128  # LSTM hidden state size\n",
    "num_layers = 3  # Stacked LSTM layers\n",
    "output_size = 1  # Binary classification\n",
    "batch_size = 16\n",
    "num_epochs = 26\n",
    "learning_rate = 0.0001\n",
    "tensorboard_log_dir = f\"runs/stacked_lstm/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = StackedLSTM(input_size, hidden_size, num_layers, output_size)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train and Evaluate\n",
    "train_and_evaluate(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs)\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Train': 508, 'Validation': 90, 'Test': 106}\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define One-Hot Encoding Function for Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"  # 20 standard amino acids\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)  # One-hot encoded matrix\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "def preprocess_sequence(full_sequence, max_length):\n",
    "    \"\"\"\n",
    "    Prepares input and target sequences while avoiding padding ('X') during training.\n",
    "    \n",
    "    Args:\n",
    "        full_sequence (str): The original sequence including 'X' padding.\n",
    "        max_length (int): Maximum sequence length (for padding).\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (input sequence, target sequence) after removing padding.\n",
    "    \"\"\"\n",
    "    # Remove trailing 'X' (padding) before slicing\n",
    "    trimmed_sequence = full_sequence.rstrip('X')\n",
    "\n",
    "    # Ensure there's at least 2 valid residues left for training\n",
    "    if len(trimmed_sequence) < 2:\n",
    "        return None, None  # Skip sequences that are too short to predict anything\n",
    "\n",
    "    # Create input & target sequences (without padding)\n",
    "    input_seq = trimmed_sequence[:-1]  # Exclude last valid residue\n",
    "    target_seq = trimmed_sequence[-1]  # Shifted by one position\n",
    "\n",
    "    # Pad back to maintain fixed-length format\n",
    "    input_seq = input_seq.ljust(max_length - 1, 'X')  # Pad to max_length - 1\n",
    "    # target_seq = target_seq.ljust(max_length - 1, 'X')  # Target also gets 'X' padding\n",
    "\n",
    "    return input_seq, target_seq\n",
    "\n",
    "# Custom Dataset for Generative Model\n",
    "class GenerativeSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, max_length, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.max_length = max_length\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        full_sequence = self.sequences.iloc[idx]  # Get sequence\n",
    "\n",
    "        # Process input & target while handling padding correctly\n",
    "        input_seq, target_seq = preprocess_sequence(full_sequence, self.max_length)\n",
    "        if input_seq is None or target_seq is None:\n",
    "            return None  # Skip sequences that are too short\n",
    "\n",
    "        # Convert to one-hot encoding\n",
    "        input_one_hot = one_hot_torch(input_seq, dtype=self.one_hot_dtype)\n",
    "        target_one_hot = one_hot_torch(target_seq, dtype=self.one_hot_dtype)\n",
    "\n",
    "        return input_one_hot, target_one_hot\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train_val, X_test = train_test_split(df[\"Sequences\"], test_size=0.15, random_state=42)\n",
    "X_train, X_val = train_test_split(X_train_val, test_size=0.15, random_state=42)  \n",
    "\n",
    "# Convert to PyTorch Dataset\n",
    "train_dataset = GenerativeSequenceDataset(X_train, max_length)\n",
    "val_dataset = GenerativeSequenceDataset(X_val, max_length)\n",
    "test_dataset = GenerativeSequenceDataset(X_test, max_length)\n",
    "\n",
    "# Define DataLoaders\n",
    "batch_size = 16\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)-\n",
    "}\n",
    "print(dataset_sizes)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from icecream import ic\n",
    "\n",
    "def train_lstm(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs=25):\n",
    "    writer = SummaryWriter(tensorboard_log_dir)\n",
    "    # Initialize the scheduler: reduce LR by factor 0.5 if validation loss doesn't improve for 5 epochs.\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, verbose=True)\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        train_losses = []\n",
    "\n",
    "        for input_seq, target_residue in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(input_seq)  # Forward pass\n",
    "            outputs = outputs.squeeze()\n",
    "\n",
    "            # Debug prints for output and target shapes\n",
    "            ic(outputs.shape)  # Expected: (batch_size, num_classes)\n",
    "            ic(target_residue.shape)  # Expected: (batch_size, 20)\n",
    "\n",
    "            # Convert one-hot target to class indices\n",
    "            target_residue = torch.argmax(target_residue, dim=1)  # (batch_size, 20) → (batch_size,)\n",
    "            target_residue = target_residue.squeeze()\n",
    "            ic(target_residue.shape)  # Expected: (batch_size,)\n",
    "            ic(target_residue)\n",
    "\n",
    "            # Compute loss (CrossEntropyLoss expects class indices as targets)\n",
    "            loss = criterion(outputs, target_residue)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses.append(loss.item())\n",
    "\n",
    "        avg_train_loss = np.mean(train_losses)\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "\n",
    "        # Validation Step\n",
    "        model.eval()\n",
    "        val_losses = []\n",
    "        with torch.no_grad():\n",
    "            for input_seq, target_residue in val_loader:\n",
    "                outputs = model(input_seq)\n",
    "                outputs = outputs.squeeze()\n",
    "\n",
    "                target_residue = torch.argmax(target_residue, dim=1)  # Converts (batch_size, 20) → (batch_size,)\n",
    "                target_residue = target_residue.squeeze()\n",
    "                \n",
    "                loss = criterion(outputs, target_residue)\n",
    "                val_losses.append(loss.item())\n",
    "\n",
    "        avg_val_loss = np.mean(val_losses)\n",
    "        writer.add_scalar('Loss/Val', avg_val_loss, epoch)\n",
    "\n",
    "        # Update the learning rate scheduler with the current validation loss\n",
    "        scheduler.step(avg_val_loss)\n",
    "\n",
    "        if epoch % 5 == 0:\n",
    "            print(f'Epoch {epoch+1}/{num_epochs}, TrainLoss: {avg_train_loss:.4f}, ValLoss: {avg_val_loss:.4f}')\n",
    "\n",
    "    writer.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Decoded Input Sequence: KWKSFLKTFKSLKKTVLHTLLKAISXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Decoded Target Sequence: S\n",
      "Input Shape: torch.Size([16, 20, 127])\n",
      "Target Shape: torch.Size([16, 20, 1])\n",
      "\n",
      "Decoded Input Sequence: KFHEKHHSHRGXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "Decoded Target Sequence: Y\n",
      "Input Shape: torch.Size([16, 20, 127])\n",
      "Target Shape:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " torch.Size([16, 20, 1])\n"
     ]
    }
   ],
   "source": [
    "def one_hot_to_sequence(one_hot_tensor):\n",
    "    \"\"\"\n",
    "    Converts a one-hot encoded tensor back into an amino acid sequence,\n",
    "    handling fully zero vectors (padding 'X').\n",
    "\n",
    "    Args:\n",
    "        one_hot_tensor (torch.Tensor): One-hot encoded tensor of shape (num_amino_acids, seq_length)\n",
    "\n",
    "    Returns:\n",
    "        str: Decoded amino acid sequence\n",
    "    \"\"\"\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"  # 20 standard amino acids\n",
    "    seq_length = one_hot_tensor.shape[1]  # Get sequence length\n",
    "    \n",
    "    decoded_sequence = \"\"\n",
    "    for i in range(seq_length):\n",
    "        column = one_hot_tensor[:, i]  # Get one-hot vector for residue\n",
    "        \n",
    "        if torch.sum(column) == 0:  # If it's fully zero, assume it's padding\n",
    "            decoded_sequence += \"X\"\n",
    "        else:\n",
    "            amino_acid_index = torch.argmax(column).item()  # Get highest probability index\n",
    "            decoded_sequence += amino_acids[amino_acid_index]  # Convert index to amino acid\n",
    "    \n",
    "    return decoded_sequence\n",
    "\n",
    "for x, y in train_loader:\n",
    "    # print(\"One-Hot Encoded Input:\\n\", x)\n",
    "    # print(\"One-Hot Encoded Target:\\n\", y)\n",
    "    \n",
    "    # Convert back to amino acid sequences\n",
    "    input_seq_decoded = one_hot_to_sequence(x[0])  # Decode first sample in batch\n",
    "    target_seq_decoded = one_hot_to_sequence(y[0])  # Decode first target sequence\n",
    "    \n",
    "    print(\"\\nDecoded Input Sequence:\", input_seq_decoded)\n",
    "    print(\"Decoded Target Sequence:\", target_seq_decoded)\n",
    "    print(\"Input Shape:\", x.shape)\n",
    "    print(\"Target Shape:\", y.shape)\n",
    "    break  # Print one batch and exit\n",
    "\n",
    "for x, y in test_loader:\n",
    "    # print(\"One-Hot Encoded Input:\\n\", x)\n",
    "    # print(\"One-Hot Encoded Target:\\n\", y)\n",
    "    \n",
    "    # Convert back to amino acid sequences\n",
    "    input_seq_decoded = one_hot_to_sequence(x[0])  # Decode first sample in batch\n",
    "    target_seq_decoded = one_hot_to_sequence(y[0])  # Decode first target sequence\n",
    "    \n",
    "    print(\"\\nDecoded Input Sequence:\", input_seq_decoded)\n",
    "    print(\"Decoded Target Sequence:\", target_seq_decoded)\n",
    "    print(\"Input Shape:\", x.shape)\n",
    "    print(\"Target Shape:\", y.shape)\n",
    "    break  # Print one batch and exit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "ic.enable()\n",
    "class LSTM_Generative(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size, dropout):\n",
    "        super(LSTM_Generative, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        self.softmax = nn.Softmax(dim=-1)  # Softmax for probability distribution\n",
    "\n",
    "    def forward(self, x):\n",
    "        ic(x.shape)\n",
    "        lstm_out, _ = self.lstm(x)  # Shape: (batch_size, seq_length, hidden_size)\n",
    "        out = self.fc(lstm_out)  # Shape: (batch_size, seq_length, output_size)\n",
    "        return out # Probability distribution over amino acids\n",
    "\n",
    "\n",
    "# class LSTM_Generative(nn.Module):\n",
    "#     def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "#         super(LSTM_Generative, self).__init__()\n",
    "#         self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "#         self.fc = nn.Linear(hidden_size, output_size)  # No softmax!\n",
    "\n",
    "#     def forward(self, x):\n",
    "#         lstm_out, _ = self.lstm(x)\n",
    "#         out = self.fc(lstm_out[:, -1, :])  # Predict next residue from last hidden state\n",
    "#         return out  # Return logits (no softmax)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/60, TrainLoss: 2.9947, ValLoss: 2.9937\n",
      "Epoch 6/60, TrainLoss: 2.9760, ValLoss: 2.9761\n",
      "Epoch 11/60, TrainLoss: 2.9383, ValLoss: 2.9728\n",
      "Epoch 16/60, TrainLoss: 2.8913, ValLoss: 2.9548\n",
      "Epoch 21/60, TrainLoss: 2.8255, ValLoss: 2.9013\n",
      "Epoch 26/60, TrainLoss: 2.7441, ValLoss: 2.8519\n",
      "Epoch 31/60, TrainLoss: 2.7049, ValLoss: 2.8736\n",
      "Epoch 36/60, TrainLoss: 2.6580, ValLoss: 2.8560\n",
      "Epoch 41/60, TrainLoss: 2.6392, ValLoss: 2.8771\n",
      "Epoch 46/60, TrainLoss: 2.6337, ValLoss: 2.8783\n",
      "Epoch 51/60, TrainLoss: 2.6264, ValLoss: 2.8761\n",
      "Epoch 56/60, TrainLoss: 2.6227, ValLoss: 2.8855\n"
     ]
    }
   ],
   "source": [
    "\n",
    "ic.disable()\n",
    "# ic.enable()\n",
    "\n",
    "# Hyperparameters\n",
    "input_size = 127 #len(amino_acids)  # Number of amino acids (20)\n",
    "hidden_size = 128  # LSTM hidden state size\n",
    "num_layers = 2  # Stacked LSTM layers\n",
    "output_size = 1 #len(amino_acids)  # Predicting one of 20 amino acids\n",
    "batch_size = 16\n",
    "num_epochs = 60\n",
    "learning_rate = 0.0001\n",
    "dropout = 0.7\n",
    "tensorboard_log_dir = f\"runs/generative_lstm/experiment_{time.strftime('%Y-%m-%d_%H-%M-%S')}\"\n",
    "\n",
    "# Model, Loss, Optimizer\n",
    "model = LSTM_Generative(input_size, hidden_size, num_layers, output_size, dropout)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Train the Model\n",
    "train_lstm(model, train_loader, val_loader, criterion, optimizer, tensorboard_log_dir, num_epochs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_seq\n",
      "KFHEKHHSHRGXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "pred_seq\n",
      "\n",
      "tensor([[ -0.2576],\n",
      "        [ -1.4460],\n",
      "        [  0.6699],\n",
      "        [ -0.1802],\n",
      "        [ -0.3858],\n",
      "        [  0.7994],\n",
      "        [ -2.3871],\n",
      "        [ -3.3349],\n",
      "        [ -8.6021],\n",
      "        [ -5.5967],\n",
      "        [ -4.2079],\n",
      "        [ -9.3004],\n",
      "        [-12.7328],\n",
      "        [-13.1089],\n",
      "        [-12.1663],\n",
      "        [-10.3028],\n",
      "        [ -7.9509],\n",
      "        [ -4.1972],\n",
      "        [ -1.0595],\n",
      "        [ -8.0496]])\n",
      "torch.Size([20, 1])\n",
      "G\n",
      "target_seq\n",
      "Y\n",
      "---------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_seq\n",
      "LVRAYHAMXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "pred_seq\n",
      "\n",
      "tensor([[-0.3253],\n",
      "        [-4.0561],\n",
      "        [-2.2803],\n",
      "        [-1.1629],\n",
      "        [-0.7106],\n",
      "        [-0.0308],\n",
      "        [-1.2208],\n",
      "        [-2.3512],\n",
      "        [-5.0872],\n",
      "        [-4.9262],\n",
      "        [-2.7832],\n",
      "        [-8.7870],\n",
      "        [-7.9234],\n",
      "        [-4.7137],\n",
      "        [-3.1058],\n",
      "        [-4.7313],\n",
      "        [-5.1510],\n",
      "        [-3.3170],\n",
      "        [-4.6499],\n",
      "        [-3.5734]])\n",
      "torch.Size([20, 1])\n",
      "G\n",
      "target_seq\n",
      "S\n",
      "---------------------------------------------\n",
      "input_seq\n",
      "AVEAXKTYVRDKPXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "pred_seq\n",
      "\n",
      "tensor([[  0.0355],\n",
      "        [ -3.2529],\n",
      "        [ -0.8319],\n",
      "        [ -4.1709],\n",
      "        [-11.3265],\n",
      "        [ -5.1903],\n",
      "        [  0.3549],\n",
      "        [ -0.5790],\n",
      "        [ -4.1414],\n",
      "        [ -6.5417],\n",
      "        [ -8.0048],\n",
      "        [ -7.5452],\n",
      "        [ -5.1974],\n",
      "        [ -2.1537],\n",
      "        [ -3.2312],\n",
      "        [ -6.8428],\n",
      "        [-10.3155],\n",
      "        [-13.7976],\n",
      "        [-11.4917],\n",
      "        [ -9.2461]])\n",
      "torch.Size([20, 1])\n",
      "H\n",
      "target_seq\n",
      "I\n",
      "---------------------------------------------\n",
      "input_seq\n",
      "QRPEYPVWREKGAXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "pred_seq\n",
      "\n",
      "tensor([[  0.1904],\n",
      "        [ -2.5725],\n",
      "        [ -0.2758],\n",
      "        [ -1.5790],\n",
      "        [ -3.7870],\n",
      "        [ -0.9253],\n",
      "        [ -1.7087],\n",
      "        [ -3.7649],\n",
      "        [ -0.9233],\n",
      "        [ -8.1001],\n",
      "        [ -6.6121],\n",
      "        [ -7.7962],\n",
      "        [-10.0973],\n",
      "        [-12.6326],\n",
      "        [ -9.1053],\n",
      "        [-10.7580],\n",
      "        [-10.6665],\n",
      "        [ -8.2840],\n",
      "        [-17.4629],\n",
      "        [-12.5252]])\n",
      "torch.Size([20, 1])\n",
      "A\n",
      "target_seq\n",
      "V\n",
      "---------------------------------------------\n",
      "input_seq\n",
      "WKIVFGWRXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "pred_seq\n",
      "\n",
      "tensor([[-0.2576],\n",
      "        [-1.4460],\n",
      "        [ 0.6699],\n",
      "        [-0.3309],\n",
      "        [ 0.0050],\n",
      "        [ 0.3228],\n",
      "        [-0.7133],\n",
      "        [-0.4850],\n",
      "        [-3.2951],\n",
      "        [-3.2076],\n",
      "        [-4.3730],\n",
      "        [-3.2457],\n",
      "        [-3.7350],\n",
      "        [-0.3533],\n",
      "        [ 4.3698],\n",
      "        [ 0.9634],\n",
      "        [-4.5806],\n",
      "        [-3.3838],\n",
      "        [-3.4885],\n",
      "        [-0.8683]])\n",
      "torch.Size([20, 1])\n",
      "R\n",
      "target_seq\n",
      "R\n",
      "---------------------------------------------\n",
      "input_seq\n",
      "LEXILSLSXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "pred_seq\n",
      "\n",
      "tensor([[-0.2576],\n",
      "        [-1.4460],\n",
      "        [ 0.6699],\n",
      "        [ 0.6201],\n",
      "        [-2.2500],\n",
      "        [ 2.6413],\n",
      "        [ 0.6567],\n",
      "        [ 0.0832],\n",
      "        [ 1.1085],\n",
      "        [-4.9167],\n",
      "        [-4.6078],\n",
      "        [-5.8449],\n",
      "        [-4.6907],\n",
      "        [-1.1380],\n",
      "        [-1.4232],\n",
      "        [-6.1510],\n",
      "        [-5.7048],\n",
      "        [-4.3520],\n",
      "        [-1.2739],\n",
      "        [ 1.6148]])\n",
      "torch.Size([20, 1])\n",
      "G\n",
      "target_seq\n",
      "I\n",
      "---------------------------------------------\n",
      "input_seq\n",
      "KHSLPDLPYDYGXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\n",
      "pred_seq\n",
      "\n",
      "tensor([[ -0.2576],\n",
      "        [ -1.4460],\n",
      "        [ -0.4183],\n",
      "        [  0.0720],\n",
      "        [ -6.4271],\n",
      "        [ -2.6422],\n",
      "        [ -5.6960],\n",
      "        [ -5.0038],\n",
      "        [ -4.9767],\n",
      "        [ -8.3074],\n",
      "        [ -9.9318],\n",
      "        [ -7.9121],\n",
      "        [ -6.7688],\n",
      "        [ -6.1388],\n",
      "        [ -7.2177],\n",
      "        [ -8.7651],\n",
      "        [-13.0087],\n",
      "        [-10.8260],\n",
      "        [-10.3750],\n",
      "        [ -4.6007]])\n",
      "torch.Size([20, 1])\n",
      "E\n",
      "target_seq\n",
      "A\n",
      "---------------------------------------------\n",
      "Test Loss: 0.0000\n",
      "Test Accuracy: 1.0000\n",
      "Test Perplexity: 1.0000\n",
      "test_loss: 0.0\n",
      "test_accuracy: 1.0\n",
      "test_perplexity: 1.0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "ic.disable()\n",
    "def evaluate_lstm(model, test_loader, criterion):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    \n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    total_perplexity = 0\n",
    "    num_samples = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq in test_loader:\n",
    "            print('input_seq')\n",
    "            print(one_hot_to_sequence(input_seq[0]))\n",
    "            outputs = model(input_seq)  # Predictions (batch_size, seq_length, num_classes)\n",
    "            print('pred_seq')\n",
    "            print()\n",
    "            print(outputs[0])\n",
    "            print(outputs[0].shape)\n",
    "            print(one_hot_to_sequence(outputs[0]))\n",
    "            print('target_seq')\n",
    "            print(one_hot_to_sequence(target_seq[0]))\n",
    "            print('---------------------------------------------')\n",
    "            # Compute loss\n",
    "            target_indices = torch.argmax(target_seq, dim=-1)  # Shape: (batch_size, seq_length\n",
    "            # ic(target_indices.shape)\n",
    "            # ic(outputs.shape)\n",
    "            # Compute loss (CrossEntropy expects target of shape (batch_size, seq_length))\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), target_indices.view(-1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Compute accuracy (compare predicted residues with ground truth)\n",
    "            predicted_indices = torch.argmax(outputs, dim=-1)  # Get highest probability residues\n",
    "            target_indices = torch.argmax(target_seq, dim=-1)  # Get actual residues\n",
    "            batch_accuracy = (predicted_indices == target_indices).float().mean().item()\n",
    "            total_accuracy += batch_accuracy\n",
    "\n",
    "            # Compute perplexity (exp of cross-entropy loss)\n",
    "            batch_perplexity = torch.exp(loss).item()\n",
    "            total_perplexity += batch_perplexity\n",
    "\n",
    "            num_samples += 1\n",
    "\n",
    "    # Calculate averages\n",
    "    avg_loss = total_loss / num_samples\n",
    "    avg_accuracy = total_accuracy / num_samples\n",
    "    avg_perplexity = total_perplexity / num_samples\n",
    "\n",
    "    print(f\"Test Loss: {avg_loss:.4f}\")\n",
    "    print(f\"Test Accuracy: {avg_accuracy:.4f}\")\n",
    "    print(f\"Test Perplexity: {avg_perplexity:.4f}\")\n",
    "\n",
    "    return avg_loss, avg_accuracy, avg_perplexity\n",
    "# Run evaluation\n",
    "test_loss, test_accuracy, test_perplexity = evaluate_lstm(model, test_loader, criterion)\n",
    "print('test_loss:',test_loss)\n",
    "print('test_accuracy:',test_accuracy)\n",
    "print('test_perplexity:',test_perplexity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
