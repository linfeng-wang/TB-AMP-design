{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# If using PyTorch\n",
    "import torch\n",
    "\n",
    "# If using TensorFlow\n",
    "\n",
    "# Optional: If using Python hash-based functions\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "\n",
    "# Set seed for base Python random\n",
    "random.seed(42)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set seed for PyTorch (CPU and GPU)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)  # if using multi-GPU\n",
    "\n",
    "# Force deterministic behavior in PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual FASTA parsing (without Biopython)\n",
    "# fasta_path = \"../data//naturalAMPs_APD2024a-ADAM.fasta.txt\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\", index=False)\n",
    "\n",
    "\n",
    "# Manual FASTA parsing (without Biopython)\n",
    "# fasta_path = \"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# fasta_path = \"../data/uniprotkb_length_5_TO_30_NOT_antimicrob_2025_04_14.fasta (1)\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Bio import SeqIO\n",
    "\n",
    "# def read_fasta_sequences(fasta_path):\n",
    "#     \"\"\"\n",
    "#     Reads sequences from a FASTA file and returns them as a list of strings.\n",
    "\n",
    "#     Args:\n",
    "#         fasta_path (str): Path to the FASTA file.\n",
    "\n",
    "#     Returns:\n",
    "#         List[str]: A list of amino acid or nucleotide sequences as strings.\n",
    "#     \"\"\"\n",
    "#     sequences = [str(record.seq) for record in SeqIO.parse(fasta_path, \"fasta\")]\n",
    "#     return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbbasp = pd.read_csv(\"../models1/database_check/DBAASP_peptides.csv\")\n",
    "dbbasp = dbbasp[dbbasp[\"SEQUENCE\"].str.len() >= 10]\n",
    "dbbasp = dbbasp[~dbbasp[\"TARGET GROUP\"].str.contains(\"Fungus\", na=False)]\n",
    "dbbasp = dbbasp[[\"ID\", \"SEQUENCE\"]]\n",
    "dbbasp.columns = [\"Peptide ID\", \"Sequence\"]\n",
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "adam_df = pd.concat([adam_df, dbbasp], ignore_index=True)\n",
    "uniprot_df = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\")\n",
    "uniprot_df1 = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\")\n",
    "uniprot_df2 = pd.read_csv(\"/mnt/storageG1/lwang/Projects/TB-AMP-design/data/uniprotkb_length_5_TO_30_NOT_antimicrob_2025_04_23 (1).tsv\", sep=\"\\t\")\n",
    "uniprot_df2\n",
    "uniprot_df2 = uniprot_df2[['Entry', 'Sequence']]\n",
    "uniprot_df2.columns = [\"Peptide ID\", \"Sequence\"]\n",
    "uniprot_df = pd.concat([uniprot_df, uniprot_df1, uniprot_df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Peptide ID</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr|A0A009HCC0|A0A009HCC0_9GAMM Acyl carrier pr...</td>\n",
       "      <td>MSDIEQRVKQAVAEQLGLKAEEIKNEASFMDDLGADSLDLVELVMS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tr|A0A009YF97|A0A009YF97_9GAMM Acyl carrier pr...</td>\n",
       "      <td>MSDIEQRVKQAVAEQLGMKVEEIKNEASFMDDLGADSLDLVELVMS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tr|A0A010SZ96|A0A010SZ96_PSEFL Acyl carrier pr...</td>\n",
       "      <td>MSTIEERVKKIVAEQLGVKEEEVVNTASFVEDLGADSLDTVELVMA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tr|A0A011NAB1|A0A011NAB1_9PAST Acyl carrier pr...</td>\n",
       "      <td>MSIEERVKKIIVDQLGAKAEDVKPEASFIEDLGADSLDTVELVMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tr|A0A011NTH9|A0A011NTH9_9PROT DNA-directed RN...</td>\n",
       "      <td>MARVTVDDCLTRIPNRFQMTLAATYRARQITAGASPLIDANRDKPT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104968</th>\n",
       "      <td>P9WEJ1</td>\n",
       "      <td>MPEQKANCSPNGNITVDSMIMSLGSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105566</th>\n",
       "      <td>Q6LEB3</td>\n",
       "      <td>LFNKYISRPRRVELAVMLNLTERHIKI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105568</th>\n",
       "      <td>Q6QLL8</td>\n",
       "      <td>VKGFSFKYGNGVWIGRTKSTNSRSGFQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105789</th>\n",
       "      <td>Q9DTV7</td>\n",
       "      <td>HFPGFGQSLLFGYPVYVFGDCVQGDWCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106056</th>\n",
       "      <td>T0YYA3</td>\n",
       "      <td>EPGLPESFNVLIKEMQSLALDIELLKTREK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38787 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Peptide ID  \\\n",
       "0       tr|A0A009HCC0|A0A009HCC0_9GAMM Acyl carrier pr...   \n",
       "6       tr|A0A009YF97|A0A009YF97_9GAMM Acyl carrier pr...   \n",
       "8       tr|A0A010SZ96|A0A010SZ96_PSEFL Acyl carrier pr...   \n",
       "9       tr|A0A011NAB1|A0A011NAB1_9PAST Acyl carrier pr...   \n",
       "10      tr|A0A011NTH9|A0A011NTH9_9PROT DNA-directed RN...   \n",
       "...                                                   ...   \n",
       "104968                                             P9WEJ1   \n",
       "105566                                             Q6LEB3   \n",
       "105568                                             Q6QLL8   \n",
       "105789                                             Q9DTV7   \n",
       "106056                                             T0YYA3   \n",
       "\n",
       "                                                 Sequence  \n",
       "0       MSDIEQRVKQAVAEQLGLKAEEIKNEASFMDDLGADSLDLVELVMS...  \n",
       "6       MSDIEQRVKQAVAEQLGMKVEEIKNEASFMDDLGADSLDLVELVMS...  \n",
       "8       MSTIEERVKKIVAEQLGVKEEEVVNTASFVEDLGADSLDTVELVMA...  \n",
       "9       MSIEERVKKIIVDQLGAKAEDVKPEASFIEDLGADSLDTVELVMAL...  \n",
       "10      MARVTVDDCLTRIPNRFQMTLAATYRARQITAGASPLIDANRDKPT...  \n",
       "...                                                   ...  \n",
       "104968                         MPEQKANCSPNGNITVDSMIMSLGSS  \n",
       "105566                        LFNKYISRPRRVELAVMLNLTERHIKI  \n",
       "105568                        VKGFSFKYGNGVWIGRTKSTNSRSGFQ  \n",
       "105789                       HFPGFGQSLLFGYPVYVFGDCVQGDWCR  \n",
       "106056                     EPGLPESFNVLIKEMQSLALDIELLKTREK  \n",
       "\n",
       "[38787 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniprot_df = uniprot_df[uniprot_df['Sequence'].apply(lambda s: isinstance(s, str) and len(s) >= 10)]\n",
    "uniprot_df = uniprot_df.drop_duplicates(subset=\"Sequence\")\n",
    "uniprot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove rows where the Sequence is not a string or is shorter than 10 characters\n",
    "adam_df = adam_df[adam_df['Sequence'].apply(lambda x: isinstance(x, str) and len(x) >= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 10\n",
      "Range of sequence lengths: 180\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sequence length: 23.215054249547922\n",
      "Median sequence length: 19.0\n",
      "Standard deviation of sequence lengths: 15.467236594869847\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate sequence lengths\n",
    "sequence_lengths = adam_df['Sequence'].apply(len)\n",
    "\n",
    "# Calculate the range of sequence lengths\n",
    "length_range = sequence_lengths.max() - sequence_lengths.min()\n",
    "print(sequence_lengths.max(),sequence_lengths.min())\n",
    "print(f\"Range of sequence lengths: {length_range}\")\n",
    "\n",
    "# Draw a box plot\n",
    "plt.boxplot(sequence_lengths)\n",
    "plt.title(\"Box Plot of Sequence Lengths\")\n",
    "plt.ylabel(\"Sequence Length\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display distribution statistics\n",
    "mean_length = sequence_lengths.mean()\n",
    "median_length = sequence_lengths.median()\n",
    "std_dev_length = sequence_lengths.std()\n",
    "\n",
    "print(f\"Mean sequence length: {mean_length}\")\n",
    "print(f\"Median sequence length: {median_length}\")\n",
    "print(f\"Standard deviation of sequence lengths: {std_dev_length}\")\n",
    "\n",
    "# adam_df = adam_df.drop(columns=['Sequence Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_115197/679251586.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_uniprot_df = uniprot_df.groupby('Sequence Length', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Peptide ID  \\\n",
      "0      sp|C0HJE2|AMYG_BACLI Glucoamylase (Fragment) O...   \n",
      "1      sp|Q03367|PSBF_CAPAN Cytochrome b559 subunit b...   \n",
      "2      sp|Q10997|SPI_HALRO Serine proteinase inhibito...   \n",
      "3      sp|P85962|RLA2_PSEMZ Large ribosomal subunit p...   \n",
      "4      sp|P02728|GLEM_HUMAN Erythrocyte membrane glyc...   \n",
      "...                                                  ...   \n",
      "15376  tr|A0A497J933|A0A497J933_9ARCH DNA-directed RN...   \n",
      "15377  tr|Q6Q0P4|Q6Q0P4_POLCT Cytochrome c oxidase su...   \n",
      "15378  tr|A0A2J4G6H0|A0A2J4G6H0_9CREN DNA-directed RN...   \n",
      "15379  tr|A0A327J0Z7|A0A327J0Z7_9BACT ATP synthase su...   \n",
      "15380  tr|A0A0G2F2K3|A0A0G2F2K3_PHACM U6 snRNA-associ...   \n",
      "\n",
      "                                                Sequence  \n",
      "0                                             SSNKLTTSWG  \n",
      "1                                             SISAMQFIQR  \n",
      "2                                             TKKDGEEKVA  \n",
      "3                                             DITEVIAAGR  \n",
      "4                                             CEGHSHDHGA  \n",
      "...                                                  ...  \n",
      "15376  MPEEFKVGRHVLVPKHEILPKEKVEEVLKRYKIQPYHLPLIKSSDP...  \n",
      "15377  IKAFSWMATIYGSKLNFSPTMMWSMGFIFLFTMGGLTGIILSNSSL...  \n",
      "15378  MPRKINVLEHELVPKHILLTREEAKKILKLMGLRKSELPWIYATDP...  \n",
      "15379  MADVKTISDLAQLGAGVAIGFGAVGAGLGIGIATKGLLESISRQPE...  \n",
      "15380  MENGAPSEGKDPAAFLGEIIGVPVTVKLNSGVVYKGELQSVDGYMN...  \n",
      "\n",
      "[15381 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate sequence lengths for uniprot_df\n",
    "uniprot_df['Sequence Length'] = uniprot_df['Sequence'].apply(len)\n",
    "\n",
    "# Calculate sequence lengths for adam_df\n",
    "adam_df['Sequence Length'] = adam_df['Sequence'].apply(len)\n",
    "\n",
    "# Perform stratified sampling to select more samples\n",
    "default_min_samples = 30\n",
    "\n",
    "# Perform lenient stratified sampling\n",
    "sampled_uniprot_df = uniprot_df.groupby('Sequence Length', group_keys=False).apply(\n",
    "    lambda x: x.sample(\n",
    "        n=min(\n",
    "            len(x), \n",
    "            int(14 * adam_df['Sequence Length'].value_counts().get(x.name, default_min_samples))\n",
    "        ),\n",
    "        random_state=42\n",
    "    )\n",
    ").reset_index(drop=True)\n",
    "# Drop the 'Sequence Length' column after sampling\n",
    "sampled_uniprot_df = sampled_uniprot_df.drop(columns=['Sequence Length'])\n",
    "adam_df = adam_df.drop(columns=['Sequence Length'])\n",
    "\n",
    "# Draw a box plot to visualize the distribution\n",
    "plt.boxplot(sampled_uniprot_df['Sequence'].apply(len))\n",
    "plt.title(\"Box Plot of Sequence Lengths (Sampled Negatives)\")\n",
    "plt.ylabel(\"Sequence Length\")\n",
    "plt.show()\n",
    "\n",
    "print(sampled_uniprot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n",
      "{'w', 'Q', 'R', 'G', 'T', '9', 'N', 'W', 'v', 'k', 'l', 'D', 'L', 'Y', 'm', 'F', 's', 'U', 'n', 'g', 'I', 'x', 'f', 'O', 'E', 'i', 'Z', 'X', 'a', 'K', 'B', 'P', ' ', 'y', 'h', 't', 'M', 'S', 'V', 'H', 'p', 'd', 'C', 'e', 'c', 'A', 'q', 'r'}\n",
      "48\n",
      "{'w', '9', 'v', 'l', 'k', 'm', 's', 'U', 'n', 'g', 'x', 'O', 'f', 'i', 'Z', 'a', 'B', ' ', 'h', 'y', 't', 'p', 'd', 'e', 'c', 'q', 'r'}\n",
      "Number of sequences after filtering: 30987\n",
      "label\n",
      "1    15839\n",
      "0    15148\n",
      "Name: count, dtype: int64\n",
      "{'Q', 'R', 'G', 'T', 'N', 'W', 'D', 'L', 'Y', 'F', 'I', 'E', 'X', 'K', 'P', 'M', 'S', 'V', 'H', 'C', 'A'}\n",
      "21\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "adam_df['label'] = 1\n",
    "sampled_uniprot_df['label'] = 0\n",
    "adam_df.columns = [\"Peptide ID\", \"Sequences\", 'label']\n",
    "sampled_uniprot_df.columns = [\"Peptide ID\", \"Sequences\" , 'label']\n",
    "df = pd.concat([adam_df, sampled_uniprot_df], ignore_index=True)\n",
    "\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWYX\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "\n",
    "# Filter out sequences containing non-standard amino acids\n",
    "df = df[~df['Sequences'].str.contains('|'.join(non_standard_amino_acids))]\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "tb_df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df[~df['Sequences'].isin(tb_df['Sequences'])]\n",
    "df = df[df['Sequences'].apply(lambda x: isinstance(x, str) and len(x) >= 10)]\n",
    "print(f\"Number of sequences after filtering: {len(df)}\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWYX\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# lengths = [len(seq) for seq in df['Sequences']]\n",
    "# print(lengths[lengths <= 0])\n",
    "\n",
    "lengths = np.array([len(seq) for seq in df['Sequences']])\n",
    "print(lengths[lengths <= 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define One-Hot Encoding Function for DNA Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        length = len(seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        return one_hot_torch(seq, dtype=self.one_hot_dtype), torch.tensor(label, dtype=torch.float32), length\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def collate_and_pack(batch):\n",
    "    # batch = list of (tensor_seq, label, length)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "\n",
    "    # Filter out sequences with zero length\n",
    "    filtered_batch = [(seq, lbl, l) for seq, lbl, l in zip(sequences, labels, lengths) if l > 0]\n",
    "\n",
    "    if len(filtered_batch) == 0:\n",
    "        raise ValueError(\"All sequences in the batch have zero length.\")\n",
    "\n",
    "    sequences, labels, lengths = zip(*filtered_batch)\n",
    "\n",
    "    # Convert lengths to tensor\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # Sort by descending length (required by pack_padded_sequence)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    labels = torch.tensor([labels[i] for i in sorted_indices])\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    # Stack to shape: (batch_size, 20, seq_len) and transpose for LSTM input\n",
    "    # LSTM expects input of shape (seq_len, batch_size, features)\n",
    "    sequences = [seq.T for seq in sequences]  # Transpose each [20, L] to [L, 20]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)  # shape: [max_len, batch, features]\n",
    "\n",
    "    # Pack the sequence\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 21690\n",
      "Validation: 4649\n",
      "Test: 4648\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        # packed_input: PackedSequence\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # hn: [num_layers, batch_size, hidden_dim]\n",
    "        # We'll use the **last layer's** hidden state as feature\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(last_hidden)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10] - Train Loss: 0.5523, Val Loss: 0.6706, Val Acc: 0.5627, Val AUC: 0.7061\n",
      "Epoch [2/10] - Train Loss: 0.4765, Val Loss: 0.3772, Val Acc: 0.8621, Val AUC: 0.9378\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 112\u001b[0m\n\u001b[1;32m    108\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, acc, auc\n\u001b[1;32m    111\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMClassifier(hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m--> 112\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, num_epochs, lr, device)\u001b[0m\n\u001b[1;32m     22\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[1;32m     23\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m packed_input, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[1;32m     26\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     27\u001b[0m     packed_input \u001b[38;5;241m=\u001b[39m packed_input\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[1;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[1;32m    714\u001b[0m ):\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[1;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/utils/data/_utils/fetch.py:52\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     50\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 52\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "Cell \u001b[0;32mIn[12], line 25\u001b[0m, in \u001b[0;36mSequenceDataset.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     23\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlabels\u001b[38;5;241m.\u001b[39miloc[idx]\n\u001b[1;32m     24\u001b[0m length \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(seq\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m))  \u001b[38;5;66;03m# unpadded length\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mone_hot_torch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mone_hot_dtype\u001b[49m\u001b[43m)\u001b[49m, torch\u001b[38;5;241m.\u001b[39mtensor(label, dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32), length\n",
      "Cell \u001b[0;32mIn[12], line 8\u001b[0m, in \u001b[0;36mone_hot_torch\u001b[0;34m(seq, dtype)\u001b[0m\n\u001b[1;32m      6\u001b[0m arr \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(amino_acids), \u001b[38;5;28mlen\u001b[39m(seq_bytes), dtype\u001b[38;5;241m=\u001b[39mdtype)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, aa \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(aa_bytes):\n\u001b[0;32m----> 8\u001b[0m     arr[i, seq_bytes \u001b[38;5;241m==\u001b[39m aa] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "    # Set up TensorBoard writer\n",
    "    log_dir = f\"runs/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Run evaluation\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return history\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # handle corner cases\n",
    "\n",
    "    # Sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        # Print metrics\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "model = LSTMClassifier(hidden_dim=64)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding in regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[1786  487]\n",
      " [  74 2302]]\n",
      "Sensitivity (Recall for Positive Class): 0.9689\n",
      "Specificity (Recall for Negative Class): 0.7857\n",
      "Epoch [1/10] - Train Loss: 0.4703, Val Loss: 0.3289, Val Acc: 0.8793, Val AUC: 0.9427\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  12 2261]\n",
      " [   1 2375]]\n",
      "Sensitivity (Recall for Positive Class): 0.9996\n",
      "Specificity (Recall for Negative Class): 0.0053\n",
      "Epoch [2/10] - Train Loss: 0.3643, Val Loss: 0.8430, Val Acc: 0.5134, Val AUC: 0.6575\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1877  396]\n",
      " [ 438 1938]]\n",
      "Sensitivity (Recall for Positive Class): 0.8157\n",
      "Specificity (Recall for Negative Class): 0.8258\n",
      "Epoch [3/10] - Train Loss: 0.6831, Val Loss: 0.6017, Val Acc: 0.8206, Val AUC: 0.8942\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2173  100]\n",
      " [ 471 1905]]\n",
      "Sensitivity (Recall for Positive Class): 0.8018\n",
      "Specificity (Recall for Negative Class): 0.9560\n",
      "Epoch [4/10] - Train Loss: 0.4295, Val Loss: 0.3204, Val Acc: 0.8772, Val AUC: 0.9526\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2082  191]\n",
      " [ 212 2164]]\n",
      "Sensitivity (Recall for Positive Class): 0.9108\n",
      "Specificity (Recall for Negative Class): 0.9160\n",
      "Epoch [5/10] - Train Loss: 0.2683, Val Loss: 0.2393, Val Acc: 0.9133, Val AUC: 0.9655\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2146  127]\n",
      " [ 304 2072]]\n",
      "Sensitivity (Recall for Positive Class): 0.8721\n",
      "Specificity (Recall for Negative Class): 0.9441\n",
      "Epoch [6/10] - Train Loss: 0.2381, Val Loss: 0.2417, Val Acc: 0.9073, Val AUC: 0.9706\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1990  283]\n",
      " [  97 2279]]\n",
      "Sensitivity (Recall for Positive Class): 0.9592\n",
      "Specificity (Recall for Negative Class): 0.8755\n",
      "Epoch [7/10] - Train Loss: 0.2354, Val Loss: 0.2040, Val Acc: 0.9183, Val AUC: 0.9766\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2048  225]\n",
      " [ 141 2235]]\n",
      "Sensitivity (Recall for Positive Class): 0.9407\n",
      "Specificity (Recall for Negative Class): 0.9010\n",
      "Epoch [8/10] - Train Loss: 0.2214, Val Loss: 0.1923, Val Acc: 0.9213, Val AUC: 0.9778\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2104  169]\n",
      " [ 164 2212]]\n",
      "Sensitivity (Recall for Positive Class): 0.9310\n",
      "Specificity (Recall for Negative Class): 0.9256\n",
      "Epoch [9/10] - Train Loss: 0.2448, Val Loss: 0.1869, Val Acc: 0.9284, Val AUC: 0.9790\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2027  246]\n",
      " [ 106 2270]]\n",
      "Sensitivity (Recall for Positive Class): 0.9554\n",
      "Specificity (Recall for Negative Class): 0.8918\n",
      "Epoch [10/10] - Train Loss: 0.1950, Val Loss: 0.1790, Val Acc: 0.9243, Val AUC: 0.9805\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # L2 regularization\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "    # Set up TensorBoard writer\n",
    "    log_dir = f\"runs/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)   \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Run evaluation\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.4f}, \"\n",
    "              f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # handle corner cases\n",
    "\n",
    "    # Sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        # Print metrics\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=64, dropout=0.5)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3,\n",
    "                      weight_decay=1e-4, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling on general AMP data (bayesian optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-05-06 10:43:08,155] A new study created in memory with name: no-name-32ca120b-d518-4e44-9e08-f233d31b1450\n",
      "[I 2025-05-06 10:48:27,944] Trial 0 finished with value: 0.19858369925250746 and parameters: {'hidden_dim': 70, 'num_layers': 1, 'dropout': 0.38027966645972233, 'lr': 0.003241504883125835, 'weight_decay': 0.005328877972977425}. Best is trial 0 with value: 0.19858369925250746.\n",
      "[I 2025-05-06 10:56:22,612] Trial 1 finished with value: 0.6929278202252845 and parameters: {'hidden_dim': 82, 'num_layers': 3, 'dropout': 0.18799202663891307, 'lr': 0.009958285769135185, 'weight_decay': 0.0033994716252035875}. Best is trial 0 with value: 0.19858369925250746.\n",
      "[I 2025-05-06 11:01:34,172] Trial 2 finished with value: 0.20074757974441737 and parameters: {'hidden_dim': 36, 'num_layers': 1, 'dropout': 0.23673358587151824, 'lr': 0.009899543492172445, 'weight_decay': 0.0018754388116351002}. Best is trial 0 with value: 0.19858369925250746.\n",
      "[I 2025-05-06 11:09:33,246] Trial 3 finished with value: 0.6929291388759874 and parameters: {'hidden_dim': 92, 'num_layers': 3, 'dropout': 0.4798532874492769, 'lr': 0.0035391236707473187, 'weight_decay': 0.003192437496598055}. Best is trial 0 with value: 0.19858369925250746.\n",
      "[I 2025-05-06 11:17:24,138] Trial 4 finished with value: 0.6929269743292299 and parameters: {'hidden_dim': 88, 'num_layers': 3, 'dropout': 0.43214107377135624, 'lr': 0.00708367999927206, 'weight_decay': 0.0017107449623304243}. Best is trial 0 with value: 0.19858369925250746.\n",
      "[I 2025-05-06 11:23:01,361] Trial 5 finished with value: 0.2091947590855703 and parameters: {'hidden_dim': 119, 'num_layers': 1, 'dropout': 0.1106695956297402, 'lr': 0.0010700190213220218, 'weight_decay': 0.004712115974510263}. Best is trial 0 with value: 0.19858369925250746.\n",
      "[I 2025-05-06 11:28:05,393] Trial 6 finished with value: 0.2569272798830516 and parameters: {'hidden_dim': 33, 'num_layers': 1, 'dropout': 0.4567422375079918, 'lr': 0.00024798262575343523, 'weight_decay': 0.004371519618736681}. Best is trial 0 with value: 0.19858369925250746.\n",
      "[I 2025-05-06 11:34:43,844] Trial 7 finished with value: 0.1246995001725138 and parameters: {'hidden_dim': 48, 'num_layers': 2, 'dropout': 0.1305874628402517, 'lr': 0.00421054415111712, 'weight_decay': 1.1250678718516062e-05}. Best is trial 7 with value: 0.1246995001725138.\n",
      "[I 2025-05-06 11:40:02,762] Trial 8 finished with value: 0.34227360907482773 and parameters: {'hidden_dim': 74, 'num_layers': 1, 'dropout': 0.34148647772175733, 'lr': 0.007189601061972823, 'weight_decay': 0.009547475249469415}. Best is trial 7 with value: 0.1246995001725138.\n",
      "[I 2025-05-06 11:46:46,715] Trial 9 finished with value: 0.6929276160997887 and parameters: {'hidden_dim': 91, 'num_layers': 2, 'dropout': 0.22432561747865787, 'lr': 0.005371173899641159, 'weight_decay': 0.006515675179096496}. Best is trial 7 with value: 0.1246995001725138.\n",
      "[I 2025-05-06 11:53:13,455] Trial 10 finished with value: 0.12483648435301976 and parameters: {'hidden_dim': 55, 'num_layers': 2, 'dropout': 0.10571446183821616, 'lr': 0.0032955081882349354, 'weight_decay': 8.200510314447716e-05}. Best is trial 7 with value: 0.1246995001725138.\n",
      "[I 2025-05-06 11:59:52,697] Trial 11 finished with value: 0.13662074194991425 and parameters: {'hidden_dim': 52, 'num_layers': 2, 'dropout': 0.10690679214894755, 'lr': 0.0027902543328929053, 'weight_decay': 7.876788406816858e-05}. Best is trial 7 with value: 0.1246995001725138.\n",
      "[I 2025-05-06 12:06:24,449] Trial 12 finished with value: 0.1481042984005523 and parameters: {'hidden_dim': 55, 'num_layers': 2, 'dropout': 0.1619998878656104, 'lr': 0.005048740720122787, 'weight_decay': 0.00021523763752400233}. Best is trial 7 with value: 0.1246995001725138.\n",
      "[I 2025-05-06 12:12:50,870] Trial 13 finished with value: 0.1939170921910299 and parameters: {'hidden_dim': 52, 'num_layers': 2, 'dropout': 0.27215414186021364, 'lr': 0.0019338066368894064, 'weight_decay': 0.00183712188697648}. Best is trial 7 with value: 0.1246995001725138.\n",
      "[I 2025-05-06 12:19:24,848] Trial 14 finished with value: 0.6929274666799258 and parameters: {'hidden_dim': 62, 'num_layers': 2, 'dropout': 0.1574558851916439, 'lr': 0.004989139085040697, 'weight_decay': 0.007880257469275566}. Best is trial 7 with value: 0.1246995001725138.\n",
      "[I 2025-05-06 12:26:01,452] Trial 15 finished with value: 0.1629132434521636 and parameters: {'hidden_dim': 43, 'num_layers': 2, 'dropout': 0.10461997907616716, 'lr': 0.004075063888823951, 'weight_decay': 0.0006221797942415956}. Best is trial 7 with value: 0.1246995001725138.\n",
      "[I 2025-05-06 12:34:08,585] Trial 16 finished with value: 0.6929269016605534 and parameters: {'hidden_dim': 107, 'num_layers': 3, 'dropout': 0.29355872696039104, 'lr': 0.006679852218557443, 'weight_decay': 0.0029668487724535675}. Best is trial 7 with value: 0.1246995001725138.\n",
      "[I 2025-05-06 12:40:54,244] Trial 17 finished with value: 0.16534172059738472 and parameters: {'hidden_dim': 65, 'num_layers': 2, 'dropout': 0.19936880667947987, 'lr': 0.001963402120299073, 'weight_decay': 0.0011755219544503292}. Best is trial 7 with value: 0.1246995001725138.\n",
      "[I 2025-05-06 12:48:35,553] Trial 18 finished with value: 0.6929252310974957 and parameters: {'hidden_dim': 45, 'num_layers': 3, 'dropout': 0.14904915034268856, 'lr': 0.004209507617451251, 'weight_decay': 0.0025078234429889426}. Best is trial 7 with value: 0.1246995001725138.\n",
      "[I 2025-05-06 12:55:13,759] Trial 19 finished with value: 0.692928430968768 and parameters: {'hidden_dim': 61, 'num_layers': 2, 'dropout': 0.246402619140822, 'lr': 0.006108579543427563, 'weight_decay': 0.006395462485243064}. Best is trial 7 with value: 0.1246995001725138.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 48, 'num_layers': 2, 'dropout': 0.1305874628402517, 'lr': 0.00421054415111712, 'weight_decay': 1.1250678718516062e-05}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    # if not train:\n",
    "    #     model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_auc = train_model(model, train_loader, val_loader, num_epochs=20, lr=lr,\n",
    "                          weight_decay=weight_decay, verbose=False, train=True)\n",
    "    return val_auc\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'R', 'Q', 'G', 'T', 'N', 'W', 'D', 'L', 'Y', 'F', 'I', 'E', 'X', 'K', 'P', 'M', 'S', 'V', 'H', 'C', 'A'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n",
      "Dataset sizes:\n",
      "Train: 21690\n",
      "Validation: 4649\n",
      "Test: 88\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Define DataLoaders\n",
    "\n",
    "\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7f72e40e95a0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[2019  254]\n",
      " [ 190 2186]]\n",
      "Sensitivity: 0.9200, Specificity: 0.8883\n",
      "Epoch [1/20] - Train Loss: 0.3328, Val Loss: 0.2347, Val Acc: 0.9045, Val AUC: 0.9667\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[2085  188]\n",
      " [ 230 2146]]\n",
      "Sensitivity: 0.9032, Specificity: 0.9173\n",
      "Epoch [2/20] - Train Loss: 0.2288, Val Loss: 0.2179, Val Acc: 0.9101, Val AUC: 0.9722\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1868  405]\n",
      " [  64 2312]]\n",
      "Sensitivity: 0.9731, Specificity: 0.8218\n",
      "Epoch [3/20] - Train Loss: 0.2147, Val Loss: 0.2132, Val Acc: 0.8991, Val AUC: 0.9772\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2047  226]\n",
      " [ 120 2256]]\n",
      "Sensitivity: 0.9495, Specificity: 0.9006\n",
      "Epoch [4/20] - Train Loss: 0.1980, Val Loss: 0.1859, Val Acc: 0.9256, Val AUC: 0.9800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1998  275]\n",
      " [  87 2289]]\n",
      "Sensitivity: 0.9634, Specificity: 0.8790\n",
      "Epoch [5/20] - Train Loss: 0.1839, Val Loss: 0.1846, Val Acc: 0.9221, Val AUC: 0.9813\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2129  144]\n",
      " [ 186 2190]]\n",
      "Sensitivity: 0.9217, Specificity: 0.9366\n",
      "Epoch [6/20] - Train Loss: 0.1705, Val Loss: 0.1628, Val Acc: 0.9290, Val AUC: 0.9840\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2139  134]\n",
      " [ 175 2201]]\n",
      "Sensitivity: 0.9263, Specificity: 0.9410\n",
      "Epoch [7/20] - Train Loss: 0.1637, Val Loss: 0.1586, Val Acc: 0.9335, Val AUC: 0.9849\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2215   58]\n",
      " [ 524 1852]]\n",
      "Sensitivity: 0.7795, Specificity: 0.9745\n",
      "Epoch [8/20] - Train Loss: 0.1933, Val Loss: 0.3001, Val Acc: 0.8748, Val AUC: 0.9472\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2105  168]\n",
      " [ 140 2236]]\n",
      "Sensitivity: 0.9411, Specificity: 0.9261\n",
      "Epoch [9/20] - Train Loss: 0.1992, Val Loss: 0.1560, Val Acc: 0.9337, Val AUC: 0.9858\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2139  134]\n",
      " [ 133 2243]]\n",
      "Sensitivity: 0.9440, Specificity: 0.9410\n",
      "Epoch [10/20] - Train Loss: 0.1525, Val Loss: 0.1431, Val Acc: 0.9426, Val AUC: 0.9876\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2169  104]\n",
      " [ 158 2218]]\n",
      "Sensitivity: 0.9335, Specificity: 0.9542\n",
      "Epoch [11/20] - Train Loss: 0.1439, Val Loss: 0.1363, Val Acc: 0.9436, Val AUC: 0.9890\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2191   82]\n",
      " [ 173 2203]]\n",
      "Sensitivity: 0.9272, Specificity: 0.9639\n",
      "Epoch [12/20] - Train Loss: 0.1333, Val Loss: 0.1335, Val Acc: 0.9451, Val AUC: 0.9898\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2162  111]\n",
      " [ 128 2248]]\n",
      "Sensitivity: 0.9461, Specificity: 0.9512\n",
      "Epoch [13/20] - Train Loss: 0.1262, Val Loss: 0.1297, Val Acc: 0.9486, Val AUC: 0.9894\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2133  140]\n",
      " [  98 2278]]\n",
      "Sensitivity: 0.9588, Specificity: 0.9384\n",
      "Epoch [14/20] - Train Loss: 0.1226, Val Loss: 0.1261, Val Acc: 0.9488, Val AUC: 0.9905\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2199   74]\n",
      " [ 153 2223]]\n",
      "Sensitivity: 0.9356, Specificity: 0.9674\n",
      "Epoch [15/20] - Train Loss: 0.1157, Val Loss: 0.1210, Val Acc: 0.9512, Val AUC: 0.9912\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2121  152]\n",
      " [  93 2283]]\n",
      "Sensitivity: 0.9609, Specificity: 0.9331\n",
      "Epoch [16/20] - Train Loss: 0.1099, Val Loss: 0.1284, Val Acc: 0.9473, Val AUC: 0.9912\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2176   97]\n",
      " [ 124 2252]]\n",
      "Sensitivity: 0.9478, Specificity: 0.9573\n",
      "Epoch [17/20] - Train Loss: 0.1232, Val Loss: 0.1216, Val Acc: 0.9525, Val AUC: 0.9906\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2181   92]\n",
      " [ 120 2256]]\n",
      "Sensitivity: 0.9495, Specificity: 0.9595\n",
      "Epoch [18/20] - Train Loss: 0.1054, Val Loss: 0.1157, Val Acc: 0.9544, Val AUC: 0.9915\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2204   69]\n",
      " [ 154 2222]]\n",
      "Sensitivity: 0.9352, Specificity: 0.9696\n",
      "Epoch [19/20] - Train Loss: 0.0984, Val Loss: 0.1169, Val Acc: 0.9520, Val AUC: 0.9920\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2191   82]\n",
      " [ 179 2197]]\n",
      "Sensitivity: 0.9247, Specificity: 0.9639\n",
      "Epoch [20/20] - Train Loss: 0.1021, Val Loss: 0.1248, Val Acc: 0.9439, Val AUC: 0.9917\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.7447\n",
      "Test Loss: 0.3776, Test Accuracy: 0.8409, Test AUC: 0.9569\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout= lstm_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_best_param['lr'],\n",
    "                      weight_decay=lstm_best_param['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'best_model_lstm_1.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 13:01:52,568] A new study created in memory with name: no-name-cc2f9fcd-612f-4e1c-94f8-b25d25b2d816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 13:05:12,340] Trial 0 finished with value: 0.2066231785367613 and parameters: {'hidden_dim': 125, 'num_layers': 3, 'dropout': 0.12023824200443928, 'lr': 0.003031862918182845, 'weight_decay': 0.0051651622324290734}. Best is trial 0 with value: 0.2066231785367613.\n",
      "[I 2025-05-06 13:07:43,189] Trial 1 finished with value: 0.1317762613602697 and parameters: {'hidden_dim': 85, 'num_layers': 2, 'dropout': 0.19156042992307892, 'lr': 0.006572817643613217, 'weight_decay': 0.00025478805676431063}. Best is trial 1 with value: 0.1317762613602697.\n",
      "[I 2025-05-06 13:10:35,908] Trial 2 finished with value: 0.14464848713107306 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.29513474079209534, 'lr': 0.00615706450445208, 'weight_decay': 0.0004853969475302689}. Best is trial 1 with value: 0.1317762613602697.\n",
      "[I 2025-05-06 13:12:59,525] Trial 3 finished with value: 0.18360891693258938 and parameters: {'hidden_dim': 118, 'num_layers': 1, 'dropout': 0.1730863015776694, 'lr': 0.008264043672596574, 'weight_decay': 0.009733634014509558}. Best is trial 1 with value: 0.1317762613602697.\n",
      "[I 2025-05-06 13:15:13,225] Trial 4 finished with value: 0.16604583036817916 and parameters: {'hidden_dim': 50, 'num_layers': 1, 'dropout': 0.2989748372025703, 'lr': 0.0012948550128187985, 'weight_decay': 0.002383649719528047}. Best is trial 1 with value: 0.1317762613602697.\n",
      "[I 2025-05-06 13:17:38,622] Trial 5 finished with value: 0.20389808949134122 and parameters: {'hidden_dim': 47, 'num_layers': 3, 'dropout': 0.30302613656514354, 'lr': 0.004438819117176622, 'weight_decay': 0.005537113315386818}. Best is trial 1 with value: 0.1317762613602697.\n",
      "[I 2025-05-06 13:19:57,135] Trial 6 finished with value: 0.14356076819439456 and parameters: {'hidden_dim': 90, 'num_layers': 1, 'dropout': 0.1349045234711777, 'lr': 0.005056225316719732, 'weight_decay': 0.0011858596689549225}. Best is trial 1 with value: 0.1317762613602697.\n",
      "[I 2025-05-06 13:22:29,302] Trial 7 finished with value: 0.6929223063873918 and parameters: {'hidden_dim': 57, 'num_layers': 3, 'dropout': 0.3198594978535788, 'lr': 0.005579751510408966, 'weight_decay': 0.007147303675695694}. Best is trial 1 with value: 0.1317762613602697.\n",
      "[I 2025-05-06 13:24:55,906] Trial 8 finished with value: 0.17702473118288875 and parameters: {'hidden_dim': 123, 'num_layers': 1, 'dropout': 0.33372532183965925, 'lr': 0.002178723204774289, 'weight_decay': 0.00479032917447085}. Best is trial 1 with value: 0.1317762613602697.\n",
      "[I 2025-05-06 13:27:23,441] Trial 9 finished with value: 0.6929260026918699 and parameters: {'hidden_dim': 39, 'num_layers': 3, 'dropout': 0.2598328115494058, 'lr': 0.002251116452845975, 'weight_decay': 0.00499717350364536}. Best is trial 1 with value: 0.1317762613602697.\n",
      "[I 2025-05-06 13:29:52,388] Trial 10 finished with value: 0.18280055455557287 and parameters: {'hidden_dim': 84, 'num_layers': 2, 'dropout': 0.4722151665479271, 'lr': 0.00962304974098827, 'weight_decay': 0.0027327507566366465}. Best is trial 1 with value: 0.1317762613602697.\n",
      "[I 2025-05-06 13:32:21,395] Trial 11 finished with value: 0.1334185274190282 and parameters: {'hidden_dim': 90, 'num_layers': 2, 'dropout': 0.18347137087705584, 'lr': 0.0070922331012354435, 'weight_decay': 0.000341616575055919}. Best is trial 1 with value: 0.1317762613602697.\n",
      "[I 2025-05-06 13:35:12,194] Trial 12 finished with value: 0.10278649241599726 and parameters: {'hidden_dim': 100, 'num_layers': 2, 'dropout': 0.20840654476885606, 'lr': 0.007257394353633846, 'weight_decay': 1.0693673700535341e-05}. Best is trial 12 with value: 0.10278649241599726.\n",
      "[I 2025-05-06 13:38:05,387] Trial 13 finished with value: 0.16456766875639353 and parameters: {'hidden_dim': 105, 'num_layers': 2, 'dropout': 0.22058486546204492, 'lr': 0.007811878349303426, 'weight_decay': 0.0023968438862364636}. Best is trial 12 with value: 0.10278649241599726.\n",
      "[I 2025-05-06 13:40:33,886] Trial 14 finished with value: 0.1055978722615193 and parameters: {'hidden_dim': 68, 'num_layers': 2, 'dropout': 0.38967643797501383, 'lr': 0.009088390903711798, 'weight_decay': 4.897543911977479e-05}. Best is trial 12 with value: 0.10278649241599726.\n",
      "[I 2025-05-06 13:43:01,411] Trial 15 finished with value: 0.18479488167452485 and parameters: {'hidden_dim': 67, 'num_layers': 2, 'dropout': 0.4032321925260039, 'lr': 0.009563300974754116, 'weight_decay': 0.003663858929066153}. Best is trial 12 with value: 0.10278649241599726.\n",
      "[I 2025-05-06 13:45:26,641] Trial 16 finished with value: 0.1637674337380553 and parameters: {'hidden_dim': 71, 'num_layers': 2, 'dropout': 0.4044899598281561, 'lr': 0.008374442424883935, 'weight_decay': 0.0015719617654352995}. Best is trial 12 with value: 0.10278649241599726.\n",
      "[I 2025-05-06 13:48:46,709] Trial 17 finished with value: 0.20742025573367942 and parameters: {'hidden_dim': 106, 'num_layers': 3, 'dropout': 0.37593211433489904, 'lr': 0.009011635631717765, 'weight_decay': 0.0071318005593237485}. Best is trial 12 with value: 0.10278649241599726.\n",
      "[I 2025-05-06 13:51:37,010] Trial 18 finished with value: 0.1551804301673419 and parameters: {'hidden_dim': 103, 'num_layers': 2, 'dropout': 0.49368281495105704, 'lr': 0.004070620770873573, 'weight_decay': 0.001507070860126844}. Best is trial 12 with value: 0.10278649241599726.\n",
      "[I 2025-05-06 13:53:45,640] Trial 19 finished with value: 0.20548080612127095 and parameters: {'hidden_dim': 72, 'num_layers': 1, 'dropout': 0.2459432382778063, 'lr': 0.007438359538633307, 'weight_decay': 0.009739293331793077}. Best is trial 12 with value: 0.10278649241599726.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 100, 'num_layers': 2, 'dropout': 0.20840654476885606, 'lr': 0.007257394353633846, 'weight_decay': 1.0693673700535341e-05}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_flatten/BiLSTM_Flatten_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "    max_seq_len = 100  # fixed for now; match your padding/truncation\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage (uncomment and run in your local environment):\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bilstm_best_param = {'hidden_dim': 59, 'num_layers': 1, 'dropout': 0.19595557432276017, 'lr': 0.009816503475479747, 'weight_decay': 2.600777275437832e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'R', 'Q', 'G', 'T', 'N', 'W', 'D', 'L', 'Y', 'F', 'I', 'E', 'X', 'K', 'P', 'M', 'S', 'V', 'H', 'C', 'A'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n",
      "Dataset sizes:\n",
      "Train: 21690\n",
      "Validation: 4649\n",
      "Test: 88\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Define DataLoaders\n",
    "\n",
    "\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[2103  170]\n",
      " [ 145 2231]]\n",
      "Sensitivity: 0.9390, Specificity: 0.9252\n",
      "Epoch [1/30] - Train Loss: 0.2308, Val Loss: 0.1702, Val Acc: 0.9322, Val AUC: 0.9826\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2089  184]\n",
      " [ 105 2271]]\n",
      "Sensitivity: 0.9558, Specificity: 0.9190\n",
      "Epoch [2/30] - Train Loss: 0.1718, Val Loss: 0.1525, Val Acc: 0.9378, Val AUC: 0.9859\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2157  116]\n",
      " [ 186 2190]]\n",
      "Sensitivity: 0.9217, Specificity: 0.9490\n",
      "Epoch [3/30] - Train Loss: 0.1440, Val Loss: 0.1526, Val Acc: 0.9350, Val AUC: 0.9872\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[2162  111]\n",
      " [ 141 2235]]\n",
      "Sensitivity: 0.9407, Specificity: 0.9512\n",
      "Epoch [4/30] - Train Loss: 0.1280, Val Loss: 0.1299, Val Acc: 0.9458, Val AUC: 0.9904\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2150  123]\n",
      " [ 109 2267]]\n",
      "Sensitivity: 0.9541, Specificity: 0.9459\n",
      "Epoch [5/30] - Train Loss: 0.1107, Val Loss: 0.1125, Val Acc: 0.9501, Val AUC: 0.9920\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2202   71]\n",
      " [ 121 2255]]\n",
      "Sensitivity: 0.9491, Specificity: 0.9688\n",
      "Epoch [6/30] - Train Loss: 0.0943, Val Loss: 0.1206, Val Acc: 0.9587, Val AUC: 0.9920\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2137  136]\n",
      " [  88 2288]]\n",
      "Sensitivity: 0.9630, Specificity: 0.9402\n",
      "Epoch [7/30] - Train Loss: 0.0878, Val Loss: 0.1123, Val Acc: 0.9518, Val AUC: 0.9923\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2160  113]\n",
      " [  82 2294]]\n",
      "Sensitivity: 0.9655, Specificity: 0.9503\n",
      "Epoch [8/30] - Train Loss: 0.0794, Val Loss: 0.1112, Val Acc: 0.9581, Val AUC: 0.9929\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2171  102]\n",
      " [  90 2286]]\n",
      "Sensitivity: 0.9621, Specificity: 0.9551\n",
      "Epoch [9/30] - Train Loss: 0.0653, Val Loss: 0.1095, Val Acc: 0.9587, Val AUC: 0.9934\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2200   73]\n",
      " [ 127 2249]]\n",
      "Sensitivity: 0.9465, Specificity: 0.9679\n",
      "Epoch [10/30] - Train Loss: 0.0589, Val Loss: 0.1154, Val Acc: 0.9570, Val AUC: 0.9936\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2164  109]\n",
      " [  73 2303]]\n",
      "Sensitivity: 0.9693, Specificity: 0.9520\n",
      "Epoch [11/30] - Train Loss: 0.0542, Val Loss: 0.1085, Val Acc: 0.9609, Val AUC: 0.9936\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2172  101]\n",
      " [  83 2293]]\n",
      "Sensitivity: 0.9651, Specificity: 0.9556\n",
      "Epoch [12/30] - Train Loss: 0.0462, Val Loss: 0.1184, Val Acc: 0.9604, Val AUC: 0.9923\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2170  103]\n",
      " [  84 2292]]\n",
      "Sensitivity: 0.9646, Specificity: 0.9547\n",
      "Epoch [13/30] - Train Loss: 0.0440, Val Loss: 0.1317, Val Acc: 0.9598, Val AUC: 0.9931\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2148  125]\n",
      " [  63 2313]]\n",
      "Sensitivity: 0.9735, Specificity: 0.9450\n",
      "Epoch [14/30] - Train Loss: 0.0409, Val Loss: 0.1263, Val Acc: 0.9596, Val AUC: 0.9930\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2184   89]\n",
      " [  99 2277]]\n",
      "Sensitivity: 0.9583, Specificity: 0.9608\n",
      "Epoch [15/30] - Train Loss: 0.0375, Val Loss: 0.1194, Val Acc: 0.9596, Val AUC: 0.9938\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2190   83]\n",
      " [ 121 2255]]\n",
      "Sensitivity: 0.9491, Specificity: 0.9635\n",
      "Epoch [16/30] - Train Loss: 0.0368, Val Loss: 0.1375, Val Acc: 0.9561, Val AUC: 0.9932\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2183   90]\n",
      " [  85 2291]]\n",
      "Sensitivity: 0.9642, Specificity: 0.9604\n",
      "Epoch [17/30] - Train Loss: 0.0307, Val Loss: 0.1357, Val Acc: 0.9624, Val AUC: 0.9942\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2158  115]\n",
      " [  50 2326]]\n",
      "Sensitivity: 0.9790, Specificity: 0.9494\n",
      "Epoch [18/30] - Train Loss: 0.0322, Val Loss: 0.1398, Val Acc: 0.9645, Val AUC: 0.9938\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2181   92]\n",
      " [  91 2285]]\n",
      "Sensitivity: 0.9617, Specificity: 0.9595\n",
      "Epoch [19/30] - Train Loss: 0.0279, Val Loss: 0.1384, Val Acc: 0.9606, Val AUC: 0.9939\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2185   88]\n",
      " [ 110 2266]]\n",
      "Sensitivity: 0.9537, Specificity: 0.9613\n",
      "Epoch [20/30] - Train Loss: 0.0300, Val Loss: 0.1426, Val Acc: 0.9574, Val AUC: 0.9935\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2157  116]\n",
      " [  73 2303]]\n",
      "Sensitivity: 0.9693, Specificity: 0.9490\n",
      "Epoch [21/30] - Train Loss: 0.0335, Val Loss: 0.1364, Val Acc: 0.9593, Val AUC: 0.9928\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2202   71]\n",
      " [ 104 2272]]\n",
      "Sensitivity: 0.9562, Specificity: 0.9688\n",
      "Epoch [22/30] - Train Loss: 0.0216, Val Loss: 0.1645, Val Acc: 0.9624, Val AUC: 0.9934\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2149  124]\n",
      " [  61 2315]]\n",
      "Sensitivity: 0.9743, Specificity: 0.9454\n",
      "Epoch [23/30] - Train Loss: 0.0229, Val Loss: 0.1605, Val Acc: 0.9602, Val AUC: 0.9930\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2195   78]\n",
      " [ 105 2271]]\n",
      "Sensitivity: 0.9558, Specificity: 0.9657\n",
      "Epoch [24/30] - Train Loss: 0.0239, Val Loss: 0.1541, Val Acc: 0.9606, Val AUC: 0.9940\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2177   96]\n",
      " [  77 2299]]\n",
      "Sensitivity: 0.9676, Specificity: 0.9578\n",
      "Epoch [25/30] - Train Loss: 0.0210, Val Loss: 0.1514, Val Acc: 0.9628, Val AUC: 0.9937\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2191   82]\n",
      " [ 121 2255]]\n",
      "Sensitivity: 0.9491, Specificity: 0.9639\n",
      "Epoch [26/30] - Train Loss: 0.0274, Val Loss: 0.1489, Val Acc: 0.9563, Val AUC: 0.9929\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2159  114]\n",
      " [  78 2298]]\n",
      "Sensitivity: 0.9672, Specificity: 0.9498\n",
      "Epoch [27/30] - Train Loss: 0.0239, Val Loss: 0.1482, Val Acc: 0.9587, Val AUC: 0.9931\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2186   87]\n",
      " [ 101 2275]]\n",
      "Sensitivity: 0.9575, Specificity: 0.9617\n",
      "Epoch [28/30] - Train Loss: 0.0232, Val Loss: 0.1631, Val Acc: 0.9596, Val AUC: 0.9929\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2187   86]\n",
      " [ 110 2266]]\n",
      "Sensitivity: 0.9537, Specificity: 0.9622\n",
      "Epoch [29/30] - Train Loss: 0.0230, Val Loss: 0.1512, Val Acc: 0.9578, Val AUC: 0.9931\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2187   86]\n",
      " [  90 2286]]\n",
      "Sensitivity: 0.9621, Specificity: 0.9622\n",
      "Epoch [30/30] - Train Loss: 0.0238, Val Loss: 0.1535, Val Acc: 0.9621, Val AUC: 0.9943\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.7660\n",
      "Test Loss: 0.6995, Test Accuracy: 0.8523, Test AUC: 0.9468\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "# to rerun\n",
    "model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout= bilstm_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=30, lr=bilstm_best_param['lr'],\n",
    "                      weight_decay=bilstm_best_param['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 14:02:08,600] A new study created in memory with name: no-name-ae44df2e-afd0-4f66-8b09-563d7d3db02e\n",
      "[I 2025-05-06 14:04:24,501] Trial 0 finished with value: 0.24316251635143202 and parameters: {'hidden_dim': 74, 'num_layers': 1, 'dropout': 0.4239820945560916, 'lr': 0.0004068800237541699, 'weight_decay': 0.0032329143666924897}. Best is trial 0 with value: 0.24316251635143202.\n",
      "[I 2025-05-06 14:06:42,314] Trial 1 finished with value: 0.1969438539180037 and parameters: {'hidden_dim': 60, 'num_layers': 1, 'dropout': 0.12195167772800822, 'lr': 0.0014966985308088598, 'weight_decay': 4.117078213730993e-06}. Best is trial 1 with value: 0.1969438539180037.\n",
      "[I 2025-05-06 14:08:56,151] Trial 2 finished with value: 0.23648012664220106 and parameters: {'hidden_dim': 53, 'num_layers': 1, 'dropout': 0.41900556002934974, 'lr': 0.0004153312893799356, 'weight_decay': 0.0022274670816993703}. Best is trial 1 with value: 0.1969438539180037.\n",
      "[I 2025-05-06 14:11:16,099] Trial 3 finished with value: 0.15970573543685757 and parameters: {'hidden_dim': 40, 'num_layers': 3, 'dropout': 0.41330893695596627, 'lr': 0.0044385809920526546, 'weight_decay': 8.469835354190241e-06}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:13:33,933] Trial 4 finished with value: 0.17455318294567604 and parameters: {'hidden_dim': 68, 'num_layers': 3, 'dropout': 0.3504217320737577, 'lr': 0.000848563045052828, 'weight_decay': 1.1382536141904105e-06}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:15:54,811] Trial 5 finished with value: 0.24212900253191386 and parameters: {'hidden_dim': 63, 'num_layers': 2, 'dropout': 0.19339465418135632, 'lr': 0.00010327714203220888, 'weight_decay': 0.001887604974759681}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:18:15,054] Trial 6 finished with value: 0.2018542458137421 and parameters: {'hidden_dim': 60, 'num_layers': 3, 'dropout': 0.49528022811102745, 'lr': 0.0002490108982159191, 'weight_decay': 3.764714710823131e-05}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:20:28,181] Trial 7 finished with value: 0.192989647898772 and parameters: {'hidden_dim': 78, 'num_layers': 2, 'dropout': 0.34190990399344146, 'lr': 0.00419109223383374, 'weight_decay': 0.0010585149469087662}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:22:43,010] Trial 8 finished with value: 0.21192847988376878 and parameters: {'hidden_dim': 92, 'num_layers': 2, 'dropout': 0.1867175285627748, 'lr': 0.0002720593282197428, 'weight_decay': 6.1535916182430025e-06}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:25:05,670] Trial 9 finished with value: 0.6928965710613826 and parameters: {'hidden_dim': 106, 'num_layers': 2, 'dropout': 0.4974597776220061, 'lr': 0.00022166501233001525, 'weight_decay': 0.005152957634766889}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:27:25,655] Trial 10 finished with value: 0.3076708886313112 and parameters: {'hidden_dim': 37, 'num_layers': 3, 'dropout': 0.2686255589791504, 'lr': 0.008916420603326113, 'weight_decay': 0.00016169375423889082}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:29:46,887] Trial 11 finished with value: 0.17330308064614255 and parameters: {'hidden_dim': 32, 'num_layers': 3, 'dropout': 0.366529049363903, 'lr': 0.001404402862893263, 'weight_decay': 1.0375866746069183e-06}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:32:09,762] Trial 12 finished with value: 0.17093593576183058 and parameters: {'hidden_dim': 36, 'num_layers': 3, 'dropout': 0.39639596077110145, 'lr': 0.002231088940529391, 'weight_decay': 1.0623163180257734e-06}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:34:47,477] Trial 13 finished with value: 0.24530010843930178 and parameters: {'hidden_dim': 126, 'num_layers': 3, 'dropout': 0.42348547445629714, 'lr': 0.0035371367243779055, 'weight_decay': 1.9331549681889146e-05}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:37:06,628] Trial 14 finished with value: 0.17080808114515592 and parameters: {'hidden_dim': 48, 'num_layers': 3, 'dropout': 0.2831634181774073, 'lr': 0.0034946404574527513, 'weight_decay': 5.481567855522816e-06}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:39:25,359] Trial 15 finished with value: 0.22683810514130004 and parameters: {'hidden_dim': 52, 'num_layers': 3, 'dropout': 0.27456198420889555, 'lr': 0.009175662576493536, 'weight_decay': 0.00017488581787131226}. Best is trial 3 with value: 0.15970573543685757.\n",
      "[I 2025-05-06 14:41:48,532] Trial 16 finished with value: 0.1494660068762629 and parameters: {'hidden_dim': 47, 'num_layers': 2, 'dropout': 0.23351604325527348, 'lr': 0.00469730039661558, 'weight_decay': 7.851860779884957e-06}. Best is trial 16 with value: 0.1494660068762629.\n",
      "[I 2025-05-06 14:44:11,475] Trial 17 finished with value: 0.15410225154602364 and parameters: {'hidden_dim': 44, 'num_layers': 2, 'dropout': 0.20633625946794218, 'lr': 0.006237558537548708, 'weight_decay': 2.9140980059331456e-05}. Best is trial 16 with value: 0.1494660068762629.\n",
      "[I 2025-05-06 14:46:29,505] Trial 18 finished with value: 0.1615747214384275 and parameters: {'hidden_dim': 47, 'num_layers': 2, 'dropout': 0.21421865647328456, 'lr': 0.005517090331547779, 'weight_decay': 4.623579766557834e-05}. Best is trial 16 with value: 0.1494660068762629.\n",
      "[I 2025-05-06 14:49:02,734] Trial 19 finished with value: 0.1992529494917556 and parameters: {'hidden_dim': 101, 'num_layers': 2, 'dropout': 0.11066254569830787, 'lr': 0.002265228932973567, 'weight_decay': 0.0006867514333016042}. Best is trial 16 with value: 0.1494660068762629.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 47, 'num_layers': 2, 'dropout': 0.23351604325527348, 'lr': 0.00469730039661558, 'weight_decay': 7.851860779884957e-06}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# LSTM with Attention classifier\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)  # shape: [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)  # shape: [batch, seq_len]\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)  # normalize\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # shape: [batch, hidden_dim]\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-attn/LSTM_Attn_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-lstm_attention.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_attn_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'R', 'Q', 'G', 'T', 'N', 'W', 'D', 'L', 'Y', 'F', 'I', 'E', 'X', 'K', 'P', 'M', 'S', 'V', 'H', 'C', 'A'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n",
      "Dataset sizes:\n",
      "Train: 21690\n",
      "Validation: 4649\n",
      "Test: 88\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Define DataLoaders\n",
    "\n",
    "\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[2154  119]\n",
      " [ 283 2093]]\n",
      "Sensitivity: 0.8809, Specificity: 0.9476\n",
      "Epoch [1/20] - Train Loss: 0.2929, Val Loss: 0.2200, Val Acc: 0.9135, Val AUC: 0.9745\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2050  223]\n",
      " [ 116 2260]]\n",
      "Sensitivity: 0.9512, Specificity: 0.9019\n",
      "Epoch [2/20] - Train Loss: 0.2089, Val Loss: 0.1923, Val Acc: 0.9271, Val AUC: 0.9765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[2050  223]\n",
      " [ 107 2269]]\n",
      "Sensitivity: 0.9550, Specificity: 0.9019\n",
      "Epoch [3/20] - Train Loss: 0.2068, Val Loss: 0.1761, Val Acc: 0.9290, Val AUC: 0.9806\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2143  130]\n",
      " [ 215 2161]]\n",
      "Sensitivity: 0.9095, Specificity: 0.9428\n",
      "Epoch [4/20] - Train Loss: 0.1804, Val Loss: 0.1813, Val Acc: 0.9258, Val AUC: 0.9817\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2149  124]\n",
      " [ 191 2185]]\n",
      "Sensitivity: 0.9196, Specificity: 0.9454\n",
      "Epoch [5/20] - Train Loss: 0.1732, Val Loss: 0.1659, Val Acc: 0.9322, Val AUC: 0.9839\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2101  172]\n",
      " [ 138 2238]]\n",
      "Sensitivity: 0.9419, Specificity: 0.9243\n",
      "Epoch [6/20] - Train Loss: 0.1900, Val Loss: 0.1634, Val Acc: 0.9333, Val AUC: 0.9833\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2166  107]\n",
      " [ 196 2180]]\n",
      "Sensitivity: 0.9175, Specificity: 0.9529\n",
      "Epoch [7/20] - Train Loss: 0.1627, Val Loss: 0.1595, Val Acc: 0.9348, Val AUC: 0.9850\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2051  222]\n",
      " [  86 2290]]\n",
      "Sensitivity: 0.9638, Specificity: 0.9023\n",
      "Epoch [8/20] - Train Loss: 0.1517, Val Loss: 0.1586, Val Acc: 0.9337, Val AUC: 0.9850\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2129  144]\n",
      " [ 118 2258]]\n",
      "Sensitivity: 0.9503, Specificity: 0.9366\n",
      "Epoch [9/20] - Train Loss: 0.1456, Val Loss: 0.1451, Val Acc: 0.9436, Val AUC: 0.9869\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2145  128]\n",
      " [ 142 2234]]\n",
      "Sensitivity: 0.9402, Specificity: 0.9437\n",
      "Epoch [10/20] - Train Loss: 0.1378, Val Loss: 0.1466, Val Acc: 0.9419, Val AUC: 0.9865\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2221   52]\n",
      " [ 245 2131]]\n",
      "Sensitivity: 0.8969, Specificity: 0.9771\n",
      "Epoch [11/20] - Train Loss: 0.1325, Val Loss: 0.1580, Val Acc: 0.9361, Val AUC: 0.9882\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2182   91]\n",
      " [ 192 2184]]\n",
      "Sensitivity: 0.9192, Specificity: 0.9600\n",
      "Epoch [12/20] - Train Loss: 0.1294, Val Loss: 0.1531, Val Acc: 0.9391, Val AUC: 0.9867\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2184   89]\n",
      " [ 160 2216]]\n",
      "Sensitivity: 0.9327, Specificity: 0.9608\n",
      "Epoch [13/20] - Train Loss: 0.1229, Val Loss: 0.1374, Val Acc: 0.9464, Val AUC: 0.9888\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2159  114]\n",
      " [ 103 2273]]\n",
      "Sensitivity: 0.9566, Specificity: 0.9498\n",
      "Epoch [14/20] - Train Loss: 0.1177, Val Loss: 0.1267, Val Acc: 0.9533, Val AUC: 0.9896\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2187   86]\n",
      " [ 151 2225]]\n",
      "Sensitivity: 0.9364, Specificity: 0.9622\n",
      "Epoch [15/20] - Train Loss: 0.1159, Val Loss: 0.1267, Val Acc: 0.9490, Val AUC: 0.9898\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2132  141]\n",
      " [  95 2281]]\n",
      "Sensitivity: 0.9600, Specificity: 0.9380\n",
      "Epoch [16/20] - Train Loss: 0.1053, Val Loss: 0.1298, Val Acc: 0.9492, Val AUC: 0.9899\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2192   81]\n",
      " [ 135 2241]]\n",
      "Sensitivity: 0.9432, Specificity: 0.9644\n",
      "Epoch [17/20] - Train Loss: 0.1021, Val Loss: 0.1284, Val Acc: 0.9535, Val AUC: 0.9905\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2204   69]\n",
      " [ 163 2213]]\n",
      "Sensitivity: 0.9314, Specificity: 0.9696\n",
      "Epoch [18/20] - Train Loss: 0.0931, Val Loss: 0.1368, Val Acc: 0.9501, Val AUC: 0.9905\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2172  101]\n",
      " [ 107 2269]]\n",
      "Sensitivity: 0.9550, Specificity: 0.9556\n",
      "Epoch [19/20] - Train Loss: 0.0876, Val Loss: 0.1198, Val Acc: 0.9553, Val AUC: 0.9913\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2143  130]\n",
      " [ 133 2243]]\n",
      "Sensitivity: 0.9440, Specificity: 0.9428\n",
      "Epoch [20/20] - Train Loss: 0.1062, Val Loss: 0.1486, Val Acc: 0.9434, Val AUC: 0.9863\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.7660\n",
      "Test Loss: 0.3954, Test Accuracy: 0.8523, Test AUC: 0.9502\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "# to rerun\n",
    "model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=lstm_attn_best_param['hidden_dim'], num_layers=lstm_attn_best_param['num_layers'], dropout= lstm_attn_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_attn_best_param['lr'],\n",
    "                      weight_decay=lstm_attn_best_param['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm + attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 16:50:01,697] A new study created in memory with name: no-name-151f0db4-65e4-42b1-af9b-e4f7278bf128\n",
      "[I 2025-05-06 16:52:22,218] Trial 0 finished with value: 0.22093634001196247 and parameters: {'hidden_dim': 36, 'num_layers': 1, 'dropout': 0.13965262942271106, 'lr': 0.0031846009005828367, 'weight_decay': 0.00888311925412897}. Best is trial 0 with value: 0.22093634001196247.\n",
      "[I 2025-05-06 16:54:49,075] Trial 1 finished with value: 0.6929273858462295 and parameters: {'hidden_dim': 47, 'num_layers': 3, 'dropout': 0.19598112471295856, 'lr': 0.003562665394381885, 'weight_decay': 0.0036575581571063383}. Best is trial 0 with value: 0.22093634001196247.\n",
      "[I 2025-05-06 16:57:11,289] Trial 2 finished with value: 0.22674822991024957 and parameters: {'hidden_dim': 122, 'num_layers': 1, 'dropout': 0.43329286010302426, 'lr': 0.008736288001734331, 'weight_decay': 0.009512680139607996}. Best is trial 0 with value: 0.22093634001196247.\n",
      "[I 2025-05-06 16:59:41,543] Trial 3 finished with value: 0.17798921297470185 and parameters: {'hidden_dim': 103, 'num_layers': 1, 'dropout': 0.35089867203013536, 'lr': 0.003281477179088501, 'weight_decay': 0.0005154064000202372}. Best is trial 3 with value: 0.17798921297470185.\n",
      "[I 2025-05-06 17:03:04,086] Trial 4 finished with value: 0.692927453615894 and parameters: {'hidden_dim': 126, 'num_layers': 3, 'dropout': 0.3174289644634769, 'lr': 0.0018537924561145471, 'weight_decay': 0.0019205832979280598}. Best is trial 3 with value: 0.17798921297470185.\n",
      "[I 2025-05-06 17:05:40,436] Trial 5 finished with value: 0.692934218334825 and parameters: {'hidden_dim': 66, 'num_layers': 3, 'dropout': 0.21382837201627433, 'lr': 0.009761575042628721, 'weight_decay': 0.0008352379774279655}. Best is trial 3 with value: 0.17798921297470185.\n",
      "[I 2025-05-06 17:08:07,701] Trial 6 finished with value: 0.1893438994884491 and parameters: {'hidden_dim': 82, 'num_layers': 2, 'dropout': 0.23131313120101327, 'lr': 0.003179540705445119, 'weight_decay': 0.0019809044206596715}. Best is trial 3 with value: 0.17798921297470185.\n",
      "[I 2025-05-06 17:10:37,364] Trial 7 finished with value: 0.6929277099975167 and parameters: {'hidden_dim': 60, 'num_layers': 3, 'dropout': 0.4574885893759624, 'lr': 0.009711085252720986, 'weight_decay': 0.0019102690421109298}. Best is trial 3 with value: 0.17798921297470185.\n",
      "[I 2025-05-06 17:13:09,286] Trial 8 finished with value: 0.2826930313077691 and parameters: {'hidden_dim': 70, 'num_layers': 1, 'dropout': 0.15422538578092948, 'lr': 0.007073062316598349, 'weight_decay': 0.008326031041421532}. Best is trial 3 with value: 0.17798921297470185.\n",
      "[I 2025-05-06 17:15:56,654] Trial 9 finished with value: 0.692970924997983 and parameters: {'hidden_dim': 102, 'num_layers': 2, 'dropout': 0.18894265204639826, 'lr': 0.008454582565601373, 'weight_decay': 0.0023539358262906315}. Best is trial 3 with value: 0.17798921297470185.\n",
      "[I 2025-05-06 17:18:12,948] Trial 10 finished with value: 0.21007245156454712 and parameters: {'hidden_dim': 96, 'num_layers': 1, 'dropout': 0.353674491811924, 'lr': 0.005528050343033291, 'weight_decay': 0.005836904458988833}. Best is trial 3 with value: 0.17798921297470185.\n",
      "[I 2025-05-06 17:20:47,581] Trial 11 finished with value: 0.1573457666864134 and parameters: {'hidden_dim': 92, 'num_layers': 2, 'dropout': 0.2613822183902435, 'lr': 0.00044884435422596833, 'weight_decay': 4.1589912862876515e-05}. Best is trial 11 with value: 0.1573457666864134.\n",
      "[I 2025-05-06 17:23:42,311] Trial 12 finished with value: 0.1598840189510829 and parameters: {'hidden_dim': 102, 'num_layers': 2, 'dropout': 0.3829630392683931, 'lr': 0.0004454600984118399, 'weight_decay': 0.0003013059155011034}. Best is trial 11 with value: 0.1573457666864134.\n",
      "[I 2025-05-06 17:26:10,296] Trial 13 finished with value: 0.6929119915178378 and parameters: {'hidden_dim': 87, 'num_layers': 2, 'dropout': 0.2729307915640465, 'lr': 0.00040820812071996003, 'weight_decay': 0.0051811974939156395}. Best is trial 11 with value: 0.1573457666864134.\n",
      "[I 2025-05-06 17:29:05,978] Trial 14 finished with value: 0.17807648555464942 and parameters: {'hidden_dim': 113, 'num_layers': 2, 'dropout': 0.40032001205331635, 'lr': 0.00019193278281990028, 'weight_decay': 0.00011511051481096145}. Best is trial 11 with value: 0.1573457666864134.\n",
      "[I 2025-05-06 17:31:58,241] Trial 15 finished with value: 0.6929274348363484 and parameters: {'hidden_dim': 112, 'num_layers': 2, 'dropout': 0.3023934486364557, 'lr': 0.0014189725584955852, 'weight_decay': 0.0037867348634673227}. Best is trial 11 with value: 0.1573457666864134.\n",
      "[I 2025-05-06 17:34:27,126] Trial 16 finished with value: 0.6929273842132255 and parameters: {'hidden_dim': 91, 'num_layers': 2, 'dropout': 0.39166618975008044, 'lr': 0.0015269336967248643, 'weight_decay': 0.006166230097608786}. Best is trial 11 with value: 0.1573457666864134.\n",
      "[I 2025-05-06 17:36:59,845] Trial 17 finished with value: 0.6929277679691576 and parameters: {'hidden_dim': 73, 'num_layers': 2, 'dropout': 0.2528442960653061, 'lr': 0.004932733493921931, 'weight_decay': 0.007350926621005113}. Best is trial 11 with value: 0.1573457666864134.\n",
      "[I 2025-05-06 17:40:13,628] Trial 18 finished with value: 0.6929274095247869 and parameters: {'hidden_dim': 112, 'num_layers': 3, 'dropout': 0.4876052293963314, 'lr': 0.002141207544974981, 'weight_decay': 0.003351769069261412}. Best is trial 11 with value: 0.1573457666864134.\n",
      "[I 2025-05-06 17:42:46,141] Trial 19 finished with value: 0.1748615773777439 and parameters: {'hidden_dim': 77, 'num_layers': 2, 'dropout': 0.3389021526535362, 'lr': 0.004664765068984211, 'weight_decay': 0.0008495525299243296}. Best is trial 11 with value: 0.1573457666864134.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 92, 'num_layers': 2, 'dropout': 0.2613822183902435, 'lr': 0.00044884435422596833, 'weight_decay': 4.1589912862876515e-05}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# BiLSTM with Attention Classifier\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'R', 'Q', 'G', 'T', 'N', 'W', 'D', 'L', 'Y', 'F', 'I', 'E', 'X', 'K', 'P', 'M', 'S', 'V', 'H', 'C', 'A'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n",
      "Dataset sizes:\n",
      "Train: 21690\n",
      "Validation: 4649\n",
      "Test: 88\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Define DataLoaders\n",
    "\n",
    "\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[2049  224]\n",
      " [ 182 2194]]\n",
      "Sensitivity: 0.9234, Specificity: 0.9015\n",
      "Epoch [1/30] - Train Loss: 0.3333, Val Loss: 0.2222, Val Acc: 0.9127, Val AUC: 0.9692\n",
      "\n",
      "Confusion Matrix:\n",
      "[[1996  277]\n",
      " [  80 2296]]\n",
      "Sensitivity: 0.9663, Specificity: 0.8781\n",
      "Epoch [2/30] - Train Loss: 0.2150, Val Loss: 0.2101, Val Acc: 0.9232, Val AUC: 0.9773\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2090  183]\n",
      " [ 139 2237]]\n",
      "Sensitivity: 0.9415, Specificity: 0.9195\n",
      "Epoch [3/30] - Train Loss: 0.1908, Val Loss: 0.1768, Val Acc: 0.9307, Val AUC: 0.9805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2101  172]\n",
      " [ 144 2232]]\n",
      "Sensitivity: 0.9394, Specificity: 0.9243\n",
      "Epoch [4/30] - Train Loss: 0.1830, Val Loss: 0.1732, Val Acc: 0.9320, Val AUC: 0.9818\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2085  188]\n",
      " [ 139 2237]]\n",
      "Sensitivity: 0.9415, Specificity: 0.9173\n",
      "Epoch [5/30] - Train Loss: 0.1756, Val Loss: 0.1716, Val Acc: 0.9297, Val AUC: 0.9823\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2120  153]\n",
      " [ 174 2202]]\n",
      "Sensitivity: 0.9268, Specificity: 0.9327\n",
      "Epoch [6/30] - Train Loss: 0.1695, Val Loss: 0.1894, Val Acc: 0.9297, Val AUC: 0.9788\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2093  180]\n",
      " [ 159 2217]]\n",
      "Sensitivity: 0.9331, Specificity: 0.9208\n",
      "Epoch [7/30] - Train Loss: 0.1697, Val Loss: 0.1699, Val Acc: 0.9271, Val AUC: 0.9824\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2048  225]\n",
      " [  95 2281]]\n",
      "Sensitivity: 0.9600, Specificity: 0.9010\n",
      "Epoch [8/30] - Train Loss: 0.1663, Val Loss: 0.1603, Val Acc: 0.9312, Val AUC: 0.9840\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2111  162]\n",
      " [ 161 2215]]\n",
      "Sensitivity: 0.9322, Specificity: 0.9287\n",
      "Epoch [9/30] - Train Loss: 0.1632, Val Loss: 0.1698, Val Acc: 0.9305, Val AUC: 0.9839\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2117  156]\n",
      " [ 148 2228]]\n",
      "Sensitivity: 0.9377, Specificity: 0.9314\n",
      "Epoch [10/30] - Train Loss: 0.1577, Val Loss: 0.1551, Val Acc: 0.9346, Val AUC: 0.9850\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2159  114]\n",
      " [ 180 2196]]\n",
      "Sensitivity: 0.9242, Specificity: 0.9498\n",
      "Epoch [11/30] - Train Loss: 0.1551, Val Loss: 0.1572, Val Acc: 0.9368, Val AUC: 0.9854\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2137  136]\n",
      " [ 190 2186]]\n",
      "Sensitivity: 0.9200, Specificity: 0.9402\n",
      "Epoch [12/30] - Train Loss: 0.1562, Val Loss: 0.1677, Val Acc: 0.9299, Val AUC: 0.9824\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2141  132]\n",
      " [ 153 2223]]\n",
      "Sensitivity: 0.9356, Specificity: 0.9419\n",
      "Epoch [13/30] - Train Loss: 0.1556, Val Loss: 0.1529, Val Acc: 0.9387, Val AUC: 0.9854\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2088  185]\n",
      " [ 105 2271]]\n",
      "Sensitivity: 0.9558, Specificity: 0.9186\n",
      "Epoch [14/30] - Train Loss: 0.1501, Val Loss: 0.1491, Val Acc: 0.9376, Val AUC: 0.9865\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2148  125]\n",
      " [ 154 2222]]\n",
      "Sensitivity: 0.9352, Specificity: 0.9450\n",
      "Epoch [15/30] - Train Loss: 0.1421, Val Loss: 0.1522, Val Acc: 0.9400, Val AUC: 0.9864\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2120  153]\n",
      " [ 121 2255]]\n",
      "Sensitivity: 0.9491, Specificity: 0.9327\n",
      "Epoch [16/30] - Train Loss: 0.1414, Val Loss: 0.1455, Val Acc: 0.9411, Val AUC: 0.9872\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2175   98]\n",
      " [ 188 2188]]\n",
      "Sensitivity: 0.9209, Specificity: 0.9569\n",
      "Epoch [17/30] - Train Loss: 0.1379, Val Loss: 0.1530, Val Acc: 0.9385, Val AUC: 0.9870\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2104  169]\n",
      " [ 115 2261]]\n",
      "Sensitivity: 0.9516, Specificity: 0.9256\n",
      "Epoch [18/30] - Train Loss: 0.1474, Val Loss: 0.1510, Val Acc: 0.9389, Val AUC: 0.9857\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2148  125]\n",
      " [ 158 2218]]\n",
      "Sensitivity: 0.9335, Specificity: 0.9450\n",
      "Epoch [19/30] - Train Loss: 0.1382, Val Loss: 0.1476, Val Acc: 0.9391, Val AUC: 0.9868\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2143  130]\n",
      " [ 130 2246]]\n",
      "Sensitivity: 0.9453, Specificity: 0.9428\n",
      "Epoch [20/30] - Train Loss: 0.1329, Val Loss: 0.1427, Val Acc: 0.9441, Val AUC: 0.9874\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2157  116]\n",
      " [ 148 2228]]\n",
      "Sensitivity: 0.9377, Specificity: 0.9490\n",
      "Epoch [21/30] - Train Loss: 0.1309, Val Loss: 0.1429, Val Acc: 0.9432, Val AUC: 0.9873\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2147  126]\n",
      " [ 167 2209]]\n",
      "Sensitivity: 0.9297, Specificity: 0.9446\n",
      "Epoch [22/30] - Train Loss: 0.1358, Val Loss: 0.1512, Val Acc: 0.9370, Val AUC: 0.9859\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2171  102]\n",
      " [ 180 2196]]\n",
      "Sensitivity: 0.9242, Specificity: 0.9551\n",
      "Epoch [23/30] - Train Loss: 0.1318, Val Loss: 0.1432, Val Acc: 0.9393, Val AUC: 0.9876\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2138  135]\n",
      " [ 149 2227]]\n",
      "Sensitivity: 0.9373, Specificity: 0.9406\n",
      "Epoch [24/30] - Train Loss: 0.1272, Val Loss: 0.1429, Val Acc: 0.9389, Val AUC: 0.9872\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2169  104]\n",
      " [ 148 2228]]\n",
      "Sensitivity: 0.9377, Specificity: 0.9542\n",
      "Epoch [25/30] - Train Loss: 0.1263, Val Loss: 0.1341, Val Acc: 0.9458, Val AUC: 0.9891\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2123  150]\n",
      " [ 118 2258]]\n",
      "Sensitivity: 0.9503, Specificity: 0.9340\n",
      "Epoch [26/30] - Train Loss: 0.1239, Val Loss: 0.1381, Val Acc: 0.9424, Val AUC: 0.9881\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2192   81]\n",
      " [ 176 2200]]\n",
      "Sensitivity: 0.9259, Specificity: 0.9644\n",
      "Epoch [27/30] - Train Loss: 0.1222, Val Loss: 0.1343, Val Acc: 0.9447, Val AUC: 0.9890\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2199   74]\n",
      " [ 178 2198]]\n",
      "Sensitivity: 0.9251, Specificity: 0.9674\n",
      "Epoch [28/30] - Train Loss: 0.1177, Val Loss: 0.1327, Val Acc: 0.9458, Val AUC: 0.9897\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2142  131]\n",
      " [ 118 2258]]\n",
      "Sensitivity: 0.9503, Specificity: 0.9424\n",
      "Epoch [29/30] - Train Loss: 0.1199, Val Loss: 0.1332, Val Acc: 0.9464, Val AUC: 0.9888\n",
      "\n",
      "Confusion Matrix:\n",
      "[[2137  136]\n",
      " [  92 2284]]\n",
      "Sensitivity: 0.9613, Specificity: 0.9402\n",
      "Epoch [30/30] - Train Loss: 0.1185, Val Loss: 0.1258, Val Acc: 0.9510, Val AUC: 0.9902\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.7447\n",
      "Test Loss: 0.3756, Test Accuracy: 0.8523, Test AUC: 0.9626\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model-bilstm_attention.pt')\n",
    "\n",
    "\n",
    "model = BiLSTMWithAttentionClassifier(\n",
    "    input_dim=20,\n",
    "    hidden_dim=bilstm_attn_best_param['hidden_dim'],\n",
    "    num_layers=bilstm_attn_best_param['num_layers'],\n",
    "    dropout=bilstm_attn_best_param['dropout']\n",
    ")\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=30, lr=bilstm_attn_best_param['lr'],\n",
    "                      weight_decay=bilstm_attn_best_param['weight_decay'], verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning to TB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'R', 'Q', 'G', 'T', 'N', 'W', 'D', 'L', 'Y', 'F', 'I', 'E', 'X', 'K', 'P', 'M', 'S', 'V', 'H', 'C', 'A'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n",
      "Dataset sizes:\n",
      "Train: 264\n",
      "Validation: 88\n",
      "Test: 88\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:47:46,680] A new study created in memory with name: no-name-e10f6457-32c7-4c09-9751-f6db75aa0510\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:47:50,529] Trial 0 finished with value: 0.6920496424039205 and parameters: {'lr': 3.238878953549854e-05, 'weight_decay': 0.000778745516037528, 'dropout': 0.2834182664605883}. Best is trial 0 with value: 0.6920496424039205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:47:54,192] Trial 1 finished with value: 0.6963010629018148 and parameters: {'lr': 2.7260673155909754e-05, 'weight_decay': 7.942348347862723e-06, 'dropout': 0.48836692029333617}. Best is trial 0 with value: 0.6920496424039205.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:47:58,000] Trial 2 finished with value: 0.6912026206652323 and parameters: {'lr': 0.0004698061961600201, 'weight_decay': 0.007494373563676619, 'dropout': 0.4632731694359643}. Best is trial 2 with value: 0.6912026206652323.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:01,612] Trial 3 finished with value: 0.685158888498942 and parameters: {'lr': 0.0020291825755736244, 'weight_decay': 1.4013573867966869e-05, 'dropout': 0.21536143021451273}. Best is trial 3 with value: 0.685158888498942.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:05,156] Trial 4 finished with value: 0.6948625842730204 and parameters: {'lr': 1.2490588945910336e-05, 'weight_decay': 8.8158471225254e-06, 'dropout': 0.3571637669412737}. Best is trial 3 with value: 0.685158888498942.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:08,673] Trial 5 finished with value: 0.6956791083017985 and parameters: {'lr': 0.00011971830185336756, 'weight_decay': 0.000293607814399378, 'dropout': 0.27652152368729566}. Best is trial 3 with value: 0.685158888498942.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:12,207] Trial 6 finished with value: 0.6760003964106241 and parameters: {'lr': 0.003507996291723816, 'weight_decay': 2.1702536014651155e-06, 'dropout': 0.22400054255233331}. Best is trial 6 with value: 0.6760003964106241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:15,747] Trial 7 finished with value: 0.677286684513092 and parameters: {'lr': 0.004855082039279353, 'weight_decay': 1.4831531144990382e-06, 'dropout': 0.11730719981161158}. Best is trial 6 with value: 0.6760003964106241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:19,304] Trial 8 finished with value: 0.6940812667210897 and parameters: {'lr': 5.618853454257407e-05, 'weight_decay': 1.147164266937134e-06, 'dropout': 0.32161743668964027}. Best is trial 6 with value: 0.6760003964106241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:22,742] Trial 9 finished with value: 0.6959663232167562 and parameters: {'lr': 3.1242221029798465e-05, 'weight_decay': 1.3735031908676117e-05, 'dropout': 0.32001038664620857}. Best is trial 6 with value: 0.6760003964106241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:26,269] Trial 10 finished with value: 0.6868101557095846 and parameters: {'lr': 0.0009349022492551378, 'weight_decay': 6.853443766799151e-05, 'dropout': 0.133956073356112}. Best is trial 6 with value: 0.6760003964106241.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:29,726] Trial 11 finished with value: 0.6601137518882751 and parameters: {'lr': 0.008941492930976973, 'weight_decay': 1.478967473919042e-06, 'dropout': 0.12850732522526792}. Best is trial 11 with value: 0.6601137518882751.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:33,266] Trial 12 finished with value: 0.670564333597819 and parameters: {'lr': 0.00707001355664626, 'weight_decay': 3.0454870160037475e-06, 'dropout': 0.1845217774190537}. Best is trial 11 with value: 0.6601137518882751.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:36,777] Trial 13 finished with value: 0.6623947421709696 and parameters: {'lr': 0.009453019611283925, 'weight_decay': 4.910599175716331e-05, 'dropout': 0.18024643005466895}. Best is trial 11 with value: 0.6601137518882751.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:40,301] Trial 14 finished with value: 0.6851493716239929 and parameters: {'lr': 0.0016897006807922055, 'weight_decay': 5.996628520818426e-05, 'dropout': 0.16226909468449519}. Best is trial 11 with value: 0.6601137518882751.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:43,890] Trial 15 finished with value: 0.6750607490539551 and parameters: {'lr': 0.007003127896976025, 'weight_decay': 0.0017599229071440414, 'dropout': 0.10091500773144138}. Best is trial 11 with value: 0.6601137518882751.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:47,426] Trial 16 finished with value: 0.663530429204305 and parameters: {'lr': 0.00980081010478889, 'weight_decay': 5.064282733760245e-05, 'dropout': 0.21553391463002858}. Best is trial 11 with value: 0.6601137518882751.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:50,986] Trial 17 finished with value: 0.6917730967203776 and parameters: {'lr': 0.00031026781348497884, 'weight_decay': 0.00023617722044805473, 'dropout': 0.1632082886874302}. Best is trial 11 with value: 0.6601137518882751.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:54,541] Trial 18 finished with value: 0.6825845042864481 and parameters: {'lr': 0.0026472555810989405, 'weight_decay': 2.8309646974260704e-05, 'dropout': 0.4170203233424184}. Best is trial 11 with value: 0.6601137518882751.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:48:58,294] Trial 19 finished with value: 0.6896892786026001 and parameters: {'lr': 0.0005497685516314975, 'weight_decay': 3.845907380652832e-06, 'dropout': 0.24042262942539278}. Best is trial 11 with value: 0.6601137518882751.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.008941492930976973, 'weight_decay': 1.478967473919042e-06, 'dropout': 0.12850732522526792}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation function with detailed output\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    # print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    # print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    # print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function for frozen encoder\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-lstm-trans-frozen/FrozenEncoder_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        # if val_auc > best_val_auc:\n",
    "        #     best_val_auc = val_auc\n",
    "        #     torch.save(model.state_dict(), 'best_model_frozen.pt')\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path, max_layers=None):\n",
    "    \"\"\"\n",
    "    Load up to `max_layers` compatible layers from a checkpoint into the model.\n",
    "    If max_layers is None, load all compatible layers.\n",
    "    \"\"\"\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter compatible layers\n",
    "    compatible_items = [\n",
    "        (k, v) for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    ]\n",
    "\n",
    "    # Limit number of layers to load\n",
    "    if max_layers is not None:\n",
    "        compatible_items = compatible_items[:max_layers]\n",
    "\n",
    "    # Convert list of tuples back to dict\n",
    "    compatible_dict = dict(compatible_items)\n",
    "\n",
    "    # Update model state dict\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "import optuna\n",
    "lstm_best_param = {'hidden_dim': 95, 'num_layers': 1, 'dropout': 0.20287746211724011, 'lr': 0.004396992152527415, 'weight_decay': 3.999214064585909e-05}\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=dropout)\n",
    "    # model.load_state_dict(torch.load('best_model_lstm_1.pt'))\n",
    "    model = load_partial_weights(model, 'best_model_lstm_1.pt', 6)\n",
    "\n",
    "    \n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_frozen_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.008941492930976973,\n",
       " 'weight_decay': 1.478967473919042e-06,\n",
       " 'dropout': 0.12850732522526792}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_frozen_best_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 1 matching layers from checkpoint.\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.6606\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [1/20] - Train Loss: 0.6886, Val Loss: 0.6891, Val Acc: 0.5341, Val AUC: 0.6606\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7042\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [2/20] - Train Loss: 0.6851, Val Loss: 0.6867, Val Acc: 0.5341, Val AUC: 0.7042\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7120\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [3/20] - Train Loss: 0.6875, Val Loss: 0.6849, Val Acc: 0.5341, Val AUC: 0.7120\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7213\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [4/20] - Train Loss: 0.6815, Val Loss: 0.6832, Val Acc: 0.5341, Val AUC: 0.7213\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7213\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [5/20] - Train Loss: 0.6803, Val Loss: 0.6815, Val Acc: 0.5341, Val AUC: 0.7213\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7260\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [6/20] - Train Loss: 0.6825, Val Loss: 0.6796, Val Acc: 0.5341, Val AUC: 0.7260\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5909, AUC: 0.7307\n",
      "Sensitivity: 0.1463, Specificity: 0.9787\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [35  6]]\n",
      "Epoch [7/20] - Train Loss: 0.6734, Val Loss: 0.6778, Val Acc: 0.5909, Val AUC: 0.7307\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5909, AUC: 0.7312\n",
      "Sensitivity: 0.1707, Specificity: 0.9574\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [34  7]]\n",
      "Epoch [8/20] - Train Loss: 0.6737, Val Loss: 0.6759, Val Acc: 0.5909, Val AUC: 0.7312\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5909, AUC: 0.7302\n",
      "Sensitivity: 0.1707, Specificity: 0.9574\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [34  7]]\n",
      "Epoch [9/20] - Train Loss: 0.6753, Val Loss: 0.6743, Val Acc: 0.5909, Val AUC: 0.7302\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6023, AUC: 0.7333\n",
      "Sensitivity: 0.2439, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [31 10]]\n",
      "Epoch [10/20] - Train Loss: 0.6709, Val Loss: 0.6726, Val Acc: 0.6023, Val AUC: 0.7333\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6023, AUC: 0.7348\n",
      "Sensitivity: 0.2439, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [31 10]]\n",
      "Epoch [11/20] - Train Loss: 0.6671, Val Loss: 0.6709, Val Acc: 0.6023, Val AUC: 0.7348\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6023, AUC: 0.7353\n",
      "Sensitivity: 0.2439, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [31 10]]\n",
      "Epoch [12/20] - Train Loss: 0.6645, Val Loss: 0.6694, Val Acc: 0.6023, Val AUC: 0.7353\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6023, AUC: 0.7353\n",
      "Sensitivity: 0.2195, Specificity: 0.9362\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [32  9]]\n",
      "Epoch [13/20] - Train Loss: 0.6615, Val Loss: 0.6681, Val Acc: 0.6023, Val AUC: 0.7353\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5909, AUC: 0.7338\n",
      "Sensitivity: 0.2195, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [32  9]]\n",
      "Epoch [14/20] - Train Loss: 0.6644, Val Loss: 0.6665, Val Acc: 0.5909, Val AUC: 0.7338\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6364, AUC: 0.7369\n",
      "Sensitivity: 0.3171, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [28 13]]\n",
      "Epoch [15/20] - Train Loss: 0.6615, Val Loss: 0.6650, Val Acc: 0.6364, Val AUC: 0.7369\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6023, AUC: 0.7364\n",
      "Sensitivity: 0.2439, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [31 10]]\n",
      "Epoch [16/20] - Train Loss: 0.6562, Val Loss: 0.6639, Val Acc: 0.6023, Val AUC: 0.7364\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5909, AUC: 0.7353\n",
      "Sensitivity: 0.2195, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [32  9]]\n",
      "Epoch [17/20] - Train Loss: 0.6540, Val Loss: 0.6628, Val Acc: 0.5909, Val AUC: 0.7353\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6023, AUC: 0.7353\n",
      "Sensitivity: 0.2439, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [31 10]]\n",
      "Epoch [18/20] - Train Loss: 0.6574, Val Loss: 0.6616, Val Acc: 0.6023, Val AUC: 0.7353\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6023, AUC: 0.7348\n",
      "Sensitivity: 0.2439, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [31 10]]\n",
      "Epoch [19/20] - Train Loss: 0.6579, Val Loss: 0.6606, Val Acc: 0.6023, Val AUC: 0.7348\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6364, AUC: 0.7385\n",
      "Sensitivity: 0.3171, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [28 13]]\n",
      "Epoch [20/20] - Train Loss: 0.6523, Val Loss: 0.6594, Val Acc: 0.6364, Val AUC: 0.7385\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6023, AUC: 0.7218\n",
      "Sensitivity: 0.2683, Specificity: 0.8936\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [30 11]]\n",
      "Test Loss: 0.6645, Test Accuracy: 0.6023, Test AUC: 0.7218\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-trans-frozen/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Best hyperparameters: {'lr': 0.009940295438316211, 'weight_decay': 1.4383289881186473e-05, 'dropout': 0.22563027249521914}\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=lstm_frozen_best_param['dropout'])\n",
    "# model.load_state_dict(torch.load('best_model_lstm_1.pt')) \n",
    "model = load_partial_weights(model, 'best_model_lstm_1.pt', 6)\n",
    "\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_frozen_best_param['lr'],\n",
    "                      weight_decay=lstm_frozen_best_param['weight_decay'], verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-06 18:49:03,504] A new study created in memory with name: no-name-c626944d-6aaa-45ba-8f15-f047f5a2d437\n",
      "[W 2025-05-06 18:49:03,515] Trial 0 failed with parameters: {'lr': 1.665968764871999e-05, 'weight_decay': 0.00035017011068879896, 'dropout': 0.4558771450942153} because of the following error: RuntimeError('Error(s) in loading state_dict for LSTMClassifier:\\n\\tUnexpected key(s) in state_dict: \"lstm.weight_ih_l1\", \"lstm.weight_hh_l1\", \"lstm.bias_ih_l1\", \"lstm.bias_hh_l1\". \\n\\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([192, 20]) from checkpoint, the shape in current model is torch.Size([380, 20]).\\n\\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([192, 48]) from checkpoint, the shape in current model is torch.Size([380, 95]).\\n\\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([380]).\\n\\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([380]).\\n\\tsize mismatch for fc.weight: copying a param with shape torch.Size([1, 48]) from checkpoint, the shape in current model is torch.Size([1, 95]).').\n",
      "Traceback (most recent call last):\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n",
      "    value_or_values = func(trial)\n",
      "  File \"/tmp/ipykernel_115197/3675335354.py\", line 179, in objective\n",
      "    model.load_state_dict(torch.load('best_model_lstm_1.pt'))\n",
      "  File \"/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/nn/modules/module.py\", line 2581, in load_state_dict\n",
      "    raise RuntimeError(\n",
      "RuntimeError: Error(s) in loading state_dict for LSTMClassifier:\n",
      "\tUnexpected key(s) in state_dict: \"lstm.weight_ih_l1\", \"lstm.weight_hh_l1\", \"lstm.bias_ih_l1\", \"lstm.bias_hh_l1\". \n",
      "\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([192, 20]) from checkpoint, the shape in current model is torch.Size([380, 20]).\n",
      "\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([192, 48]) from checkpoint, the shape in current model is torch.Size([380, 95]).\n",
      "\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([380]).\n",
      "\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([380]).\n",
      "\tsize mismatch for fc.weight: copying a param with shape torch.Size([1, 48]) from checkpoint, the shape in current model is torch.Size([1, 95]).\n",
      "[W 2025-05-06 18:49:03,518] Trial 0 failed with value None.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for LSTMClassifier:\n\tUnexpected key(s) in state_dict: \"lstm.weight_ih_l1\", \"lstm.weight_hh_l1\", \"lstm.bias_ih_l1\", \"lstm.bias_hh_l1\". \n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([192, 20]) from checkpoint, the shape in current model is torch.Size([380, 20]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([192, 48]) from checkpoint, the shape in current model is torch.Size([380, 95]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([380]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([380]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1, 48]) from checkpoint, the shape in current model is torch.Size([1, 95]).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 188\u001b[0m\n\u001b[1;32m    182\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_finetune_model(\n\u001b[1;32m    183\u001b[0m         model, train_loader, val_loader,\n\u001b[1;32m    184\u001b[0m         num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay\n\u001b[1;32m    185\u001b[0m     )\n\u001b[1;32m    187\u001b[0m study \u001b[38;5;241m=\u001b[39m optuna\u001b[38;5;241m.\u001b[39mcreate_study(direction\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mminimize\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 188\u001b[0m \u001b[43mstudy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobjective\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    189\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest hyperparameters:\u001b[39m\u001b[38;5;124m\"\u001b[39m, study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams)\n\u001b[1;32m    190\u001b[0m lstm_fullbackprop_best_param \u001b[38;5;241m=\u001b[39m study\u001b[38;5;241m.\u001b[39mbest_trial\u001b[38;5;241m.\u001b[39mparams\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/study.py:475\u001b[0m, in \u001b[0;36mStudy.optimize\u001b[0;34m(self, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21moptimize\u001b[39m(\n\u001b[1;32m    374\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    375\u001b[0m     func: ObjectiveFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    382\u001b[0m     show_progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    383\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    384\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Optimize an objective function.\u001b[39;00m\n\u001b[1;32m    385\u001b[0m \n\u001b[1;32m    386\u001b[0m \u001b[38;5;124;03m    Optimization is done by choosing a suitable set of hyperparameter values from a given\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;124;03m            If nested invocation of this method occurs.\u001b[39;00m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 475\u001b[0m     \u001b[43m_optimize\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstudy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    478\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcatch\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mtuple\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIterable\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    484\u001b[0m \u001b[43m        \u001b[49m\u001b[43mshow_progress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshow_progress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    485\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:63\u001b[0m, in \u001b[0;36m_optimize\u001b[0;34m(study, func, n_trials, timeout, n_jobs, catch, callbacks, gc_after_trial, show_progress_bar)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m---> 63\u001b[0m         \u001b[43m_optimize_sequential\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[43m            \u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     65\u001b[0m \u001b[43m            \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[43m            \u001b[49m\u001b[43mn_trials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     67\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     68\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     69\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     70\u001b[0m \u001b[43m            \u001b[49m\u001b[43mgc_after_trial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[43m            \u001b[49m\u001b[43mreseed_sampler_rng\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     72\u001b[0m \u001b[43m            \u001b[49m\u001b[43mtime_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     73\u001b[0m \u001b[43m            \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     74\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     75\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     76\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m n_jobs \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:160\u001b[0m, in \u001b[0;36m_optimize_sequential\u001b[0;34m(study, func, n_trials, timeout, catch, callbacks, gc_after_trial, reseed_sampler_rng, time_start, progress_bar)\u001b[0m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 160\u001b[0m     frozen_trial \u001b[38;5;241m=\u001b[39m \u001b[43m_run_trial\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstudy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    162\u001b[0m     \u001b[38;5;66;03m# The following line mitigates memory problems that can be occurred in some\u001b[39;00m\n\u001b[1;32m    163\u001b[0m     \u001b[38;5;66;03m# environments (e.g., services that use computing containers such as GitHub Actions).\u001b[39;00m\n\u001b[1;32m    164\u001b[0m     \u001b[38;5;66;03m# Please refer to the following PR for further details:\u001b[39;00m\n\u001b[1;32m    165\u001b[0m     \u001b[38;5;66;03m# https://github.com/optuna/optuna/pull/325.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m gc_after_trial:\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:248\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    241\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mShould not reach.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    243\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    244\u001b[0m     frozen_trial\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m==\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mFAIL\n\u001b[1;32m    245\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m func_err \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    246\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(func_err, catch)\n\u001b[1;32m    247\u001b[0m ):\n\u001b[0;32m--> 248\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m func_err\n\u001b[1;32m    249\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m frozen_trial\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/optuna/study/_optimize.py:197\u001b[0m, in \u001b[0;36m_run_trial\u001b[0;34m(study, func, catch)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m get_heartbeat_thread(trial\u001b[38;5;241m.\u001b[39m_trial_id, study\u001b[38;5;241m.\u001b[39m_storage):\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 197\u001b[0m         value_or_values \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m exceptions\u001b[38;5;241m.\u001b[39mTrialPruned \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    199\u001b[0m         \u001b[38;5;66;03m# TODO(mamu): Handle multi-objective cases.\u001b[39;00m\n\u001b[1;32m    200\u001b[0m         state \u001b[38;5;241m=\u001b[39m TrialState\u001b[38;5;241m.\u001b[39mPRUNED\n",
      "Cell \u001b[0;32mIn[35], line 179\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(trial)\u001b[0m\n\u001b[1;32m    176\u001b[0m dropout \u001b[38;5;241m=\u001b[39m trial\u001b[38;5;241m.\u001b[39msuggest_float(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m0.1\u001b[39m, \u001b[38;5;241m0.5\u001b[39m)\n\u001b[1;32m    178\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMClassifier(input_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m20\u001b[39m, hidden_dim\u001b[38;5;241m=\u001b[39mlstm_best_param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhidden_dim\u001b[39m\u001b[38;5;124m'\u001b[39m], num_layers\u001b[38;5;241m=\u001b[39mlstm_best_param[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_layers\u001b[39m\u001b[38;5;124m'\u001b[39m], dropout\u001b[38;5;241m=\u001b[39mdropout)\n\u001b[0;32m--> 179\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbest_model_lstm_1.pt\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;66;03m# freeze_encoder(model)\u001b[39;00m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m train_finetune_model(\n\u001b[1;32m    183\u001b[0m     model, train_loader, val_loader,\n\u001b[1;32m    184\u001b[0m     num_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, lr\u001b[38;5;241m=\u001b[39mlr, weight_decay\u001b[38;5;241m=\u001b[39mweight_decay\n\u001b[1;32m    185\u001b[0m )\n",
      "File \u001b[0;32m/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/torch/nn/modules/module.py:2581\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2573\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2574\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2575\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2576\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2577\u001b[0m             ),\n\u001b[1;32m   2578\u001b[0m         )\n\u001b[1;32m   2580\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2581\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2582\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2583\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2584\u001b[0m         )\n\u001b[1;32m   2585\u001b[0m     )\n\u001b[1;32m   2586\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for LSTMClassifier:\n\tUnexpected key(s) in state_dict: \"lstm.weight_ih_l1\", \"lstm.weight_hh_l1\", \"lstm.bias_ih_l1\", \"lstm.bias_hh_l1\". \n\tsize mismatch for lstm.weight_ih_l0: copying a param with shape torch.Size([192, 20]) from checkpoint, the shape in current model is torch.Size([380, 20]).\n\tsize mismatch for lstm.weight_hh_l0: copying a param with shape torch.Size([192, 48]) from checkpoint, the shape in current model is torch.Size([380, 95]).\n\tsize mismatch for lstm.bias_ih_l0: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([380]).\n\tsize mismatch for lstm.bias_hh_l0: copying a param with shape torch.Size([192]) from checkpoint, the shape in current model is torch.Size([380]).\n\tsize mismatch for fc.weight: copying a param with shape torch.Size([1, 48]) from checkpoint, the shape in current model is torch.Size([1, 95])."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation function with detailed output\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    # print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    # print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    # print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function for frozen encoder\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    log_dir = f\"runs-lstm-transfer-fullbackprop/fullbackprop_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        # if val_auc > best_val_auc:\n",
    "        #     best_val_auc = val_auc\n",
    "        #     torch.save(model.state_dict(), 'best_model_frozen.pt')\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_full_backprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# # Load the best pretrained model and fine-tune\n",
    "# def finetune_with_frozen_encoder(pretrained_model_path, train_loader, val_loader, hidden_dim, num_layers, dropout):\n",
    "#     model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "#     model.load_state_dict(torch.load(pretrained_model_path))\n",
    "#     # freeze_encoder(model)\n",
    "\n",
    "#     best_auc = train_finetune_model(\n",
    "#         model=model,\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         num_epochs=10,\n",
    "#         lr=1e-3,\n",
    "#         weight_decay=1e-4\n",
    "#     )\n",
    "\n",
    "#     model.load_state_dict(torch.load('best_model_frozen.pt'))\n",
    "#     evaluate_model(model, val_loader, nn.BCELoss())\n",
    "\n",
    "#     return model, best_auc\n",
    "\n",
    "# model, best_auc = finetune_with_frozen_encoder(\n",
    "#     pretrained_model_path='best_model-lstm.pt',\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     hidden_dim=47,  # or from Optuna\n",
    "#     num_layers=2,\n",
    "#     dropout=0.3\n",
    "# )\n",
    "\n",
    "\n",
    "# lstm_best_param = {'hidden_dim': 74,\n",
    "#  'num_layers': 3,\n",
    "#  'dropout': 0.3037059572844035,\n",
    "#  'lr': 0.00774103421243492,\n",
    "#  'weight_decay': 2.4221276513292614e-05}\n",
    "\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=dropout)\n",
    "    model.load_state_dict(torch.load('best_model_lstm_1.pt'))\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_fullbackprop_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7026\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [1/19] - Train Loss: 0.6957, Val Loss: 334.0223, Val Acc: 0.5341, Val AUC: 0.7026\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7374\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [2/19] - Train Loss: 0.6866, Val Loss: 334.0200, Val Acc: 0.5341, Val AUC: 0.7374\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5455, AUC: 0.7499\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Epoch [3/19] - Train Loss: 0.6850, Val Loss: 334.0169, Val Acc: 0.5455, Val AUC: 0.7499\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5568, AUC: 0.7602\n",
      "Sensitivity: 0.0732, Specificity: 0.9787\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [38  3]]\n",
      "Epoch [4/19] - Train Loss: 0.6838, Val Loss: 334.0121, Val Acc: 0.5568, Val AUC: 0.7602\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6818, AUC: 0.7914\n",
      "Sensitivity: 0.4634, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [22 19]]\n",
      "Epoch [5/19] - Train Loss: 0.6736, Val Loss: 334.0038, Val Acc: 0.6818, Val AUC: 0.7914\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.6477, AUC: 0.7800\n",
      "Sensitivity: 0.3659, Specificity: 0.8936\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [26 15]]\n",
      "Epoch [6/19] - Train Loss: 0.6540, Val Loss: 333.9714, Val Acc: 0.6477, Val AUC: 0.7800\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7614, AUC: 0.7924\n",
      "Sensitivity: 0.6341, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [15 26]]\n",
      "Epoch [7/19] - Train Loss: 0.6076, Val Loss: 333.9338, Val Acc: 0.7614, Val AUC: 0.7924\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7500, AUC: 0.8428\n",
      "Sensitivity: 0.6585, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [14 27]]\n",
      "Epoch [8/19] - Train Loss: 0.5736, Val Loss: 333.9096, Val Acc: 0.7500, Val AUC: 0.8428\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7841, AUC: 0.8563\n",
      "Sensitivity: 0.7805, Specificity: 0.7872\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 9 32]]\n",
      "Epoch [9/19] - Train Loss: 0.5672, Val Loss: 333.9127, Val Acc: 0.7841, Val AUC: 0.8563\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8068, AUC: 0.8749\n",
      "Sensitivity: 0.9512, Specificity: 0.6809\n",
      "Confusion Matrix:\n",
      "[[32 15]\n",
      " [ 2 39]]\n",
      "Epoch [10/19] - Train Loss: 0.5759, Val Loss: 333.8201, Val Acc: 0.8068, Val AUC: 0.8749\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7386, AUC: 0.8329\n",
      "Sensitivity: 0.9024, Specificity: 0.5957\n",
      "Confusion Matrix:\n",
      "[[28 19]\n",
      " [ 4 37]]\n",
      "Epoch [11/19] - Train Loss: 0.5839, Val Loss: 333.9444, Val Acc: 0.7386, Val AUC: 0.8329\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7159, AUC: 0.8381\n",
      "Sensitivity: 0.4878, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [21 20]]\n",
      "Epoch [12/19] - Train Loss: 0.5765, Val Loss: 333.9310, Val Acc: 0.7159, Val AUC: 0.8381\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7727, AUC: 0.8656\n",
      "Sensitivity: 0.8780, Specificity: 0.6809\n",
      "Confusion Matrix:\n",
      "[[32 15]\n",
      " [ 5 36]]\n",
      "Epoch [13/19] - Train Loss: 0.5551, Val Loss: 333.8299, Val Acc: 0.7727, Val AUC: 0.8656\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7500, AUC: 0.8718\n",
      "Sensitivity: 0.9512, Specificity: 0.5745\n",
      "Confusion Matrix:\n",
      "[[27 20]\n",
      " [ 2 39]]\n",
      "Epoch [14/19] - Train Loss: 0.5419, Val Loss: 333.9107, Val Acc: 0.7500, Val AUC: 0.8718\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8182, AUC: 0.8339\n",
      "Sensitivity: 0.7805, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 9 32]]\n",
      "Epoch [15/19] - Train Loss: 0.5260, Val Loss: 333.8601, Val Acc: 0.8182, Val AUC: 0.8339\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8068, AUC: 0.8900\n",
      "Sensitivity: 0.9756, Specificity: 0.6596\n",
      "Confusion Matrix:\n",
      "[[31 16]\n",
      " [ 1 40]]\n",
      "Epoch [16/19] - Train Loss: 0.4790, Val Loss: 333.8317, Val Acc: 0.8068, Val AUC: 0.8900\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7273, AUC: 0.8422\n",
      "Sensitivity: 0.9024, Specificity: 0.5745\n",
      "Confusion Matrix:\n",
      "[[27 20]\n",
      " [ 4 37]]\n",
      "Epoch [17/19] - Train Loss: 0.5834, Val Loss: 333.9366, Val Acc: 0.7273, Val AUC: 0.8422\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7955, AUC: 0.8428\n",
      "Sensitivity: 0.7561, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [10 31]]\n",
      "Epoch [18/19] - Train Loss: 0.5947, Val Loss: 333.9193, Val Acc: 0.7955, Val AUC: 0.8428\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7159, AUC: 0.8153\n",
      "Sensitivity: 0.4878, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [21 20]]\n",
      "Epoch [19/19] - Train Loss: 0.5373, Val Loss: 333.9305, Val Acc: 0.7159, Val AUC: 0.8153\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7841, AUC: 0.8433\n",
      "Sensitivity: 0.6098, Specificity: 0.9362\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [16 25]]\n",
      "Test Loss: 333.8973, Test Accuracy: 0.7841, Test AUC: 0.8433\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 1000.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Best hyperparameters: {'lr': 0.008986542560528932, 'weight_decay': 2.3033044758439348e-06, 'dropout': 0.17164705350229123}\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=113, num_layers=1, dropout=lstm_fullbackprop_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=19, lr=lstm_fullbackprop_best_param['lr'],\n",
    "                      weight_decay=lstm_fullbackprop_best_param['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:21:50,353] A new study created in memory with name: no-name-5b7b9158-2c95-45e5-8606-63e37fe2c6b9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:21:55,279] Trial 0 finished with value: 0.6758488416671753 and parameters: {'lr': 2.534958014869424e-05, 'weight_decay': 5.775892490287451e-06, 'dropout': 0.47044132331479194}. Best is trial 0 with value: 0.6758488416671753.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:21:59,783] Trial 1 finished with value: 0.3800984521706899 and parameters: {'lr': 0.0009222464469545083, 'weight_decay': 0.003685347601942048, 'dropout': 0.2798552175051443}. Best is trial 1 with value: 0.3800984521706899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:04,239] Trial 2 finished with value: 0.6288740833600363 and parameters: {'lr': 8.372552686429017e-05, 'weight_decay': 0.00016289020356816616, 'dropout': 0.10072481950675467}. Best is trial 1 with value: 0.3800984521706899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:08,807] Trial 3 finished with value: 0.24294357374310493 and parameters: {'lr': 0.004837242820313321, 'weight_decay': 2.03252861488189e-06, 'dropout': 0.4939852104684296}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:13,321] Trial 4 finished with value: 0.6850630839665731 and parameters: {'lr': 1.0527937585663375e-05, 'weight_decay': 0.001708621952299083, 'dropout': 0.4666507585113433}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:17,882] Trial 5 finished with value: 0.3745337128639221 and parameters: {'lr': 0.0011722529196370862, 'weight_decay': 8.048278633277898e-06, 'dropout': 0.1515965521268682}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:22,596] Trial 6 finished with value: 0.47653934359550476 and parameters: {'lr': 0.0002960287195176182, 'weight_decay': 0.0003671978912784095, 'dropout': 0.26206207816825255}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:27,299] Trial 7 finished with value: 0.658479650815328 and parameters: {'lr': 5.387194863178497e-05, 'weight_decay': 4.560847872197345e-06, 'dropout': 0.4926631250252146}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:31,814] Trial 8 finished with value: 0.2901750902334849 and parameters: {'lr': 0.0038245641496077658, 'weight_decay': 1.4064561809619517e-06, 'dropout': 0.1380299222181979}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:36,332] Trial 9 finished with value: 0.34490164120992023 and parameters: {'lr': 0.0014760765335687716, 'weight_decay': 3.790189709338313e-05, 'dropout': 0.20762947327028114}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:40,977] Trial 10 finished with value: 0.258048386623462 and parameters: {'lr': 0.008615096803226382, 'weight_decay': 3.1643483102718286e-05, 'dropout': 0.38750793280278395}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:46,007] Trial 11 finished with value: 0.2891167104244232 and parameters: {'lr': 0.009747807202707017, 'weight_decay': 3.595043545313575e-05, 'dropout': 0.38243509970707334}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:50,713] Trial 12 finished with value: 0.2363847754895687 and parameters: {'lr': 0.009981673641982228, 'weight_decay': 1.5244563205828208e-06, 'dropout': 0.38560031839200354}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:55,369] Trial 13 finished with value: 0.29435809701681137 and parameters: {'lr': 0.00265186525287254, 'weight_decay': 1.1576991126832467e-06, 'dropout': 0.37902768424249167}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:23:00,064] Trial 14 finished with value: 0.428882896900177 and parameters: {'lr': 0.00044234575302963883, 'weight_decay': 3.310321963477673e-06, 'dropout': 0.41532635632373033}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:23:04,939] Trial 15 finished with value: 0.24932534930606684 and parameters: {'lr': 0.00519679053034669, 'weight_decay': 1.1648959481001289e-05, 'dropout': 0.3225174342173375}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:23:09,778] Trial 16 finished with value: 0.271179494758447 and parameters: {'lr': 0.002625673993374063, 'weight_decay': 1.210976339858137e-06, 'dropout': 0.4298577593923169}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:23:14,316] Trial 17 finished with value: 0.46983620524406433 and parameters: {'lr': 0.0003176335273641833, 'weight_decay': 0.00044223216635708146, 'dropout': 0.34063831470826716}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:23:19,014] Trial 18 finished with value: 0.26390478014945984 and parameters: {'lr': 0.005400587032946064, 'weight_decay': 1.5738645643086362e-05, 'dropout': 0.43652416422873896}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:23:23,675] Trial 19 finished with value: 0.41215527057647705 and parameters: {'lr': 0.0006554042007735221, 'weight_decay': 2.715948234261771e-06, 'dropout': 0.49822741500077933}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.009981673641982228, 'weight_decay': 1.5244563205828208e-06, 'dropout': 0.38560031839200354}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm-trans-frozen/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path, max_layers=None):\n",
    "    \"\"\"\n",
    "    Load up to `max_layers` compatible layers from a checkpoint into the model.\n",
    "    If max_layers is None, load all compatible layers.\n",
    "    \"\"\"\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter compatible layers\n",
    "    compatible_items = [\n",
    "        (k, v) for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    ]\n",
    "\n",
    "    # Limit number of layers to load\n",
    "    if max_layers is not None:\n",
    "        compatible_items = compatible_items[:max_layers]\n",
    "\n",
    "    # Convert list of tuples back to dict\n",
    "    compatible_dict = dict(compatible_items)\n",
    "\n",
    "    # Update model state dict\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def freeze_encoder(model, num_layers_to_freeze):\n",
    "    \"\"\"\n",
    "    Freezes the first `num_layers_to_freeze` LSTM layers of the model.\n",
    "    Assumes parameter names follow standard PyTorch LSTM naming.\n",
    "    \"\"\"\n",
    "    if num_layers_to_freeze <= 0:\n",
    "        print(\"⚠️ No LSTM layers frozen.\")\n",
    "        return\n",
    "\n",
    "    layer_prefixes = [f'lstm.weight_ih_l{i}' for i in range(num_layers_to_freeze)]\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(prefix in name for prefix in layer_prefixes):\n",
    "            param.requires_grad = False\n",
    "\n",
    "    print(f\"✅ Frozen first {num_layers_to_freeze} LSTM layers.\")\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=dropout, max_seq_len=100)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt', 4)\n",
    "    freeze_encoder(model, 4)\n",
    "\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.009981673641982228,\n",
       " 'weight_decay': 1.5244563205828208e-06,\n",
       " 'dropout': 0.38560031839200354}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bilstm_frozen_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8298\n",
      "Epoch [1/15] - Train Loss: 0.5835, Val Loss: 0.4087, Val Acc: 0.8182, Val AUC: 0.9030\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.9362\n",
      "Epoch [2/15] - Train Loss: 0.3249, Val Loss: 0.3755, Val Acc: 0.8182, Val AUC: 0.9377\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.7447\n",
      "Epoch [3/15] - Train Loss: 0.2763, Val Loss: 0.3600, Val Acc: 0.8523, Val AUC: 0.9481\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.8511\n",
      "Epoch [4/15] - Train Loss: 0.2220, Val Loss: 0.2900, Val Acc: 0.8750, Val AUC: 0.9538\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8936\n",
      "Epoch [5/15] - Train Loss: 0.1717, Val Loss: 0.2487, Val Acc: 0.8750, Val AUC: 0.9637\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.7872\n",
      "Epoch [6/15] - Train Loss: 0.1659, Val Loss: 0.3464, Val Acc: 0.8523, Val AUC: 0.9528\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8936\n",
      "Epoch [7/15] - Train Loss: 0.0936, Val Loss: 0.2837, Val Acc: 0.8636, Val AUC: 0.9689\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Epoch [8/15] - Train Loss: 0.0693, Val Loss: 0.2551, Val Acc: 0.9205, Val AUC: 0.9751\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.9362\n",
      "Epoch [9/15] - Train Loss: 0.0349, Val Loss: 0.5979, Val Acc: 0.8636, Val AUC: 0.9481\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.8936\n",
      "Epoch [10/15] - Train Loss: 0.0481, Val Loss: 0.2824, Val Acc: 0.8977, Val AUC: 0.9715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Epoch [11/15] - Train Loss: 0.0222, Val Loss: 0.3010, Val Acc: 0.9205, Val AUC: 0.9704\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Epoch [12/15] - Train Loss: 0.0204, Val Loss: 0.3112, Val Acc: 0.9205, Val AUC: 0.9730\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.9362\n",
      "Epoch [13/15] - Train Loss: 0.0087, Val Loss: 0.2676, Val Acc: 0.9205, Val AUC: 0.9761\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.8936\n",
      "Epoch [14/15] - Train Loss: 0.0108, Val Loss: 0.2732, Val Acc: 0.9091, Val AUC: 0.9751\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.9149\n",
      "Epoch [15/15] - Train Loss: 0.0060, Val Loss: 0.2447, Val Acc: 0.9091, Val AUC: 0.9772\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.9787\n",
      "Test Loss: 0.3509, Test Accuracy: 0.9091, Test AUC: 0.9580\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'])\n",
    "model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'], max_seq_len=100)\n",
    "\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt', 4)\n",
    "# freeze_encoder(model)\n",
    "freeze_encoder(model, 4)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=15, lr=bilstm_frozen_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:22,477] A new study created in memory with name: no-name-55b57740-d999-48bf-bfc7-ed8bf7f76410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:28,751] Trial 0 finished with value: 0.6472770969072977 and parameters: {'lr': 2.7468020893093288e-05, 'weight_decay': 1.9823888137898948e-05, 'dropout': 0.3614048468861061}. Best is trial 0 with value: 0.6472770969072977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:34,951] Trial 1 finished with value: 0.5378240942955017 and parameters: {'lr': 0.00011050232434267229, 'weight_decay': 0.002430591078406706, 'dropout': 0.44456875419188324}. Best is trial 1 with value: 0.5378240942955017.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:41,269] Trial 2 finished with value: 0.40694797535737354 and parameters: {'lr': 0.00040441757443146373, 'weight_decay': 0.0007244162314611645, 'dropout': 0.2543936645248715}. Best is trial 2 with value: 0.40694797535737354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:47,574] Trial 3 finished with value: 0.6023574670155843 and parameters: {'lr': 5.552704854986336e-05, 'weight_decay': 5.1417614802022544e-05, 'dropout': 0.24668768873375035}. Best is trial 2 with value: 0.40694797535737354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:53,829] Trial 4 finished with value: 0.41414891680081684 and parameters: {'lr': 0.0001858988397340464, 'weight_decay': 9.383893221533215e-05, 'dropout': 0.10417637110993555}. Best is trial 2 with value: 0.40694797535737354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:59,670] Trial 5 finished with value: 0.6527635852495829 and parameters: {'lr': 2.417719030171581e-05, 'weight_decay': 4.461338135656818e-06, 'dropout': 0.12875554245744866}. Best is trial 2 with value: 0.40694797535737354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:05,847] Trial 6 finished with value: 0.31085051223635674 and parameters: {'lr': 0.00926595862633317, 'weight_decay': 2.0121744840074475e-05, 'dropout': 0.46616976355966055}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:12,176] Trial 7 finished with value: 0.6513139009475708 and parameters: {'lr': 2.7391283145004265e-05, 'weight_decay': 0.0030115904593620655, 'dropout': 0.40326048753502586}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:18,357] Trial 8 finished with value: 0.3694236824909846 and parameters: {'lr': 0.0024766730126500607, 'weight_decay': 0.00023421014949312951, 'dropout': 0.4115624504911498}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:24,634] Trial 9 finished with value: 0.46098150809605914 and parameters: {'lr': 0.00018670885203011189, 'weight_decay': 0.004383737500132786, 'dropout': 0.20913943083997089}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:30,951] Trial 10 finished with value: 0.3658249229192734 and parameters: {'lr': 0.008125491550040213, 'weight_decay': 1.6142305843854046e-06, 'dropout': 0.49824230257692326}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:37,836] Trial 11 finished with value: 0.3162108088533084 and parameters: {'lr': 0.009357507940506937, 'weight_decay': 1.1938352503357078e-06, 'dropout': 0.49880941136603546}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:44,780] Trial 12 finished with value: 0.31403346980611485 and parameters: {'lr': 0.009659907971180574, 'weight_decay': 8.925280816078873e-06, 'dropout': 0.49912495000798535}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:53,491] Trial 13 finished with value: 0.38085274398326874 and parameters: {'lr': 0.001595409448541909, 'weight_decay': 1.1779100082492163e-05, 'dropout': 0.315948475819138}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:32:00,650] Trial 14 finished with value: 0.3830023507277171 and parameters: {'lr': 0.0025971744526537134, 'weight_decay': 1.0979574052414046e-05, 'dropout': 0.4516587988428966}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:32:09,012] Trial 15 finished with value: 0.39438508947690326 and parameters: {'lr': 0.0008109533614610762, 'weight_decay': 2.6958452498126817e-05, 'dropout': 0.3488070866340102}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:32:16,277] Trial 16 finished with value: 0.35764744877815247 and parameters: {'lr': 0.0047700970692806915, 'weight_decay': 4.393842864792311e-06, 'dropout': 0.4536291628595147}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:32:22,962] Trial 17 finished with value: 0.40200090408325195 and parameters: {'lr': 0.0009538114802220414, 'weight_decay': 0.00024331960402126517, 'dropout': 0.3978024930675375}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:32:33,702] Trial 18 finished with value: 0.3766237248977025 and parameters: {'lr': 0.004245728608577005, 'weight_decay': 3.8442241149315545e-06, 'dropout': 0.4758037419174857}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:32:42,339] Trial 19 finished with value: 0.40634193023045856 and parameters: {'lr': 0.0005770924461404385, 'weight_decay': 5.446901693669708e-05, 'dropout': 0.3651356881275941}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.00926595862633317, 'weight_decay': 2.0121744840074475e-05, 'dropout': 0.46616976355966055}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm-trans-fullbackprop/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_partial_weights(model, checkpoint_path, max_layers=None):\n",
    "    \"\"\"\n",
    "    Load up to `max_layers` compatible layers from a checkpoint into the model.\n",
    "    If max_layers is None, load all compatible layers.\n",
    "    \"\"\"\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter compatible layers\n",
    "    compatible_items = [\n",
    "        (k, v) for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    ]\n",
    "\n",
    "    # Limit number of layers to load\n",
    "    if max_layers is not None:\n",
    "        compatible_items = compatible_items[:max_layers]\n",
    "\n",
    "    # Convert list of tuples back to dict\n",
    "    compatible_dict = dict(compatible_items)\n",
    "\n",
    "    # Update model state dict\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=dropout, max_seq_len=100)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt', 6)\n",
    "\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=20, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Epoch [1/10] - Train Loss: 0.6852, Val Loss: 0.6715, Val Acc: 0.5455, Val AUC: 0.8630\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25 22]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.5319\n",
      "Epoch [2/10] - Train Loss: 0.6356, Val Loss: 0.5856, Val Acc: 0.6818, Val AUC: 0.8215\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [19 22]]\n",
      "Sensitivity: 0.5366, Specificity: 0.9362\n",
      "Epoch [3/10] - Train Loss: 0.4994, Val Loss: 0.5150, Val Acc: 0.7500, Val AUC: 0.8988\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.8723\n",
      "Epoch [4/10] - Train Loss: 0.4706, Val Loss: 0.5210, Val Acc: 0.7841, Val AUC: 0.8775\n",
      "\n",
      "Confusion Matrix:\n",
      "[[31 16]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.6596\n",
      "Epoch [5/10] - Train Loss: 0.4291, Val Loss: 0.5328, Val Acc: 0.8068, Val AUC: 0.8983\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.7234\n",
      "Epoch [6/10] - Train Loss: 0.4508, Val Loss: 0.4519, Val Acc: 0.8182, Val AUC: 0.9139\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.8511\n",
      "Epoch [7/10] - Train Loss: 0.4081, Val Loss: 0.4241, Val Acc: 0.7955, Val AUC: 0.9045\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8511\n",
      "Epoch [8/10] - Train Loss: 0.3681, Val Loss: 0.3904, Val Acc: 0.8182, Val AUC: 0.9237\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8511\n",
      "Epoch [9/10] - Train Loss: 0.4223, Val Loss: 0.4311, Val Acc: 0.8523, Val AUC: 0.9305\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8936\n",
      "Epoch [10/10] - Train Loss: 0.3686, Val Loss: 0.3527, Val Acc: 0.8636, Val AUC: 0.9471\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.9362\n",
      "Test Loss: 0.3634, Test Accuracy: 0.8409, Test AUC: 0.9362\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "# to rerun\n",
    "model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_fullbackprop_best_parameters['dropout'])\n",
    "# model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt', 6)\n",
    "\n",
    "# freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10, lr=bilstm_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:21,490] A new study created in memory with name: no-name-a7d840e6-6b44-4aac-bb65-e37e9b4eab81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:25,753] Trial 0 finished with value: 0.7674408356348673 and parameters: {'lr': 0.0013602003194664756, 'weight_decay': 1.5712677101296663e-05, 'dropout': 0.4169138472289945}. Best is trial 0 with value: 0.7674408356348673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:30,111] Trial 1 finished with value: 0.895917534828186 and parameters: {'lr': 0.00013430607061839717, 'weight_decay': 0.0042018236062449435, 'dropout': 0.15353305409980217}. Best is trial 0 with value: 0.7674408356348673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:34,547] Trial 2 finished with value: 0.692759116490682 and parameters: {'lr': 0.003780147510933303, 'weight_decay': 4.147651339063238e-05, 'dropout': 0.15484650861534519}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:38,612] Trial 3 finished with value: 0.7345146139462789 and parameters: {'lr': 0.0016964887404221295, 'weight_decay': 0.0025817663584606415, 'dropout': 0.3348237829311172}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:42,484] Trial 4 finished with value: 0.8995350400606791 and parameters: {'lr': 4.9828507299876767e-05, 'weight_decay': 1.0333765354408845e-05, 'dropout': 0.47991499384796643}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:46,517] Trial 5 finished with value: 0.8928668101628622 and parameters: {'lr': 0.00011988681439998793, 'weight_decay': 0.0002677504823600038, 'dropout': 0.1378990760927295}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:50,750] Trial 6 finished with value: 0.9062303105990092 and parameters: {'lr': 3.1463892370362697e-05, 'weight_decay': 0.006769770872029123, 'dropout': 0.1857350862362233}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:54,930] Trial 7 finished with value: 0.9076404372851054 and parameters: {'lr': 1.0546460736553949e-05, 'weight_decay': 2.218658426160035e-05, 'dropout': 0.1200372432481383}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:59,195] Trial 8 finished with value: 0.893417239189148 and parameters: {'lr': 2.5182458713641945e-05, 'weight_decay': 0.001656918686782995, 'dropout': 0.1668816125518131}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:44:03,516] Trial 9 finished with value: 0.9056477149327596 and parameters: {'lr': 4.6009844895884564e-05, 'weight_decay': 0.0006708921290933786, 'dropout': 0.19471185829869456}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:44:07,962] Trial 10 finished with value: 0.6912072499593099 and parameters: {'lr': 0.009315794991764154, 'weight_decay': 1.3701958442320686e-06, 'dropout': 0.26912897628898586}. Best is trial 10 with value: 0.6912072499593099.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:44:12,344] Trial 11 finished with value: 0.691220243771871 and parameters: {'lr': 0.009073800913924703, 'weight_decay': 1.3122066920704696e-06, 'dropout': 0.27779103538774164}. Best is trial 10 with value: 0.6912072499593099.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.009315794991764154, 'weight_decay': 1.3701958442320686e-06, 'dropout': 0.26912897628898586}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM with Attention\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze LSTM encoder\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-att-trans/LSTMAttTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_att_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Load compatible weights\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=dropout)\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=12)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_att_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lr': 0.009315794991764154,\n",
       " 'weight_decay': 1.3701958442320686e-06,\n",
       " 'dropout': 0.26912897628898586}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_att_frozen_best_parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [1/20] - Train Loss: 0.8951, Val Loss: 0.8342, Val Acc: 0.4659, Val AUC: 0.3259\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [2/20] - Train Loss: 0.7994, Val Loss: 0.7735, Val Acc: 0.4659, Val AUC: 0.3840\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [3/20] - Train Loss: 0.7444, Val Loss: 0.7343, Val Acc: 0.4659, Val AUC: 0.4043\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [4/20] - Train Loss: 0.7259, Val Loss: 0.7112, Val Acc: 0.4659, Val AUC: 0.4089\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [5/20] - Train Loss: 0.7094, Val Loss: 0.6980, Val Acc: 0.4659, Val AUC: 0.4152\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 2 45]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.0426\n",
      "Epoch [6/20] - Train Loss: 0.6928, Val Loss: 0.6934, Val Acc: 0.4773, Val AUC: 0.4203\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/20] - Train Loss: 0.6979, Val Loss: 0.6919, Val Acc: 0.5341, Val AUC: 0.4219\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [8/20] - Train Loss: 0.6964, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [9/20] - Train Loss: 0.6918, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4292\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [10/20] - Train Loss: 0.6949, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4338\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [11/20] - Train Loss: 0.6928, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4395\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [12/20] - Train Loss: 0.6911, Val Loss: 0.6914, Val Acc: 0.5341, Val AUC: 0.4453\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [13/20] - Train Loss: 0.6950, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4468\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [14/20] - Train Loss: 0.6892, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.4520\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [15/20] - Train Loss: 0.6937, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.4582\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [16/20] - Train Loss: 0.6932, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.4619\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [17/20] - Train Loss: 0.6924, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.4665\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [18/20] - Train Loss: 0.6943, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4691\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [19/20] - Train Loss: 0.6887, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.4743\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [20/20] - Train Loss: 0.6979, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.4748\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Test Loss: 0.6926, Test Accuracy: 0.5341, Test AUC: 0.4517\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# to rerun\n",
    "model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=lstm_att_frozen_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_att_frozen_best_parameters['lr'],\n",
    "                      weight_decay=lstm_att_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full back prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:45:47,883] A new study created in memory with name: no-name-76b0eca7-46cb-4162-8dba-40a3f14a8454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:45:52,416] Trial 0 finished with value: 0.4709471066792806 and parameters: {'lr': 0.00442440456463527, 'weight_decay': 0.0008226696089013947, 'dropout': 0.30955810155370944}. Best is trial 0 with value: 0.4709471066792806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:45:56,951] Trial 1 finished with value: 0.4392253557840983 and parameters: {'lr': 0.003661950605748677, 'weight_decay': 1.6001438653977643e-06, 'dropout': 0.4778793892133558}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:01,439] Trial 2 finished with value: 0.8960553606351217 and parameters: {'lr': 3.032894962764756e-05, 'weight_decay': 0.008733957208604901, 'dropout': 0.44586469529109785}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:06,022] Trial 3 finished with value: 0.48641664783159894 and parameters: {'lr': 0.0017527997703626144, 'weight_decay': 1.491832353517625e-06, 'dropout': 0.10531062549569291}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:10,496] Trial 4 finished with value: 0.6915572881698608 and parameters: {'lr': 0.00020896990510620403, 'weight_decay': 0.00124155424080721, 'dropout': 0.3618972607271408}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:14,442] Trial 5 finished with value: 0.8237904111544291 and parameters: {'lr': 7.533312097210972e-05, 'weight_decay': 0.00317457248026703, 'dropout': 0.37683960635297187}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:18,810] Trial 6 finished with value: 0.8280006845792135 and parameters: {'lr': 8.40556321905403e-05, 'weight_decay': 0.003096918223786138, 'dropout': 0.22357908876439977}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:23,312] Trial 7 finished with value: 0.6959420045216879 and parameters: {'lr': 9.169637422694053e-05, 'weight_decay': 1.0675046993269794e-05, 'dropout': 0.22834866915121146}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:27,884] Trial 8 finished with value: 0.6871453921000162 and parameters: {'lr': 0.0005610513630148932, 'weight_decay': 0.0004256836841638055, 'dropout': 0.18336677071677587}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:32,063] Trial 9 finished with value: 0.44175009926160175 and parameters: {'lr': 0.0033097919893395897, 'weight_decay': 3.749354030450557e-05, 'dropout': 0.4276219521659518}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:36,405] Trial 10 finished with value: 0.6771309971809387 and parameters: {'lr': 0.0009671460718926826, 'weight_decay': 1.1083631440082833e-06, 'dropout': 0.4784058248877746}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:40,698] Trial 11 finished with value: 0.4735666811466217 and parameters: {'lr': 0.009846905354476123, 'weight_decay': 3.597164154555872e-05, 'dropout': 0.4219270858412268}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.003661950605748677, 'weight_decay': 1.6001438653977643e-06, 'dropout': 0.4778793892133558}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM with Attention\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze LSTM encoder\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-att-trans-fullback/LSTMAttTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_att_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Load compatible weights\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=dropout)\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=12)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_att_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/20] - Train Loss: 0.8039, Val Loss: 0.6922, Val Acc: 0.5341, Val AUC: 0.4805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [2/20] - Train Loss: 0.6991, Val Loss: 0.6965, Val Acc: 0.4659, Val AUC: 0.5791\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/20] - Train Loss: 0.7087, Val Loss: 0.6958, Val Acc: 0.5341, Val AUC: 0.6186\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/20] - Train Loss: 0.6984, Val Loss: 0.6901, Val Acc: 0.5341, Val AUC: 0.6632\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/20] - Train Loss: 0.7040, Val Loss: 0.6896, Val Acc: 0.5341, Val AUC: 0.7063\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/20] - Train Loss: 0.6950, Val Loss: 0.6902, Val Acc: 0.5341, Val AUC: 0.7478\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/20] - Train Loss: 0.6891, Val Loss: 0.6854, Val Acc: 0.5341, Val AUC: 0.8023\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [27 14]]\n",
      "Sensitivity: 0.3415, Specificity: 0.9574\n",
      "Epoch [8/20] - Train Loss: 0.6922, Val Loss: 0.6802, Val Acc: 0.6705, Val AUC: 0.8485\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [23 18]]\n",
      "Sensitivity: 0.4390, Specificity: 0.9574\n",
      "Epoch [9/20] - Train Loss: 0.6574, Val Loss: 0.5817, Val Acc: 0.7159, Val AUC: 0.8443\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.7021\n",
      "Epoch [10/20] - Train Loss: 0.5097, Val Loss: 0.5273, Val Acc: 0.7841, Val AUC: 0.8588\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18 29]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.3830\n",
      "Epoch [11/20] - Train Loss: 0.5707, Val Loss: 0.6275, Val Acc: 0.6705, Val AUC: 0.8651\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [12/20] - Train Loss: 0.6811, Val Loss: 0.6349, Val Acc: 0.5341, Val AUC: 0.8163\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [13/20] - Train Loss: 0.6504, Val Loss: 0.5608, Val Acc: 0.5341, Val AUC: 0.8594\n",
      "\n",
      "Confusion Matrix:\n",
      "[[29 18]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.6170\n",
      "Epoch [14/20] - Train Loss: 0.5714, Val Loss: 0.5312, Val Acc: 0.7614, Val AUC: 0.8625\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [15 26]]\n",
      "Sensitivity: 0.6341, Specificity: 0.8085\n",
      "Epoch [15/20] - Train Loss: 0.5132, Val Loss: 0.8228, Val Acc: 0.7273, Val AUC: 0.8365\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.9362\n",
      "Epoch [16/20] - Train Loss: 0.5892, Val Loss: 0.5312, Val Acc: 0.7614, Val AUC: 0.8438\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.7234\n",
      "Epoch [17/20] - Train Loss: 0.4860, Val Loss: 0.4988, Val Acc: 0.7500, Val AUC: 0.8448\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8298\n",
      "Epoch [18/20] - Train Loss: 0.4626, Val Loss: 0.4933, Val Acc: 0.7727, Val AUC: 0.8557\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.8936\n",
      "Epoch [19/20] - Train Loss: 0.4305, Val Loss: 0.5858, Val Acc: 0.7386, Val AUC: 0.8588\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.8723\n",
      "Epoch [20/20] - Train Loss: 0.4433, Val Loss: 0.4643, Val Acc: 0.7500, Val AUC: 0.8537\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8936\n",
      "Test Loss: 0.3579, Test Accuracy: 0.8523, Test AUC: 0.9315\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=lstm_att_fullbackprop_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_att_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=lstm_att_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:31,867] A new study created in memory with name: no-name-6f67a702-3116-4719-a1f3-fabc0ce7b300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:36,317] Trial 0 finished with value: 0.6913818120956421 and parameters: {'lr': 0.00010396705802047225, 'weight_decay': 2.606722106246024e-05, 'dropout': 0.27484829374411573}. Best is trial 0 with value: 0.6913818120956421.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:40,608] Trial 1 finished with value: 0.6909677386283875 and parameters: {'lr': 0.001603372192461072, 'weight_decay': 2.7392594780050672e-05, 'dropout': 0.35549236639803505}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:45,004] Trial 2 finished with value: 0.6912177602450053 and parameters: {'lr': 0.001028278887873135, 'weight_decay': 2.7421423263802487e-06, 'dropout': 0.3645635923789524}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:49,294] Trial 3 finished with value: 0.6910070578257242 and parameters: {'lr': 0.009932459261507275, 'weight_decay': 0.0010550357036470013, 'dropout': 0.4391675249545186}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:53,716] Trial 4 finished with value: 0.6915017167727152 and parameters: {'lr': 4.4562322008763944e-05, 'weight_decay': 0.0011432662106658348, 'dropout': 0.1812756174961075}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:58,132] Trial 5 finished with value: 0.6912265221277872 and parameters: {'lr': 6.892477912590756e-05, 'weight_decay': 0.0010490351025928817, 'dropout': 0.36219203807204225}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:02,570] Trial 6 finished with value: 0.6912133097648621 and parameters: {'lr': 0.00027652020377975293, 'weight_decay': 0.005115876742108018, 'dropout': 0.21390603329189248}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:06,418] Trial 7 finished with value: 0.6912065744400024 and parameters: {'lr': 0.008822275785741721, 'weight_decay': 0.00013168808579933257, 'dropout': 0.25332206236182137}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:10,417] Trial 8 finished with value: 0.6913290421168009 and parameters: {'lr': 0.00014108015644636373, 'weight_decay': 0.0005935795593691928, 'dropout': 0.11745937162575203}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:14,267] Trial 9 finished with value: 0.6910712718963623 and parameters: {'lr': 0.0019145400759922671, 'weight_decay': 1.3258180609809638e-06, 'dropout': 0.17373351254916286}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:18,210] Trial 10 finished with value: 0.6919327179590861 and parameters: {'lr': 1.387364637976795e-05, 'weight_decay': 1.441591616962705e-05, 'dropout': 0.4993958742066162}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:22,101] Trial 11 finished with value: 0.6910941203435262 and parameters: {'lr': 0.009644222341479193, 'weight_decay': 8.642345129084171e-05, 'dropout': 0.46389529216658204}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:26,005] Trial 12 finished with value: 0.6910844445228577 and parameters: {'lr': 0.0024496849132849673, 'weight_decay': 0.006860473021053124, 'dropout': 0.4014735436121461}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:30,002] Trial 13 finished with value: 0.6913674672444662 and parameters: {'lr': 0.0008034113243593062, 'weight_decay': 0.00017901786277895685, 'dropout': 0.4267274754567762}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:33,943] Trial 14 finished with value: 0.6910897493362427 and parameters: {'lr': 0.003944461039328581, 'weight_decay': 1.205315555437901e-05, 'dropout': 0.3224619251674377}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:37,986] Trial 15 finished with value: 0.6911685268084208 and parameters: {'lr': 0.0004764677716866242, 'weight_decay': 4.112887309935719e-05, 'dropout': 0.33150172104875636}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:42,037] Trial 16 finished with value: 0.6910964250564575 and parameters: {'lr': 0.005196798019258291, 'weight_decay': 0.00040071957730875225, 'dropout': 0.4193980346042576}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:45,943] Trial 17 finished with value: 0.69124835729599 and parameters: {'lr': 0.0016855656468750069, 'weight_decay': 6.06374318250068e-06, 'dropout': 0.4647991614128991}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:49,833] Trial 18 finished with value: 0.6911240418752035 and parameters: {'lr': 0.004291227007661284, 'weight_decay': 0.0021430179976216273, 'dropout': 0.3917833116789575}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:54,293] Trial 19 finished with value: 0.6911638975143433 and parameters: {'lr': 0.00037995929454300183, 'weight_decay': 0.00031219145374794077, 'dropout': 0.28841007466281104}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.001603372192461072, 'weight_decay': 2.7392594780050672e-05, 'dropout': 0.35549236639803505}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm_attn-trans-frozen/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_attn_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=dropout)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/50] - Train Loss: 0.6919, Val Loss: 0.6915, Val Acc: 0.5341, Val AUC: 0.6544\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/50] - Train Loss: 0.6907, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.6518\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/50] - Train Loss: 0.6944, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6508\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/50] - Train Loss: 0.6912, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6471\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/50] - Train Loss: 0.6875, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6435\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/50] - Train Loss: 0.6908, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6409\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/50] - Train Loss: 0.6868, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6404\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [8/50] - Train Loss: 0.6904, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6375\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [9/50] - Train Loss: 0.6895, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6409\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [10/50] - Train Loss: 0.6923, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6445\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [11/50] - Train Loss: 0.6921, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6528\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [12/50] - Train Loss: 0.6945, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6596\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [13/50] - Train Loss: 0.6911, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6668\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [14/50] - Train Loss: 0.6890, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6718\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [15/50] - Train Loss: 0.6922, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6744\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [16/50] - Train Loss: 0.6916, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6793\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [17/50] - Train Loss: 0.6920, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6788\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [18/50] - Train Loss: 0.6910, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6842\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [19/50] - Train Loss: 0.6897, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6881\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [20/50] - Train Loss: 0.6923, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6881\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [21/50] - Train Loss: 0.6912, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6886\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [22/50] - Train Loss: 0.6893, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6938\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [23/50] - Train Loss: 0.6928, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6938\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [24/50] - Train Loss: 0.6900, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6949\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [25/50] - Train Loss: 0.6915, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6995\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [26/50] - Train Loss: 0.6950, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7003\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [27/50] - Train Loss: 0.6930, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7047\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [28/50] - Train Loss: 0.6929, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7063\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [29/50] - Train Loss: 0.6921, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.7104\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [30/50] - Train Loss: 0.6910, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7078\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [31/50] - Train Loss: 0.6896, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7115\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [32/50] - Train Loss: 0.6902, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7151\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [33/50] - Train Loss: 0.6934, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7151\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [34/50] - Train Loss: 0.6914, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7161\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [35/50] - Train Loss: 0.6943, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7156\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [36/50] - Train Loss: 0.6863, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7193\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [37/50] - Train Loss: 0.6902, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7182\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [38/50] - Train Loss: 0.6923, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7146\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [39/50] - Train Loss: 0.6922, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7187\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [40/50] - Train Loss: 0.6914, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7177\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [41/50] - Train Loss: 0.6938, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7193\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [42/50] - Train Loss: 0.6908, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7250\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [43/50] - Train Loss: 0.6891, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7265\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [44/50] - Train Loss: 0.6912, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7278\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [45/50] - Train Loss: 0.6888, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7309\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [46/50] - Train Loss: 0.6911, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7353\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [47/50] - Train Loss: 0.6945, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7369\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [48/50] - Train Loss: 0.6949, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7387\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [49/50] - Train Loss: 0.6859, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7499\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [50/50] - Train Loss: 0.6938, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7483\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Test Loss: 0.6924, Test Accuracy: 0.5341, Test AUC: 0.6622\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attn-trans-frozen/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# to rerun\n",
    "model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=bilstm_attn_frozen_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=50, lr=bilstm_attn_frozen_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_attn_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full back prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:08,676] A new study created in memory with name: no-name-bb29b48d-f95e-4f82-bd4f-627eb384a827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:12,600] Trial 0 finished with value: 0.6919108629226685 and parameters: {'lr': 7.861113045771693e-05, 'weight_decay': 1.4127732016108189e-06, 'dropout': 0.3592199466945646}. Best is trial 0 with value: 0.6919108629226685.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:16,574] Trial 1 finished with value: 0.6915312012036642 and parameters: {'lr': 3.626443880830906e-05, 'weight_decay': 1.2961818653876822e-05, 'dropout': 0.3464371450549254}. Best is trial 1 with value: 0.6915312012036642.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:20,716] Trial 2 finished with value: 0.6911666790644327 and parameters: {'lr': 2.7565591674814572e-05, 'weight_decay': 0.00016592249203051755, 'dropout': 0.3963016233506764}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:24,764] Trial 3 finished with value: 0.6911691228548685 and parameters: {'lr': 0.001956726158117691, 'weight_decay': 2.0522585791503508e-05, 'dropout': 0.33785336420806644}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:29,197] Trial 4 finished with value: 0.6911851763725281 and parameters: {'lr': 0.0018456346129685436, 'weight_decay': 1.3374238068711262e-05, 'dropout': 0.4166964491092321}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:33,644] Trial 5 finished with value: 0.6916813254356384 and parameters: {'lr': 1.645749040342259e-05, 'weight_decay': 0.006048875394564759, 'dropout': 0.3382932817294666}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:37,619] Trial 6 finished with value: 0.6912331978480021 and parameters: {'lr': 2.3006779878890447e-05, 'weight_decay': 0.0005144087182014116, 'dropout': 0.1673895308849253}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:41,541] Trial 7 finished with value: 0.691234310468038 and parameters: {'lr': 0.0005244862073215171, 'weight_decay': 4.783349577976966e-06, 'dropout': 0.1578606009585733}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:45,443] Trial 8 finished with value: 0.6911967794100443 and parameters: {'lr': 4.44117100165219e-05, 'weight_decay': 2.1866785989418446e-06, 'dropout': 0.30810178545726064}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:49,611] Trial 9 finished with value: 0.691170891125997 and parameters: {'lr': 5.923540180131155e-05, 'weight_decay': 3.974123505671287e-05, 'dropout': 0.4864122476074447}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:53,925] Trial 10 finished with value: 0.6913396914800009 and parameters: {'lr': 0.00024242668811504204, 'weight_decay': 0.000269896650980584, 'dropout': 0.22681777131386222}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:57,986] Trial 11 finished with value: 0.6911083658536276 and parameters: {'lr': 0.009023890166622224, 'weight_decay': 0.00010779058678715841, 'dropout': 0.4535673475985836}. Best is trial 11 with value: 0.6911083658536276.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:02,056] Trial 12 finished with value: 0.6910683314005533 and parameters: {'lr': 0.006575991272077654, 'weight_decay': 0.0012171960694781323, 'dropout': 0.49038586373028537}. Best is trial 12 with value: 0.6910683314005533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:05,891] Trial 13 finished with value: 0.691091517607371 and parameters: {'lr': 0.008979998474401603, 'weight_decay': 0.0016277825579878843, 'dropout': 0.49510349781243346}. Best is trial 12 with value: 0.6910683314005533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:10,215] Trial 14 finished with value: 0.6910977959632874 and parameters: {'lr': 0.008378064727562523, 'weight_decay': 0.0018987159227296963, 'dropout': 0.48731650267050336}. Best is trial 12 with value: 0.6910683314005533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:14,570] Trial 15 finished with value: 0.6911635796229044 and parameters: {'lr': 0.002924883905604112, 'weight_decay': 0.0013506202633295314, 'dropout': 0.4384318444073999}. Best is trial 12 with value: 0.6910683314005533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:18,760] Trial 16 finished with value: 0.6911545395851135 and parameters: {'lr': 0.003904690298858426, 'weight_decay': 0.009615624075107606, 'dropout': 0.25972480298293266}. Best is trial 12 with value: 0.6910683314005533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:22,773] Trial 17 finished with value: 0.6912190715471903 and parameters: {'lr': 0.0007172112286993747, 'weight_decay': 0.0019101752799604538, 'dropout': 0.10642009632957608}. Best is trial 12 with value: 0.6910683314005533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:26,835] Trial 18 finished with value: 0.6910647551218668 and parameters: {'lr': 0.004843097327507198, 'weight_decay': 0.0006598357188430301, 'dropout': 0.49558732282929335}. Best is trial 18 with value: 0.6910647551218668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:31,364] Trial 19 finished with value: 0.691186785697937 and parameters: {'lr': 0.0010800143901562472, 'weight_decay': 0.0005160704668832621, 'dropout': 0.3869721104280036}. Best is trial 18 with value: 0.6910647551218668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.004843097327507198, 'weight_decay': 0.0006598357188430301, 'dropout': 0.49558732282929335}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm_attn-trans-fullbackprop/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_attn_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=dropout)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/120] - Train Loss: 0.6943, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5568\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/120] - Train Loss: 0.6922, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5706\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/120] - Train Loss: 0.6895, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5677\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/120] - Train Loss: 0.6887, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.5638\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/120] - Train Loss: 0.6899, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.5659\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/120] - Train Loss: 0.6962, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5428\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/120] - Train Loss: 0.6957, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5605\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [8/120] - Train Loss: 0.6865, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5605\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [9/120] - Train Loss: 0.6907, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5540\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [10/120] - Train Loss: 0.6894, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.5573\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [11/120] - Train Loss: 0.6879, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5758\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [12/120] - Train Loss: 0.6930, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5802\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [13/120] - Train Loss: 0.6942, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5859\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [14/120] - Train Loss: 0.6940, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.5929\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [15/120] - Train Loss: 0.6929, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6077\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [16/120] - Train Loss: 0.6933, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6207\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [17/120] - Train Loss: 0.6967, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6248\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [18/120] - Train Loss: 0.6895, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.6492\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [19/120] - Train Loss: 0.6936, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6430\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [20/120] - Train Loss: 0.6871, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6487\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [21/120] - Train Loss: 0.6893, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6471\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [22/120] - Train Loss: 0.6877, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6424\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [23/120] - Train Loss: 0.6955, Val Loss: 0.6914, Val Acc: 0.5341, Val AUC: 0.6217\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [24/120] - Train Loss: 0.6919, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6362\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [25/120] - Train Loss: 0.6955, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6601\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [26/120] - Train Loss: 0.6950, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6907\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [27/120] - Train Loss: 0.6928, Val Loss: 0.6916, Val Acc: 0.5341, Val AUC: 0.7052\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [28/120] - Train Loss: 0.6938, Val Loss: 0.6916, Val Acc: 0.5341, Val AUC: 0.7029\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [29/120] - Train Loss: 0.6936, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6847\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [30/120] - Train Loss: 0.6895, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6860\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [31/120] - Train Loss: 0.6909, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6845\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [32/120] - Train Loss: 0.6919, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6809\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [33/120] - Train Loss: 0.6892, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6767\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [34/120] - Train Loss: 0.6935, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6720\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [35/120] - Train Loss: 0.6893, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6798\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [36/120] - Train Loss: 0.6912, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6772\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [37/120] - Train Loss: 0.6882, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6762\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [38/120] - Train Loss: 0.6938, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7016\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [39/120] - Train Loss: 0.6921, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7193\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [40/120] - Train Loss: 0.6934, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7291\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [41/120] - Train Loss: 0.6909, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.7348\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [42/120] - Train Loss: 0.6929, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7234\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [43/120] - Train Loss: 0.6942, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7224\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [44/120] - Train Loss: 0.6894, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7104\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [45/120] - Train Loss: 0.6919, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6933\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [46/120] - Train Loss: 0.6912, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6923\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [47/120] - Train Loss: 0.6929, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6967\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [48/120] - Train Loss: 0.6897, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6995\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [49/120] - Train Loss: 0.6871, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7052\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [50/120] - Train Loss: 0.6879, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7013\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [51/120] - Train Loss: 0.6908, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6990\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [52/120] - Train Loss: 0.6925, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7026\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [53/120] - Train Loss: 0.6896, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6798\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [54/120] - Train Loss: 0.6929, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6790\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [55/120] - Train Loss: 0.6953, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6814\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [56/120] - Train Loss: 0.6883, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6855\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [57/120] - Train Loss: 0.6902, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6700\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [58/120] - Train Loss: 0.6910, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6642\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [59/120] - Train Loss: 0.6897, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6674\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [60/120] - Train Loss: 0.6878, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6689\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [61/120] - Train Loss: 0.6893, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6684\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [62/120] - Train Loss: 0.6914, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6617\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [63/120] - Train Loss: 0.6912, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6554\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [64/120] - Train Loss: 0.6889, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6627\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [65/120] - Train Loss: 0.6911, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6596\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [66/120] - Train Loss: 0.6906, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6627\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [67/120] - Train Loss: 0.6892, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6565\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [68/120] - Train Loss: 0.6925, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6445\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [69/120] - Train Loss: 0.6882, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6424\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [70/120] - Train Loss: 0.6874, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6497\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [71/120] - Train Loss: 0.6932, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6533\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [72/120] - Train Loss: 0.6864, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6617\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [73/120] - Train Loss: 0.6926, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6668\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [74/120] - Train Loss: 0.6912, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6707\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [75/120] - Train Loss: 0.6950, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6679\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [76/120] - Train Loss: 0.6885, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6736\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [77/120] - Train Loss: 0.6915, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6655\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [78/120] - Train Loss: 0.6925, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6578\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [79/120] - Train Loss: 0.6895, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6588\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [80/120] - Train Loss: 0.6915, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6559\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [81/120] - Train Loss: 0.6909, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6466\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [82/120] - Train Loss: 0.6889, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6539\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [83/120] - Train Loss: 0.6882, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6637\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [84/120] - Train Loss: 0.6924, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6606\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [85/120] - Train Loss: 0.6920, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6622\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [86/120] - Train Loss: 0.6888, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6617\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [87/120] - Train Loss: 0.6915, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6518\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [88/120] - Train Loss: 0.6926, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6456\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [89/120] - Train Loss: 0.6911, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6435\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [90/120] - Train Loss: 0.6957, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6435\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [91/120] - Train Loss: 0.6910, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6611\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [92/120] - Train Loss: 0.6935, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6679\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [93/120] - Train Loss: 0.6884, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6860\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [94/120] - Train Loss: 0.6878, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6824\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [95/120] - Train Loss: 0.6870, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6746\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [96/120] - Train Loss: 0.6918, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6694\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [97/120] - Train Loss: 0.6874, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6679\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [98/120] - Train Loss: 0.6877, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6658\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [99/120] - Train Loss: 0.6909, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6642\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [100/120] - Train Loss: 0.6912, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6622\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [101/120] - Train Loss: 0.6921, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6648\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [102/120] - Train Loss: 0.6914, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6694\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [103/120] - Train Loss: 0.6892, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6788\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [104/120] - Train Loss: 0.6882, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.6860\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [105/120] - Train Loss: 0.6944, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6855\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [106/120] - Train Loss: 0.6915, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6917\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [107/120] - Train Loss: 0.6904, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6936\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [108/120] - Train Loss: 0.6919, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.6912\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [109/120] - Train Loss: 0.6898, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.6897\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [110/120] - Train Loss: 0.6887, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6897\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [111/120] - Train Loss: 0.6912, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6871\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [112/120] - Train Loss: 0.6918, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6834\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [113/120] - Train Loss: 0.6915, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6853\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [114/120] - Train Loss: 0.6911, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6860\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [115/120] - Train Loss: 0.6934, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.6866\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [116/120] - Train Loss: 0.6902, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.6834\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [117/120] - Train Loss: 0.6937, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6767\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [118/120] - Train Loss: 0.6912, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.6718\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [119/120] - Train Loss: 0.6908, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6627\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [120/120] - Train Loss: 0.6912, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6601\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Test Loss: 0.6921, Test Accuracy: 0.5341, Test AUC: 0.7032\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attn-trans-fullbackprop/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=bilstm_attn_fullbackprop_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=120, lr=bilstm_attn_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_attn_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
