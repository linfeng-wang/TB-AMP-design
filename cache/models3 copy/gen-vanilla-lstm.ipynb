{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# If using PyTorch\n",
    "import torch\n",
    "\n",
    "# If using TensorFlow\n",
    "\n",
    "# Optional: If using Python hash-based functions\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "\n",
    "# Set seed for base Python random\n",
    "random.seed(42)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set seed for PyTorch (CPU and GPU)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)  # if using multi-GPU\n",
    "\n",
    "# Force deterministic behavior in PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General AMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbbasp = pd.read_csv(\"../models1/database_check/DBAASP_peptides.csv\")\n",
    "dbbasp = dbbasp[dbbasp[\"SEQUENCE\"].str.len() >= 10]\n",
    "dbbasp = dbbasp[~dbbasp[\"TARGET GROUP\"].str.contains(\"Fungus\", na=False)]\n",
    "dbbasp = dbbasp[[\"ID\", \"SEQUENCE\"]]\n",
    "dbbasp.columns = [\"Peptide ID\", \"Sequence\"]\n",
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "adam_df = pd.concat([adam_df, dbbasp], ignore_index=True)\n",
    "# uniprot_df = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\")\n",
    "# uniprot_df1 = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\")\n",
    "# uniprot_df = pd.concat([uniprot_df, uniprot_df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### raw data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'K', 'C', 'Q', 'G', 'F', 'A', 'D', 'Y', 'P', 'M', 'H', 'E', 'I', 'V', 'L', 'S', 'T', 'W', 'R', 'N'}\n",
      "20\n",
      "Number of sequences after filtering: 3306\n",
      "Dataset sizes: {'Train': 2219, 'Validation': 475, 'Test': 476}\n",
      "Input shape: torch.Size([1793, 20])\n",
      "Target shape: torch.Size([1793, 20])\n",
      "Lengths: tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 59, 57, 55, 53, 53, 50,\n",
      "        46, 45, 40, 40, 38, 35, 32, 31, 31, 30, 29, 29, 28, 26, 20, 19, 16, 14,\n",
      "        13, 10,  7,  5,  5,  4,  4,  4,  4,  3,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load only positive (AMP) sequences\n",
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "\n",
    "unique_letters = set(''.join(adam_df[\"Sequence\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "print(f\"Number of sequences after filtering: {len(adam_df)}\")\n",
    "adam_df = adam_df.drop_duplicates(subset='Sequence')\n",
    "tb_df = pd.read_csv('../data/all_seq702.csv')\n",
    "adam_df = adam_df[~adam_df['Sequence'].isin(tb_df['Sequences'])]\n",
    "adam_df = adam_df[adam_df[\"Sequence\"].str.len() >= 10]\n",
    "generation_seqs = adam_df[\"Sequence\"].reset_index(drop=True)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "class GenerativeSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        input_seq = seq[:-1]  # all residues except the last\n",
    "        target_seq = seq[1:]  # all residues except the first\n",
    "        length = len(input_seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        input_one_hot = one_hot_torch(input_seq, dtype=self.one_hot_dtype)\n",
    "        target_one_hot = one_hot_torch(target_seq, dtype=self.one_hot_dtype)\n",
    "        # target_indices = torch.tensor([\"ACDEFGHIKLMNPQRSTVWY\".index(res) for res in target_seq], dtype=torch.long)\n",
    "        return input_one_hot, target_one_hot, length\n",
    "\n",
    "def generative_collate_and_pack(batch):\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "\n",
    "    lengths = torch.tensor(lengths)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    targets = [targets[i] for i in sorted_indices]\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    sequences = [seq.T for seq in sequences]  # transpose to [seq_len, features]\n",
    "    targets = [tgt.T for tgt in targets]      # transpose targets as well\n",
    "\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)\n",
    "    padded_targets = pad_sequence(targets, batch_first=False)\n",
    "\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "    packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, packed_target, lengths\n",
    "\n",
    "\n",
    "# Train/val/test split\n",
    "train_seqs, test_seqs = train_test_split(generation_seqs, test_size=0.3, random_state=42)\n",
    "val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = GenerativeSequenceDataset(train_seqs)\n",
    "val_dataset = GenerativeSequenceDataset(val_seqs)\n",
    "test_dataset = GenerativeSequenceDataset(test_seqs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=generative_collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "\n",
    "# Dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\", dataset_sizes)\n",
    "\n",
    "for x, y, l in train_loader:\n",
    "    print(\"Input shape:\", x.data.shape)  # [L, B, 20]\n",
    "    print(\"Target shape:\", y.data.shape)  # [L, B, 20]\n",
    "    print(\"Lengths:\", y.batch_sizes)  # Lengths of sequences in the batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sliding window data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-import required libraries after environment reset\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load only positive (AMP) sequences\n",
    "# adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "\n",
    "# # Clean non-standard amino acids\n",
    "# unique_letters = set(''.join(adam_df[\"Sequence\"]))\n",
    "# amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "# # non_standard_amino_acids = set(unique_letters) - set(amino_acids)\n",
    "# # adam_df = adam_df[~adam_df[\"Sequence\"].str.contains('|'.join(non_standard_amino_acids))]\n",
    "\n",
    "# # Apply sliding window to generate fragments\n",
    "# def generate_fragments(sequences, window_size=15, stride=5):\n",
    "#     fragments = []\n",
    "#     for seq in sequences:\n",
    "#         for start in range(0, len(seq) - window_size + 1, stride):\n",
    "#             fragment = seq[start:start + window_size]\n",
    "#             fragments.append(fragment)\n",
    "#     return fragments\n",
    "\n",
    "# generation_fragments = generate_fragments(adam_df[\"Sequence\"].tolist())\n",
    "\n",
    "# # Define one-hot encoding function\n",
    "# def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "#     amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "#     seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "#     aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "#     arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "#     for i, aa in enumerate(aa_bytes):\n",
    "#         arr[i, seq_bytes == aa] = 1\n",
    "#     return arr\n",
    "\n",
    "# # Dataset using one-hot encoding for generative modeling\n",
    "# class AMPGenerationOneHotDataset(Dataset):\n",
    "#     def __init__(self, sequences):\n",
    "#         self.sequences = sequences\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         seq = self.sequences[idx]\n",
    "#         input_seq = seq[:-1]  # all residues except the last\n",
    "#         target_seq = seq[1:]  # all residues except the first\n",
    "#         input_one_hot = one_hot_torch(input_seq)  # shape: [20, seq_len - 1]\n",
    "#         target_one_hot = one_hot_torch(target_seq)  # shape: [20, seq_len - 1]\n",
    "#         length = input_one_hot.shape[1]\n",
    "#         return input_one_hot, target_one_hot, length\n",
    "\n",
    "# # Collate function for packing one-hot sequences\n",
    "# def collate_and_pack_for_generation(batch):\n",
    "#     sequences, targets, lengths = zip(*batch)\n",
    "#     lengths = torch.tensor(lengths)\n",
    "\n",
    "#     # Sort by length (required for packing)\n",
    "#     sorted_indices = torch.argsort(lengths, descending=True)\n",
    "#     sequences = [sequences[i] for i in sorted_indices]\n",
    "#     targets = [targets[i] for i in sorted_indices]\n",
    "#     lengths = lengths[sorted_indices]\n",
    "\n",
    "#     # Transpose each to [L, 20] and pad\n",
    "#     sequences = [seq.T for seq in sequences]  # from [20, L] to [L, 20]\n",
    "#     targets = [tgt.T for tgt in targets]      # from [20, L] to [L, 20]\n",
    "\n",
    "#     padded_seqs = pad_sequence(sequences, batch_first=False)  # [L, B, 20]\n",
    "#     padded_targets = pad_sequence(targets, batch_first=False)  # [L, B, 20]\n",
    "\n",
    "#     packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "#     packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "\n",
    "#     return packed_input, packed_target, lengths\n",
    "\n",
    "# # Train/val/test split\n",
    "# train_seqs, test_seqs = train_test_split(generation_fragments, test_size=0.3, random_state=42)\n",
    "# val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "\n",
    "# train_dataset = AMPGenerationOneHotDataset(train_seqs)\n",
    "# val_dataset = AMPGenerationOneHotDataset(val_seqs)\n",
    "# test_dataset = AMPGenerationOneHotDataset(test_seqs)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_and_pack_for_generation)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_and_pack_for_generation)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_and_pack_for_generation)\n",
    "\n",
    "# # Dataset sizes for verification\n",
    "# dataset_sizes = {\n",
    "#     \"Train\": len(train_dataset),\n",
    "#     \"Validation\": len(val_dataset),\n",
    "#     \"Test\": len(test_dataset)\n",
    "# }\n",
    "# print(\"Dataset sizes:\", dataset_sizes)\n",
    "\n",
    "# for x, y, l in train_loader:\n",
    "#     print(\"Input shape:\", x.data.shape)  # [L, B, 20]\n",
    "#     print(\"Target shape:\", y.data.shape)  # [L, B, 20]\n",
    "#     print(\"Lengths:\", y.batch_sizes)  # Lengths of sequences in the batch\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=128, num_layers=1, dropout=0.3, output_dim=20):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        dropped = self.dropout(packed_output.data)\n",
    "        logits = self.fc(dropped)\n",
    "        return logits  # shape: [total_timesteps, 20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GenerativeLSTM()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and eval functions\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for packed_input, packed_target, _ in dataloader:\n",
    "        inputs = packed_input.to(device)\n",
    "        targets = torch.argmax(packed_target.data, dim=1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, packed_target, _ in dataloader:\n",
    "            inputs = packed_input.to(device)\n",
    "            targets = torch.argmax(packed_target.data, dim=1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Final test evaluation\n",
    "test_loss = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "class GenerativeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=128, num_layers=1, dropout=0.3):\n",
    "        super(GenerativeLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle packed input\n",
    "        if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n",
    "            packed_output, _ = self.lstm(x)\n",
    "            unpacked_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            return self.fc(unpacked_output)\n",
    "        else:\n",
    "            out, _ = self.lstm(x)\n",
    "            return self.fc(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General AMP - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 10:15:27,715] A new study created in memory with name: no-name-f1f9add1-e643-4a26-a9ec-34bc8c6709d0\n",
      "[I 2025-05-01 10:16:06,930] Trial 0 finished with value: 0.9415398016571999 and parameters: {'hidden_dim': 188, 'num_layers': 3, 'dropout': 0.1784113931084026, 'lr': 0.003916594127886763, 'weight_decay': 0.0005862332247206099}. Best is trial 0 with value: 0.9415398016571999.\n",
      "[I 2025-05-01 10:16:46,849] Trial 1 finished with value: 2.5822975635528564 and parameters: {'hidden_dim': 197, 'num_layers': 3, 'dropout': 0.2848177027449981, 'lr': 0.0006319414902667608, 'weight_decay': 0.0006788940039732188}. Best is trial 0 with value: 0.9415398016571999.\n",
      "[I 2025-05-01 10:17:30,858] Trial 2 finished with value: 2.237904667854309 and parameters: {'hidden_dim': 233, 'num_layers': 3, 'dropout': 0.3197777425446538, 'lr': 0.001122873472308211, 'weight_decay': 0.0002095358074265867}. Best is trial 0 with value: 0.9415398016571999.\n",
      "[I 2025-05-01 10:18:13,597] Trial 3 finished with value: 0.4089527167379856 and parameters: {'hidden_dim': 217, 'num_layers': 3, 'dropout': 0.3776399249751917, 'lr': 0.007053702882194899, 'weight_decay': 0.0002456620169779992}. Best is trial 3 with value: 0.4089527167379856.\n",
      "[I 2025-05-01 10:18:42,241] Trial 4 finished with value: 0.4070202708244324 and parameters: {'hidden_dim': 95, 'num_layers': 1, 'dropout': 0.3783676507844047, 'lr': 0.006661918011336966, 'weight_decay': 0.00027443012783422144}. Best is trial 4 with value: 0.4070202708244324.\n",
      "[I 2025-05-01 10:19:18,757] Trial 5 finished with value: 0.4712855666875839 and parameters: {'hidden_dim': 163, 'num_layers': 3, 'dropout': 0.2864766992449764, 'lr': 0.006248042934446088, 'weight_decay': 0.0009890973602234175}. Best is trial 4 with value: 0.4070202708244324.\n",
      "[I 2025-05-01 10:20:01,087] Trial 6 finished with value: 0.9271863326430321 and parameters: {'hidden_dim': 214, 'num_layers': 3, 'dropout': 0.43832404678111603, 'lr': 0.0040854545579036205, 'weight_decay': 0.0001181940338985567}. Best is trial 4 with value: 0.4070202708244324.\n",
      "[I 2025-05-01 10:20:37,447] Trial 7 finished with value: 0.2649224251508713 and parameters: {'hidden_dim': 115, 'num_layers': 2, 'dropout': 0.28440357323785054, 'lr': 0.008945663493539245, 'weight_decay': 0.0007176906714228661}. Best is trial 7 with value: 0.2649224251508713.\n",
      "[I 2025-05-01 10:21:12,059] Trial 8 finished with value: 1.121833175420761 and parameters: {'hidden_dim': 91, 'num_layers': 1, 'dropout': 0.13693750258578855, 'lr': 0.0035212741552060776, 'weight_decay': 0.000538661353772583}. Best is trial 7 with value: 0.2649224251508713.\n",
      "[I 2025-05-01 10:21:46,300] Trial 9 finished with value: 0.22188957780599594 and parameters: {'hidden_dim': 100, 'num_layers': 2, 'dropout': 0.1494929554458434, 'lr': 0.009759749055557456, 'weight_decay': 0.00016234831853182585}. Best is trial 9 with value: 0.22188957780599594.\n",
      "[I 2025-05-01 10:22:19,651] Trial 10 finished with value: 0.25011663883924484 and parameters: {'hidden_dim': 131, 'num_layers': 2, 'dropout': 0.20343867253126294, 'lr': 0.009082430977798893, 'weight_decay': 5.059778582166282e-05}. Best is trial 9 with value: 0.22188957780599594.\n",
      "[I 2025-05-01 10:22:52,358] Trial 11 finished with value: 0.23038380965590477 and parameters: {'hidden_dim': 134, 'num_layers': 2, 'dropout': 0.19627815948000318, 'lr': 0.00969752369931221, 'weight_decay': 3.5826979874727994e-06}. Best is trial 9 with value: 0.22188957780599594.\n",
      "[I 2025-05-01 10:23:26,303] Trial 12 finished with value: 0.2505199909210205 and parameters: {'hidden_dim': 141, 'num_layers': 2, 'dropout': 0.10083559146490016, 'lr': 0.009309206167988231, 'weight_decay': 0.00035490157942941756}. Best is trial 9 with value: 0.22188957780599594.\n",
      "[I 2025-05-01 10:24:01,807] Trial 13 finished with value: 0.2180823851376772 and parameters: {'hidden_dim': 71, 'num_layers': 2, 'dropout': 0.2090149412354028, 'lr': 0.009865638191669987, 'weight_decay': 1.8872353298433333e-05}. Best is trial 13 with value: 0.2180823851376772.\n",
      "[I 2025-05-01 10:24:32,509] Trial 14 finished with value: 0.31580497696995735 and parameters: {'hidden_dim': 81, 'num_layers': 1, 'dropout': 0.22496241032401176, 'lr': 0.007849353698517693, 'weight_decay': 0.0003680749664314482}. Best is trial 13 with value: 0.2180823851376772.\n",
      "[I 2025-05-01 10:25:09,074] Trial 15 finished with value: 0.3153325952589512 and parameters: {'hidden_dim': 66, 'num_layers': 2, 'dropout': 0.14474560535402953, 'lr': 0.007916632524880207, 'weight_decay': 0.00013447916797347856}. Best is trial 13 with value: 0.2180823851376772.\n",
      "[I 2025-05-01 10:25:42,022] Trial 16 finished with value: 0.6107366681098938 and parameters: {'hidden_dim': 67, 'num_layers': 2, 'dropout': 0.23512857057933612, 'lr': 0.005591121139251066, 'weight_decay': 0.00039439394749100904}. Best is trial 13 with value: 0.2180823851376772.\n",
      "[I 2025-05-01 10:26:11,899] Trial 17 finished with value: 0.21757740899920464 and parameters: {'hidden_dim': 106, 'num_layers': 1, 'dropout': 0.14740305321048008, 'lr': 0.009969866325800866, 'weight_decay': 5.187258217307484e-06}. Best is trial 17 with value: 0.21757740899920464.\n",
      "[I 2025-05-01 10:26:39,359] Trial 18 finished with value: 0.3090316243469715 and parameters: {'hidden_dim': 116, 'num_layers': 1, 'dropout': 0.10531126160083544, 'lr': 0.008211156513102391, 'weight_decay': 0.0008715172248464653}. Best is trial 17 with value: 0.21757740899920464.\n",
      "[I 2025-05-01 10:27:09,302] Trial 19 finished with value: 1.3747986406087875 and parameters: {'hidden_dim': 153, 'num_layers': 1, 'dropout': 0.24355637517227083, 'lr': 0.0027479669383470486, 'weight_decay': 8.452191220006338e-06}. Best is trial 17 with value: 0.21757740899920464.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dim': 106, 'num_layers': 1, 'dropout': 0.14740305321048008, 'lr': 0.009969866325800866, 'weight_decay': 5.187258217307484e-06}\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary packages after reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Criterion\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss on the last time step of each sequence.\n",
    "    \n",
    "    Args:\n",
    "        output: Tensor of shape [B, L, vocab_size]\n",
    "        target_seq: Tensor of shape [B, L] containing target class indices\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss computed only on the last token of each sequence\n",
    "    \"\"\"\n",
    "    # Get last time step for each sequence\n",
    "    last_token_logits = output[:, -1, :]        # [B, vocab_size]\n",
    "    last_token_targets = target_seq[:, -1, :]      # [B]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "    # print('last_token_logits',last_token_logits.shape)\n",
    "    # print('last_token_targets',last_token_targets.shape)\n",
    "\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen/AMP_LSTM_GEN_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])      # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            # print('target_shape before reshape',target_seq.shape)\n",
    "            # target_seq = target_seq.reshape(-1)\n",
    "            # print(f\"Output shape: {output.shape}, Target shape: {target_seq.shape}\")\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_generator.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])  # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # # target_seq = target_seq.reshape(-1)\n",
    "            # # target_seq = target_seq.reshape(-1, target_seq.shape[-1])\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            # assert output.size(0) == target_seq.size(0), f\"Mismatch: {output.size(0)} vs {target_seq.size(0)}\"\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            # print('loss done')\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            preds = output[:, -1, :]        # shape: [B, vocab_size]\n",
    "            preds = torch.argmax(preds, dim=1)  # shape: [B]\n",
    "\n",
    "            targets = target_seq[:, -1, :]      # shape: [B, vocab_size]\n",
    "            targets = torch.argmax(targets, dim=-1)  # shape: [B]\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Objective for Optuna tuning\n",
    "def objective_generation(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "\n",
    "lstm_gen_best_params = study.best_trial.params\n",
    "print(lstm_gen_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_gen_best_params = {'hidden_dim': 106, 'num_layers': 1, 'dropout': 0.14740305321048008, 'lr': 0.009969866325800866, 'weight_decay': 5.187258217307484e-06}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir runs-lstm-gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.7806 | Val Loss = 2.4566 | Acc = 0.9832 | AUC = undefined | Perplexity = 11.6652\n",
      "Epoch 2: Train Loss = 2.1617 | Val Loss = 1.8715 | Acc = 0.9832 | AUC = undefined | Perplexity = 6.4982\n",
      "Epoch 3: Train Loss = 1.6136 | Val Loss = 1.3766 | Acc = 0.9832 | AUC = undefined | Perplexity = 3.9615\n",
      "Epoch 4: Train Loss = 1.1763 | Val Loss = 0.9982 | Acc = 0.9832 | AUC = undefined | Perplexity = 2.7134\n",
      "Epoch 5: Train Loss = 0.8453 | Val Loss = 0.7492 | Acc = 0.9832 | AUC = undefined | Perplexity = 2.1154\n",
      "Epoch 6: Train Loss = 0.6172 | Val Loss = 0.5569 | Acc = 0.9832 | AUC = undefined | Perplexity = 1.7452\n",
      "Epoch 7: Train Loss = 0.4617 | Val Loss = 0.4207 | Acc = 0.9853 | AUC = undefined | Perplexity = 1.5230\n",
      "Epoch 8: Train Loss = 0.3594 | Val Loss = 0.3449 | Acc = 0.9853 | AUC = undefined | Perplexity = 1.4118\n",
      "Epoch 9: Train Loss = 0.2935 | Val Loss = 0.2825 | Acc = 0.9853 | AUC = undefined | Perplexity = 1.3265\n",
      "Epoch 10: Train Loss = 0.2424 | Val Loss = 0.2422 | Acc = 0.9832 | AUC = undefined | Perplexity = 1.2741\n",
      "Epoch 11: Train Loss = 0.2175 | Val Loss = 0.2230 | Acc = 0.9832 | AUC = undefined | Perplexity = 1.2498\n",
      "Epoch 12: Train Loss = 0.1880 | Val Loss = 0.2051 | Acc = 0.9853 | AUC = undefined | Perplexity = 1.2276\n",
      "Epoch 13: Train Loss = 0.1668 | Val Loss = 0.1859 | Acc = 0.9853 | AUC = undefined | Perplexity = 1.2043\n",
      "Epoch 14: Train Loss = 0.1530 | Val Loss = 0.1730 | Acc = 0.9832 | AUC = undefined | Perplexity = 1.1889\n",
      "Epoch 15: Train Loss = 0.1291 | Val Loss = 0.1694 | Acc = 0.9832 | AUC = undefined | Perplexity = 1.1846\n",
      "Epoch 16: Train Loss = 0.1202 | Val Loss = 0.1540 | Acc = 0.9832 | AUC = undefined | Perplexity = 1.1665\n",
      "Epoch 17: Train Loss = 0.1096 | Val Loss = 0.1470 | Acc = 0.9832 | AUC = undefined | Perplexity = 1.1584\n",
      "Epoch 18: Train Loss = 0.1038 | Val Loss = 0.1434 | Acc = 0.9832 | AUC = undefined | Perplexity = 1.1542\n",
      "Epoch 19: Train Loss = 0.0947 | Val Loss = 0.1357 | Acc = 0.9832 | AUC = undefined | Perplexity = 1.1453\n",
      "Epoch 20: Train Loss = 0.0890 | Val Loss = 0.1292 | Acc = 0.9832 | AUC = undefined | Perplexity = 1.1380\n",
      "\n",
      "✅ Final Test Metrics:\n",
      "Loss = 0.1292, Accuracy = 0.9832, AUC = undefined, Perplexity = 1.1380\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_best_params[\"lr\"], weight_decay=lstm_gen_best_params[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "# lstm_gen_best_params = {'hidden_dim': 133, 'num_layers': 2, 'dropout': 0.10063270147175422, 'lr': 0.003237280156212186, 'weight_decay': 2.2594437829479466e-05}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=20)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\"\\n✅ Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n",
    "torch.save(trained_model.state_dict(), 'best_model_lstm_generator_final.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tb amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'K', 'C', 'Q', 'G', 'X', 'F', 'A', 'D', 'Y', 'P', 'M', 'H', 'E', 'I', 'V', 'L', 'S', 'T', 'W', 'R', 'N'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "# df_filtered = df[\n",
    "#     (df['Sequences'].str.len() >= 10) &\n",
    "#     (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "#     (~df['Sequences'].str.contains('X'))\n",
    "# ]\n",
    "df_filtered = df\n",
    "\n",
    "def split_sequence(seq, chunk_size=20):\n",
    "    return [seq[i:i+chunk_size] for i in range(0, len(seq), chunk_size)]\n",
    "\n",
    "new_rows = []\n",
    "for _, row in df_filtered.iterrows():\n",
    "    seq = row['Sequences']\n",
    "    amp_label = row['AMP']\n",
    "    if len(seq) > 40:\n",
    "        for chunk in split_sequence(seq, 20):\n",
    "            new_rows.append({'Sequences': chunk, 'AMP': amp_label})\n",
    "    else:\n",
    "        new_rows.append({'Sequences': seq, 'AMP': amp_label})\n",
    "\n",
    "df_filtered = pd.DataFrame(new_rows)\n",
    "\n",
    "\n",
    "df_filtered = df_filtered[\n",
    "    (df_filtered['Sequences'].str.len() >= 10) &\n",
    "    (df_filtered['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df_filtered['Sequences'].str.contains('X'))\n",
    "]\n",
    "df_filtered = df_filtered[df_filtered['AMP']==1]\n",
    "df_filtered = df_filtered.drop_duplicates(subset='Sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(min([len(x) for x in df_filtered['Sequences']]))\n",
    "print(max([len(x) for x in df_filtered['Sequences']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Input shape': torch.Size([1203, 20]),\n",
       " 'Target shape': torch.Size([1203, 20]),\n",
       " 'Lengths': tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 55, 53, 46, 42, 40, 39, 39, 34, 34,\n",
       "         34, 25, 25, 23, 22, 21, 15, 11, 10, 10,  9,  7,  6,  6,  6,  6,  3,  3,\n",
       "          2,  1])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import libraries after environment reset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "df = df_filtered\n",
    "\n",
    "# Clean and inspect\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "# non_standard_amino_acids = unique_letters - amino_acids\n",
    "# df = df[~df[\"Sequences\"].str.contains('|'.join(non_standard_amino_acids))]\n",
    "\n",
    "# Extract sequences\n",
    "sequences = df[\"Sequences\"].reset_index(drop=True)\n",
    "\n",
    "# Define one-hot function\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "# Define dataset class\n",
    "class GenerativeSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        input_seq = seq[:-1]\n",
    "        target_seq = seq[1:]\n",
    "        length = len(input_seq.replace(\"X\", \"\"))\n",
    "        input_one_hot = one_hot_torch(input_seq, dtype=self.one_hot_dtype)\n",
    "        target_one_hot = one_hot_torch(target_seq, dtype=self.one_hot_dtype)\n",
    "        return input_one_hot, target_one_hot, length\n",
    "\n",
    "# Define collate function\n",
    "def generative_collate_and_pack(batch):\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    targets = [targets[i] for i in sorted_indices]\n",
    "    lengths = lengths[sorted_indices]\n",
    "    sequences = [seq.T for seq in sequences]\n",
    "    targets = [tgt.T for tgt in targets]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)\n",
    "    padded_targets = pad_sequence(targets, batch_first=False)\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "    packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "    return packed_input, packed_target, lengths\n",
    "\n",
    "# Split and load data\n",
    "train_seqs, test_seqs = train_test_split(sequences, test_size=0.3, random_state=42)\n",
    "val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "train_dataset = GenerativeSequenceDataset(train_seqs)\n",
    "val_dataset = GenerativeSequenceDataset(val_seqs)\n",
    "test_dataset = GenerativeSequenceDataset(test_seqs)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=generative_collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "\n",
    "# Preview batch\n",
    "batch_sample = next(iter(train_loader))\n",
    "batch_sample_shapes = {\n",
    "    \"Input shape\": batch_sample[0].data.shape,\n",
    "    \"Target shape\": batch_sample[1].data.shape,\n",
    "    \"Lengths\": batch_sample[0].batch_sizes\n",
    "}\n",
    "batch_sample_shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 171\n",
      "Validation: 37\n",
      "Test: 37\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train for full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:19:55,547] A new study created in memory with name: no-name-9b82d52b-315f-49f9-a9a9-5218d9c4a7ad\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:20:47,954] Trial 0 finished with value: 0.05580922123044729 and parameters: {'dropout': 0.16137250444161905, 'lr': 0.006821184839221076, 'weight_decay': 0.00017561080126782017}. Best is trial 0 with value: 0.05580922123044729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:21:38,999] Trial 1 finished with value: 0.07151259062811732 and parameters: {'dropout': 0.2281357258980474, 'lr': 0.003554233540590442, 'weight_decay': 0.0004775635364938583}. Best is trial 0 with value: 0.05580922123044729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:22:29,497] Trial 2 finished with value: 0.06283493572846055 and parameters: {'dropout': 0.3382564231642894, 'lr': 0.006296652275817763, 'weight_decay': 0.0009204393829331862}. Best is trial 0 with value: 0.05580922123044729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:23:21,786] Trial 3 finished with value: 0.06758854305371642 and parameters: {'dropout': 0.4943587720542153, 'lr': 0.001464581255892956, 'weight_decay': 0.00016449516026215914}. Best is trial 0 with value: 0.05580922123044729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:24:13,156] Trial 4 finished with value: 0.08432310912758112 and parameters: {'dropout': 0.15163581451791633, 'lr': 0.0005873878373832947, 'weight_decay': 0.0004997069899391908}. Best is trial 0 with value: 0.05580922123044729.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:25:05,761] Trial 5 finished with value: 0.053100173361599445 and parameters: {'dropout': 0.2576315958956391, 'lr': 0.009252915314094317, 'weight_decay': 0.00031734323068074543}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:25:59,607] Trial 6 finished with value: 0.06669122399762273 and parameters: {'dropout': 0.10414419140251759, 'lr': 0.0018215218966184843, 'weight_decay': 0.00021165010912347886}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:26:53,112] Trial 7 finished with value: 0.06706126127392054 and parameters: {'dropout': 0.3693911979617652, 'lr': 0.005460634568352292, 'weight_decay': 0.0005155368426372023}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:27:45,595] Trial 8 finished with value: 0.060398975387215614 and parameters: {'dropout': 0.3803639375351807, 'lr': 0.004894591853007688, 'weight_decay': 0.0006872145869630581}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:28:38,054] Trial 9 finished with value: 0.0717171560972929 and parameters: {'dropout': 0.19387977778283602, 'lr': 0.0030558343383497765, 'weight_decay': 0.00023680260504264667}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:29:33,841] Trial 10 finished with value: 0.05382060678675771 and parameters: {'dropout': 0.25233062823986974, 'lr': 0.009942797154413093, 'weight_decay': 1.053008942079089e-05}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:30:25,718] Trial 11 finished with value: 0.059463777113705873 and parameters: {'dropout': 0.2664666922601705, 'lr': 0.009758181158408883, 'weight_decay': 1.579044533030503e-05}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:31:17,483] Trial 12 finished with value: 0.05704248289112002 and parameters: {'dropout': 0.2841165592620222, 'lr': 0.009909311181013677, 'weight_decay': 4.4136037235088506e-05}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:32:11,270] Trial 13 finished with value: 0.07085083518177271 and parameters: {'dropout': 0.24052347961213183, 'lr': 0.008526777573588228, 'weight_decay': 0.00035267988749535135}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:33:05,159] Trial 14 finished with value: 0.05653113918378949 and parameters: {'dropout': 0.32814414741748715, 'lr': 0.008270792278931982, 'weight_decay': 0.0003481660971995551}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:34:00,289] Trial 15 finished with value: 0.06229461310431361 and parameters: {'dropout': 0.43750116969994557, 'lr': 0.008014231522366848, 'weight_decay': 0.0006949059443235783}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:34:53,572] Trial 16 finished with value: 0.06175466626882553 and parameters: {'dropout': 0.201131982976489, 'lr': 0.008881799414471652, 'weight_decay': 0.0003320709981493193}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:35:47,050] Trial 17 finished with value: 0.060917569790035486 and parameters: {'dropout': 0.2992276981158616, 'lr': 0.007088610964283416, 'weight_decay': 9.190345278821998e-05}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:36:41,585] Trial 18 finished with value: 0.07095807697623968 and parameters: {'dropout': 0.25449697798070703, 'lr': 0.009311996216002795, 'weight_decay': 0.0009831429548891662}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 11:37:33,788] Trial 19 finished with value: 0.07094359816983342 and parameters: {'dropout': 0.394241434335746, 'lr': 0.007543332937379096, 'weight_decay': 0.0006249489292065357}. Best is trial 5 with value: 0.053100173361599445.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best transfer learning hyperparameters: {'dropout': 0.2576315958956391, 'lr': 0.009252915314094317, 'weight_decay': 0.00031734323068074543}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Criterion\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Transfer Learning Loader\n",
    "def load_pretrained_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    last_token_logits = output[:, -1, :]\n",
    "    last_token_targets = target_seq[:, -1, :]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen-tb/AMP_LSTM_GEN_TRANSFER_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # if train:\n",
    "            #     torch.save(model.state_dict(), 'best_model_lstm_transfer.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "    \n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path, max_layers=None):\n",
    "    \"\"\"\n",
    "    Load up to `max_layers` compatible layers from a checkpoint into the model.\n",
    "    If max_layers is None, load all compatible layers.\n",
    "    \"\"\"\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter compatible layers\n",
    "    compatible_items = [\n",
    "        (k, v) for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    ]\n",
    "\n",
    "    # Limit number of layers to load\n",
    "    if max_layers is not None:\n",
    "        compatible_items = compatible_items[:max_layers]\n",
    "\n",
    "    # Convert list of tuples back to dict\n",
    "    compatible_dict = dict(compatible_items)\n",
    "\n",
    "    # Update model state dict\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def freeze_encoder(model, num_layers_to_freeze):\n",
    "    \"\"\"\n",
    "    Freezes the first `num_layers_to_freeze` LSTM layers of the model.\n",
    "    Assumes parameter names follow standard PyTorch LSTM naming.\n",
    "    \"\"\"\n",
    "    if num_layers_to_freeze <= 0:\n",
    "        print(\"⚠️ No LSTM layers frozen.\")\n",
    "        return\n",
    "\n",
    "    layer_prefixes = [f'lstm.weight_ih_l{i}' for i in range(num_layers_to_freeze)]\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(prefix in name for prefix in layer_prefixes):\n",
    "            param.requires_grad = False\n",
    "\n",
    "    print(f\"✅ Frozen first {num_layers_to_freeze} LSTM layers.\")\n",
    "\n",
    "# Optuna objective for fine-tuning\n",
    "def objective_generation(trial):\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(    \n",
    "                hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "                num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "                dropout=dropout\n",
    "                # dropout=lstm_gen_best_params[\"dropout\"]\n",
    "                )\n",
    "    model = load_partial_weights(model, 'best_model_lstm_generator_final.pt', 6)  # path to the general AMP model\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=20, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "lstm_gen_best_params_tb = study.best_trial.params\n",
    "print(\"Best transfer learning hyperparameters:\", lstm_gen_best_params_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "Epoch 1: Train Loss = 2.9824 | Val Loss = 2.9351 | Acc = 0.0270 | AUC = undefined | Perplexity = 18.8228\n",
      "Epoch 2: Train Loss = 2.9245 | Val Loss = 2.8535 | Acc = 1.0000 | AUC = undefined | Perplexity = 17.3477\n",
      "Epoch 3: Train Loss = 2.8669 | Val Loss = 2.8353 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.0356\n",
      "Epoch 4: Train Loss = 2.8081 | Val Loss = 2.7463 | Acc = 1.0000 | AUC = undefined | Perplexity = 15.5846\n",
      "Epoch 5: Train Loss = 2.7636 | Val Loss = 2.6848 | Acc = 1.0000 | AUC = undefined | Perplexity = 14.6560\n",
      "Epoch 6: Train Loss = 2.6948 | Val Loss = 2.6642 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.3566\n",
      "Epoch 7: Train Loss = 2.6443 | Val Loss = 2.6036 | Acc = 0.9730 | AUC = undefined | Perplexity = 13.5118\n",
      "Epoch 8: Train Loss = 2.5905 | Val Loss = 2.5321 | Acc = 1.0000 | AUC = undefined | Perplexity = 12.5796\n",
      "Epoch 9: Train Loss = 2.5280 | Val Loss = 2.4807 | Acc = 1.0000 | AUC = undefined | Perplexity = 11.9492\n",
      "Epoch 10: Train Loss = 2.4709 | Val Loss = 2.4276 | Acc = 1.0000 | AUC = undefined | Perplexity = 11.3315\n",
      "Epoch 11: Train Loss = 2.4380 | Val Loss = 2.3750 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.7506\n",
      "Epoch 12: Train Loss = 2.3616 | Val Loss = 2.3227 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.2035\n",
      "Epoch 13: Train Loss = 2.3262 | Val Loss = 2.2712 | Acc = 1.0000 | AUC = undefined | Perplexity = 9.6909\n",
      "Epoch 14: Train Loss = 2.2815 | Val Loss = 2.2210 | Acc = 1.0000 | AUC = undefined | Perplexity = 9.2164\n",
      "Epoch 15: Train Loss = 2.2033 | Val Loss = 2.1721 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.7771\n",
      "Epoch 16: Train Loss = 2.1598 | Val Loss = 2.1246 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.3693\n",
      "Epoch 17: Train Loss = 2.1161 | Val Loss = 2.0774 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.9834\n",
      "Epoch 18: Train Loss = 2.0607 | Val Loss = 2.0317 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.6271\n",
      "Epoch 19: Train Loss = 2.0155 | Val Loss = 1.9863 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.2887\n",
      "Epoch 20: Train Loss = 1.9632 | Val Loss = 1.9410 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.9656\n",
      "Epoch 21: Train Loss = 1.9129 | Val Loss = 1.8938 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.6444\n",
      "Epoch 22: Train Loss = 1.8627 | Val Loss = 1.8448 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.3270\n",
      "Epoch 23: Train Loss = 1.8180 | Val Loss = 1.7955 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.0223\n",
      "Epoch 24: Train Loss = 1.7691 | Val Loss = 1.7467 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.7359\n",
      "Epoch 25: Train Loss = 1.7206 | Val Loss = 1.6988 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.4672\n",
      "Epoch 26: Train Loss = 1.6962 | Val Loss = 1.6619 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.2694\n",
      "Epoch 27: Train Loss = 1.6440 | Val Loss = 1.6233 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.0697\n",
      "Epoch 28: Train Loss = 1.6058 | Val Loss = 1.5857 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.8828\n",
      "Epoch 29: Train Loss = 1.5665 | Val Loss = 1.5470 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.6973\n",
      "Epoch 30: Train Loss = 1.5322 | Val Loss = 1.5015 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.4886\n",
      "Epoch 31: Train Loss = 1.4743 | Val Loss = 1.4621 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.3150\n",
      "Epoch 32: Train Loss = 1.4390 | Val Loss = 1.4318 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.1863\n",
      "Epoch 33: Train Loss = 1.3912 | Val Loss = 1.3932 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.0279\n",
      "Epoch 34: Train Loss = 1.3524 | Val Loss = 1.3469 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.8456\n",
      "Epoch 35: Train Loss = 1.3145 | Val Loss = 1.3068 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.6944\n",
      "Epoch 36: Train Loss = 1.2947 | Val Loss = 1.2316 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.4268\n",
      "Epoch 37: Train Loss = 1.2494 | Val Loss = 1.2812 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.6011\n",
      "Epoch 38: Train Loss = 1.2344 | Val Loss = 1.2465 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.4780\n",
      "Epoch 39: Train Loss = 1.1789 | Val Loss = 1.1454 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.1435\n",
      "Epoch 40: Train Loss = 1.1536 | Val Loss = 1.1052 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.0197\n",
      "Epoch 41: Train Loss = 1.1403 | Val Loss = 1.1364 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.1156\n",
      "Epoch 42: Train Loss = 1.1330 | Val Loss = 1.0375 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.8221\n",
      "Epoch 43: Train Loss = 1.0731 | Val Loss = 1.0721 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.9215\n",
      "Epoch 44: Train Loss = 1.0419 | Val Loss = 1.0247 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.7864\n",
      "Epoch 45: Train Loss = 1.0502 | Val Loss = 0.9977 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.7120\n",
      "Epoch 46: Train Loss = 0.9846 | Val Loss = 0.9690 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.6353\n",
      "Epoch 47: Train Loss = 0.9450 | Val Loss = 0.9471 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.5782\n",
      "Epoch 48: Train Loss = 0.9221 | Val Loss = 0.9139 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.4940\n",
      "Epoch 49: Train Loss = 0.9000 | Val Loss = 0.8714 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.3903\n",
      "Epoch 50: Train Loss = 0.8688 | Val Loss = 0.8456 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.3294\n",
      "Epoch 51: Train Loss = 0.8563 | Val Loss = 0.8250 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.2818\n",
      "Epoch 52: Train Loss = 0.8170 | Val Loss = 0.8111 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.2505\n",
      "Epoch 53: Train Loss = 0.8032 | Val Loss = 0.7919 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.2077\n",
      "Epoch 54: Train Loss = 0.7788 | Val Loss = 0.7726 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.1653\n",
      "Epoch 55: Train Loss = 0.7587 | Val Loss = 0.7598 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.1379\n",
      "Epoch 56: Train Loss = 0.7690 | Val Loss = 0.7440 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.1043\n",
      "Epoch 57: Train Loss = 0.7121 | Val Loss = 0.7246 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0638\n",
      "Epoch 58: Train Loss = 0.7140 | Val Loss = 0.7044 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0227\n",
      "Epoch 59: Train Loss = 0.6758 | Val Loss = 0.6728 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9598\n",
      "Epoch 60: Train Loss = 0.6522 | Val Loss = 0.6543 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9238\n",
      "Epoch 61: Train Loss = 0.6559 | Val Loss = 0.6389 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.8945\n",
      "Epoch 62: Train Loss = 0.6405 | Val Loss = 0.6563 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9276\n",
      "Epoch 63: Train Loss = 0.6154 | Val Loss = 0.6235 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.8655\n",
      "Epoch 64: Train Loss = 0.5913 | Val Loss = 0.6041 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.8296\n",
      "Epoch 65: Train Loss = 0.5743 | Val Loss = 0.5807 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.7873\n",
      "Epoch 66: Train Loss = 0.5523 | Val Loss = 0.5951 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.8132\n",
      "Epoch 67: Train Loss = 0.5425 | Val Loss = 0.5713 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.7705\n",
      "Epoch 68: Train Loss = 0.5222 | Val Loss = 0.5495 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.7323\n",
      "Epoch 69: Train Loss = 0.5053 | Val Loss = 0.5348 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.7072\n",
      "Epoch 70: Train Loss = 0.4948 | Val Loss = 0.5370 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.7108\n",
      "Epoch 71: Train Loss = 0.4929 | Val Loss = 0.5153 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.6742\n",
      "Epoch 72: Train Loss = 0.4677 | Val Loss = 0.5055 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.6578\n",
      "Epoch 73: Train Loss = 0.4928 | Val Loss = 0.5006 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.6498\n",
      "Epoch 74: Train Loss = 0.4914 | Val Loss = 0.5056 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.6580\n",
      "Epoch 75: Train Loss = 0.4525 | Val Loss = 0.4899 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.6322\n",
      "Epoch 76: Train Loss = 0.4635 | Val Loss = 0.4756 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.6090\n",
      "Epoch 77: Train Loss = 0.4264 | Val Loss = 0.4787 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.6140\n",
      "Epoch 78: Train Loss = 0.4110 | Val Loss = 0.4389 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.5509\n",
      "Epoch 79: Train Loss = 0.3951 | Val Loss = 0.4449 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.5603\n",
      "Epoch 80: Train Loss = 0.4177 | Val Loss = 0.4317 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.5399\n",
      "Epoch 81: Train Loss = 0.3761 | Val Loss = 0.4904 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.6329\n",
      "Epoch 82: Train Loss = 0.3690 | Val Loss = 0.4217 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.5245\n",
      "Epoch 83: Train Loss = 0.3758 | Val Loss = 0.4833 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.6214\n",
      "Epoch 84: Train Loss = 0.4193 | Val Loss = 0.4066 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.5016\n",
      "Epoch 85: Train Loss = 0.3538 | Val Loss = 0.3658 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.4416\n",
      "Epoch 86: Train Loss = 0.3910 | Val Loss = 0.3561 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.4278\n",
      "Epoch 87: Train Loss = 0.3511 | Val Loss = 0.3464 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.4139\n",
      "Epoch 88: Train Loss = 0.3601 | Val Loss = 0.3490 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.4177\n",
      "Epoch 89: Train Loss = 0.3730 | Val Loss = 0.3619 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.4361\n",
      "Epoch 90: Train Loss = 0.3477 | Val Loss = 0.3507 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.4200\n",
      "Epoch 91: Train Loss = 0.3447 | Val Loss = 0.3402 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.4052\n",
      "Epoch 92: Train Loss = 0.3159 | Val Loss = 0.3303 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3914\n",
      "Epoch 93: Train Loss = 0.3275 | Val Loss = 0.3150 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3703\n",
      "Epoch 94: Train Loss = 0.3287 | Val Loss = 0.3063 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.3584\n",
      "Epoch 95: Train Loss = 0.3054 | Val Loss = 0.3146 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3697\n",
      "Epoch 96: Train Loss = 0.3582 | Val Loss = 0.3198 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3768\n",
      "Epoch 97: Train Loss = 0.3317 | Val Loss = 0.3129 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3674\n",
      "Epoch 98: Train Loss = 0.3114 | Val Loss = 0.3061 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3581\n",
      "Epoch 99: Train Loss = 0.2837 | Val Loss = 0.2914 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3384\n",
      "Epoch 100: Train Loss = 0.2911 | Val Loss = 0.2807 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.3241\n",
      "Epoch 101: Train Loss = 0.2986 | Val Loss = 0.2746 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.3160\n",
      "Epoch 102: Train Loss = 0.2698 | Val Loss = 0.2718 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.3123\n",
      "Epoch 103: Train Loss = 0.3720 | Val Loss = 0.2731 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3140\n",
      "Epoch 104: Train Loss = 0.2734 | Val Loss = 0.2817 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3254\n",
      "Epoch 105: Train Loss = 0.2887 | Val Loss = 0.2776 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3199\n",
      "Epoch 106: Train Loss = 0.2636 | Val Loss = 0.2755 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3171\n",
      "Epoch 107: Train Loss = 0.2659 | Val Loss = 0.2655 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3040\n",
      "Epoch 108: Train Loss = 0.2651 | Val Loss = 0.2647 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3030\n",
      "Epoch 109: Train Loss = 0.2526 | Val Loss = 0.2525 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2872\n",
      "Epoch 110: Train Loss = 0.2515 | Val Loss = 0.2478 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2812\n",
      "Epoch 111: Train Loss = 0.2439 | Val Loss = 0.2405 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2719\n",
      "Epoch 112: Train Loss = 0.2669 | Val Loss = 0.2416 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2733\n",
      "Epoch 113: Train Loss = 0.2542 | Val Loss = 0.2477 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2811\n",
      "Epoch 114: Train Loss = 0.2607 | Val Loss = 0.2611 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2984\n",
      "Epoch 115: Train Loss = 0.2614 | Val Loss = 0.2481 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2816\n",
      "Epoch 116: Train Loss = 0.2238 | Val Loss = 0.2364 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2667\n",
      "Epoch 117: Train Loss = 0.2380 | Val Loss = 0.2333 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2627\n",
      "Epoch 118: Train Loss = 0.2727 | Val Loss = 0.2418 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2735\n",
      "Epoch 119: Train Loss = 0.2148 | Val Loss = 0.2397 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2708\n",
      "Epoch 120: Train Loss = 0.2106 | Val Loss = 0.2334 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2628\n",
      "Epoch 121: Train Loss = 0.2097 | Val Loss = 0.2078 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2310\n",
      "Epoch 122: Train Loss = 0.2060 | Val Loss = 0.2209 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2472\n",
      "Epoch 123: Train Loss = 0.2046 | Val Loss = 0.2296 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2582\n",
      "Epoch 124: Train Loss = 0.2073 | Val Loss = 0.2053 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2279\n",
      "Epoch 125: Train Loss = 0.2108 | Val Loss = 0.2105 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2343\n",
      "Epoch 126: Train Loss = 0.1910 | Val Loss = 0.2404 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2717\n",
      "Epoch 127: Train Loss = 0.2097 | Val Loss = 0.2286 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2569\n",
      "Epoch 128: Train Loss = 0.1838 | Val Loss = 0.2237 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2507\n",
      "Epoch 129: Train Loss = 0.1801 | Val Loss = 0.2253 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2527\n",
      "Epoch 130: Train Loss = 0.1790 | Val Loss = 0.2521 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2867\n",
      "Epoch 131: Train Loss = 0.1832 | Val Loss = 0.1989 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2200\n",
      "Epoch 132: Train Loss = 0.2057 | Val Loss = 0.2177 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2433\n",
      "Epoch 133: Train Loss = 0.2060 | Val Loss = 0.2159 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2410\n",
      "Epoch 134: Train Loss = 0.1845 | Val Loss = 0.2020 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2239\n",
      "Epoch 135: Train Loss = 0.1984 | Val Loss = 0.1837 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2016\n",
      "Epoch 136: Train Loss = 0.2052 | Val Loss = 0.1888 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2078\n",
      "Epoch 137: Train Loss = 0.1850 | Val Loss = 0.2059 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2287\n",
      "Epoch 138: Train Loss = 0.1685 | Val Loss = 0.2067 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2296\n",
      "Epoch 139: Train Loss = 0.1624 | Val Loss = 0.2114 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2354\n",
      "Epoch 140: Train Loss = 0.1944 | Val Loss = 0.2252 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2526\n",
      "Epoch 141: Train Loss = 0.1499 | Val Loss = 0.1954 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2158\n",
      "Epoch 142: Train Loss = 0.1547 | Val Loss = 0.1919 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2116\n",
      "Epoch 143: Train Loss = 0.1966 | Val Loss = 0.2030 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2251\n",
      "Epoch 144: Train Loss = 0.1654 | Val Loss = 0.1989 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2201\n",
      "Epoch 145: Train Loss = 0.1573 | Val Loss = 0.1902 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2095\n",
      "Epoch 146: Train Loss = 0.1641 | Val Loss = 0.1667 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1814\n",
      "Epoch 147: Train Loss = 0.1749 | Val Loss = 0.1635 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1777\n",
      "Epoch 148: Train Loss = 0.1637 | Val Loss = 0.1754 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1918\n",
      "Epoch 149: Train Loss = 0.1808 | Val Loss = 0.1923 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2120\n",
      "Epoch 150: Train Loss = 0.1619 | Val Loss = 0.1800 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1972\n",
      "Epoch 151: Train Loss = 0.1573 | Val Loss = 0.1749 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1912\n",
      "Epoch 152: Train Loss = 0.1528 | Val Loss = 0.1728 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1886\n",
      "Epoch 153: Train Loss = 0.1587 | Val Loss = 0.1811 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1986\n",
      "Epoch 154: Train Loss = 0.1609 | Val Loss = 0.2117 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2358\n",
      "Epoch 155: Train Loss = 0.1438 | Val Loss = 0.1917 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2113\n",
      "Epoch 156: Train Loss = 0.1433 | Val Loss = 0.1783 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1952\n",
      "Epoch 157: Train Loss = 0.1394 | Val Loss = 0.1902 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2095\n",
      "Epoch 158: Train Loss = 0.1389 | Val Loss = 0.1778 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1946\n",
      "Epoch 159: Train Loss = 0.1393 | Val Loss = 0.1874 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2062\n",
      "Epoch 160: Train Loss = 0.1340 | Val Loss = 0.1873 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2059\n",
      "Epoch 161: Train Loss = 0.1412 | Val Loss = 0.1741 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1902\n",
      "Epoch 162: Train Loss = 0.1365 | Val Loss = 0.1570 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1701\n",
      "Epoch 163: Train Loss = 0.1322 | Val Loss = 0.1619 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1757\n",
      "Epoch 164: Train Loss = 0.1342 | Val Loss = 0.1874 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2061\n",
      "Epoch 165: Train Loss = 0.1292 | Val Loss = 0.1995 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2208\n",
      "Epoch 166: Train Loss = 0.1223 | Val Loss = 0.2019 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2237\n",
      "Epoch 167: Train Loss = 0.1154 | Val Loss = 0.2035 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2257\n",
      "Epoch 168: Train Loss = 0.1452 | Val Loss = 0.1986 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2196\n",
      "Epoch 169: Train Loss = 0.1205 | Val Loss = 0.2088 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2322\n",
      "Epoch 170: Train Loss = 0.1318 | Val Loss = 0.2081 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2314\n",
      "Epoch 171: Train Loss = 0.1171 | Val Loss = 0.2021 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2240\n",
      "Epoch 172: Train Loss = 0.1090 | Val Loss = 0.2303 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2590\n",
      "Epoch 173: Train Loss = 0.1483 | Val Loss = 0.2264 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2541\n",
      "Epoch 174: Train Loss = 0.1101 | Val Loss = 0.1251 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1333\n",
      "Epoch 175: Train Loss = 0.1235 | Val Loss = 0.1029 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1084\n",
      "Epoch 176: Train Loss = 0.1231 | Val Loss = 0.2595 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2963\n",
      "Epoch 177: Train Loss = 0.1532 | Val Loss = 0.0996 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1047\n",
      "Epoch 178: Train Loss = 0.1407 | Val Loss = 0.2503 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2844\n",
      "Epoch 179: Train Loss = 0.1789 | Val Loss = 0.2309 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2598\n",
      "Epoch 180: Train Loss = 0.1745 | Val Loss = 0.1902 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2095\n",
      "Epoch 181: Train Loss = 0.2033 | Val Loss = 0.2259 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2535\n",
      "Epoch 182: Train Loss = 0.1414 | Val Loss = 0.2287 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2570\n",
      "Epoch 183: Train Loss = 0.1183 | Val Loss = 0.2279 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2560\n",
      "Epoch 184: Train Loss = 0.1165 | Val Loss = 0.2239 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2509\n",
      "Epoch 185: Train Loss = 0.1169 | Val Loss = 0.2100 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2337\n",
      "Epoch 186: Train Loss = 0.1251 | Val Loss = 0.2040 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2263\n",
      "Epoch 187: Train Loss = 0.1103 | Val Loss = 0.2183 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2440\n",
      "Epoch 188: Train Loss = 0.1141 | Val Loss = 0.2223 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2490\n",
      "Epoch 189: Train Loss = 0.1521 | Val Loss = 0.2073 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2303\n",
      "Epoch 190: Train Loss = 0.1425 | Val Loss = 0.2044 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2267\n",
      "Epoch 191: Train Loss = 0.1012 | Val Loss = 0.2158 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2409\n",
      "Epoch 192: Train Loss = 0.1141 | Val Loss = 0.2166 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2419\n",
      "Epoch 193: Train Loss = 0.0950 | Val Loss = 0.2103 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2340\n",
      "Epoch 194: Train Loss = 0.1185 | Val Loss = 0.2065 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2294\n",
      "Epoch 195: Train Loss = 0.1559 | Val Loss = 0.2081 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2313\n",
      "Epoch 196: Train Loss = 0.1002 | Val Loss = 0.2066 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2295\n",
      "Epoch 197: Train Loss = 0.1288 | Val Loss = 0.2005 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2220\n",
      "Epoch 198: Train Loss = 0.1064 | Val Loss = 0.1763 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1928\n",
      "Epoch 199: Train Loss = 0.1002 | Val Loss = 0.1919 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2116\n",
      "Epoch 200: Train Loss = 0.0984 | Val Loss = 0.2167 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2420\n",
      " Final Test Metrics:\n",
      "Loss = 0.2167, Accuracy = 0.9730, AUC = undefined, Perplexity = 1.2420\n",
      "Model saved to final_amp_generator_lstm.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_best_params_tb[\"lr\"], weight_decay=lstm_gen_best_params_tb[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "# lstm_gen_best_params = {'hidden_dim': 133, 'num_layers': 2}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params_tb[\"dropout\"],\n",
    "    # weights_decay=lstm_gen_best_params_tb[\"weight_decay\"],\n",
    "    # lr=lstm_gen_best_params_tb[\"lr\"]\n",
    ")\n",
    "final_model = load_partial_weights(final_model, 'best_model_lstm_generator_final.pt', 4)  # path to the general AMP model\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=200)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\" Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n",
    "# Save model weights\n",
    "torch.save(trained_model.state_dict(), \"final_amp_generator_lstm.pt\")\n",
    "print(\"Model saved to final_amp_generator_lstm.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Recreate your amino acid vocab\n",
    "aa_vocab = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aa_to_idx = {aa: i for i, aa in enumerate(aa_vocab)}\n",
    "idx_to_aa = {i: aa for aa, i in aa_to_idx.items()}\n",
    "\n",
    "def one_hot_encode_amino_acid(aa, vocab=aa_vocab):\n",
    "    vec = torch.zeros(len(vocab))\n",
    "    vec[aa_to_idx[aa]] = 1.0\n",
    "    return vec\n",
    "\n",
    "def generate_sequence_from_seed(model, seed, max_length=30, temperature=1.0, device='cpu'):\n",
    "    model.eval()\n",
    "    input_seq = [one_hot_encode_amino_acid(aa).to(device) for aa in seed]\n",
    "    input_tensor = torch.stack(input_seq).unsqueeze(0)  # [1, L, 20]\n",
    "\n",
    "    generated = seed.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(seed)):\n",
    "            output = model(input_tensor)  # [1, L, vocab]\n",
    "            logits = output[0, -1, :]  # Last time step → [vocab]\n",
    "\n",
    "            # Apply temperature and sample\n",
    "            probs = F.softmax(logits / temperature, dim=-1).cpu().numpy()\n",
    "            next_idx = np.random.choice(len(aa_vocab), p=probs)\n",
    "            next_aa = idx_to_aa[next_idx]\n",
    "\n",
    "            # Update sequence\n",
    "            next_aa_vec = one_hot_encode_amino_acid(next_aa).to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, 20]\n",
    "            input_tensor = torch.cat([input_tensor, next_aa_vec], dim=1)\n",
    "            generated.append(next_aa)\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class LengthSampler:\n",
    "    def __init__(self, sequence_lengths):\n",
    "        \"\"\"\n",
    "        Initialize sampler from observed sequence lengths.\n",
    "        \n",
    "        Args:\n",
    "            sequence_lengths (list[int]): List of sequence lengths (e.g., [20, 21, 20, 23, ...])\n",
    "        \"\"\"\n",
    "        self.length_counts = Counter(sequence_lengths)\n",
    "        self.lengths = np.array(sorted(self.length_counts.keys()))\n",
    "        counts = np.array([self.length_counts[l] for l in self.lengths])\n",
    "        self.probs = counts / counts.sum()  # Empirical probabilities\n",
    "\n",
    "    def sample(self, n=1):\n",
    "        \"\"\"\n",
    "        Sample one or more lengths based on the learned distribution.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray of sampled lengths\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.lengths, size=n, p=self.probs)\n",
    "length_sampler = LengthSampler([len(seq) for seq in df.loc[df['AMP'] == 1, :]['Sequences']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: ESKLDGDYRYDYYKHH\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    # dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "gen_model.eval()\n",
    "# Define a seed and generate a sequence\n",
    "sampled_length = length_sampler.sample()[0]\n",
    "start_aa = sample_start_amino_acid()\n",
    "seed_sequence = list(start_aa)  # start with valine\n",
    "generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1.0, device=device)\n",
    "\n",
    "print(\"Generated AMP sequence:\", generated_peptide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: ESKLDGDYRYDYYKHH\n",
      "Generated AMP sequence: AAAAGAGGGLGPPD\n",
      "Generated AMP sequence: KAAVYPGDPPGPDYLWPPPK\n",
      "Generated AMP sequence: IATSKYDHDNPLYPLPHYDYYKAYYYYDPHYSPDPPYSY\n",
      "Generated AMP sequence: IAHNLSPPPDGDSPPYKPY\n",
      "Generated AMP sequence: FAAATLKRPHYP\n",
      "Generated AMP sequence: EMAADKSYAPLKHPYPPYPYYLPPND\n",
      "Generated AMP sequence: VAAAQCGGYGPSKYPSSPEY\n",
      "Generated AMP sequence: DAAGKAKHPHSPYH\n",
      "Generated AMP sequence: WASSENVPPKEYYS\n",
      "Generated AMP sequence: QAGSTQPDHYRCHT\n",
      "Generated AMP sequence: CAAKKGPIKY\n",
      "Generated AMP sequence: ADAGAGGHYNYSYPPPKYMD\n",
      "Generated AMP sequence: DATWVKAYLYYYLPYPHPYY\n",
      "Generated AMP sequence: HADYAKYSSSPLYYYYPPYS\n",
      "Generated AMP sequence: IDSGKDPDPPLQDDYPHPYK\n",
      "Generated AMP sequence: TAAGKKPYPLYLPDDYYYPH\n",
      "Generated AMP sequence: YAAKKGYRPPPK\n",
      "Generated AMP sequence: AAAAAAKGGGKKGPP\n",
      "Generated AMP sequence: VAAAGGLKGDPDPYPPSPPY\n",
      "Generated AMP sequence: HTSAACADPDKYDYLH\n",
      "Generated AMP sequence: VAPKPGGTYYPPYPYYPYYH\n",
      "Generated AMP sequence: QALASHAVDKYYPRPNPWYYYFPDPPYPHHYSHE\n",
      "Generated AMP sequence: IALLAAYKKYYYYPDYPPYH\n",
      "Generated AMP sequence: RAAAAAKKKTKKYKYCYDYP\n",
      "Generated AMP sequence: WAAWHKLKPPYDLYYPSLIPPPDYYYPPYYHYYYYSPYYD\n",
      "Generated AMP sequence: KALYCPKYYY\n",
      "Generated AMP sequence: AAAAKKYYKPYYKPHYRKW\n",
      "Generated AMP sequence: GAAKKPKYPPYPHDYSYKHC\n",
      "Generated AMP sequence: QAAARGILKPYYHYP\n",
      "Generated AMP sequence: MAYGHPKDHHHHS\n",
      "Generated AMP sequence: KARHPGHDHLHE\n",
      "Generated AMP sequence: FAAAGKAPKDY\n",
      "Generated AMP sequence: HAAMLEGKPPPPYDLYYPPHPPKLPDPKPHYRYYP\n",
      "Generated AMP sequence: MAAKGCHKPPP\n",
      "Generated AMP sequence: EAAAAAAGGGLGKKYGPYYYLHYYPPMYIPPPKHSNSHP\n",
      "Generated AMP sequence: DAAAAYGKGKQPPPPYYYHY\n",
      "Generated AMP sequence: PAALKSFSHHHYTPPYPYPPPPYPKYPPYSHYSPHY\n",
      "Generated AMP sequence: EARPCKKYDPYYPYHYHPYH\n",
      "Generated AMP sequence: NAGKPLKLHYPP\n",
      "Generated AMP sequence: NSAIKNAYYPSYYHPRPYY\n",
      "Generated AMP sequence: YAAYQCYYPPPDPYALPPYPPYPKPHHPPYPPSCW\n",
      "Generated AMP sequence: KTAGAYLPDYKY\n",
      "Generated AMP sequence: CAAEKAPKLMPP\n",
      "Generated AMP sequence: RAQYKKQPDHYT\n",
      "Generated AMP sequence: VAAGAAKGKG\n",
      "Generated AMP sequence: EAHRQKPILYC\n",
      "Generated AMP sequence: PAAAHGRPKK\n",
      "Generated AMP sequence: DAAHGKKKYYHYPYYP\n",
      "Generated AMP sequence: VAAGAAKWKG\n",
      "Generated AMP sequence: LASAKGLPYKAHY\n",
      "Generated AMP sequence: YADKDYMPPDHYDSKPMPYPPPSYYKDLRDP\n",
      "Generated AMP sequence: NAIAAKIKKKYYTKQPEYHY\n",
      "Generated AMP sequence: WAAQKPKPTKPKYYDYYH\n",
      "Generated AMP sequence: HPAMHYYYPYYPYDRKHDYP\n",
      "Generated AMP sequence: DAAGNGPYPKDYDYPYYYPYPP\n",
      "Generated AMP sequence: CKYQKPVKGPLNKDDYPPWK\n",
      "Generated AMP sequence: ICAGWYPPHYKYSSPKHKIYPDYYYYHPYYHYKKPRP\n",
      "Generated AMP sequence: LLAAVYWSPPD\n",
      "Generated AMP sequence: DAAKHAGMDYPPPYPVP\n",
      "Generated AMP sequence: IKPPVYTYWYSYH\n",
      "Generated AMP sequence: EKARDKSHYYPAKSYSKSPYHYPYPSYHYYDP\n",
      "Generated AMP sequence: PADDNAYYYYHYDDPPYYPLY\n",
      "Generated AMP sequence: KAALKGDPPKYHPYPSPL\n",
      "Generated AMP sequence: RARDGARLYPYIYYYYYSHDYSYPHHYPKPDQLYPPCDP\n",
      "Generated AMP sequence: NAAHPLAKDKYHPRYPAMPE\n",
      "Generated AMP sequence: GAADAKLPGHFPVDY\n",
      "Generated AMP sequence: NAGSPRKYDY\n",
      "Generated AMP sequence: NFAKKRDDKHYKYPIYVPPY\n",
      "Generated AMP sequence: HYYPFQDDKPYY\n",
      "Generated AMP sequence: KAAAYALKKMKYYPLQYKSSYHY\n",
      "Generated AMP sequence: DAAWAPIGSKPMPHPYLVPYHYPKDHPPYPLSYHH\n",
      "Generated AMP sequence: YAGHSSPYKG\n",
      "Generated AMP sequence: GAAVAAGSGKDGY\n",
      "Generated AMP sequence: VATIRKKPQHDNYKYYFVPYHYYYYPSYPDYYYKWPSH\n",
      "Generated AMP sequence: IYAYKYYPYYPPSYYYPYHP\n",
      "Generated AMP sequence: GAADLKKKMK\n",
      "Generated AMP sequence: RAILKLYPPD\n",
      "Generated AMP sequence: PAAHKKRSSYCKAS\n",
      "Generated AMP sequence: KASAWHDKKKLYPPRYMPHIYIPYHPLYPYPDSY\n",
      "Generated AMP sequence: VFRDGPPYYPKYLYPVFSPP\n",
      "Generated AMP sequence: IAPAAAKKKHKP\n",
      "Generated AMP sequence: MYALKLSSYHGYPLDEYKPK\n",
      "Generated AMP sequence: CAAALGKRLPKMSYV\n",
      "Generated AMP sequence: IAAAGVGLHPDPYPSYYKPH\n",
      "Generated AMP sequence: CQESMPDYHYYYDPLPEYPPPPRPYPYYKYHYPYYYYPDP\n",
      "Generated AMP sequence: MAAYKPEVYPPY\n",
      "Generated AMP sequence: PAAAFAAGTKYK\n",
      "Generated AMP sequence: KAAAIEKKGDYPDY\n",
      "Generated AMP sequence: DAYAAAGEKD\n",
      "Generated AMP sequence: HYPLKPPPHYPKPPYYYCPP\n",
      "Generated AMP sequence: WAKAKGAHYW\n",
      "Generated AMP sequence: MAAGGRELHLYPPPPYYYYPYPLYDYPSVYSSDPHYPW\n",
      "Generated AMP sequence: HAHAAAWLHH\n",
      "Generated AMP sequence: SAFAAAKLGADYKYR\n",
      "Generated AMP sequence: PAAHPKLCYYLYRPYY\n",
      "Generated AMP sequence: RAAVDKKLPYKHKYSLYYYY\n",
      "Generated AMP sequence: FAAQMPMGPYYD\n",
      "Generated AMP sequence: KAIQDLYVPPPL\n",
      "Generated AMP sequence: FAQKRGPAKYYGPYKPYSIP\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    # dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "gen_model.eval()\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptidesFullback.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:35:58,944] A new study created in memory with name: no-name-11effea8-22fc-45fe-8f19-2c3770a74e45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:03,260] Trial 0 finished with value: 0.1755082756280899 and parameters: {'dropout': 0.48226628572434316, 'lr': 0.0006821168350646403, 'weight_decay': 0.00023604294180079854}. Best is trial 0 with value: 0.1755082756280899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:07,509] Trial 1 finished with value: 0.17400945723056793 and parameters: {'dropout': 0.38551514827720557, 'lr': 0.0006341134747285574, 'weight_decay': 0.0007398026271855571}. Best is trial 1 with value: 0.17400945723056793.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:12,277] Trial 2 finished with value: 0.18788397312164307 and parameters: {'dropout': 0.3817009386510213, 'lr': 0.006140314607255589, 'weight_decay': 2.6150613802277304e-05}. Best is trial 1 with value: 0.17400945723056793.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:16,381] Trial 3 finished with value: 0.1828785389661789 and parameters: {'dropout': 0.25018051076906866, 'lr': 0.006911200001676759, 'weight_decay': 0.0008366391036811805}. Best is trial 1 with value: 0.17400945723056793.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:20,214] Trial 4 finished with value: 0.20788905024528503 and parameters: {'dropout': 0.35157497265151116, 'lr': 0.009712639719830562, 'weight_decay': 0.00022174905614099342}. Best is trial 1 with value: 0.17400945723056793.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:23,865] Trial 5 finished with value: 0.1966792792081833 and parameters: {'dropout': 0.4857775314557615, 'lr': 0.009360694886012572, 'weight_decay': 1.6502116263242312e-05}. Best is trial 1 with value: 0.17400945723056793.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:27,672] Trial 6 finished with value: 0.16907334327697754 and parameters: {'dropout': 0.3064821494977372, 'lr': 0.008385604009341855, 'weight_decay': 0.0008831466393808281}. Best is trial 6 with value: 0.16907334327697754.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:31,609] Trial 7 finished with value: 0.19636847078800201 and parameters: {'dropout': 0.3712213862120247, 'lr': 0.0027867958751891485, 'weight_decay': 0.0004589811274669594}. Best is trial 6 with value: 0.16907334327697754.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:35,556] Trial 8 finished with value: 0.1686840057373047 and parameters: {'dropout': 0.43762387817195103, 'lr': 0.00818905875426827, 'weight_decay': 0.00017032490159272614}. Best is trial 8 with value: 0.1686840057373047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:39,573] Trial 9 finished with value: 0.17050457000732422 and parameters: {'dropout': 0.47486530446396935, 'lr': 0.0043724729847743, 'weight_decay': 0.0002157185826018631}. Best is trial 8 with value: 0.1686840057373047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:44,502] Trial 10 finished with value: 0.270433634519577 and parameters: {'dropout': 0.1747956881853147, 'lr': 0.007573748516733935, 'weight_decay': 0.0005611213262369222}. Best is trial 8 with value: 0.1686840057373047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:48,534] Trial 11 finished with value: 0.18712729215621948 and parameters: {'dropout': 0.2596102894620508, 'lr': 0.007741494150564815, 'weight_decay': 0.0009845084023799241}. Best is trial 8 with value: 0.1686840057373047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:52,106] Trial 12 finished with value: 0.21051616966724396 and parameters: {'dropout': 0.12158797919965172, 'lr': 0.008436311628558244, 'weight_decay': 0.00048653773834579547}. Best is trial 8 with value: 0.1686840057373047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:55,941] Trial 13 finished with value: 0.18539530038833618 and parameters: {'dropout': 0.30449714535246053, 'lr': 0.005407928391727309, 'weight_decay': 0.0007396917787039752}. Best is trial 8 with value: 0.1686840057373047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:36:59,932] Trial 14 finished with value: 0.17021183669567108 and parameters: {'dropout': 0.41661675057378666, 'lr': 0.0036785520266853903, 'weight_decay': 0.00034699197108381834}. Best is trial 8 with value: 0.1686840057373047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:37:03,919] Trial 15 finished with value: 0.20867696404457092 and parameters: {'dropout': 0.29710624107867256, 'lr': 0.008601039075634543, 'weight_decay': 0.0006145654782784139}. Best is trial 8 with value: 0.1686840057373047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:37:07,848] Trial 16 finished with value: 0.26581302285194397 and parameters: {'dropout': 0.17657640214145703, 'lr': 0.006244980301077365, 'weight_decay': 0.0009808126734961603}. Best is trial 8 with value: 0.1686840057373047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:37:12,420] Trial 17 finished with value: 0.25367027521133423 and parameters: {'dropout': 0.42457773857777714, 'lr': 0.009851501412763404, 'weight_decay': 0.0003505495287414805}. Best is trial 8 with value: 0.1686840057373047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:37:16,483] Trial 18 finished with value: 0.2031043916940689 and parameters: {'dropout': 0.32707182307592225, 'lr': 0.008336894500320755, 'weight_decay': 0.0008308701021206918}. Best is trial 8 with value: 0.1686840057373047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-01 11:37:20,526] Trial 19 finished with value: 0.20394910871982574 and parameters: {'dropout': 0.43345592233747454, 'lr': 0.006926688238652978, 'weight_decay': 0.00012292462298675778}. Best is trial 8 with value: 0.1686840057373047.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best transfer learning hyperparameters: {'dropout': 0.43762387817195103, 'lr': 0.00818905875426827, 'weight_decay': 0.00017032490159272614}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Criterion\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Transfer Learning Loader\n",
    "def load_pretrained_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    def freeze_encoder(model, num_layers_to_freeze):\n",
    "        \"\"\"\n",
    "        Freezes the first `num_layers_to_freeze` LSTM layers of the model.\n",
    "        Assumes parameter names follow standard PyTorch LSTM naming.\n",
    "        \"\"\"\n",
    "        if num_layers_to_freeze <= 0:\n",
    "            print(\"⚠️ No LSTM layers frozen.\")\n",
    "            return\n",
    "\n",
    "        layer_prefixes = [f'lstm.weight_ih_l{i}' for i in range(num_layers_to_freeze)]\n",
    "        for name, param in model.named_parameters():\n",
    "            if any(prefix in name for prefix in layer_prefixes):\n",
    "                param.requires_grad = False\n",
    "\n",
    "        print(f\"✅ Frozen first {num_layers_to_freeze} LSTM layers.\")\n",
    "\n",
    "\n",
    "\n",
    "def freeze_encoder(model, num_layers_to_freeze):\n",
    "    \"\"\"\n",
    "    Freezes the first `num_layers_to_freeze` LSTM layers of the model.\n",
    "    Assumes parameter names follow standard PyTorch LSTM naming.\n",
    "    \"\"\"\n",
    "    if num_layers_to_freeze <= 0:\n",
    "        print(\"⚠️ No LSTM layers frozen.\")\n",
    "        return\n",
    "\n",
    "    layer_prefixes = [f'lstm.weight_ih_l{i}' for i in range(num_layers_to_freeze)]\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(prefix in name for prefix in layer_prefixes):\n",
    "            param.requires_grad = False\n",
    "\n",
    "    print(f\"✅ Frozen first {num_layers_to_freeze} LSTM layers.\")\n",
    "    \n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    last_token_logits = output[:, -1, :]\n",
    "    last_token_targets = target_seq[:, -1, :]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen-tb/AMP_LSTM_GEN_TRANSFER_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # if train:\n",
    "            #     torch.save(model.state_dict(), 'best_model_lstm_transfer.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "    \n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Optuna objective for fine-tuning\n",
    "def objective_generation(trial):\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(    \n",
    "                hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "                num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "                dropout=dropout\n",
    "                # dropout=lstm_gen_best_params[\"dropout\"]\n",
    "                )\n",
    "    model = load_pretrained_weights(model, 'best_model_lstm_generator_final.pt')  # path to the general AMP model\n",
    "\n",
    "    freeze_encoder(model, 4)\n",
    "\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=20, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "lstm_gen_frozen_best_params_tb = study.best_trial.params\n",
    "print(\"Best transfer learning hyperparameters:\", lstm_gen_frozen_best_params_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Frozen first 2 LSTM layers.\n",
      "Epoch 1: Train Loss = 2.9264 | Val Loss = 2.9156 | Acc = 0.9730 | AUC = undefined | Perplexity = 18.4606\n",
      "Epoch 2: Train Loss = 2.9097 | Val Loss = 2.9004 | Acc = 0.9730 | AUC = undefined | Perplexity = 18.1817\n",
      "Epoch 3: Train Loss = 2.8965 | Val Loss = 2.8853 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.9091\n",
      "Epoch 4: Train Loss = 2.8804 | Val Loss = 2.8699 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.6361\n",
      "Epoch 5: Train Loss = 2.8649 | Val Loss = 2.8537 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.3521\n",
      "Epoch 6: Train Loss = 2.8514 | Val Loss = 2.8351 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.0327\n",
      "Epoch 7: Train Loss = 2.8321 | Val Loss = 2.8113 | Acc = 0.9730 | AUC = undefined | Perplexity = 16.6314\n",
      "Epoch 8: Train Loss = 2.8114 | Val Loss = 2.7805 | Acc = 1.0000 | AUC = undefined | Perplexity = 16.1273\n",
      "Epoch 9: Train Loss = 2.7812 | Val Loss = 2.7606 | Acc = 1.0000 | AUC = undefined | Perplexity = 15.8086\n",
      "Epoch 10: Train Loss = 2.7631 | Val Loss = 2.7501 | Acc = 0.9730 | AUC = undefined | Perplexity = 15.6441\n",
      "Epoch 11: Train Loss = 2.7892 | Val Loss = 2.7403 | Acc = 0.9730 | AUC = undefined | Perplexity = 15.4919\n",
      "Epoch 12: Train Loss = 2.7648 | Val Loss = 2.7267 | Acc = 0.9730 | AUC = undefined | Perplexity = 15.2831\n",
      "Epoch 13: Train Loss = 2.7732 | Val Loss = 2.7138 | Acc = 0.9730 | AUC = undefined | Perplexity = 15.0872\n",
      "Epoch 14: Train Loss = 2.7087 | Val Loss = 2.7009 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.8933\n",
      "Epoch 15: Train Loss = 2.7444 | Val Loss = 2.6891 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.7188\n",
      "Epoch 16: Train Loss = 2.6959 | Val Loss = 2.6784 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.5615\n",
      "Epoch 17: Train Loss = 2.6787 | Val Loss = 2.6666 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.3917\n",
      "Epoch 18: Train Loss = 2.6702 | Val Loss = 2.6530 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.1961\n",
      "Epoch 19: Train Loss = 2.6486 | Val Loss = 2.6382 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.9875\n",
      "Epoch 20: Train Loss = 2.6416 | Val Loss = 2.6220 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.7630\n",
      "Epoch 21: Train Loss = 2.6193 | Val Loss = 2.6061 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.5461\n",
      "Epoch 22: Train Loss = 2.6112 | Val Loss = 2.5907 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.3398\n",
      "Epoch 23: Train Loss = 2.5866 | Val Loss = 2.5761 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.1455\n",
      "Epoch 24: Train Loss = 2.5836 | Val Loss = 2.5623 | Acc = 1.0000 | AUC = undefined | Perplexity = 12.9658\n",
      "Epoch 25: Train Loss = 2.5905 | Val Loss = 2.5483 | Acc = 1.0000 | AUC = undefined | Perplexity = 12.7849\n",
      "Epoch 26: Train Loss = 2.5554 | Val Loss = 2.5351 | Acc = 1.0000 | AUC = undefined | Perplexity = 12.6172\n",
      "Epoch 27: Train Loss = 2.5407 | Val Loss = 2.5209 | Acc = 1.0000 | AUC = undefined | Perplexity = 12.4396\n",
      "Epoch 28: Train Loss = 2.5195 | Val Loss = 2.5078 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.2773\n",
      "Epoch 29: Train Loss = 2.5087 | Val Loss = 2.4947 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.1178\n",
      "Epoch 30: Train Loss = 2.5016 | Val Loss = 2.4808 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.9503\n",
      "Epoch 31: Train Loss = 2.4818 | Val Loss = 2.4678 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.7962\n",
      "Epoch 32: Train Loss = 2.4713 | Val Loss = 2.4539 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.6336\n",
      "Epoch 33: Train Loss = 2.4558 | Val Loss = 2.4409 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.4828\n",
      "Epoch 34: Train Loss = 2.4494 | Val Loss = 2.4270 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.3247\n",
      "Epoch 35: Train Loss = 2.4303 | Val Loss = 2.4148 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.1874\n",
      "Epoch 36: Train Loss = 2.4124 | Val Loss = 2.4026 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.0514\n",
      "Epoch 37: Train Loss = 2.4007 | Val Loss = 2.3911 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.9252\n",
      "Epoch 38: Train Loss = 2.3852 | Val Loss = 2.3784 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.7874\n",
      "Epoch 39: Train Loss = 2.3796 | Val Loss = 2.3644 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.6380\n",
      "Epoch 40: Train Loss = 2.3668 | Val Loss = 2.3496 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.4817\n",
      "Epoch 41: Train Loss = 2.3486 | Val Loss = 2.3346 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.3249\n",
      "Epoch 42: Train Loss = 2.3342 | Val Loss = 2.3199 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.1749\n",
      "Epoch 43: Train Loss = 2.3370 | Val Loss = 2.3061 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.0355\n",
      "Epoch 44: Train Loss = 2.3092 | Val Loss = 2.2933 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.9077\n",
      "Epoch 45: Train Loss = 2.3093 | Val Loss = 2.2835 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.8110\n",
      "Epoch 46: Train Loss = 2.2878 | Val Loss = 2.2701 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.6807\n",
      "Epoch 47: Train Loss = 2.2757 | Val Loss = 2.2554 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.5388\n",
      "Epoch 48: Train Loss = 2.2751 | Val Loss = 2.2433 | Acc = 1.0000 | AUC = undefined | Perplexity = 9.4247\n",
      "Epoch 49: Train Loss = 2.2469 | Val Loss = 2.2278 | Acc = 1.0000 | AUC = undefined | Perplexity = 9.2796\n",
      "Epoch 50: Train Loss = 2.2322 | Val Loss = 2.2121 | Acc = 1.0000 | AUC = undefined | Perplexity = 9.1346\n",
      "Epoch 51: Train Loss = 2.2156 | Val Loss = 2.1976 | Acc = 1.0000 | AUC = undefined | Perplexity = 9.0037\n",
      "Epoch 52: Train Loss = 2.2208 | Val Loss = 2.1827 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.8703\n",
      "Epoch 53: Train Loss = 2.1896 | Val Loss = 2.1681 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.7413\n",
      "Epoch 54: Train Loss = 2.1914 | Val Loss = 2.1532 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.6123\n",
      "Epoch 55: Train Loss = 2.2601 | Val Loss = 2.1461 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.5511\n",
      "Epoch 56: Train Loss = 2.1659 | Val Loss = 2.1448 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.5406\n",
      "Epoch 57: Train Loss = 2.1486 | Val Loss = 2.1363 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.4680\n",
      "Epoch 58: Train Loss = 2.1362 | Val Loss = 2.1232 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.3577\n",
      "Epoch 59: Train Loss = 2.1447 | Val Loss = 2.1083 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.2345\n",
      "Epoch 60: Train Loss = 2.1172 | Val Loss = 2.0947 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.1226\n",
      " Final Test Metrics:\n",
      "Loss = 2.0947, Accuracy = 1.0000, AUC = undefined, Perplexity = 8.1226\n",
      "Model saved to final_amp_frozen_generator_lstm.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_frozen_best_params_tb[\"lr\"], weight_decay=lstm_gen_frozen_best_params_tb[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-frozen-gen-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "# lstm_gen_best_params = {'hidden_dim': 133, 'num_layers': 2, 'dropout': 0.10063270147175422, 'lr': 0.003237280156212186, 'weight_decay': 2.2594437829479466e-05}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_frozen_best_params_tb[\"dropout\"],\n",
    "    # weights_decay=lstm_gen_frozen_best_params_tb[\"weight_decay\"],\n",
    "    # lr=lstm_gen_frozen_best_params_tb[\"lr\"]\n",
    ")\n",
    "freeze_encoder(final_model, 4)\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\" Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n",
    "# Save model weights\n",
    "torch.save(trained_model.state_dict(), \"final_amp_frozen_generator_lstm.pt\")\n",
    "print(\"Model saved to final_amp_frozen_generator_lstm.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EKAHKHKHKHKSKHKH\n",
      "Generated AMP sequence: AAAHKHKHKHKHKH\n",
      "Generated AMP sequence: KAASYKHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: IARSKSDKHKHKHKHKHKHSHKDKHKHKHKHKHKHKHKH\n",
      "Generated AMP sequence: IAANKRHKHKHKHKHKHKH\n",
      "Generated AMP sequence: FAAAVRHKHKHK\n",
      "Generated AMP sequence: EAAAGHKHCHKHKHKHKHKSKHKHKH\n",
      "Generated AMP sequence: VAAASKHKYKHKHKHKHKHK\n",
      "Generated AMP sequence: DAAHLAHKHKHKLK\n",
      "Generated AMP sequence: WAQRKHKHKHKHKH\n",
      "Generated AMP sequence: QAASRHKHKHKDKH\n",
      "Generated AMP sequence: CAALMIHKHK\n",
      "Generated AMP sequence: AAAHKHKHSHKHKHKHKHKH\n",
      "Generated AMP sequence: DAQVTKAKHRRKHKHKHKHK\n",
      "Generated AMP sequence: HAAYAHSHKHKHKHKHKHKH\n",
      "Generated AMP sequence: IAKAHKHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: TAAKHKHSHKHKHKHKHKHK\n",
      "Generated AMP sequence: YAAMKHRHKHKH\n",
      "Generated AMP sequence: AAAAHHKHKHKHKHK\n",
      "Generated AMP sequence: VAAAHKHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: HNHAADADKHKHKHKH\n",
      "Generated AMP sequence: VAHLRKHKHSHKHKHKHKHK\n",
      "Generated AMP sequence: QAGASIAPKHKPKHKHKHKHKHKHKHKHKHKHKH\n",
      "Generated AMP sequence: IADKADYKHKPSHKHKHKHK\n",
      "Generated AMP sequence: RAAAGHKHKRKHKHKDLHKH\n",
      "Generated AMP sequence: WAAYKHKHKHKHKPKHKHKHKHKWSHKHKHKHKPKHKHKH\n",
      "Generated AMP sequence: KAEWHKHSHK\n",
      "Generated AMP sequence: AAAAKHYYKHKHKHKPKHK\n",
      "Generated AMP sequence: GAAKHKHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: QAAHRHKHKHKPKHK\n",
      "Generated AMP sequence: MAVDGKHKHKHKH\n",
      "Generated AMP sequence: KAMGHKHKHKHK\n",
      "Generated AMP sequence: FAAAHKDKHKH\n",
      "Generated AMP sequence: HAAPRGHHKHKHKHKHKHKHKHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: MAAMKGKHKHK\n",
      "Generated AMP sequence: EAAADHKHKHKHKHSHKRKHKHKHKHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: DAAAAWHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: PAAPKRHKKHKHKHKHKHKHKHKHKHKHKHKLKHKH\n",
      "Generated AMP sequence: EAKNAKHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: NAAHKHKHKHKH\n",
      "Generated AMP sequence: NIAGKHHHKHKHKHKHKHK\n",
      "Generated AMP sequence: YAAYRHKHKHKHKHGHKHKHKHKHKHKHKHKHKDK\n",
      "Generated AMP sequence: KNAAAYHKHKHK\n",
      "Generated AMP sequence: CAAGKDHKHKHK\n",
      "Generated AMP sequence: RAHWKHKHKHKH\n",
      "Generated AMP sequence: VAAKAHHKHK\n",
      "Generated AMP sequence: EAARRHKHKHH\n",
      "Generated AMP sequence: PAAALHSRHH\n",
      "Generated AMP sequence: DAAHHKHKQKHKHKHK\n",
      "Generated AMP sequence: VAAKGKHSHK\n",
      "Generated AMP sequence: LANAKHKHKHHHK\n",
      "Generated AMP sequence: YAAKKPKHKHKHKHKHKHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: NAAAAHKHKHKHKHKHKHKP\n",
      "Generated AMP sequence: WAARKHKHKHKHSNKHKH\n",
      "Generated AMP sequence: HAAKGTSRKHKHKHKHKHKH\n",
      "Generated AMP sequence: DAAGRHHKHKFKGKHKHYHKHK\n",
      "Generated AMP sequence: CAWPHKHKHKHKHKHSHKHK\n",
      "Generated AMP sequence: IAAASSHKHSHKHKHKHKHKHKRKHSHKHKHKHKHKH\n",
      "Generated AMP sequence: LAAARTKHKHK\n",
      "Generated AMP sequence: DAALHGHKHKHKHKHKH\n",
      "Generated AMP sequence: IAAKSRKHKHKHK\n",
      "Generated AMP sequence: EAAPAHKHKHKDKHSHKHKHKHKPKHKHKHKH\n",
      "Generated AMP sequence: PAAANASVWKHKHKHKHKHKY\n",
      "Generated AMP sequence: KAAKHKHKHKPKHKHKHK\n",
      "Generated AMP sequence: RAKAEAKKHKRKHKHKHKHKHKHKHKHKHKHKHKHKDKH\n",
      "Generated AMP sequence: NAAIRHKHKHSHKHKHHHKH\n",
      "Generated AMP sequence: GAAAHKHKHKHKHKI\n",
      "Generated AMP sequence: NAARKHKHKH\n",
      "Generated AMP sequence: NAAHKHKHKHKHSHKHKHKH\n",
      "Generated AMP sequence: HYTICPAAKHKH\n",
      "Generated AMP sequence: KAAAYKHKHKHKHKHKHKHKHKH\n",
      "Generated AMP sequence: DAAYFRHKHKHKHKHNHKHKHMHKHKHKHKHKHKH\n",
      "Generated AMP sequence: YAAHRHKHKH\n",
      "Generated AMP sequence: GAASAKHKHKHKY\n",
      "Generated AMP sequence: VARHRKHKHKHKHKHPHKHKHKHKHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: IYAYKPKHKHKHKHKHKHKH\n",
      "Generated AMP sequence: GAAAHKHKHK\n",
      "Generated AMP sequence: RAAHKHKHKH\n",
      "Generated AMP sequence: PAAHKKQKKRKHHH\n",
      "Generated AMP sequence: KAQARKHKHKHKHKHKHKHKHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: VAIAEKHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: IAHAAAHKHKHK\n",
      "Generated AMP sequence: MVAHKHKHSHKHKHKHKHKH\n",
      "Generated AMP sequence: CAAARKHKHKHKHSH\n",
      "Generated AMP sequence: IAAAKTHKHKHKHKHKHKHK\n",
      "Generated AMP sequence: CHAPKHKHKHKHKHKHKIKHKHKHKHSHKHKHKHKHKHKH\n",
      "Generated AMP sequence: MAAYKPKHKHKH\n",
      "Generated AMP sequence: PAAAIDAGTKTK\n",
      "Generated AMP sequence: KAAAHKHKHKHKHS\n",
      "Generated AMP sequence: DAWACKHKHK\n",
      "Generated AMP sequence: HYAHKHKHKHKHKHKHKDKH\n",
      "Generated AMP sequence: WAAAKHGHSH\n",
      "Generated AMP sequence: MAAHKRGHKHKHKHKHKPKHKHKHKHKHKHKHKHKHKH\n",
      "Generated AMP sequence: HAAAAAVKHK\n",
      "Generated AMP sequence: SAAAGKHKHKHPHKH\n",
      "Generated AMP sequence: PAAHRHKASHKYKHKH\n",
      "Generated AMP sequence: RAATKHKHKPKHKHKHKHKH\n",
      "Generated AMP sequence: FAARPRHKHSPK\n",
      "Generated AMP sequence: KAAMHKHKHKHK\n",
      "Generated AMP sequence: FAHHRKHEHKYKHKHKHKHK\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptidesFrozen.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TB no transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 10:55:14,561] A new study created in memory with name: no-name-f7a5cdc2-a106-4dcf-a41d-0c75ebdcb8f2\n",
      "[I 2025-05-02 10:55:22,868] Trial 0 finished with value: 2.7173891067504883 and parameters: {'hidden_dim': 144, 'num_layers': 3, 'dropout': 0.15437274023341982, 'lr': 0.005771881998865653, 'weight_decay': 0.0002682713506887346}. Best is trial 0 with value: 2.7173891067504883.\n",
      "[I 2025-05-02 10:55:25,214] Trial 1 finished with value: 2.7665016651153564 and parameters: {'hidden_dim': 227, 'num_layers': 2, 'dropout': 0.4424763441823395, 'lr': 0.005704011585947213, 'weight_decay': 0.00041701060111245934}. Best is trial 0 with value: 2.7173891067504883.\n",
      "[I 2025-05-02 10:55:27,423] Trial 2 finished with value: 2.9343535900115967 and parameters: {'hidden_dim': 250, 'num_layers': 1, 'dropout': 0.32673827998102084, 'lr': 0.00154077252086226, 'weight_decay': 0.0008423535539033275}. Best is trial 0 with value: 2.7173891067504883.\n",
      "[I 2025-05-02 10:55:29,530] Trial 3 finished with value: 2.921525478363037 and parameters: {'hidden_dim': 203, 'num_layers': 1, 'dropout': 0.2910025775992817, 'lr': 0.002777183171745712, 'weight_decay': 0.0003434969516472462}. Best is trial 0 with value: 2.7173891067504883.\n",
      "[I 2025-05-02 10:55:31,689] Trial 4 finished with value: 2.6613221168518066 and parameters: {'hidden_dim': 94, 'num_layers': 3, 'dropout': 0.26014567080645096, 'lr': 0.0070399821490159725, 'weight_decay': 0.00035452053677446617}. Best is trial 4 with value: 2.6613221168518066.\n",
      "[I 2025-05-02 10:55:33,703] Trial 5 finished with value: 2.4661667346954346 and parameters: {'hidden_dim': 149, 'num_layers': 2, 'dropout': 0.4350981856205326, 'lr': 0.009996913527093828, 'weight_decay': 0.00012991309919791892}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:36,002] Trial 6 finished with value: 2.5921413898468018 and parameters: {'hidden_dim': 156, 'num_layers': 2, 'dropout': 0.26233032429984915, 'lr': 0.007085314917207174, 'weight_decay': 0.0003734833010050261}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:38,023] Trial 7 finished with value: 2.6170382499694824 and parameters: {'hidden_dim': 198, 'num_layers': 1, 'dropout': 0.2992198925463133, 'lr': 0.007478531405196831, 'weight_decay': 0.00032844451955447204}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:40,070] Trial 8 finished with value: 2.840430498123169 and parameters: {'hidden_dim': 246, 'num_layers': 2, 'dropout': 0.2860246538822809, 'lr': 0.0028624639927731003, 'weight_decay': 0.0006479475216515398}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:41,973] Trial 9 finished with value: 2.7308855056762695 and parameters: {'hidden_dim': 251, 'num_layers': 1, 'dropout': 0.1253868148614699, 'lr': 0.0061950686700516615, 'weight_decay': 0.00037620391007206044}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:44,080] Trial 10 finished with value: 2.4994585514068604 and parameters: {'hidden_dim': 104, 'num_layers': 3, 'dropout': 0.4761814996060996, 'lr': 0.008950795438477129, 'weight_decay': 4.841308347150881e-05}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:46,228] Trial 11 finished with value: 2.473280429840088 and parameters: {'hidden_dim': 110, 'num_layers': 3, 'dropout': 0.493711162895993, 'lr': 0.009976896028417542, 'weight_decay': 3.8410958555953375e-05}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:48,456] Trial 12 finished with value: 2.5803210735321045 and parameters: {'hidden_dim': 128, 'num_layers': 3, 'dropout': 0.4030390884872124, 'lr': 0.009912634416465286, 'weight_decay': 1.4941241000134039e-05}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:50,833] Trial 13 finished with value: 2.5807318687438965 and parameters: {'hidden_dim': 119, 'num_layers': 2, 'dropout': 0.4955914901115842, 'lr': 0.008935117102889415, 'weight_decay': 0.0001581504847002647}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:53,191] Trial 14 finished with value: 2.508286952972412 and parameters: {'hidden_dim': 72, 'num_layers': 2, 'dropout': 0.3929849973560849, 'lr': 0.009956845066137525, 'weight_decay': 0.0006072891214864552}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:55,624] Trial 15 finished with value: 2.591632843017578 and parameters: {'hidden_dim': 173, 'num_layers': 3, 'dropout': 0.37166428354469105, 'lr': 0.008408121428336614, 'weight_decay': 0.00016500535403210457}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:57,749] Trial 16 finished with value: 2.700115919113159 and parameters: {'hidden_dim': 73, 'num_layers': 2, 'dropout': 0.44696270216232137, 'lr': 0.004629756215012773, 'weight_decay': 0.0001677033310943217}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:56:00,317] Trial 17 finished with value: 2.5816473960876465 and parameters: {'hidden_dim': 171, 'num_layers': 3, 'dropout': 0.1988244547342201, 'lr': 0.008242167192370142, 'weight_decay': 0.0009716342592541768}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:56:02,703] Trial 18 finished with value: 2.7583444118499756 and parameters: {'hidden_dim': 135, 'num_layers': 2, 'dropout': 0.35954400375975193, 'lr': 0.004800561503195884, 'weight_decay': 0.0005332670932732454}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:56:05,108] Trial 19 finished with value: 2.605470657348633 and parameters: {'hidden_dim': 99, 'num_layers': 2, 'dropout': 0.4359927569570083, 'lr': 0.009518826156893808, 'weight_decay': 0.00010606058205186216}. Best is trial 5 with value: 2.4661667346954346.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dim': 149, 'num_layers': 2, 'dropout': 0.4350981856205326, 'lr': 0.009996913527093828, 'weight_decay': 0.00012991309919791892}\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary packages after reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "class GenerativeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=128, num_layers=1, dropout=0.3):\n",
    "        super(GenerativeLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle packed input\n",
    "        if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n",
    "            packed_output, _ = self.lstm(x)\n",
    "            unpacked_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            return self.fc(unpacked_output)\n",
    "        else:\n",
    "            out, _ = self.lstm(x)\n",
    "            return self.fc(out)\n",
    "\n",
    "# Criterion\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss on the last time step of each sequence.\n",
    "    \n",
    "    Args:\n",
    "        output: Tensor of shape [B, L, vocab_size]\n",
    "        target_seq: Tensor of shape [B, L] containing target class indices\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss computed only on the last token of each sequence\n",
    "    \"\"\"\n",
    "    # Get last time step for each sequence\n",
    "    last_token_logits = output[:, -1, :]        # [B, vocab_size]\n",
    "    last_token_targets = target_seq[:, -1, :]      # [B]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "    # print('last_token_logits',last_token_logits.shape)\n",
    "    # print('last_token_targets',last_token_targets.shape)\n",
    "\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen-notrans-tb/AMP_LSTM_GEN_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])      # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            # print('target_shape before reshape',target_seq.shape)\n",
    "            # target_seq = target_seq.reshape(-1)\n",
    "            # print(f\"Output shape: {output.shape}, Target shape: {target_seq.shape}\")\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_generator-notrans-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])  # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # # target_seq = target_seq.reshape(-1)\n",
    "            # # target_seq = target_seq.reshape(-1, target_seq.shape[-1])\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            # assert output.size(0) == target_seq.size(0), f\"Mismatch: {output.size(0)} vs {target_seq.size(0)}\"\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            # print('loss done')\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            preds = output[:, -1, :]        # shape: [B, vocab_size]\n",
    "            preds = torch.argmax(preds, dim=1)  # shape: [B]\n",
    "\n",
    "            targets = target_seq[:, -1, :]      # shape: [B, vocab_size]\n",
    "            targets = torch.argmax(targets, dim=-1)  # shape: [B]\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Objective for Optuna tuning\n",
    "def objective_generation(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "\n",
    "lstm_gen_notrans_tb_best_params = study.best_trial.params\n",
    "print(lstm_gen_notrans_tb_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 3.0548 | Val Loss = 3.0090 | Acc = 0.0270 | AUC = undefined | Perplexity = 20.2662\n",
      "Epoch 2: Train Loss = 2.9965 | Val Loss = 2.9435 | Acc = 0.0270 | AUC = undefined | Perplexity = 18.9822\n",
      "Epoch 3: Train Loss = 2.9277 | Val Loss = 2.8642 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.5345\n",
      "Epoch 4: Train Loss = 2.8628 | Val Loss = 2.7988 | Acc = 1.0000 | AUC = undefined | Perplexity = 16.4257\n",
      "Epoch 5: Train Loss = 2.8049 | Val Loss = 2.7393 | Acc = 1.0000 | AUC = undefined | Perplexity = 15.4760\n",
      "Epoch 6: Train Loss = 2.7479 | Val Loss = 2.6935 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.7829\n",
      "Epoch 7: Train Loss = 2.6925 | Val Loss = 2.6404 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.0191\n",
      "Epoch 8: Train Loss = 2.6369 | Val Loss = 2.5883 | Acc = 0.9730 | AUC = undefined | Perplexity = 13.3075\n",
      "Epoch 9: Train Loss = 2.5930 | Val Loss = 2.5379 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.6535\n",
      "Epoch 10: Train Loss = 2.5679 | Val Loss = 2.4886 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.0445\n",
      "Epoch 11: Train Loss = 2.4979 | Val Loss = 2.4429 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.5069\n",
      "Epoch 12: Train Loss = 2.4344 | Val Loss = 2.3924 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.9399\n",
      "Epoch 13: Train Loss = 2.3910 | Val Loss = 2.3338 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.3175\n",
      "Epoch 14: Train Loss = 2.3210 | Val Loss = 2.2806 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.7830\n",
      "Epoch 15: Train Loss = 2.2695 | Val Loss = 2.2254 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.2571\n",
      "Epoch 16: Train Loss = 2.2189 | Val Loss = 2.1678 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.7391\n",
      "Epoch 17: Train Loss = 2.1710 | Val Loss = 2.1145 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.2853\n",
      "Epoch 18: Train Loss = 2.1090 | Val Loss = 2.0647 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.8831\n",
      "Epoch 19: Train Loss = 2.0967 | Val Loss = 2.0239 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.5675\n",
      "Epoch 20: Train Loss = 2.0232 | Val Loss = 1.9821 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.2579\n",
      "Epoch 21: Train Loss = 1.9692 | Val Loss = 1.9328 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.9090\n",
      "Epoch 22: Train Loss = 1.9208 | Val Loss = 1.8839 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.5791\n",
      "Epoch 23: Train Loss = 1.8725 | Val Loss = 1.8355 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.2683\n",
      "Epoch 24: Train Loss = 1.8243 | Val Loss = 1.7873 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.9734\n",
      "Epoch 25: Train Loss = 1.7727 | Val Loss = 1.7418 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.7077\n",
      "Epoch 26: Train Loss = 1.7375 | Val Loss = 1.7069 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.5119\n",
      "Epoch 27: Train Loss = 1.6830 | Val Loss = 1.6516 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.2151\n",
      "Epoch 28: Train Loss = 1.6418 | Val Loss = 1.6205 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.0554\n",
      "Epoch 29: Train Loss = 1.6265 | Val Loss = 1.5928 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.9173\n",
      "Epoch 30: Train Loss = 1.5590 | Val Loss = 1.5435 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.6811\n",
      "Epoch 31: Train Loss = 1.5096 | Val Loss = 1.4809 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.3969\n",
      "Epoch 32: Train Loss = 1.4711 | Val Loss = 1.4596 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.3044\n",
      "Epoch 33: Train Loss = 1.4701 | Val Loss = 1.4189 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.1324\n",
      "Epoch 34: Train Loss = 1.4025 | Val Loss = 1.3763 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.9602\n",
      "Epoch 35: Train Loss = 1.3666 | Val Loss = 1.3143 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.7222\n",
      "Epoch 36: Train Loss = 1.3193 | Val Loss = 1.2723 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.5691\n",
      "Epoch 37: Train Loss = 1.2764 | Val Loss = 1.2426 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.4647\n",
      "Epoch 38: Train Loss = 1.2628 | Val Loss = 1.2157 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.3726\n",
      "Epoch 39: Train Loss = 1.2075 | Val Loss = 1.2056 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.3388\n",
      "Epoch 40: Train Loss = 1.1708 | Val Loss = 1.1870 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.2773\n",
      "Epoch 41: Train Loss = 1.1270 | Val Loss = 1.1746 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.2369\n",
      "Epoch 42: Train Loss = 1.1500 | Val Loss = 1.1117 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.0397\n",
      "Epoch 43: Train Loss = 1.0546 | Val Loss = 1.0840 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.9566\n",
      "Epoch 44: Train Loss = 1.0816 | Val Loss = 1.0778 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.9382\n",
      "Epoch 45: Train Loss = 1.0346 | Val Loss = 1.0787 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.9408\n",
      "Epoch 46: Train Loss = 1.0063 | Val Loss = 0.9876 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.6849\n",
      "Epoch 47: Train Loss = 1.0096 | Val Loss = 0.9313 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.5379\n",
      "Epoch 48: Train Loss = 1.0302 | Val Loss = 0.9131 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.4920\n",
      "Epoch 49: Train Loss = 0.9184 | Val Loss = 0.8908 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.4371\n",
      "Epoch 50: Train Loss = 0.8861 | Val Loss = 0.8640 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.3727\n",
      "Epoch 51: Train Loss = 0.8515 | Val Loss = 0.8388 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.3136\n",
      "Epoch 52: Train Loss = 0.8409 | Val Loss = 0.8135 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.2557\n",
      "Epoch 53: Train Loss = 0.8157 | Val Loss = 0.7907 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.2050\n",
      "Epoch 54: Train Loss = 0.8092 | Val Loss = 0.7698 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.1594\n",
      "Epoch 55: Train Loss = 0.7674 | Val Loss = 0.7473 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.1114\n",
      "Epoch 56: Train Loss = 0.7351 | Val Loss = 0.7270 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0688\n",
      "Epoch 57: Train Loss = 0.7307 | Val Loss = 0.7027 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.0193\n",
      "Epoch 58: Train Loss = 0.7172 | Val Loss = 0.6816 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.9771\n",
      "Epoch 59: Train Loss = 0.6898 | Val Loss = 0.6711 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9564\n",
      "Epoch 60: Train Loss = 0.6594 | Val Loss = 0.6509 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9174\n",
      "\n",
      "✅ Final Test Metrics:\n",
      "Loss = 0.6509, Accuracy = 0.9730, AUC = undefined, Perplexity = 1.9174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_notrans_tb_best_params[\"lr\"], weight_decay=lstm_gen_notrans_tb_best_params[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen-notrans-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_notrans_tb_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_notrans_tb_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_notrans_tb_best_params[\"dropout\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\"\\n✅ Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EYPPGGGYPPGYYGGG\n",
      "Generated AMP sequence: AHIHPGHKKSGLPG\n",
      "Generated AMP sequence: KAAYYYHGPKGKGYHPHPPG\n",
      "Generated AMP sequence: IQYYPYGGGHKHYIHPGYGYRGDYPPRGKGYPHGHHPPY\n",
      "Generated AMP sequence: IAPRPRKLKGGGPHLYHKR\n",
      "Generated AMP sequence: FAGGYYPYYGYP\n",
      "Generated AMP sequence: EVHGGKYYDLKGGHYHPPKYYHKHHG\n",
      "Generated AMP sequence: VHAHYHGKYHPRHPKPPPGY\n",
      "Generated AMP sequence: DAAPPGKGPGPKYG\n",
      "Generated AMP sequence: WAYYHPYPNHGYYP\n",
      "Generated AMP sequence: QEPYYRPGGYPFGP\n",
      "Generated AMP sequence: CAKPPGPHHR\n",
      "Generated AMP sequence: ASPLGIHHYKYPYKPKGPHG\n",
      "Generated AMP sequence: DAYYYKGYKYYYHKYHGPYP\n",
      "Generated AMP sequence: HANYGLYQPPHHYYYYLKYP\n",
      "Generated AMP sequence: IRYHKGPGKPHPGGYKGPRG\n",
      "Generated AMP sequence: TAAKLPPYLHYHKGGYYPKG\n",
      "Generated AMP sequence: YAKPPHYPPPKH\n",
      "Generated AMP sequence: APAGGGYPKGKKGKK\n",
      "Generated AMP sequence: VLAHPKYPGGPGPYPKPKPY\n",
      "Generated AMP sequence: HYYGGGGGPGHYGYHG\n",
      "Generated AMP sequence: VLVPSHGRYYKKRHYYKRRG\n",
      "Generated AMP sequence: QHSHYKFYGHYYPPKHHPRYWGKGPKYHGGRPGG\n",
      "Generated AMP sequence: IASPGGYKKYYYRKGRPKYG\n",
      "Generated AMP sequence: RAKGGGPRPYKHYGYGYGYP\n",
      "Generated AMP sequence: WAKYKPPKPPYGHYYKPHGKHPGYYPPHYPGYYYPPKYYG\n",
      "Generated AMP sequence: KGSYGPKYYY\n",
      "Generated AMP sequence: AGGFYYYYPRYYHKGYPHP\n",
      "Generated AMP sequence: GFEPLRLYPPYKGGRPPGGG\n",
      "Generated AMP sequence: QKHKYHLSKPYYGYK\n",
      "Generated AMP sequence: MGYKHPHGGGGGP\n",
      "Generated AMP sequence: KFYKPGGGGHGG\n",
      "Generated AMP sequence: FGGHKPGYPGY\n",
      "Generated AMP sequence: HAGSRGGKKPKKRGHPYLPGKPHHKGHGHGYPPWK\n",
      "Generated AMP sequence: MIMQKGHIPPH\n",
      "Generated AMP sequence: EKGGGHGGHGYGPKYGKYYYHGPYPPHRGHKLHGPHPGK\n",
      "Generated AMP sequence: DAHGGYHYHPRPKKHYYYGP\n",
      "Generated AMP sequence: PAAPPYGWGGGYPPKYKYKKKHRLHYKPYPGYPHGY\n",
      "Generated AMP sequence: EIYRGHHRGPRYHYGYGKYG\n",
      "Generated AMP sequence: NPPPPPHHGYPK\n",
      "Generated AMP sequence: NYFPKPGYYPPYPGPPKPY\n",
      "Generated AMP sequence: YGDYYGYYLPKGHYDHKPYHHQKGKGGKKYKPPGP\n",
      "Generated AMP sequence: KYGKGYKPGRGY\n",
      "Generated AMP sequence: CAAKPGRKLKKP\n",
      "Generated AMP sequence: RAVYKHPKGGPP\n",
      "Generated AMP sequence: VAGPEGWGPH\n",
      "Generated AMP sequence: EAPYYKPGHYG\n",
      "Generated AMP sequence: PAAGPKYYHH\n",
      "Generated AMP sequence: DKGPKPKHYRGYKYYK\n",
      "Generated AMP sequence: VAKPHGYYMG\n",
      "Generated AMP sequence: LAYGPHPPYGGGY\n",
      "Generated AMP sequence: YKMPGYKKLGGQGPHKHHPHPKPYPGGHPGK\n",
      "Generated AMP sequence: NDRGGPKPKHYYPHPKGYGY\n",
      "Generated AMP sequence: WAKYPWPPYHKGYYGPYG\n",
      "Generated AMP sequence: HWAPHYYYKYYHYGPHGGPH\n",
      "Generated AMP sequence: DAHLRHPYPHGYGYPYYYHRKK\n",
      "Generated AMP sequence: CTYRKKPHGHHHHGGYKKPG\n",
      "Generated AMP sequence: IQDKYYKPGYHYPPLHGGGRHGYYYYGKRPGYGGPPP\n",
      "Generated AMP sequence: LTGGYYYYPPG\n",
      "Generated AMP sequence: DFGPKGGPGYKKPYHPP\n",
      "Generated AMP sequence: ITRPYYPYPPPRG\n",
      "Generated AMP sequence: ETAVGHRGYYLDHPYPHPPRGRPYHPYGYPGH\n",
      "Generated AMP sequence: PALHPGYYYRGRGGHKRPKHY\n",
      "Generated AMP sequence: KHGSPGGPPHYGKYKPKH\n",
      "Generated AMP sequence: RGYGGGPKYKYGYRRYRPGGYPYKGGPKGKGPHYHKGGK\n",
      "Generated AMP sequence: NKGPYRGKGHYGKPPPFHNG\n",
      "Generated AMP sequence: GAAKGPPYGGGPPGY\n",
      "Generated AMP sequence: NIPYPYHYGY\n",
      "Generated AMP sequence: NRKPLPGGHGYGYHGWPKPP\n",
      "Generated AMP sequence: HYYPGPGGHKYP\n",
      "Generated AMP sequence: KGHIYGYPKPKYYKHPYGPPPGP\n",
      "Generated AMP sequence: DAGYGYKHYKLIPGKYHPHRGYKHGGPHYKHPPGG\n",
      "Generated AMP sequence: YPPKYYPYGG\n",
      "Generated AMP sequence: GAKYGGKYHKGGY\n",
      "Generated AMP sequence: VAYPYKLKPGGHPGPYGPKYGPYYPKPYPGYPPGPKPG\n",
      "Generated AMP sequence: IYAYKYRPPYKHPYRYKYGP\n",
      "Generated AMP sequence: GEHKPKKHKG\n",
      "Generated AMP sequence: RKPPKKYLKG\n",
      "Generated AMP sequence: PFHPLPRYPYGGEP\n",
      "Generated AMP sequence: KAYGYHGKHHHYPHPPHKGGPGPPGPHYKYPGPY\n",
      "Generated AMP sequence: VRWGGPKYYHHRHYKPGPKP\n",
      "Generated AMP sequence: IPWGGGPPPHKP\n",
      "Generated AMP sequence: MYEPKKPPYGGPPHGGYGHG\n",
      "Generated AMP sequence: CAKGSGPYPPHHPYP\n",
      "Generated AMP sequence: IPKGPYGPGPGKRHPYYHKG\n",
      "Generated AMP sequence: CWKYKPGRGYRPGHHKGYPHKKPMYKYYGYGYKYYYWHGK\n",
      "Generated AMP sequence: MNHYPYGYYKPR\n",
      "Generated AMP sequence: PHHGHGGGYPYK\n",
      "Generated AMP sequence: KHKGPGPPGGRKGY\n",
      "Generated AMP sequence: DPYGGGHGPG\n",
      "Generated AMP sequence: HYSPHPLKGYKGKKYPRGKK\n",
      "Generated AMP sequence: WAQGLGFHYY\n",
      "Generated AMP sequence: MHDLKYGKGHPKKKKSYYTHPKHYGYKPPYPPGPGQLP\n",
      "Generated AMP sequence: HAPGGFYPHH\n",
      "Generated AMP sequence: SQPGGGPRHGGYHRP\n",
      "Generated AMP sequence: PACPRPMGYYHYPPRY\n",
      "Generated AMP sequence: RAHYGKKKPYHGGYPHYYYY\n",
      "Generated AMP sequence: FPKYPRPGPYYG\n",
      "Generated AMP sequence: KAPWGKYPPPLH\n",
      "Generated AMP sequence: FNWPRGKFHPYGKYGPPPGH\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "aa_vocab = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aa_to_idx = {aa: i for i, aa in enumerate(aa_vocab)}\n",
    "idx_to_aa = {i: aa for aa, i in aa_to_idx.items()}\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "def one_hot_encode_amino_acid(aa, vocab=aa_vocab):\n",
    "    vec = torch.zeros(len(vocab))\n",
    "    vec[aa_to_idx[aa]] = 1.0\n",
    "    return vec\n",
    "\n",
    "\n",
    "def generate_sequence_from_seed(model, seed, max_length=30, temperature=1.0, device='cpu'):\n",
    "    model.eval()\n",
    "    input_seq = [one_hot_encode_amino_acid(aa).to(device) for aa in seed]\n",
    "    input_tensor = torch.stack(input_seq).unsqueeze(0)  # [1, L, 20]\n",
    "\n",
    "    generated = seed.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(seed)):\n",
    "            output = model(input_tensor)  # [1, L, vocab]\n",
    "            logits = output[0, -1, :]  # Last time step → [vocab]\n",
    "\n",
    "            # Apply temperature and sample\n",
    "            probs = F.softmax(logits / temperature, dim=-1).cpu().numpy()\n",
    "            next_idx = np.random.choice(len(aa_vocab), p=probs)\n",
    "            next_aa = idx_to_aa[next_idx]\n",
    "\n",
    "            # Update sequence\n",
    "            next_aa_vec = one_hot_encode_amino_acid(next_aa).to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, 20]\n",
    "            input_tensor = torch.cat([input_tensor, next_aa_vec], dim=1)\n",
    "            generated.append(next_aa)\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "\n",
    "class LengthSampler:\n",
    "    def __init__(self, sequence_lengths):\n",
    "        \"\"\"\n",
    "        Initialize sampler from observed sequence lengths.\n",
    "        \n",
    "        Args:\n",
    "            sequence_lengths (list[int]): List of sequence lengths (e.g., [20, 21, 20, 23, ...])\n",
    "        \"\"\"\n",
    "        self.length_counts = Counter(sequence_lengths)\n",
    "        self.lengths = np.array(sorted(self.length_counts.keys()))\n",
    "        counts = np.array([self.length_counts[l] for l in self.lengths])\n",
    "        self.probs = counts / counts.sum()  # Empirical probabilities\n",
    "\n",
    "    def sample(self, n=1):\n",
    "        \"\"\"\n",
    "        Sample one or more lengths based on the learned distribution.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray of sampled lengths\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.lengths, size=n, p=self.probs)\n",
    "length_sampler = LengthSampler([len(seq) for seq in df.loc[df['AMP'] == 1, :]['Sequences']])\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = trained_model\n",
    "gen_model.to(device)\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides-notrans.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "class GenerativeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=128, num_layers=1, dropout=0.3):\n",
    "        super(GenerativeLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle packed input\n",
    "        if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n",
    "            packed_output, _ = self.lstm(x)\n",
    "            unpacked_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            return self.fc(unpacked_output)\n",
    "        else:\n",
    "            out, _ = self.lstm(x)\n",
    "            return self.fc(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### no trans - test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.9193 | Val Loss = 2.8783 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.7838\n",
      "Epoch 2: Train Loss = 2.8637 | Val Loss = 2.8221 | Acc = 0.9730 | AUC = undefined | Perplexity = 16.8114\n",
      "Epoch 3: Train Loss = 2.8002 | Val Loss = 2.7504 | Acc = 0.9730 | AUC = undefined | Perplexity = 15.6490\n",
      "Epoch 4: Train Loss = 2.7289 | Val Loss = 2.6782 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.5591\n",
      "Epoch 5: Train Loss = 2.6731 | Val Loss = 2.6309 | Acc = 0.9730 | AUC = undefined | Perplexity = 13.8861\n",
      "Epoch 6: Train Loss = 2.6404 | Val Loss = 2.5762 | Acc = 0.9730 | AUC = undefined | Perplexity = 13.1466\n",
      "Epoch 7: Train Loss = 2.5704 | Val Loss = 2.5244 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.4831\n",
      "Epoch 8: Train Loss = 2.5093 | Val Loss = 2.4685 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.8052\n",
      "Epoch 9: Train Loss = 2.5077 | Val Loss = 2.4155 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.1951\n",
      "Epoch 10: Train Loss = 2.4048 | Val Loss = 2.3679 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.6746\n",
      "Epoch 11: Train Loss = 2.3527 | Val Loss = 2.3166 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.1411\n",
      "Epoch 12: Train Loss = 2.3010 | Val Loss = 2.2645 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.6266\n",
      "Epoch 13: Train Loss = 2.2512 | Val Loss = 2.2103 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.1182\n",
      "Epoch 14: Train Loss = 2.2688 | Val Loss = 2.1687 | Acc = 0.9730 | AUC = undefined | Perplexity = 8.7469\n",
      "Epoch 15: Train Loss = 2.1540 | Val Loss = 2.1172 | Acc = 0.9730 | AUC = undefined | Perplexity = 8.3076\n",
      "Epoch 16: Train Loss = 2.1145 | Val Loss = 2.0638 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.8755\n",
      "Epoch 17: Train Loss = 2.0592 | Val Loss = 2.0124 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.4814\n",
      "Epoch 18: Train Loss = 2.0036 | Val Loss = 1.9635 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.1245\n",
      "Epoch 19: Train Loss = 1.9730 | Val Loss = 1.9119 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.7662\n",
      "Epoch 20: Train Loss = 1.9095 | Val Loss = 1.8641 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.4500\n",
      "Epoch 21: Train Loss = 1.8654 | Val Loss = 1.8141 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.1357\n",
      "Epoch 22: Train Loss = 1.8186 | Val Loss = 1.7679 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.8583\n",
      "Epoch 23: Train Loss = 1.7688 | Val Loss = 1.7202 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.5856\n",
      "Epoch 24: Train Loss = 1.7210 | Val Loss = 1.6825 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.3792\n",
      "Epoch 25: Train Loss = 1.6735 | Val Loss = 1.6397 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.1538\n",
      "Epoch 26: Train Loss = 1.6346 | Val Loss = 1.6008 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.9570\n",
      "Epoch 27: Train Loss = 1.5937 | Val Loss = 1.5539 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.7298\n",
      "Epoch 28: Train Loss = 1.5500 | Val Loss = 1.5164 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.5557\n",
      "Epoch 29: Train Loss = 1.5063 | Val Loss = 1.4691 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.3452\n",
      "Epoch 30: Train Loss = 1.5066 | Val Loss = 1.4294 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.1761\n",
      "Epoch 31: Train Loss = 1.4285 | Val Loss = 1.3940 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.0310\n",
      "Epoch 32: Train Loss = 1.3868 | Val Loss = 1.3550 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.8766\n",
      "Epoch 33: Train Loss = 1.3628 | Val Loss = 1.3139 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.7206\n",
      "Epoch 34: Train Loss = 1.3075 | Val Loss = 1.2761 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.5826\n",
      "Epoch 35: Train Loss = 1.2936 | Val Loss = 1.2462 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.4771\n",
      "Epoch 36: Train Loss = 1.2406 | Val Loss = 1.2076 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.3455\n",
      "Epoch 37: Train Loss = 1.2875 | Val Loss = 1.1810 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.2578\n",
      "Epoch 38: Train Loss = 1.1887 | Val Loss = 1.1546 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.1728\n",
      "Epoch 39: Train Loss = 1.1406 | Val Loss = 1.1175 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.0573\n",
      "Epoch 40: Train Loss = 1.1188 | Val Loss = 1.0835 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.9550\n",
      "Epoch 41: Train Loss = 1.0857 | Val Loss = 1.0531 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.8665\n",
      "Epoch 42: Train Loss = 1.0503 | Val Loss = 1.0253 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.7879\n",
      "Epoch 43: Train Loss = 1.0633 | Val Loss = 0.9924 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.6976\n",
      "Epoch 44: Train Loss = 0.9837 | Val Loss = 0.9721 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.6434\n",
      "Epoch 45: Train Loss = 0.9735 | Val Loss = 0.9412 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.5629\n",
      "Epoch 46: Train Loss = 0.9402 | Val Loss = 0.9114 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.4879\n",
      "Epoch 47: Train Loss = 0.9162 | Val Loss = 0.8837 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.4199\n",
      "Epoch 48: Train Loss = 0.9012 | Val Loss = 0.8604 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.3640\n",
      "Epoch 49: Train Loss = 0.8592 | Val Loss = 0.8386 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.3132\n",
      "Epoch 50: Train Loss = 0.8248 | Val Loss = 0.8168 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.2633\n",
      "Epoch 51: Train Loss = 0.8145 | Val Loss = 0.7971 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.2191\n",
      "Epoch 52: Train Loss = 0.7914 | Val Loss = 0.7762 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.1732\n",
      "Epoch 53: Train Loss = 0.7925 | Val Loss = 0.7522 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.1216\n",
      "Epoch 54: Train Loss = 0.7622 | Val Loss = 0.7394 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0947\n",
      "Epoch 55: Train Loss = 0.7218 | Val Loss = 0.7138 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0417\n",
      "Epoch 56: Train Loss = 0.6996 | Val Loss = 0.6916 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9969\n",
      "Epoch 57: Train Loss = 0.6905 | Val Loss = 0.6707 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9557\n",
      "Epoch 58: Train Loss = 0.6773 | Val Loss = 0.6526 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9206\n",
      "Epoch 59: Train Loss = 0.6592 | Val Loss = 0.6348 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.8866\n",
      "Epoch 60: Train Loss = 0.6474 | Val Loss = 0.6204 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.8597\n",
      "\n",
      "✅ Final Test Metrics:\n",
      "Loss = 0.6204, Accuracy = 0.9730, AUC = undefined, Perplexity = 1.8597\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss on the last time step of each sequence.\n",
    "    \n",
    "    Args:\n",
    "        output: Tensor of shape [B, L, vocab_size]\n",
    "        target_seq: Tensor of shape [B, L] containing target class indices\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss computed only on the last token of each sequence\n",
    "    \"\"\"\n",
    "    # Get last time step for each sequence\n",
    "    last_token_logits = output[:, -1, :]        # [B, vocab_size]\n",
    "    last_token_targets = target_seq[:, -1, :]      # [B]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "    # print('last_token_logits',last_token_logits.shape)\n",
    "    # print('last_token_targets',last_token_targets.shape)\n",
    "\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_notrans_tb_best_params[\"lr\"], weight_decay=lstm_gen_notrans_tb_best_params[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen-notrans-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "lstm_gen_notrans_tb_best_params = {'hidden_dim': 125, 'num_layers': 3, 'dropout': 0.1809741409069741, 'lr': 0.009903431049726066, 'weight_decay': 0.00095634187480499}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_notrans_tb_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_notrans_tb_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_notrans_tb_best_params[\"dropout\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\"\\n✅ Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n",
    "# torch.save(trained_model.state_dict(), \"final_amp_notrans_generator_lstm.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# final_model = GenerativeLSTM(\n",
    "#     input_dim=20,\n",
    "#     hidden_dim=lstm_gen_notrans_tb_best_params[\"hidden_dim\"],\n",
    "#     num_layers=lstm_gen_notrans_tb_best_params[\"num_layers\"],\n",
    "#     dropout=lstm_gen_notrans_tb_best_params[\"dropout\"]\n",
    "# )\n",
    "# trained_model = GenerativeLSTM(\n",
    "#     input_dim=20,\n",
    "#     hidden_dim=176,\n",
    "#     num_layers=lstm_gen_notrans_tb_best_params[\"num_layers\"],\n",
    "#     dropout=lstm_gen_notrans_tb_best_params[\"dropout\"]\n",
    "# )\n",
    "\n",
    "\n",
    "# final_model.load_state_dict(torch.load(\"best_model_lstm_generator-notrans-tb.pt\"))\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "class LengthSampler:\n",
    "    def __init__(self, sequence_lengths):\n",
    "        \"\"\"\n",
    "        Initialize sampler from observed sequence lengths.\n",
    "        \n",
    "        Args:\n",
    "            sequence_lengths (list[int]): List of sequence lengths (e.g., [20, 21, 20, 23, ...])\n",
    "        \"\"\"\n",
    "        self.length_counts = Counter(sequence_lengths)\n",
    "        self.lengths = np.array(sorted(self.length_counts.keys()))\n",
    "        counts = np.array([self.length_counts[l] for l in self.lengths])\n",
    "        self.probs = counts / counts.sum()  # Empirical probabilities\n",
    "\n",
    "    def sample(self, n=1):\n",
    "        \"\"\"\n",
    "        Sample one or more lengths based on the learned distribution.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray of sampled lengths\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.lengths, size=n, p=self.probs)\n",
    "length_sampler = LengthSampler([len(seq) for seq in df.loc[df['AMP'] == 1, :]['Sequences']])\n",
    "\n",
    "\n",
    "# Recreate your amino acid vocab\n",
    "aa_vocab = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aa_to_idx = {aa: i for i, aa in enumerate(aa_vocab)}\n",
    "idx_to_aa = {i: aa for aa, i in aa_to_idx.items()}\n",
    "\n",
    "def one_hot_encode_amino_acid(aa, vocab=aa_vocab):\n",
    "    vec = torch.zeros(len(vocab))\n",
    "    vec[aa_to_idx[aa]] = 1.0\n",
    "    return vec\n",
    "\n",
    "def generate_sequence_from_seed(model, seed, max_length=30, temperature=1.0, device='cpu'):\n",
    "    model.eval()\n",
    "    input_seq = [one_hot_encode_amino_acid(aa).to(device) for aa in seed]\n",
    "    input_tensor = torch.stack(input_seq).unsqueeze(0)  # [1, L, 20]\n",
    "\n",
    "    generated = seed.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(seed)):\n",
    "            output = model(input_tensor)  # [1, L, vocab]\n",
    "            logits = output[0, -1, :]  # Last time step → [vocab]\n",
    "\n",
    "            # Apply temperature and sample\n",
    "            probs = F.softmax(logits / temperature, dim=-1).cpu().numpy()\n",
    "            next_idx = np.random.choice(len(aa_vocab), p=probs)\n",
    "            next_aa = idx_to_aa[next_idx]\n",
    "\n",
    "            # Update sequence\n",
    "            next_aa_vec = one_hot_encode_amino_acid(next_aa).to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, 20]\n",
    "            input_tensor = torch.cat([input_tensor, next_aa_vec], dim=1)\n",
    "            generated.append(next_aa)\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = trained_model\n",
    "gen_model.to(device)\n",
    "\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "model\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides-notrans.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## No trans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EWPKCECWKPCYSGGG\n",
      "Generated AMP sequence: AEFDLDGHKQGKKD\n",
      "Generated AMP sequence: KAAYYSHENKGKDYGKHKKG\n",
      "Generated AMP sequence: IPWVKYDGDHHHSHHKGRGYPGASNPPGHGYKHEHHPKY\n",
      "Generated AMP sequence: IANQKQKKKCGDKHKYGHP\n",
      "Generated AMP sequence: FAAAYSLWRGYK\n",
      "Generated AMP sequence: ETAAEHSVAKIGGHYHKMHYYGKHHD\n",
      "Generated AMP sequence: VDADWGGKYGKPGPHKKKGS\n",
      "Generated AMP sequence: DAAKPAKGKGMHYG\n",
      "Generated AMP sequence: WAWTGNSKKGGYYK\n",
      "Generated AMP sequence: QANVVRLDGYKAGK\n",
      "Generated AMP sequence: CAHNMGPGHP\n",
      "Generated AMP sequence: ARLKAHGGYHYKQKKKGPHD\n",
      "Generated AMP sequence: DAWYYHAYKYYVHHTHGKYL\n",
      "Generated AMP sequence: HAKYAKWPPPHHRRYYKKQK\n",
      "Generated AMP sequence: IPTFHCKAKKHKDDSHGKPG\n",
      "Generated AMP sequence: TAAKKLPYKHQHKGDYSLHG\n",
      "Generated AMP sequence: YAHPNGYPKKIG\n",
      "Generated AMP sequence: ANAAAATPKGKKGKH\n",
      "Generated AMP sequence: VIAGLKTLGDKDKYKHKKKY\n",
      "Generated AMP sequence: HWVAACADNDHSDSHG\n",
      "Generated AMP sequence: VHTPRGGPSYHHPHYWKPPG\n",
      "Generated AMP sequence: QDRFWHAYFHYYKKKHHKPPPGKDKKYHGGPKGG\n",
      "Generated AMP sequence: IARPAAYHHSYYPHGPKKYG\n",
      "Generated AMP sequence: RAGAADLPKYHHYGYAYDYK\n",
      "Generated AMP sequence: WAHYKLPKKKYDHYYKKHGKHKGYYLKHSKGYSYPKKYYD\n",
      "Generated AMP sequence: KARYCKHYSS\n",
      "Generated AMP sequence: AAAATSYYKPYTGKGYKGK\n",
      "Generated AMP sequence: GAAMKQKTKKYHGDPKNGGC\n",
      "Generated AMP sequence: QGDHWHKQHKWYGYK\n",
      "Generated AMP sequence: MCYHGLGDGGGGK\n",
      "Generated AMP sequence: KATHNFGCGHGG\n",
      "Generated AMP sequence: FCAGKPARKEY\n",
      "Generated AMP sequence: HAARPGGHKKHKPDGPYKKGKKGHHDHGHGYKKPK\n",
      "Generated AMP sequence: MFKPHDGHKKH\n",
      "Generated AMP sequence: EHAACGGGHGYGKHYGHYYSGGKYKKHPGHKKGGKHKGK\n",
      "Generated AMP sequence: DACAAYHSGMPKKHHYSYGP\n",
      "Generated AMP sequence: PAAPKSFRGGGSKKHYHSKHKHPKGYHKYKGYKHGQ\n",
      "Generated AMP sequence: EFTRCGGPCKPYHSGSGHQG\n",
      "Generated AMP sequence: NMNLPKGHGYKH\n",
      "Generated AMP sequence: NWAKKKAWYKNYPGKKKPY\n",
      "Generated AMP sequence: YCAYSDYWKKHDHRAHHKYHHPKGKGGKKYHKKCK\n",
      "Generated AMP sequence: KWAGAYKKDPGY\n",
      "Generated AMP sequence: CAAHKAQKKKHK\n",
      "Generated AMP sequence: RASYHGLHAGPK\n",
      "Generated AMP sequence: VAANADRGKG\n",
      "Generated AMP sequence: EANSSHNGHYA\n",
      "Generated AMP sequence: PAAAKHYSHG\n",
      "Generated AMP sequence: DGALHKIHYPGYKYQK\n",
      "Generated AMP sequence: VAHLGDSYKG\n",
      "Generated AMP sequence: LAVAKGKLSGAGY\n",
      "Generated AMP sequence: YHKMDYHHKDGPGKGHHHPHKKKYPGDGKDK\n",
      "Generated AMP sequence: NAPAAPKPKGSRMHKHGYGY\n",
      "Generated AMP sequence: WAHTPSMPTGKGYYDNYG\n",
      "Generated AMP sequence: HTAPGYYYKWSHSDKGGGLH\n",
      "Generated AMP sequence: DAAKRHKYKGCYCYKYQYHPHK\n",
      "Generated AMP sequence: CSYQHHPGGHHHGDCYKHKG\n",
      "Generated AMP sequence: IPAGVYKKGYGYKKKGGGGPHDYYYYGHPLGSGGKKK\n",
      "Generated AMP sequence: LSAAVYVSKKG\n",
      "Generated AMP sequence: DAAPKAGPDTKKKSHKK\n",
      "Generated AMP sequence: ISQMSVPSPLKPG\n",
      "Generated AMP sequence: ESASAHPGSSKAHKYKGKKPGPKYHKYGYLFH\n",
      "Generated AMP sequence: PAIGLAWYYPGPDDHKPKKHY\n",
      "Generated AMP sequence: KCARPGDNKHYGKYKKHH\n",
      "Generated AMP sequence: RATCEALHWKYGYPPSPKGDYKQKGGKKGKEKHQHKAGH\n",
      "Generated AMP sequence: NHAPTRAKGGYGKKNKAHKG\n",
      "Generated AMP sequence: GAAGAKPRGGGKPDY\n",
      "Generated AMP sequence: NFNTPSHSDY\n",
      "Generated AMP sequence: NQGLKPDDGGYGYHGPKKKP\n",
      "Generated AMP sequence: HYYMDPDAGKYP\n",
      "Generated AMP sequence: KACGYAVKKPKYYKHKYGKKPGP\n",
      "Generated AMP sequence: DAAYDTKGYKKHKGHYGKHPGYKGGGKHSKGKNGG\n",
      "Generated AMP sequence: YMLHSSKWGG\n",
      "Generated AMP sequence: GAHYADKYHKDGY\n",
      "Generated AMP sequence: VAYLTKKKLGDHPGKYGKKYGLSYKKKYKDYLKGKHKG\n",
      "Generated AMP sequence: IYAYHVRKPYHHKRPQKSGK\n",
      "Generated AMP sequence: GADGPHKGHG\n",
      "Generated AMP sequence: RHLKIHVKKC\n",
      "Generated AMP sequence: PADKKKQSPYAGAK\n",
      "Generated AMP sequence: KAWAYHDKGHHRKHKPHIGGKGKNGKGYKRKDKY\n",
      "Generated AMP sequence: VQTCEKHSYHGPHSKKGKKK\n",
      "Generated AMP sequence: IMSAAALKKGHK\n",
      "Generated AMP sequence: MYAPHHPPYGGPKHGGYGHG\n",
      "Generated AMP sequence: CAHARGKYKKHHKYK\n",
      "Generated AMP sequence: IMGAKYGPGKGKPHKQYGHG\n",
      "Generated AMP sequence: CVHSKLDPGSPPDHGHGYKHHKKKYKYTGYGSHYYRPHGH\n",
      "Generated AMP sequence: MLCYMSFVYKKP\n",
      "Generated AMP sequence: PDDAHAAGYKYK\n",
      "Generated AMP sequence: KDFAMGKLGDPKDY\n",
      "Generated AMP sequence: DLYCCDHEPD\n",
      "Generated AMP sequence: HYRKGLKIGYKGHISPPAIK\n",
      "Generated AMP sequence: WAPAKGAGYR\n",
      "Generated AMP sequence: MDAKHSDKGHPKHHKPYYPHLKGSDYKKKYKKFKGPKK\n",
      "Generated AMP sequence: HANAAAYPHH\n",
      "Generated AMP sequence: SPLADCMPGAFYHPK\n",
      "Generated AMP sequence: PAAKQMKCYRHYKKPS\n",
      "Generated AMP sequence: RACWEHIKKYGGGYKHSWSY\n",
      "Generated AMP sequence: FNGSPQMGKYYD\n",
      "Generated AMP sequence: KANSEKSPKKKH\n",
      "Generated AMP sequence: FKSLQEKAGPYGHRGKPKGH\n",
      "Generated AMP sequence: IFAQAPGYHHGHYYHKPYKHILAKH\n",
      "Generated AMP sequence: VAKHMAKKKKLRHSKGDHPNPKKGH\n",
      "Generated AMP sequence: VAAKPYYKIKQGHSHKKG\n",
      "Generated AMP sequence: KAAASGKHYGKYSGR\n",
      "Generated AMP sequence: WDAAPKIPPGKGPKYFDHG\n",
      "Generated AMP sequence: QTADQIQDKDDY\n",
      "Generated AMP sequence: WFEGWAMYKYDI\n",
      "Generated AMP sequence: PMYDMGKKYKHGG\n",
      "Generated AMP sequence: NASKKDVPDPKG\n",
      "Generated AMP sequence: IDAYSNKHKKDKRKDPGGG\n",
      "Generated AMP sequence: FNHAVWWGKYPDEK\n",
      "Generated AMP sequence: TFGAMSHQYGKHQKKQHYKGLQHVYHYGGGYKGKK\n",
      "Generated AMP sequence: SSQIYHYMKG\n",
      "Generated AMP sequence: DAAAKKPYDPGVSKPHKSKHKSHGPSGGGHKPK\n",
      "Generated AMP sequence: CASHYLENHMDGVGSHKLKH\n",
      "Generated AMP sequence: EAAMYVQRLGYK\n",
      "Generated AMP sequence: FAFHTKHYGIDPYHWHYPRC\n",
      "Generated AMP sequence: GMASGVDPKGGAKHKHHPDYVKKYGGGQDKYVKESKGG\n",
      "Generated AMP sequence: QKRTKKIHKPKYKKKKNGKGHKKYKPKKFGHKDGH\n",
      "Generated AMP sequence: YAGAKGGKAAGDGHRKKHYHGKGHKYKMYGHHKHYPKHS\n",
      "Generated AMP sequence: DQALAGCKNGHKHGKPFGHYKKHKAKYKYPGYFKGAK\n",
      "Generated AMP sequence: PDIAAKHPKSNKK\n",
      "Generated AMP sequence: PAQHRSTYKHKHK\n",
      "Generated AMP sequence: YKDAHDYHGCKKHFHHKKHG\n",
      "Generated AMP sequence: RCMAKAKYHDDHGDPKHYGYHYKSYHYHHHKGKKPSKHGG\n",
      "Generated AMP sequence: TAAYKHKGKK\n",
      "Generated AMP sequence: KAYTLKKGHDKKGYYPKYGP\n",
      "Generated AMP sequence: VPATYDKYKWK\n",
      "Generated AMP sequence: AAAQHKKKWYHSPKYG\n",
      "Generated AMP sequence: ERFSWAYKKKY\n",
      "Generated AMP sequence: VDMPPDHHRH\n",
      "Generated AMP sequence: KWAYPSHAPSPG\n",
      "Generated AMP sequence: MNLQSKDKDHHDHGRYFYKHGKQN\n",
      "Generated AMP sequence: EAEDFGSYHY\n",
      "Generated AMP sequence: LDALYKHLGYKKKK\n",
      "Generated AMP sequence: QKAPSSKKGP\n",
      "Generated AMP sequence: GDAALAPHAKDHP\n",
      "Generated AMP sequence: RRACPGDHGS\n",
      "Generated AMP sequence: APTHYHYDNKHKNA\n",
      "Generated AMP sequence: KLLYQYPHGPKD\n",
      "Generated AMP sequence: TWGHHGHHDPISTGK\n",
      "Generated AMP sequence: GSANKGFLPHKHPGYHHDG\n",
      "Generated AMP sequence: TASAHHAGHKHKKAKYKKHP\n",
      "Generated AMP sequence: EEYAAQSGGAHPKHHAHKYTDYPPK\n",
      "Generated AMP sequence: LAACFKPKDYYPYKYGH\n",
      "Generated AMP sequence: TAALRKGSKHLG\n",
      "Generated AMP sequence: YVAHWPCGAKKGYKG\n",
      "Generated AMP sequence: HKADSKKKDGKGDSYQVGYG\n",
      "Generated AMP sequence: FYNYKGKKDKYGQYYYKYPGKPGKKGKPKAAHYGKGH\n",
      "Generated AMP sequence: NMAHAKMGHHYYDPHHPYYKHPGKHKGGY\n",
      "Generated AMP sequence: GDAAAFVHYKK\n",
      "Generated AMP sequence: VEPDGYYYIPKKGKGHVGFF\n",
      "Generated AMP sequence: TEERSHPPKGYKHKDGPAIKDYGHHGAPYGKPTKHHP\n",
      "Generated AMP sequence: AADVSYPHKKFKPGPKYDHKHPPGKK\n",
      "Generated AMP sequence: YVRPPKYPPWHRGHHY\n",
      "Generated AMP sequence: MLNKYKHGQPHDSANYKGNSKYDKPWGQKH\n",
      "Generated AMP sequence: SSAEGVNRGKK\n",
      "Generated AMP sequence: ATANAKGHHKHPHEPHKKHDHYKKKG\n",
      "Generated AMP sequence: ENADGHYDKGYGHPNKH\n",
      "Generated AMP sequence: NAAHGGMYKEEYHGKKK\n",
      "Generated AMP sequence: LAYDPPKHGKWYG\n",
      "Generated AMP sequence: IWPTDKPNHAKG\n",
      "Generated AMP sequence: CWDSDPYEAKGPYKGKPAK\n",
      "Generated AMP sequence: IIAPAGWPAGGHGDKPKKGY\n",
      "Generated AMP sequence: WQRYKGKPPKDHYKKHHYKPKKGNKK\n",
      "Generated AMP sequence: DVASGIAHKDKYGPHKGGGF\n",
      "Generated AMP sequence: DKTRPDGHGKKH\n",
      "Generated AMP sequence: SQQEYYHHKYYGHKK\n",
      "Generated AMP sequence: DRLSTPKKGLGM\n",
      "Generated AMP sequence: VAAAPPKGEAGY\n",
      "Generated AMP sequence: FAAAKKGDHGGY\n",
      "Generated AMP sequence: FGEHKPPGKHYKKGKGHGLG\n",
      "Generated AMP sequence: STAIHARDKKKLGAPKS\n",
      "Generated AMP sequence: VRGFRIGTYKEHRYGEYHKYFKGHRK\n",
      "Generated AMP sequence: GGAAGHDHKPKPAGKKARPKFPKPPRKGKKPGGYPYPKKY\n",
      "Generated AMP sequence: KIAHPGGHGGKYKGGLHSKQ\n",
      "Generated AMP sequence: TKAYLHGGPGKGK\n",
      "Generated AMP sequence: YPPAAPHKCGDYGKDKHYYHYYSSGK\n",
      "Generated AMP sequence: QAYFPSVYGD\n",
      "Generated AMP sequence: HAAGRKHLGAPKPDSKK\n",
      "Generated AMP sequence: VSKKYQKPHGKYYHKGYYKR\n",
      "Generated AMP sequence: HHAHNPHYGKMNKHGKKGNKKS\n",
      "Generated AMP sequence: LAQKPSSYKKHG\n",
      "Generated AMP sequence: PAVKKHAAIFKYYKGWYGPH\n",
      "Generated AMP sequence: NYAVLYKTDHDYSVYGHFGGKHGGG\n",
      "Generated AMP sequence: RNYHGRHHGGGHKDGRGHYGGKHKRGKKGKHKGKHSDKK\n",
      "Generated AMP sequence: TGLQTGKSYYHGKPKKSHWHHK\n",
      "Generated AMP sequence: RAAYDKKKKGKHKGKGHGGSHYYK\n",
      "Generated AMP sequence: EAQGAKSRGLGG\n",
      "Generated AMP sequence: IAAATRKPGKHGKYDHGGYKKPGKKKYPHSG\n",
      "Generated AMP sequence: IACILYKKDD\n",
      "Generated AMP sequence: DASAAIGDPGKHGHRHPGPG\n",
      "Generated AMP sequence: MGWGAASKPGHDYKYGKKHT\n",
      "Generated AMP sequence: ASPDGYAPPKYHHGKGHSYGHYEGGKGYKKGKKPPKY\n",
      "Generated AMP sequence: WNAAENHHYYYHLHGGHKSGPKHKHK\n",
      "Generated AMP sequence: VAAKWHPLYSKKYHHASQ\n",
      "Generated AMP sequence: IGGAYGDKPDK\n",
      "Generated AMP sequence: WAAKGYSYGYKHDDLKGKGK\n",
      "Generated AMP sequence: IQRHGHSHGFKKSGH\n",
      "Generated AMP sequence: AQDHGYEYNKDKHGKYKPFIGKYKGH\n",
      "Generated AMP sequence: DARRLHGFHHLKEHGGSDKG\n",
      "Generated AMP sequence: CGAIPYGYYYKKHPKKKPSGSKSKHYGSKKHK\n",
      "Generated AMP sequence: IAAAYYHPHLRYHP\n",
      "Generated AMP sequence: DYAKNPEYGKGHK\n",
      "Generated AMP sequence: CAAGQYDHGAKKGGDSY\n",
      "Generated AMP sequence: MASWHAAKHCK\n",
      "Generated AMP sequence: DPAAKHYKHP\n",
      "Generated AMP sequence: TTDKAHSSGGSYG\n",
      "Generated AMP sequence: IASAEPAKKHHKGKRPGGH\n",
      "Generated AMP sequence: KAMAAHPGKGYYHGH\n",
      "Generated AMP sequence: SKRNSHGPHLHG\n",
      "Generated AMP sequence: HAYYRNKWHKKHKHKTGYLSKPCSDYYHGPGYGGYKTHKK\n",
      "Generated AMP sequence: VKIHGQGPGKPGDGKTDPHK\n",
      "Generated AMP sequence: FAGAAGAKKKGHKYDYKKKFGKHYHGPHKLYPKKY\n",
      "Generated AMP sequence: WAAANLKAPHKKH\n",
      "Generated AMP sequence: WARWMDHPQGA\n",
      "Generated AMP sequence: SAAAKTYKKFKHKHGGHGGYK\n",
      "Generated AMP sequence: ISYKSHHHLMHGKGKLMPYK\n",
      "Generated AMP sequence: SHAAGIKTGGYGKKKGDGKKGK\n",
      "Generated AMP sequence: QAGNGGKYSKPHKKGMKHYDHDPMHK\n",
      "Generated AMP sequence: HNGGYGEKYHDHKGKYK\n",
      "Generated AMP sequence: EAATHHMKKDSYKYG\n",
      "Generated AMP sequence: EHIGGDHGPGDKKYKHRKGK\n",
      "Generated AMP sequence: QAAACHKDYPKSGKKSPHYGYYGPDHKKYKKRYDY\n",
      "Generated AMP sequence: NQAAHMHDKYH\n",
      "Generated AMP sequence: QAASSKHRKSKP\n",
      "Generated AMP sequence: QAYKKGKKGDKPKGKGKGGG\n",
      "Generated AMP sequence: RADIKHSGKDEKKYKY\n",
      "Generated AMP sequence: CAAAKMAYSH\n",
      "Generated AMP sequence: EAAAKGHYNY\n",
      "Generated AMP sequence: CAHAANLHKICH\n",
      "Generated AMP sequence: PAYKPGHGFGRL\n",
      "Generated AMP sequence: MCVAGHRYGYSGHHYGQHPK\n",
      "Generated AMP sequence: ERQTAPFHKHKYGKHGGYYY\n",
      "Generated AMP sequence: IACMKHHKKGKKKGPDHPKGLKQPKKGGKKKHAKHPDKYG\n",
      "Generated AMP sequence: HAKKHKYGAWKYKRKPYKG\n",
      "Generated AMP sequence: HAAEKKIKDFGKG\n",
      "Generated AMP sequence: VARAMNRYAGYKK\n",
      "Generated AMP sequence: RKWYPGWDGYGGGWKKGPPH\n",
      "Generated AMP sequence: FAYLYGRPPYHGKPKGK\n",
      "Generated AMP sequence: QAKPFHGGQYHPYS\n",
      "Generated AMP sequence: GWDFAKDGHSYH\n",
      "Generated AMP sequence: KRPAPYPHKKKGHRWGYHKPRYHWGNPKGY\n",
      "Generated AMP sequence: RANHSHGSKGPK\n",
      "Generated AMP sequence: ICRSPDSMPHYPGHYDYKKH\n",
      "Generated AMP sequence: DWAQHDAKYHHH\n",
      "Generated AMP sequence: RATIKDYGDKKIKPGSKKKG\n",
      "Generated AMP sequence: VEAKYHYHDGIGAYSPGGFPYKGK\n",
      "Generated AMP sequence: EAKPAAYGDGDHRHK\n",
      "Generated AMP sequence: CESTARKYKYPGKYVGNHAH\n",
      "Generated AMP sequence: VQCSAGGGDGKH\n",
      "Generated AMP sequence: AATVKHDYDHHHGPGKG\n",
      "Generated AMP sequence: DTASKAYGVG\n",
      "Generated AMP sequence: IAHQYYSYGHHHHKGLGHKQPKGKGHIDHY\n",
      "Generated AMP sequence: GAAAHYGPGYHPHKGGYYYKRFHHKHGKSHGGGHKKL\n",
      "Generated AMP sequence: QQAHRFDYHMCMHDK\n",
      "Generated AMP sequence: SAAAGCYAKGHY\n",
      "Generated AMP sequence: SYAPKWKKGGYH\n",
      "Generated AMP sequence: HNCYPKWDGKGPYKHY\n",
      "Generated AMP sequence: PDCWKQSHKTHQYKPRSH\n",
      "Generated AMP sequence: CALHPHLGPK\n",
      "Generated AMP sequence: GAPHEYPYKKYK\n",
      "Generated AMP sequence: PAFADAAYGKSKGPKPPYKHK\n",
      "Generated AMP sequence: ATAFSKHKVH\n",
      "Generated AMP sequence: PAPSDHGPKKGGDHKWKKYPYKGDPGHKKKG\n",
      "Generated AMP sequence: KHYPYQKGLKKKKHKG\n",
      "Generated AMP sequence: RAAHNIGKHYYKHPDKHYAKPKGYKK\n",
      "Generated AMP sequence: LDFAASGGKPDK\n",
      "Generated AMP sequence: QAASAAKSGID\n",
      "Generated AMP sequence: VGIFGDHEPYYWKNHKKKP\n",
      "Generated AMP sequence: SAWAKYKKGHTKK\n",
      "Generated AMP sequence: FAYGKAPYKGK\n",
      "Generated AMP sequence: HATLWLFHGYPKHGPKGKDHKGYG\n",
      "Generated AMP sequence: LAPHKAAKSYG\n",
      "Generated AMP sequence: HTDSAYPYMHYHHNPYGQPE\n",
      "Generated AMP sequence: CAAYMPYHRKYGYGHP\n",
      "Generated AMP sequence: WHFPEAHKGHHDKYGKK\n",
      "Generated AMP sequence: VLEIAPMCHYIKYPKQY\n",
      "Generated AMP sequence: CCDYPAVKDKIYGSKDYKH\n",
      "Generated AMP sequence: MRAAAGQKGGHYHYKGYKP\n",
      "Generated AMP sequence: CAAPGGAAPGKKKDPAPYKKKHYYHKYYKKHYGKHHHYD\n",
      "Generated AMP sequence: CAAHKGHDHYYHKYARYGGY\n",
      "Generated AMP sequence: WAAHPPYHKKHGYVKKGKLK\n",
      "Generated AMP sequence: SHDQDHHKQKQKHRGGKGAKKKYKYGKYKPHGKHYGYYGH\n",
      "Generated AMP sequence: TYFFKPGPDDHHHKHGHYHKKYGKPKGKEKPGSD\n",
      "Generated AMP sequence: TCAAYAGHKKGHGGHNGHHYYQGFHHK\n",
      "Generated AMP sequence: GRSGPGCPAHHMPHSHKPLYGKKYG\n",
      "Generated AMP sequence: CGAHGKYKKKDGKHGKHGKGYYNG\n",
      "Generated AMP sequence: TQPAMSKLKKDKGKHKGLQGGGYKK\n",
      "Generated AMP sequence: DISSLADDGGG\n",
      "Generated AMP sequence: GWKSPDPGKHYGHYDM\n",
      "Generated AMP sequence: DRYYASLLGKYYKDHSMKDKKY\n",
      "Generated AMP sequence: YAHAYHKHNKPHHGKDKYYK\n",
      "Generated AMP sequence: DALSFHSYGKGGYSPDSKH\n",
      "Generated AMP sequence: IAPKDGGDKSADKSHHKHDP\n",
      "Generated AMP sequence: PGPNDHEGMVYKGGSKKPMS\n",
      "Generated AMP sequence: ENSDMGYAKG\n",
      "Generated AMP sequence: WAAHYGGPKYGKGGKKPHKHY\n",
      "Generated AMP sequence: IACKGYGYGKYGY\n",
      "Generated AMP sequence: WWLEDGDGGKQGGKGGGYS\n",
      "Generated AMP sequence: YKHNPLGCKYKSYGL\n",
      "Generated AMP sequence: CYYNFRDQYHKKKHPYYGYP\n",
      "Generated AMP sequence: YAAWLPGPCHKG\n",
      "Generated AMP sequence: DALGKDAYGSYGGKYYKQGYYGHGHS\n",
      "Generated AMP sequence: QACYLHGYSHEDGKYK\n",
      "Generated AMP sequence: WANPDHLDHKYKSKQKYHKKIGGCYGG\n",
      "Generated AMP sequence: WAHAYTVHPGACKKKAKPKPGSK\n",
      "Generated AMP sequence: TAKSHGVMKKGK\n",
      "Generated AMP sequence: MYGHKYRKPKKGKGY\n",
      "Generated AMP sequence: KWHLKDHEKKPGGH\n",
      "Generated AMP sequence: HLEIAYNHKGYGKGPPKKYGHGYHPKGGHHGDYYH\n",
      "Generated AMP sequence: MAMSCERKDKHHPR\n",
      "Generated AMP sequence: IWEQKYYDYSQGRTHYD\n",
      "Generated AMP sequence: KFILAYAGAKYK\n",
      "Generated AMP sequence: PAHHKAPKPKGKGYKGYKGKGRSYKK\n",
      "Generated AMP sequence: FAHKAHTVFGGYGPGKHPGKGPYG\n",
      "Generated AMP sequence: LHAHDPPFKSYKYKHTKKKSSHHKKGQKHG\n",
      "Generated AMP sequence: RWAAIGAHKKHK\n",
      "Generated AMP sequence: MAACHQGPAIPHKGPDVGYHPHYGGKGGYYHYKHMKYH\n",
      "Generated AMP sequence: DAAAYSRKHGHHPKGCGHYK\n",
      "Generated AMP sequence: ACDAVRGKKKYKKDEGDPPGSGHGH\n",
      "Generated AMP sequence: RDEYADAKDSKKK\n",
      "Generated AMP sequence: YAAKDDYARQHG\n",
      "Generated AMP sequence: WLKQYPGGSGEP\n",
      "Generated AMP sequence: EVLAYGHHDKGHHPRGH\n",
      "Generated AMP sequence: DAQPDHDKGYPHKYKG\n",
      "Generated AMP sequence: VRKMTYYEKPPKGQPG\n",
      "Generated AMP sequence: HACAHKKLSGGTGYSKYGH\n",
      "Generated AMP sequence: TAWAKKMGHSGSKPPGKYKY\n",
      "Generated AMP sequence: KADYGVWHEPHPKKPHKHH\n",
      "Generated AMP sequence: FQDINGGYPKKDKKKHGGYH\n",
      "Generated AMP sequence: NAAPKKGPGGKHGKKSIGH\n",
      "Generated AMP sequence: DHNAKAKYYKKYKEKGKKKHGKSKHYYGDPSYYKKDGKK\n",
      "Generated AMP sequence: IMAKAHPPYHKHKVLYGYDY\n",
      "Generated AMP sequence: NAHYAHTHGKHYPK\n",
      "Generated AMP sequence: LCADHEYPGKPYYKPKKGGKHDPDKKYKEKSYKHYGKHDH\n",
      "Generated AMP sequence: GKWDSAKDYPKSK\n",
      "Generated AMP sequence: RLWIAKEKKKAGKYHKPKHHGYGHDGPGGKAGRPYVKDP\n",
      "Generated AMP sequence: VPHCSPYGGGQGEYDSHSKYYAGGKYKKG\n",
      "Generated AMP sequence: LYAHAKKGKHKKAPPHDPYGHGKGHL\n",
      "Generated AMP sequence: YAEKAAWDPYHHKYYPGKHK\n",
      "Generated AMP sequence: TAQHGHTPGG\n",
      "Generated AMP sequence: AVIKKHSKGGKY\n",
      "Generated AMP sequence: VSPCADPGHKPGHK\n",
      "Generated AMP sequence: LQARPKPHGKKKHYKGKPQK\n",
      "Generated AMP sequence: EAEVWPVDWHGGIGHGHKKHPYPHPGGHRGEGWK\n",
      "Generated AMP sequence: FGHMFVHPAHHKF\n",
      "Generated AMP sequence: KALRKPMHSHKPHHKSYKKYGGNSV\n",
      "Generated AMP sequence: EAFCGQDSKKGHGYRKG\n",
      "Generated AMP sequence: EAAKYAGYGGKWKYKGYHHKHGAKSKYHHGGHPYYG\n",
      "Generated AMP sequence: VAATLEIHPKYDHKKHPYYRTKSHHHKHYHGK\n",
      "Generated AMP sequence: FAAHHRPIHH\n",
      "Generated AMP sequence: KAARIDGYVPKKYYGRGGKHYGKYY\n",
      "Generated AMP sequence: LAKAYYGGEYGI\n",
      "Generated AMP sequence: YHAAYYQAHKHYKFYYGK\n",
      "Generated AMP sequence: HAAKYKDKHVKGGP\n",
      "Generated AMP sequence: MNAFIYYGHGKKY\n",
      "Generated AMP sequence: HAGVSAHIGGDKGDGRTKIYGGHGHYKHYDGKKDH\n",
      "Generated AMP sequence: KLKFAHFYKHPKHSYKDKPG\n",
      "Generated AMP sequence: TSAPIIASHKLP\n",
      "Generated AMP sequence: SFDYMGESHKKGKDIKKPSPDGKYSSHKKGDYGGGYKK\n",
      "Generated AMP sequence: KAKADSQHHKAGGGRHGKYGGHPKSYPKKDYHGKH\n",
      "Generated AMP sequence: CGFSGIKLQHKGYHPDGKYK\n",
      "Generated AMP sequence: DDGARGYYYKLPGKDKGGHKKGKGKKKPHPKKKDK\n",
      "Generated AMP sequence: QAAAYPEHKSDGYAGYKGFD\n",
      "Generated AMP sequence: KSEPYHGKYRDM\n",
      "Generated AMP sequence: CTAKVHKDHHYHKKGKPYKK\n",
      "Generated AMP sequence: AWAQGYPPGKK\n",
      "Generated AMP sequence: MAYVGSKKHKPHKMPDWKGGKYHPG\n",
      "Generated AMP sequence: FYPAKGHQPGYKHKKKKKGS\n",
      "Generated AMP sequence: KAPYYHFKGYGGHHQKGHKG\n",
      "Generated AMP sequence: GGNYYYGGHYYWGYKAKDKH\n",
      "Generated AMP sequence: RHSMHSYGKHDKWGSVYHY\n",
      "Generated AMP sequence: VFAAPIGGLARFGH\n",
      "Generated AMP sequence: QAAYMAIYHGPKKDGKGKKH\n",
      "Generated AMP sequence: VRNAAKGYGKHKGKHYDYSHPDYKHHFYDKKYGKKMF\n",
      "Generated AMP sequence: AAPAKGVYKHGD\n",
      "Generated AMP sequence: EIHYHPKKGKKPY\n",
      "Generated AMP sequence: DAGKDQYYGYPKKT\n",
      "Generated AMP sequence: FAGKDYNKGKGFKHKGKGYK\n",
      "Generated AMP sequence: VAAYGHLASKHDGYLGELKH\n",
      "Generated AMP sequence: CALYDKHKGSKKHGG\n",
      "Generated AMP sequence: NAMGHKHEHKKHKGGDYGGHG\n",
      "Generated AMP sequence: WAALGPKGKGKKHKYYKHGG\n",
      "Generated AMP sequence: VAVGHHSLKHSDYKHKKAYK\n",
      "Generated AMP sequence: FAFHHPHYKSGHHKW\n",
      "Generated AMP sequence: QASANKSGGK\n",
      "Generated AMP sequence: FAAKYSYAGKSHGPDHKKPHKKKKGGGGKSMPHHHGE\n",
      "Generated AMP sequence: CYAARGSPYHKKKKGKKRPK\n",
      "Generated AMP sequence: LMGVYASCHKG\n",
      "Generated AMP sequence: NAAKPYPMHAGKKIPKGGSPYGY\n",
      "Generated AMP sequence: CAYDNFKYHLGK\n",
      "Generated AMP sequence: NAAAWHGKHPYKWK\n",
      "Generated AMP sequence: HKHMWIYPKKHY\n",
      "Generated AMP sequence: IAALAAKHKGGLKGHKFGGS\n",
      "Generated AMP sequence: ENNMIMAKQKHHKLYHPKYKKKPY\n",
      "Generated AMP sequence: NAWKLKDPPH\n",
      "Generated AMP sequence: VRATGKYNHNHHYP\n",
      "Generated AMP sequence: QAAAKQKSCHGKFYGYKYKY\n",
      "Generated AMP sequence: YNVAAHYKKG\n",
      "Generated AMP sequence: FQALYGNDKG\n",
      "Generated AMP sequence: IAGKDPHLGNGKGPGYYGKGHPK\n",
      "Generated AMP sequence: GADAPKPGHHCHRYKKDYAY\n",
      "Generated AMP sequence: GAWAKKGIKPAYGGGDGYHG\n",
      "Generated AMP sequence: QMSDAGCEKGK\n",
      "Generated AMP sequence: AVVKGHHPKVYHK\n",
      "Generated AMP sequence: GAEAVEKKPHY\n",
      "Generated AMP sequence: MAAHSKSKGAHNQHKKHGGG\n",
      "Generated AMP sequence: QAALGYGHHA\n",
      "Generated AMP sequence: IAWVKKMHKDDKKYGSGHHHGKQKKT\n",
      "Generated AMP sequence: KDAVKGGRDYGQHKYIKDAG\n",
      "Generated AMP sequence: GAAGMPGDYGVWPGSPKHGG\n",
      "Generated AMP sequence: EGCHHLYGHGKPYHYEKDYK\n",
      "Generated AMP sequence: PQAAVGFKKDHDKYYKGYKG\n",
      "Generated AMP sequence: CNPPDGPGKPPKQSKGHHGHGKYGPKGYKHPGLDS\n",
      "Generated AMP sequence: SIANQKKKYKSRHGHSGK\n",
      "Generated AMP sequence: IAKCARDGVHYYHDNKGNDYGGGYLQ\n",
      "Generated AMP sequence: HAAHKLGYGYAKHKKKI\n",
      "Generated AMP sequence: RKRDKKHKHGGGLKYPDYWKKY\n",
      "Generated AMP sequence: NTAAHKSYHDRKGKPPKHYK\n",
      "Generated AMP sequence: LAPCDKGKGKHAGYYGP\n",
      "Generated AMP sequence: IYDNAHYKHGPKHYDHYHGK\n",
      "Generated AMP sequence: IAWRDPHHQYGKKHHDKKGSRKHKHG\n",
      "Generated AMP sequence: AAAACAYHGGYKHYHHGHHKHK\n",
      "Generated AMP sequence: HAEYHNGGDH\n",
      "Generated AMP sequence: PAPAKHKKHHPY\n",
      "Generated AMP sequence: MATGKHEKKGKQ\n",
      "Generated AMP sequence: KAHPRLGPMCHHKGKYQYYA\n",
      "Generated AMP sequence: DASDYKRDGPCSAKHKLHGF\n",
      "Generated AMP sequence: KEPIHHAHYYHKTKCGKYLF\n",
      "Generated AMP sequence: NPSKMASRGPGD\n",
      "Generated AMP sequence: TACGKKGAKHHY\n",
      "Generated AMP sequence: PRGAPGKDSGYWKPKRKPKYKKP\n",
      "Generated AMP sequence: VFAHYPSAKKSDVKGHHKDGSGGKKP\n",
      "Generated AMP sequence: MPKACIHPKHGGKKKGKV\n",
      "Generated AMP sequence: ADYFKCKDYPQPK\n",
      "Generated AMP sequence: ERYHYKKKGAGKHKYHKGD\n",
      "Generated AMP sequence: KGPAKGGAGHGYHGGKEWYGKDKIH\n",
      "Generated AMP sequence: GIEAPKVGQLGM\n",
      "Generated AMP sequence: WWALHYHYYGRYGKKHCG\n",
      "Generated AMP sequence: KAATPALLYAGYY\n",
      "Generated AMP sequence: CNAIFFSGKYGYYKGHKHHYGP\n",
      "Generated AMP sequence: EHVDDMPPHG\n",
      "Generated AMP sequence: YVGSHHGIKCKPLKYCKPKHGYKYKKRKHKRHHGYGKI\n",
      "Generated AMP sequence: QKAKQSGYKYKKYYKHH\n",
      "Generated AMP sequence: NAASAPGHDGPYGKKYP\n",
      "Generated AMP sequence: MKSAHYPKKHKKIPWGKHLG\n",
      "Generated AMP sequence: QSEWYDGYYGAGYGKTDKHW\n",
      "Generated AMP sequence: YKLAAHGSRKKKS\n",
      "Generated AMP sequence: TAAKHIDPGGHGKHKKGG\n",
      "Generated AMP sequence: EAEAKKKPGGSYGKGDHGPKGARGKHKHGGGK\n",
      "Generated AMP sequence: PAAIKYKKKYSHGGKPKFPR\n",
      "Generated AMP sequence: WKFVSKPYKHPGH\n",
      "Generated AMP sequence: HMEAGGGPGKHYHGGKKKKDKKGGGGKGGG\n",
      "Generated AMP sequence: KRAFDKAMKYGRQGHGGGKHYGYGPHGWHKKKKPGKYD\n",
      "Generated AMP sequence: CLFGAKEKPGKY\n",
      "Generated AMP sequence: QREQADGHTHKPKHKHGYGG\n",
      "Generated AMP sequence: AALFHGHHPKYDKKYDKVGYDPYKKDYKDLKG\n",
      "Generated AMP sequence: TAILKSVKAPIK\n",
      "Generated AMP sequence: VAVANAYFFHHGKKDHRLPL\n",
      "Generated AMP sequence: HAASYKAGHKEYHY\n",
      "Generated AMP sequence: NNKLLLKYPPYGGKKSKKKDKSGKK\n",
      "Generated AMP sequence: QAYGEFNKKFYH\n",
      "Generated AMP sequence: DCDTYGVQYGKHSA\n",
      "Generated AMP sequence: MAALSKTGKKAKKKPKEGKH\n",
      "Generated AMP sequence: YAHFPMHPKK\n",
      "Generated AMP sequence: MAMGYSKGGHKHKGYGYYKP\n",
      "Generated AMP sequence: EICEKKKHHFDGFGSDKH\n",
      "Generated AMP sequence: LDAIHKDYKHPHSKPGSPGGHYGKDHKYKSK\n",
      "Generated AMP sequence: TAIYTGGGYT\n",
      "Generated AMP sequence: LAGHGKAGYPKKHPPPPGHKHPHGH\n",
      "Generated AMP sequence: QAKHAKPKYHGYHKQHPGHGRGPYKKKHKKYYWYKKYHY\n",
      "Generated AMP sequence: MHNYFPKGPKKHKKHGY\n",
      "Generated AMP sequence: PMEGHHKKKGKGGYKKHYKP\n",
      "Generated AMP sequence: LWRSDGGGVDEHGGQK\n",
      "Generated AMP sequence: VAEAPPPPAY\n",
      "Generated AMP sequence: FTAYSSDPYLKYKKKYGKLPKYYEHD\n",
      "Generated AMP sequence: HFVSACKKGYYHYH\n",
      "Generated AMP sequence: QDAWHPHAGKMHMMNKYKKGHDK\n",
      "Generated AMP sequence: PAPFFFQGKKMK\n",
      "Generated AMP sequence: GAADCGHKLHPHHKDGIYPGSPKSHG\n",
      "Generated AMP sequence: YFHKYYWRYGGKTKPGGPGH\n",
      "Generated AMP sequence: WANGKCSHKPGYK\n",
      "Generated AMP sequence: LAWPDHSPPKPHGKYHKK\n",
      "Generated AMP sequence: PRPAYPVLVGHYGGPSGVLK\n",
      "Generated AMP sequence: VAADKKSRKPGDYGGLHYHAHKHKYY\n",
      "Generated AMP sequence: ACHYYGPHAAKKD\n",
      "Generated AMP sequence: LAHEDHGKGKGKPGKFKGKK\n",
      "Generated AMP sequence: LFIGYQGSKSSGNHHHPGAPGGGHKDYPHHKKGGW\n",
      "Generated AMP sequence: HASMGPAPKSYYY\n",
      "Generated AMP sequence: QAGNVGYSYYHGGHKRYKAG\n",
      "Generated AMP sequence: WAGAGFLKKKY\n",
      "Generated AMP sequence: YEPKYHKDKHGK\n",
      "Generated AMP sequence: MAMLDKPKHHKYKFKYGYYG\n",
      "Generated AMP sequence: RASGHLQAKGHHKHGGDKYGYPKKSGKLKSGLKHHK\n",
      "Generated AMP sequence: RDRGAAAHHPPDGHKGGYRGHHPKGPHGHKHGHD\n",
      "Generated AMP sequence: RAAMQSKGPPGYGYWLYPGKFHPHH\n",
      "Generated AMP sequence: HSAGCPDMHGYCKDKKHGHK\n",
      "Generated AMP sequence: TEKAGEGGKKYGKSKGGDHPDGNPK\n",
      "Generated AMP sequence: SAAGKYDKHKCHSKKHHHSY\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "lstm_gen_best_params = {'hidden_dim': 125, 'num_layers': 3, 'dropout': 0.1809741409069741, 'lr': 0.009903431049726066, 'weight_decay': 0.00095634187480499}\n",
    "\n",
    "# lstm_gen_best_params = {'dropout': 0.13882938162931305, 'lr': 0.001464264209101335, 'weight_decay': 0.0002912839503135125}\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_notrans_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(500):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides-notrans1-500.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EWPLAAATLPAYSDDD\n",
      "Generated AMP sequence: AIHFMAGGHRDKLA\n",
      "Generated AMP sequence: KAAYYSGAPHAIAWFNGKKD\n",
      "Generated AMP sequence: IQWVLWADAGHFSGFKASAYRDASPPRAGATMGAGGPNV\n",
      "Generated AMP sequence: IAPQKRIKHAAANGKWEHQ\n",
      "Generated AMP sequence: FAEAWSNTSDVK\n",
      "Generated AMP sequence: EVEADHSTAKHDAGYGKPGYYEIGFA\n",
      "Generated AMP sequence: VHAEWEAIYEPQEPGMNKAS\n",
      "Generated AMP sequence: DAALPAKDNCPHYA\n",
      "Generated AMP sequence: WAWVENSKKEAVVN\n",
      "Generated AMP sequence: QEPVVRNACVMAAP\n",
      "Generated AMP sequence: CAKPNDPEGQ\n",
      "Generated AMP sequence: ASNKAGFEYHVMRKLIDPFA\n",
      "Generated AMP sequence: DAWYWGAWHYYTGGTGCKYP\n",
      "Generated AMP sequence: HAMYAKVQPPGGSSTWKKRN\n",
      "Generated AMP sequence: IRVGGALAHKGLAASGAKRD\n",
      "Generated AMP sequence: TAAKKNPYKGRFHAAYSPHC\n",
      "Generated AMP sequence: YAKPNFYQKMHE\n",
      "Generated AMP sequence: AQAAAATPIAIICHH\n",
      "Generated AMP sequence: VMAGMKTNCANALYLHNHKY\n",
      "Generated AMP sequence: HYVCAAAAPAGTASFA\n",
      "Generated AMP sequence: VMTQSFCQSYHGRGWTHQQA\n",
      "Generated AMP sequence: QHSFVGAWAGYYLMHGGPQRRAIAKHVGAAQMAA\n",
      "Generated AMP sequence: IASPAAYGGSYYQGARKHWA\n",
      "Generated AMP sequence: RAHAAANQLYGFTDYAYAVK\n",
      "Generated AMP sequence: WAKYKMPHMLVAFYVHMFDHGLAYYPKGSPCWSYPMHYTA\n",
      "Generated AMP sequence: KFSYALGYTS\n",
      "Generated AMP sequence: AGDATSYYKRYTEHAYMEP\n",
      "Generated AMP sequence: GEAPKRKTKKVHAAQMPDAA\n",
      "Generated AMP sequence: QKGHVGKRHMTYAWI\n",
      "Generated AMP sequence: MGYIGMEAAACAN\n",
      "Generated AMP sequence: KEVHPCDACFCA\n",
      "Generated AMP sequence: FHAGIPARMAV\n",
      "Generated AMP sequence: HADSQCDGIMGHQAEPVKKAHKEFGAGDGAVLPRI\n",
      "Generated AMP sequence: MILQHAFGNKG\n",
      "Generated AMP sequence: ELCAAECDFCVAKHYAHYTSFCPWKKFRDGHKEAMGLCI\n",
      "Generated AMP sequence: DAFAAYGSEPQLIHGWSYAP\n",
      "Generated AMP sequence: PAAQLTARDCCSPKGVHSHGHGQKEVGKWMAYMGAR\n",
      "Generated AMP sequence: EIVRAGEQAKQVGSATAHRC\n",
      "Generated AMP sequence: NPPNPKEGCWLH\n",
      "Generated AMP sequence: NWALKMATWKPWPCLMHQY\n",
      "Generated AMP sequence: YGAYSAWTKLHAGSAGHKWGGQHDHACIHWGLMAP\n",
      "Generated AMP sequence: KYAHAYKLAQDV\n",
      "Generated AMP sequence: CAAHKARHKHHK\n",
      "Generated AMP sequence: RATYGFNHACPN\n",
      "Generated AMP sequence: VACPAARCNE\n",
      "Generated AMP sequence: EAPTSHPDGVA\n",
      "Generated AMP sequence: PAACLHVSGF\n",
      "Generated AMP sequence: DKDMHKHGYQATIVRH\n",
      "Generated AMP sequence: VCKMEASYKC\n",
      "Generated AMP sequence: LAWALFKNSDAAV\n",
      "Generated AMP sequence: YLLNCWHGKACQAMEHGGPGKIPYPDAFLAI\n",
      "Generated AMP sequence: NDRAAPIPHESRPFLGAWAY\n",
      "Generated AMP sequence: WAKTPSNPTEIDYYAPWD\n",
      "Generated AMP sequence: HVAPFWYYITTGSALEAAPG\n",
      "Generated AMP sequence: DAEKRGMVMEATAVKYRYGRHI\n",
      "Generated AMP sequence: CTYRHHQEAGGGEAAYHGPD\n",
      "Generated AMP sequence: IRAHVYHMCYEYNMKECDDRGAYVWYCHQPCSDDKLL\n",
      "Generated AMP sequence: LTDAVYTSNKA\n",
      "Generated AMP sequence: DFDPIADPATIILSGPK\n",
      "Generated AMP sequence: ITRNTTPSPPMQC\n",
      "Generated AMP sequence: ETASAGQCSSKAGMYMFNKRAQKYGNWAYPAG\n",
      "Generated AMP sequence: PALGMATYYQAQAAGIRPHFY\n",
      "Generated AMP sequence: KGARPDAPNFYCHYHNHF\n",
      "Generated AMP sequence: RGVDDANGTIYDTRRTQMAAWMRIAAPHDIALFRGHAAH\n",
      "Generated AMP sequence: NLEPTRAIAEYAIMPKAGKA\n",
      "Generated AMP sequence: GAAGAKPRDCANPAY\n",
      "Generated AMP sequence: NIPTPRGSAV\n",
      "Generated AMP sequence: NSHNKPAAECTDYGDRNIKP\n",
      "Generated AMP sequence: HYYNDPAAEHTP\n",
      "Generated AMP sequence: KGFGYATLHPIVVHFLWDMMPAP\n",
      "Generated AMP sequence: DACYCTIETHKGLCHYFNGRAYHEACKGSHEMPCC\n",
      "Generated AMP sequence: YPNHTSLTDA\n",
      "Generated AMP sequence: GAKWAAHYGKAAY\n",
      "Generated AMP sequence: VAYNTHKIPAAGPDPYAPHTCPSYPIMTKAWPPDNHNA\n",
      "Generated AMP sequence: IYAYHTRKQVHGPSQRHSAK\n",
      "Generated AMP sequence: GDGGPGHEGE\n",
      "Generated AMP sequence: RLNMHGTKIA\n",
      "Generated AMP sequence: PFGLKMQSPYADAN\n",
      "Generated AMP sequence: KAWAWGAIEFGSKGMPFHADPDKPAKFYISKANY\n",
      "Generated AMP sequence: VSVDDMHTVGERFSHPAMHL\n",
      "Generated AMP sequence: IPTDAANMKEHL\n",
      "Generated AMP sequence: MYAPHHPPYAAPLFAAVDGE\n",
      "Generated AMP sequence: CAIARDKVNKGGPYN\n",
      "Generated AMP sequence: IPHAKYCPDKAHQGPRWEHC\n",
      "Generated AMP sequence: CWKTKMAQASRPAGFGAYKGHHLKWIYTDWASGVTSRGAH\n",
      "Generated AMP sequence: MPFYNSATWIMQ\n",
      "Generated AMP sequence: PHGAGAAAYKYH\n",
      "Generated AMP sequence: KHHANDMNCARHAY\n",
      "Generated AMP sequence: DPYDAAGAPA\n",
      "Generated AMP sequence: HYSMGMKHAVHDGHSPRAHI\n",
      "Generated AMP sequence: WAQAKDAEYR\n",
      "Generated AMP sequence: MHAKHTAIAGQIGHHRVYRGPHETAVHNNVNMAKCQKP\n",
      "Generated AMP sequence: HAPAAAYQGG\n",
      "Generated AMP sequence: SQNCCANQFAAYGRM\n",
      "Generated AMP sequence: PAAMRNKAYRGYLLQS\n",
      "Generated AMP sequence: RAFWDGHHMYEADVNFSTTW\n",
      "Generated AMP sequence: FQHTPRNCKYYA\n",
      "Generated AMP sequence: KAPSDKTPKLKG\n",
      "Generated AMP sequence: FNTMQCHAEPYAHRDKPMDG\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "lstm_gen_best_params = {'hidden_dim': 247, 'num_layers': 3, 'dropout': 0.12915638774083413, 'lr': 0.009983517577195865, 'weight_decay': 0.0007901836295730717}\n",
    "\n",
    "# lstm_gen_best_params = {'dropout': 0.13882938162931305, 'lr': 0.001464264209101335, 'weight_decay': 0.0002912839503135125}\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_frozen_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides-.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EWQMDDAVNPAYRGGG\n",
      "Generated AMP sequence: AKHFNDGHKRGKKD\n",
      "Generated AMP sequence: KAAYYSGDPIGKDYHPHKKG\n",
      "Generated AMP sequence: IRWVMYCGAHHHRHHKGPGYPGDPPPPGHGRKHGHHPLY\n",
      "Generated AMP sequence: IAQRLRKKIAGDPHKYGHP\n",
      "Generated AMP sequence: FAFCYSPVRGYK\n",
      "Generated AMP sequence: EVFAEISTAKHGGHYHKPHYYGKHHG\n",
      "Generated AMP sequence: VKAFWGDKYGPPGPHPPKGP\n",
      "Generated AMP sequence: DAAMPAKGPGPHYG\n",
      "Generated AMP sequence: WAWVFPSLKGFYYP\n",
      "Generated AMP sequence: QFQVVRPDGYNAGP\n",
      "Generated AMP sequence: CALPPFPGHP\n",
      "Generated AMP sequence: ATPLAHGGYHYPPKKKGPHD\n",
      "Generated AMP sequence: DAWYWHAYIYYSHHSHGKYP\n",
      "Generated AMP sequence: HANYCKVQPPHHQQWYKKPP\n",
      "Generated AMP sequence: ISVGHAMAKKHLDDQHGKPG\n",
      "Generated AMP sequence: TAAKLPQYKHRHHGDYQPHG\n",
      "Generated AMP sequence: YDLQPGYQKPHG\n",
      "Generated AMP sequence: ARAAAATPKEKKGHH\n",
      "Generated AMP sequence: VNAHNKTPGDPDKYKHPHKY\n",
      "Generated AMP sequence: HYVDAAADPDHSDQHG\n",
      "Generated AMP sequence: VNVQSGEQRYHHPHYSHPPG\n",
      "Generated AMP sequence: QKSGVHAYDHYYKPHHHPPPPGKGKHYHGGPKGG\n",
      "Generated AMP sequence: IASPAAYHHRYYPHGPKHYG\n",
      "Generated AMP sequence: RAIAACPQMYHHWGYDYGYK\n",
      "Generated AMP sequence: WALYKPPKPLYDHYYHKHGHHKGYYPKHPPGYPYPKHYSG\n",
      "Generated AMP sequence: KGSYCMHYSS\n",
      "Generated AMP sequence: AHEATSYYKQYSGHGYKGP\n",
      "Generated AMP sequence: GGAPKRKTKKWHGDPLPGGD\n",
      "Generated AMP sequence: QMHIWHKRHPVYGYK\n",
      "Generated AMP sequence: MIYKGPGCEGGGP\n",
      "Generated AMP sequence: KFVKPDFAGGGG\n",
      "Generated AMP sequence: FIDGKPARPDW\n",
      "Generated AMP sequence: HAESREGHKPHKPDGPYKKGHKGHHDHGHGYKPPK\n",
      "Generated AMP sequence: MKMQICGHPLH\n",
      "Generated AMP sequence: EMDADGEGGGYFKHYGHYSPHGPYKKHPGHHKGGKHKGH\n",
      "Generated AMP sequence: DAGAAYHSGPPLKHHYPYGP\n",
      "Generated AMP sequence: PAAQMTDRGGGRPKHYHPHHHHPKGYHKYKGYKHGP\n",
      "Generated AMP sequence: EKVRCGGQALPYHRGRGHPG\n",
      "Generated AMP sequence: NQQPPLGHGYLH\n",
      "Generated AMP sequence: NWAMKNAVYKPYPGKKHPY\n",
      "Generated AMP sequence: YHAYSCYVKMHDHPAHHKYHHPHGHGGKHYHKKDP\n",
      "Generated AMP sequence: KYAHAYKNDPGY\n",
      "Generated AMP sequence: CCAKLARKKIHK\n",
      "Generated AMP sequence: RATYHGPHAGPP\n",
      "Generated AMP sequence: VCDPACRGPG\n",
      "Generated AMP sequence: EDQTSHPGHYA\n",
      "Generated AMP sequence: PCADMHWSHG\n",
      "Generated AMP sequence: DMENHLIGYQGWKYPH\n",
      "Generated AMP sequence: VDLNFDSYKG\n",
      "Generated AMP sequence: LAWAMGLPSGAGY\n",
      "Generated AMP sequence: YMMPDWHHKCGPGKGHHHPHKHPYPGGHKGK\n",
      "Generated AMP sequence: NFRAAQKPIGRQPHKHGYGY\n",
      "Generated AMP sequence: WALVQSPPTGKGYYDPYG\n",
      "Generated AMP sequence: HWAPGYYYKVSHRDKGGGPH\n",
      "Generated AMP sequence: DAFLRGPWPGAWDYKYPYHPHH\n",
      "Generated AMP sequence: CTYRHHQGDHHHGDDYHHPG\n",
      "Generated AMP sequence: IRAHVYKNGYGYPLKGGGGPHGYYYYGHPPGPGGKKK\n",
      "Generated AMP sequence: LVEAVYTSPKE\n",
      "Generated AMP sequence: DGEQKAGPDTKKKRHPK\n",
      "Generated AMP sequence: IVSPTVPSPPPPG\n",
      "Generated AMP sequence: ETASCHQFSSKAHMYPHPKPGPKYHKYGYPGH\n",
      "Generated AMP sequence: PAMGNAVYYPGPDDHKPPHHY\n",
      "Generated AMP sequence: KHASPFCPPGYGHYHPHH\n",
      "Generated AMP sequence: RHVEEAPHVKYGWPPRPLGGYKPHGGPHGKGKHPHHDGH\n",
      "Generated AMP sequence: NMFPTRAKDGYGKMPKDHKG\n",
      "Generated AMP sequence: GAAHCLPRGGGPPDY\n",
      "Generated AMP sequence: NLQVQSHSDY\n",
      "Generated AMP sequence: NSIPKQAAGGWGYHGPPKKP\n",
      "Generated AMP sequence: HYYPDPAAGKVP\n",
      "Generated AMP sequence: KHGHYAVMIPKWYHHKYGKLPGP\n",
      "Generated AMP sequence: DADYDTKGVKKHKGHYHPHPGYHGGGKHPHGKPGG\n",
      "Generated AMP sequence: YQPKTSMVGF\n",
      "Generated AMP sequence: GAKWACKYHKDGY\n",
      "Generated AMP sequence: VAYPTKKKPGCHPGPYGPHTGPPYPKKSKGYPPGPHPG\n",
      "Generated AMP sequence: IYAYHVRLPWHHPPPPHPGK\n",
      "Generated AMP sequence: GFGHPHKGHG\n",
      "Generated AMP sequence: RMPNKHTKKA\n",
      "Generated AMP sequence: PGHMLNRSPYAGAP\n",
      "Generated AMP sequence: KDWAYHCKGHHRKHKPHHGGPGKPGKHYHPKGKY\n",
      "Generated AMP sequence: VSVEENHSYHGPHRHPGKHK\n",
      "Generated AMP sequence: IQTDAAPNKGHL\n",
      "Generated AMP sequence: MYAPHIPPYGGPKHGGYGHG\n",
      "Generated AMP sequence: CAKASFKVPKHHPYP\n",
      "Generated AMP sequence: IQIALYEPGKFKPHPPYGHG\n",
      "Generated AMP sequence: CWLTKPCQESQPDHHHGYKHHHKKYKYQGYGPHYSPPHGH\n",
      "Generated AMP sequence: MPGYPSDTYKNP\n",
      "Generated AMP sequence: PKHCHAADYKYH\n",
      "Generated AMP sequence: KKHAPFPPGDQKEY\n",
      "Generated AMP sequence: DPYECCGDPD\n",
      "Generated AMP sequence: HYSNGPKIEYKGHHRPPDHI\n",
      "Generated AMP sequence: WDRALFAGYR\n",
      "Generated AMP sequence: MIALITCKGHPKHHHPYYPHPHGPGYHMPYLKGKGPKP\n",
      "Generated AMP sequence: HAQAAAYQHH\n",
      "Generated AMP sequence: SRPDDAPRGAEYHPN\n",
      "Generated AMP sequence: PCANRPKAYRHYLKPP\n",
      "Generated AMP sequence: RAGWEHIIPYGGGYPHPSRY\n",
      "Generated AMP sequence: FRITQRPFKYYD\n",
      "Generated AMP sequence: KAQSEKSPKMKH\n",
      "Generated AMP sequence: FPVNRDKAGPYGHPGKPKGH\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "lstm_gen_best_params = {'hidden_dim': 247, 'num_layers': 3, 'dropout': 0.12915638774083413, 'lr': 0.009983517577195865, 'weight_decay': 0.0007901836295730717}\n",
    "\n",
    "# lstm_gen_best_params = {'dropout': 0.13882938162931305, 'lr': 0.001464264209101335, 'weight_decay': 0.0002912839503135125}\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides-fullback1.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
