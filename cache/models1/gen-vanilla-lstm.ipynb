{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General AMP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "# uniprot_df = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\")\n",
    "# uniprot_df1 = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\")\n",
    "# uniprot_df = pd.concat([uniprot_df, uniprot_df1], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### raw data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'W', 'L', 'Q', 'F', 'K', 'C', 'H', 'T', 'E', 'P', 'R', 'V', 'D', 'G', 'N', 'S', 'Y', 'I', 'M', 'A'}\n",
      "20\n",
      "Number of sequences after filtering: 3306\n",
      "Dataset sizes: {'Train': 2276, 'Validation': 488, 'Test': 488}\n",
      "Input shape: torch.Size([2495, 20])\n",
      "Target shape: torch.Size([2495, 20])\n",
      "Lengths: tensor([64, 64, 64, 64, 64, 64, 63, 63, 63, 63, 63, 63, 61, 61, 60, 58, 55, 54,\n",
      "        52, 52, 49, 48, 48, 47, 46, 45, 43, 41, 37, 35, 33, 29, 27, 25, 25, 23,\n",
      "        23, 23, 22, 21, 21, 20, 18, 17, 16, 14, 14, 12, 10, 10,  9,  9,  9,  9,\n",
      "         9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,  9,\n",
      "         9,  9,  8,  8,  7,  7,  6,  6,  6,  6,  6,  6,  6,  6,  5,  5,  5,  5,\n",
      "         5,  5,  5,  5,  5,  5,  5,  4,  4,  4,  4,  4,  4,  4,  4,  4,  4,  3,\n",
      "         3,  3,  3,  3,  3,  3,  3,  3,  3,  3,  2,  2,  2,  2,  2,  2,  2,  2])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load only positive (AMP) sequences\n",
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "\n",
    "unique_letters = set(''.join(adam_df[\"Sequence\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "print(f\"Number of sequences after filtering: {len(adam_df)}\")\n",
    "adam_df = adam_df.drop_duplicates(subset='Sequence')\n",
    "tb_df = pd.read_csv('../data/all_seq702.csv')\n",
    "adam_df = adam_df[~adam_df['Sequence'].isin(tb_df['Sequences'])]\n",
    "\n",
    "generation_seqs = adam_df[\"Sequence\"].reset_index(drop=True)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "class GenerativeSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        input_seq = seq[:-1]  # all residues except the last\n",
    "        target_seq = seq[1:]  # all residues except the first\n",
    "        length = len(input_seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        input_one_hot = one_hot_torch(input_seq, dtype=self.one_hot_dtype)\n",
    "        target_one_hot = one_hot_torch(target_seq, dtype=self.one_hot_dtype)\n",
    "        # target_indices = torch.tensor([\"ACDEFGHIKLMNPQRSTVWY\".index(res) for res in target_seq], dtype=torch.long)\n",
    "        return input_one_hot, target_one_hot, length\n",
    "\n",
    "def generative_collate_and_pack(batch):\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "\n",
    "    lengths = torch.tensor(lengths)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    targets = [targets[i] for i in sorted_indices]\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    sequences = [seq.T for seq in sequences]  # transpose to [seq_len, features]\n",
    "    targets = [tgt.T for tgt in targets]      # transpose targets as well\n",
    "\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)\n",
    "    padded_targets = pad_sequence(targets, batch_first=False)\n",
    "\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "    packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, packed_target, lengths\n",
    "\n",
    "\n",
    "# Train/val/test split\n",
    "train_seqs, test_seqs = train_test_split(generation_seqs, test_size=0.3, random_state=42)\n",
    "val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = GenerativeSequenceDataset(train_seqs)\n",
    "val_dataset = GenerativeSequenceDataset(val_seqs)\n",
    "test_dataset = GenerativeSequenceDataset(test_seqs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=generative_collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "\n",
    "# Dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\", dataset_sizes)\n",
    "\n",
    "for x, y, l in train_loader:\n",
    "    print(\"Input shape:\", x.data.shape)  # [L, B, 20]\n",
    "    print(\"Target shape:\", y.data.shape)  # [L, B, 20]\n",
    "    print(\"Lengths:\", y.batch_sizes)  # Lengths of sequences in the batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### sliding window data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Re-import required libraries after environment reset\n",
    "# import torch\n",
    "# import pandas as pd\n",
    "# from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "# from sklearn.model_selection import train_test_split\n",
    "\n",
    "# # Load only positive (AMP) sequences\n",
    "# adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "\n",
    "# # Clean non-standard amino acids\n",
    "# unique_letters = set(''.join(adam_df[\"Sequence\"]))\n",
    "# amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "# # non_standard_amino_acids = set(unique_letters) - set(amino_acids)\n",
    "# # adam_df = adam_df[~adam_df[\"Sequence\"].str.contains('|'.join(non_standard_amino_acids))]\n",
    "\n",
    "# # Apply sliding window to generate fragments\n",
    "# def generate_fragments(sequences, window_size=15, stride=5):\n",
    "#     fragments = []\n",
    "#     for seq in sequences:\n",
    "#         for start in range(0, len(seq) - window_size + 1, stride):\n",
    "#             fragment = seq[start:start + window_size]\n",
    "#             fragments.append(fragment)\n",
    "#     return fragments\n",
    "\n",
    "# generation_fragments = generate_fragments(adam_df[\"Sequence\"].tolist())\n",
    "\n",
    "# # Define one-hot encoding function\n",
    "# def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "#     amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "#     seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "#     aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "#     arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "#     for i, aa in enumerate(aa_bytes):\n",
    "#         arr[i, seq_bytes == aa] = 1\n",
    "#     return arr\n",
    "\n",
    "# # Dataset using one-hot encoding for generative modeling\n",
    "# class AMPGenerationOneHotDataset(Dataset):\n",
    "#     def __init__(self, sequences):\n",
    "#         self.sequences = sequences\n",
    "\n",
    "#     def __len__(self):\n",
    "#         return len(self.sequences)\n",
    "\n",
    "#     def __getitem__(self, idx):\n",
    "#         seq = self.sequences[idx]\n",
    "#         input_seq = seq[:-1]  # all residues except the last\n",
    "#         target_seq = seq[1:]  # all residues except the first\n",
    "#         input_one_hot = one_hot_torch(input_seq)  # shape: [20, seq_len - 1]\n",
    "#         target_one_hot = one_hot_torch(target_seq)  # shape: [20, seq_len - 1]\n",
    "#         length = input_one_hot.shape[1]\n",
    "#         return input_one_hot, target_one_hot, length\n",
    "\n",
    "# # Collate function for packing one-hot sequences\n",
    "# def collate_and_pack_for_generation(batch):\n",
    "#     sequences, targets, lengths = zip(*batch)\n",
    "#     lengths = torch.tensor(lengths)\n",
    "\n",
    "#     # Sort by length (required for packing)\n",
    "#     sorted_indices = torch.argsort(lengths, descending=True)\n",
    "#     sequences = [sequences[i] for i in sorted_indices]\n",
    "#     targets = [targets[i] for i in sorted_indices]\n",
    "#     lengths = lengths[sorted_indices]\n",
    "\n",
    "#     # Transpose each to [L, 20] and pad\n",
    "#     sequences = [seq.T for seq in sequences]  # from [20, L] to [L, 20]\n",
    "#     targets = [tgt.T for tgt in targets]      # from [20, L] to [L, 20]\n",
    "\n",
    "#     padded_seqs = pad_sequence(sequences, batch_first=False)  # [L, B, 20]\n",
    "#     padded_targets = pad_sequence(targets, batch_first=False)  # [L, B, 20]\n",
    "\n",
    "#     packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "#     packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "\n",
    "#     return packed_input, packed_target, lengths\n",
    "\n",
    "# # Train/val/test split\n",
    "# train_seqs, test_seqs = train_test_split(generation_fragments, test_size=0.3, random_state=42)\n",
    "# val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "\n",
    "# train_dataset = AMPGenerationOneHotDataset(train_seqs)\n",
    "# val_dataset = AMPGenerationOneHotDataset(val_seqs)\n",
    "# test_dataset = AMPGenerationOneHotDataset(test_seqs)\n",
    "\n",
    "# train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=collate_and_pack_for_generation)\n",
    "# val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=collate_and_pack_for_generation)\n",
    "# test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=collate_and_pack_for_generation)\n",
    "\n",
    "# # Dataset sizes for verification\n",
    "# dataset_sizes = {\n",
    "#     \"Train\": len(train_dataset),\n",
    "#     \"Validation\": len(val_dataset),\n",
    "#     \"Test\": len(test_dataset)\n",
    "# }\n",
    "# print(\"Dataset sizes:\", dataset_sizes)\n",
    "\n",
    "# for x, y, l in train_loader:\n",
    "#     print(\"Input shape:\", x.data.shape)  # [L, B, 20]\n",
    "#     print(\"Target shape:\", y.data.shape)  # [L, B, 20]\n",
    "#     print(\"Lengths:\", y.batch_sizes)  # Lengths of sequences in the batch\n",
    "#     break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerativeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=128, num_layers=1, dropout=0.3, output_dim=20):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, dropout=dropout if num_layers > 1 else 0)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, _ = self.lstm(packed_input)\n",
    "        dropped = self.dropout(packed_output.data)\n",
    "        logits = self.fc(dropped)\n",
    "        return logits  # shape: [total_timesteps, 20]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Train Loss: 2.9062 - Val Loss: 2.8592\n",
      "Epoch 2/10 - Train Loss: 2.8528 - Val Loss: 2.8409\n",
      "Epoch 3/10 - Train Loss: 2.8234 - Val Loss: 2.8101\n",
      "Epoch 4/10 - Train Loss: 2.8029 - Val Loss: 2.7917\n",
      "Epoch 5/10 - Train Loss: 2.7897 - Val Loss: 2.7748\n",
      "Epoch 6/10 - Train Loss: 2.7756 - Val Loss: 2.7624\n",
      "Epoch 7/10 - Train Loss: 2.7663 - Val Loss: 2.7500\n",
      "Epoch 8/10 - Train Loss: 2.7544 - Val Loss: 2.7458\n",
      "Epoch 9/10 - Train Loss: 2.7522 - Val Loss: 2.7432\n",
      "Epoch 10/10 - Train Loss: 2.7495 - Val Loss: 2.7287\n",
      "Test Loss: 2.7355\n"
     ]
    }
   ],
   "source": [
    "model = GenerativeLSTM()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer and loss\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train and eval functions\n",
    "def train(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for packed_input, packed_target, _ in dataloader:\n",
    "        inputs = packed_input.to(device)\n",
    "        targets = torch.argmax(packed_target.data, dim=1).to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "def evaluate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, packed_target, _ in dataloader:\n",
    "            inputs = packed_input.to(device)\n",
    "            targets = torch.argmax(packed_target.data, dim=1).to(device)\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            total_loss += loss.item()\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 10\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train(model, train_loader, optimizer, criterion, device)\n",
    "    val_loss = evaluate(model, val_loader, criterion, device)\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs} - Train Loss: {train_loss:.4f} - Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Final test evaluation\n",
    "test_loss = evaluate(model, test_loader, criterion, device)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "class GenerativeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=128, num_layers=1, dropout=0.3):\n",
    "        super(GenerativeLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle packed input\n",
    "        if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n",
    "            packed_output, _ = self.lstm(x)\n",
    "            unpacked_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            return self.fc(unpacked_output)\n",
    "        else:\n",
    "            out, _ = self.lstm(x)\n",
    "            return self.fc(out)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### General AMP - train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:01:33,836] A new study created in memory with name: no-name-ac1e7109-a285-42bc-85d1-80d0c8f1751c\n",
      "[I 2025-04-23 14:02:14,459] Trial 0 finished with value: 2.352133423089981 and parameters: {'hidden_dim': 193, 'num_layers': 2, 'dropout': 0.16639264160797632, 'lr': 0.0009584674035512115, 'weight_decay': 0.0006112688207040144}. Best is trial 0 with value: 2.352133423089981.\n",
      "[I 2025-04-23 14:02:53,079] Trial 1 finished with value: 1.9836155623197556 and parameters: {'hidden_dim': 253, 'num_layers': 3, 'dropout': 0.4272420846439058, 'lr': 0.0016812232747644258, 'weight_decay': 0.0004198276532947544}. Best is trial 1 with value: 1.9836155623197556.\n",
      "[I 2025-04-23 14:03:32,865] Trial 2 finished with value: 0.5449075996875763 and parameters: {'hidden_dim': 117, 'num_layers': 3, 'dropout': 0.3971753738117332, 'lr': 0.005669630280173859, 'weight_decay': 0.0005079920092703994}. Best is trial 2 with value: 0.5449075996875763.\n",
      "[I 2025-04-23 14:04:06,377] Trial 3 finished with value: 1.6985166817903519 and parameters: {'hidden_dim': 132, 'num_layers': 1, 'dropout': 0.29738714180882797, 'lr': 0.002189465929730427, 'weight_decay': 0.0004772932535223261}. Best is trial 2 with value: 0.5449075996875763.\n",
      "[I 2025-04-23 14:04:40,164] Trial 4 finished with value: 0.4838373586535454 and parameters: {'hidden_dim': 192, 'num_layers': 2, 'dropout': 0.4612065083549679, 'lr': 0.006159429302004391, 'weight_decay': 0.0004398220467026661}. Best is trial 4 with value: 0.4838373586535454.\n",
      "[I 2025-04-23 14:05:20,227] Trial 5 finished with value: 0.2613733634352684 and parameters: {'hidden_dim': 80, 'num_layers': 2, 'dropout': 0.25305464069545724, 'lr': 0.008522080952525957, 'weight_decay': 7.937015120563174e-05}. Best is trial 5 with value: 0.2613733634352684.\n",
      "[I 2025-04-23 14:05:57,405] Trial 6 finished with value: 0.4286224730312824 and parameters: {'hidden_dim': 106, 'num_layers': 3, 'dropout': 0.46980320367854544, 'lr': 0.0064102863427614775, 'weight_decay': 0.0009373996184464516}. Best is trial 5 with value: 0.2613733634352684.\n",
      "[I 2025-04-23 14:06:40,342] Trial 7 finished with value: 1.2733717113733292 and parameters: {'hidden_dim': 196, 'num_layers': 3, 'dropout': 0.2046930931225942, 'lr': 0.0030482204746422195, 'weight_decay': 0.00038826570631922993}. Best is trial 5 with value: 0.2613733634352684.\n",
      "[I 2025-04-23 14:07:14,409] Trial 8 finished with value: 2.2647345066070557 and parameters: {'hidden_dim': 210, 'num_layers': 2, 'dropout': 0.20074590344674179, 'lr': 0.0011843877670480998, 'weight_decay': 0.0004745703656962667}. Best is trial 5 with value: 0.2613733634352684.\n",
      "[I 2025-04-23 14:07:46,373] Trial 9 finished with value: 0.868241623044014 and parameters: {'hidden_dim': 217, 'num_layers': 1, 'dropout': 0.4870139488045493, 'lr': 0.004212109434515076, 'weight_decay': 0.0006566058723862926}. Best is trial 5 with value: 0.2613733634352684.\n",
      "[I 2025-04-23 14:08:19,918] Trial 10 finished with value: 0.23270189762115479 and parameters: {'hidden_dim': 69, 'num_layers': 1, 'dropout': 0.3016557482356841, 'lr': 0.009595057973336141, 'weight_decay': 4.2445882679333783e-05}. Best is trial 10 with value: 0.23270189762115479.\n",
      "[I 2025-04-23 14:08:50,195] Trial 11 finished with value: 0.23356282897293568 and parameters: {'hidden_dim': 69, 'num_layers': 1, 'dropout': 0.31385293721584945, 'lr': 0.009369185056553934, 'weight_decay': 3.007446212661364e-06}. Best is trial 10 with value: 0.23270189762115479.\n",
      "[I 2025-04-23 14:09:31,357] Trial 12 finished with value: 0.24586280062794685 and parameters: {'hidden_dim': 66, 'num_layers': 1, 'dropout': 0.3539198844903619, 'lr': 0.00948521604861462, 'weight_decay': 3.5317853693036436e-05}. Best is trial 10 with value: 0.23270189762115479.\n",
      "[I 2025-04-23 14:10:03,610] Trial 13 finished with value: 0.324273694306612 and parameters: {'hidden_dim': 151, 'num_layers': 1, 'dropout': 0.32478823534475243, 'lr': 0.0077575027563932105, 'weight_decay': 0.00021669040856524545}. Best is trial 10 with value: 0.23270189762115479.\n",
      "[I 2025-04-23 14:10:39,132] Trial 14 finished with value: 0.21493979170918465 and parameters: {'hidden_dim': 101, 'num_layers': 1, 'dropout': 0.11058091088425542, 'lr': 0.009976100969164275, 'weight_decay': 0.00020340520395482203}. Best is trial 14 with value: 0.21493979170918465.\n",
      "[I 2025-04-23 14:11:17,460] Trial 15 finished with value: 0.31537556275725365 and parameters: {'hidden_dim': 95, 'num_layers': 1, 'dropout': 0.10008655688918394, 'lr': 0.007630992100710814, 'weight_decay': 0.00020890842899665965}. Best is trial 14 with value: 0.21493979170918465.\n",
      "[I 2025-04-23 14:11:51,586] Trial 16 finished with value: 0.23222488351166248 and parameters: {'hidden_dim': 152, 'num_layers': 1, 'dropout': 0.25777117758342943, 'lr': 0.00981568449655331, 'weight_decay': 0.00018575097830986374}. Best is trial 14 with value: 0.21493979170918465.\n",
      "[I 2025-04-23 14:12:30,656] Trial 17 finished with value: 0.3474557213485241 and parameters: {'hidden_dim': 157, 'num_layers': 2, 'dropout': 0.11415100898913438, 'lr': 0.007377500813127706, 'weight_decay': 0.0002501864586064863}. Best is trial 14 with value: 0.21493979170918465.\n",
      "[I 2025-04-23 14:12:59,441] Trial 18 finished with value: 0.22444638051092625 and parameters: {'hidden_dim': 134, 'num_layers': 1, 'dropout': 0.15224617337292592, 'lr': 0.009967276807899206, 'weight_decay': 0.0003099327344969195}. Best is trial 14 with value: 0.21493979170918465.\n",
      "[I 2025-04-23 14:13:30,238] Trial 19 finished with value: 0.8249271661043167 and parameters: {'hidden_dim': 126, 'num_layers': 1, 'dropout': 0.14782299845123265, 'lr': 0.004394027112545067, 'weight_decay': 0.0003041116559982781}. Best is trial 14 with value: 0.21493979170918465.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dim': 101, 'num_layers': 1, 'dropout': 0.11058091088425542, 'lr': 0.009976100969164275, 'weight_decay': 0.00020340520395482203}\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary packages after reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Criterion\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss on the last time step of each sequence.\n",
    "    \n",
    "    Args:\n",
    "        output: Tensor of shape [B, L, vocab_size]\n",
    "        target_seq: Tensor of shape [B, L] containing target class indices\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss computed only on the last token of each sequence\n",
    "    \"\"\"\n",
    "    # Get last time step for each sequence\n",
    "    last_token_logits = output[:, -1, :]        # [B, vocab_size]\n",
    "    last_token_targets = target_seq[:, -1, :]      # [B]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "    # print('last_token_logits',last_token_logits.shape)\n",
    "    # print('last_token_targets',last_token_targets.shape)\n",
    "\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen/AMP_LSTM_GEN_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])      # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            # print('target_shape before reshape',target_seq.shape)\n",
    "            # target_seq = target_seq.reshape(-1)\n",
    "            # print(f\"Output shape: {output.shape}, Target shape: {target_seq.shape}\")\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_generator.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])  # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # # target_seq = target_seq.reshape(-1)\n",
    "            # # target_seq = target_seq.reshape(-1, target_seq.shape[-1])\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            # assert output.size(0) == target_seq.size(0), f\"Mismatch: {output.size(0)} vs {target_seq.size(0)}\"\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            # print('loss done')\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            preds = output[:, -1, :]        # shape: [B, vocab_size]\n",
    "            preds = torch.argmax(preds, dim=1)  # shape: [B]\n",
    "\n",
    "            targets = target_seq[:, -1, :]      # shape: [B, vocab_size]\n",
    "            targets = torch.argmax(targets, dim=-1)  # shape: [B]\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Objective for Optuna tuning\n",
    "def objective_generation(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "\n",
    "lstm_gen_best_params = study.best_trial.params\n",
    "print(lstm_gen_best_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!tensorboard --logdir runs-lstm-gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.6271 | Val Loss = 2.2939 | Acc = 0.9877 | AUC = undefined | Perplexity = 9.9133\n",
      "Epoch 2: Train Loss = 2.0037 | Val Loss = 1.7072 | Acc = 0.9877 | AUC = undefined | Perplexity = 5.5137\n",
      "Epoch 3: Train Loss = 1.4672 | Val Loss = 1.2243 | Acc = 0.9877 | AUC = undefined | Perplexity = 3.4016\n",
      "Epoch 4: Train Loss = 1.0436 | Val Loss = 0.8678 | Acc = 0.9857 | AUC = undefined | Perplexity = 2.3817\n",
      "Epoch 5: Train Loss = 0.7415 | Val Loss = 0.6213 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.8614\n",
      "Epoch 6: Train Loss = 0.5398 | Val Loss = 0.4616 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.5866\n",
      "Epoch 7: Train Loss = 0.4091 | Val Loss = 0.3588 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.4316\n",
      "Epoch 8: Train Loss = 0.3251 | Val Loss = 0.2927 | Acc = 0.9857 | AUC = undefined | Perplexity = 1.3400\n",
      "Epoch 9: Train Loss = 0.2648 | Val Loss = 0.2521 | Acc = 0.9857 | AUC = undefined | Perplexity = 1.2868\n",
      "Epoch 10: Train Loss = 0.2229 | Val Loss = 0.2172 | Acc = 0.9857 | AUC = undefined | Perplexity = 1.2426\n",
      "Epoch 11: Train Loss = 0.1934 | Val Loss = 0.1852 | Acc = 0.9836 | AUC = undefined | Perplexity = 1.2035\n",
      "Epoch 12: Train Loss = 0.1705 | Val Loss = 0.1612 | Acc = 0.9857 | AUC = undefined | Perplexity = 1.1749\n",
      "Epoch 13: Train Loss = 0.1527 | Val Loss = 0.1461 | Acc = 0.9857 | AUC = undefined | Perplexity = 1.1573\n",
      "Epoch 14: Train Loss = 0.1409 | Val Loss = 0.1332 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.1424\n",
      "Epoch 15: Train Loss = 0.1292 | Val Loss = 0.1229 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.1308\n",
      "Epoch 16: Train Loss = 0.1189 | Val Loss = 0.1244 | Acc = 0.9857 | AUC = undefined | Perplexity = 1.1325\n",
      "Epoch 17: Train Loss = 0.1104 | Val Loss = 0.1160 | Acc = 0.9857 | AUC = undefined | Perplexity = 1.1230\n",
      "Epoch 18: Train Loss = 0.1076 | Val Loss = 0.1067 | Acc = 0.9857 | AUC = undefined | Perplexity = 1.1126\n",
      "Epoch 19: Train Loss = 0.1005 | Val Loss = 0.1009 | Acc = 0.9836 | AUC = undefined | Perplexity = 1.1062\n",
      "Epoch 20: Train Loss = 0.0969 | Val Loss = 0.0928 | Acc = 0.9877 | AUC = undefined | Perplexity = 1.0972\n",
      "\n",
      "✅ Final Test Metrics:\n",
      "Loss = 0.0928, Accuracy = 0.9877, AUC = undefined, Perplexity = 1.0972\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_best_params[\"lr\"], weight_decay=lstm_gen_best_params[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "# lstm_gen_best_params = {'hidden_dim': 133, 'num_layers': 2, 'dropout': 0.10063270147175422, 'lr': 0.003237280156212186, 'weight_decay': 2.2594437829479466e-05}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=20)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\"\\n✅ Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n",
    "torch.save(trained_model.state_dict(), 'best_model_lstm_generator_final.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tb amp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'L', 'T', 'M', 'E', 'Y', 'R', 'H', 'X', 'W', 'G', 'N', 'K', 'S', 'C', 'P', 'V', 'I', 'Q', 'F', 'D', 'A'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "# df_filtered = df[\n",
    "#     (df['Sequences'].str.len() >= 10) &\n",
    "#     (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "#     (~df['Sequences'].str.contains('X'))\n",
    "# ]\n",
    "df_filtered = df\n",
    "\n",
    "def split_sequence(seq, chunk_size=20):\n",
    "    return [seq[i:i+chunk_size] for i in range(0, len(seq), chunk_size)]\n",
    "\n",
    "new_rows = []\n",
    "for _, row in df_filtered.iterrows():\n",
    "    seq = row['Sequences']\n",
    "    amp_label = row['AMP']\n",
    "    if len(seq) > 40:\n",
    "        for chunk in split_sequence(seq, 20):\n",
    "            new_rows.append({'Sequences': chunk, 'AMP': amp_label})\n",
    "    else:\n",
    "        new_rows.append({'Sequences': seq, 'AMP': amp_label})\n",
    "\n",
    "df_filtered = pd.DataFrame(new_rows)\n",
    "\n",
    "\n",
    "df_filtered = df_filtered[\n",
    "    (df_filtered['Sequences'].str.len() >= 10) &\n",
    "    (df_filtered['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df_filtered['Sequences'].str.contains('X'))\n",
    "]\n",
    "df_filtered = df_filtered[df_filtered['AMP']==1]\n",
    "df_filtered = df_filtered.drop_duplicates(subset='Sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "40\n"
     ]
    }
   ],
   "source": [
    "print(min([len(x) for x in df_filtered['Sequences']]))\n",
    "print(max([len(x) for x in df_filtered['Sequences']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Input shape': torch.Size([1120, 20]),\n",
       " 'Target shape': torch.Size([1120, 20]),\n",
       " 'Lengths': tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 58, 54, 47, 42, 39, 37, 36, 35, 32,\n",
       "         30, 16, 15, 14, 14, 11, 10,  8,  8,  8,  7,  4,  4,  4,  4,  3,  1,  1,\n",
       "          1,  1])}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import libraries after environment reset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "df = df_filtered\n",
    "\n",
    "# Clean and inspect\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "# non_standard_amino_acids = unique_letters - amino_acids\n",
    "# df = df[~df[\"Sequences\"].str.contains('|'.join(non_standard_amino_acids))]\n",
    "\n",
    "# Extract sequences\n",
    "sequences = df[\"Sequences\"].reset_index(drop=True)\n",
    "\n",
    "# Define one-hot function\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "# Define dataset class\n",
    "class GenerativeSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        input_seq = seq[:-1]\n",
    "        target_seq = seq[1:]\n",
    "        length = len(input_seq.replace(\"X\", \"\"))\n",
    "        input_one_hot = one_hot_torch(input_seq, dtype=self.one_hot_dtype)\n",
    "        target_one_hot = one_hot_torch(target_seq, dtype=self.one_hot_dtype)\n",
    "        return input_one_hot, target_one_hot, length\n",
    "\n",
    "# Define collate function\n",
    "def generative_collate_and_pack(batch):\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    targets = [targets[i] for i in sorted_indices]\n",
    "    lengths = lengths[sorted_indices]\n",
    "    sequences = [seq.T for seq in sequences]\n",
    "    targets = [tgt.T for tgt in targets]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)\n",
    "    padded_targets = pad_sequence(targets, batch_first=False)\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "    packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "    return packed_input, packed_target, lengths\n",
    "\n",
    "# Split and load data\n",
    "train_seqs, test_seqs = train_test_split(sequences, test_size=0.3, random_state=42)\n",
    "val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "train_dataset = GenerativeSequenceDataset(train_seqs)\n",
    "val_dataset = GenerativeSequenceDataset(val_seqs)\n",
    "test_dataset = GenerativeSequenceDataset(test_seqs)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=generative_collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "\n",
    "# Preview batch\n",
    "batch_sample = next(iter(train_loader))\n",
    "batch_sample_shapes = {\n",
    "    \"Input shape\": batch_sample[0].data.shape,\n",
    "    \"Target shape\": batch_sample[1].data.shape,\n",
    "    \"Lengths\": batch_sample[0].batch_sizes\n",
    "}\n",
    "batch_sample_shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 171\n",
      "Validation: 37\n",
      "Test: 37\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### train for full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:30:17,699] A new study created in memory with name: no-name-73cda9ff-9f49-4694-ae74-377f6103161d\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:30:22,613] Trial 0 finished with value: 0.17065972089767456 and parameters: {'dropout': 0.3980787728630053, 'lr': 0.003262618150370296, 'weight_decay': 0.0007610779642297202}. Best is trial 0 with value: 0.17065972089767456.\n",
      "[I 2025-04-23 14:30:28,779] Trial 1 finished with value: 0.16326631605625153 and parameters: {'dropout': 0.17500559295476195, 'lr': 0.008779716388199644, 'weight_decay': 0.0006768118069278002}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:30:34,271] Trial 2 finished with value: 0.1712217628955841 and parameters: {'dropout': 0.28931630707511136, 'lr': 0.0056108644742778055, 'weight_decay': 0.00041883086845768166}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:30:38,407] Trial 3 finished with value: 0.1696176379919052 and parameters: {'dropout': 0.3656290382610786, 'lr': 0.0025633203154633794, 'weight_decay': 0.00033162615249359686}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:30:42,514] Trial 4 finished with value: 0.16328835487365723 and parameters: {'dropout': 0.24665646395599264, 'lr': 0.004230960441811617, 'weight_decay': 0.00034839423192348104}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:30:46,646] Trial 5 finished with value: 0.17240388691425323 and parameters: {'dropout': 0.23820800649254248, 'lr': 0.007897319294654932, 'weight_decay': 0.0004019510503883494}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:30:50,721] Trial 6 finished with value: 0.1669636219739914 and parameters: {'dropout': 0.13976351728909112, 'lr': 0.00444922629938417, 'weight_decay': 0.0004361735176279666}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:30:54,862] Trial 7 finished with value: 0.17328640818595886 and parameters: {'dropout': 0.32264799137940414, 'lr': 0.0010573403473316276, 'weight_decay': 0.0002641908919414505}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:30:59,027] Trial 8 finished with value: 0.16730506718158722 and parameters: {'dropout': 0.3308247492879753, 'lr': 0.009153902633303198, 'weight_decay': 0.0009076103344006626}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:31:03,083] Trial 9 finished with value: 0.18108707666397095 and parameters: {'dropout': 0.11218978456309876, 'lr': 0.0026169730832984474, 'weight_decay': 0.0009189705939270551}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:31:07,459] Trial 10 finished with value: 0.17716781795024872 and parameters: {'dropout': 0.4870819987021694, 'lr': 0.006440756078568113, 'weight_decay': 3.562740630542159e-05}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:31:11,982] Trial 11 finished with value: 0.16340947151184082 and parameters: {'dropout': 0.19920115389718074, 'lr': 0.009947598852769777, 'weight_decay': 0.0006220497834528689}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:31:16,380] Trial 12 finished with value: 0.1721547544002533 and parameters: {'dropout': 0.19756397744523999, 'lr': 0.007189968783557099, 'weight_decay': 0.000627488820238981}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:31:20,951] Trial 13 finished with value: 0.1812155842781067 and parameters: {'dropout': 0.2502000418640612, 'lr': 0.0045017027476924095, 'weight_decay': 0.00015964301896418347}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:31:25,291] Trial 14 finished with value: 0.16525690257549286 and parameters: {'dropout': 0.17170480276483635, 'lr': 0.00880251836925263, 'weight_decay': 0.000583371449638262}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:31:29,617] Trial 15 finished with value: 0.1733725517988205 and parameters: {'dropout': 0.26884036240228504, 'lr': 0.0007834031450856516, 'weight_decay': 0.000744353131937464}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:31:34,095] Trial 16 finished with value: 0.1667252779006958 and parameters: {'dropout': 0.10466320659082964, 'lr': 0.005723300254605163, 'weight_decay': 0.00020012838670733206}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:31:38,547] Trial 17 finished with value: 0.1693485826253891 and parameters: {'dropout': 0.20922994683766571, 'lr': 0.003935734748793612, 'weight_decay': 0.0007737022476539153}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:31:42,814] Trial 18 finished with value: 0.16946829855442047 and parameters: {'dropout': 0.1553153305646126, 'lr': 0.007281771186514235, 'weight_decay': 0.0005291001599257091}. Best is trial 1 with value: 0.16326631605625153.\n",
      "[I 2025-04-23 14:31:47,617] Trial 19 finished with value: 0.17488627135753632 and parameters: {'dropout': 0.4367949114138724, 'lr': 0.001875956712008838, 'weight_decay': 2.154505974099344e-05}. Best is trial 1 with value: 0.16326631605625153.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best transfer learning hyperparameters: {'dropout': 0.17500559295476195, 'lr': 0.008779716388199644, 'weight_decay': 0.0006768118069278002}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Criterion\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Transfer Learning Loader\n",
    "def load_pretrained_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    last_token_logits = output[:, -1, :]\n",
    "    last_token_targets = target_seq[:, -1, :]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen-tb/AMP_LSTM_GEN_TRANSFER_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # if train:\n",
    "            #     torch.save(model.state_dict(), 'best_model_lstm_transfer.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "    \n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Optuna objective for fine-tuning\n",
    "def objective_generation(trial):\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(    \n",
    "                hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "                num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "                dropout=dropout\n",
    "                # dropout=lstm_gen_best_params[\"dropout\"]\n",
    "                )\n",
    "    model = load_pretrained_weights(model, 'best_model_lstm_generator_final.pt')  # path to the general AMP model\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=20, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "lstm_gen_best_params_tb = study.best_trial.params\n",
    "print(\"Best transfer learning hyperparameters:\", lstm_gen_best_params_tb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.9377 | Val Loss = 2.9043 | Acc = 0.0000 | AUC = undefined | Perplexity = 18.2530\n",
      "Epoch 2: Train Loss = 2.8846 | Val Loss = 2.8489 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.2682\n",
      "Epoch 3: Train Loss = 2.8364 | Val Loss = 2.7850 | Acc = 0.9730 | AUC = undefined | Perplexity = 16.1997\n",
      "Epoch 4: Train Loss = 2.7719 | Val Loss = 2.7146 | Acc = 1.0000 | AUC = undefined | Perplexity = 15.0982\n",
      "Epoch 5: Train Loss = 2.7198 | Val Loss = 2.6705 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.4476\n",
      "Epoch 6: Train Loss = 2.6647 | Val Loss = 2.6288 | Acc = 0.9730 | AUC = undefined | Perplexity = 13.8569\n",
      "Epoch 7: Train Loss = 2.6211 | Val Loss = 2.5836 | Acc = 0.9730 | AUC = undefined | Perplexity = 13.2453\n",
      "Epoch 8: Train Loss = 2.5820 | Val Loss = 2.5400 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.6799\n",
      "Epoch 9: Train Loss = 2.5216 | Val Loss = 2.4870 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.0257\n",
      "Epoch 10: Train Loss = 2.4830 | Val Loss = 2.4301 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.3599\n",
      "Epoch 11: Train Loss = 2.4380 | Val Loss = 2.3826 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.8326\n",
      "Epoch 12: Train Loss = 2.4000 | Val Loss = 2.3411 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.3927\n",
      "Epoch 13: Train Loss = 2.3565 | Val Loss = 2.3060 | Acc = 1.0000 | AUC = undefined | Perplexity = 10.0347\n",
      "Epoch 14: Train Loss = 2.3011 | Val Loss = 2.2665 | Acc = 1.0000 | AUC = undefined | Perplexity = 9.6458\n",
      "Epoch 15: Train Loss = 2.2525 | Val Loss = 2.2172 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.1817\n",
      "Epoch 16: Train Loss = 2.2057 | Val Loss = 2.1695 | Acc = 0.9730 | AUC = undefined | Perplexity = 8.7543\n",
      "Epoch 17: Train Loss = 2.1613 | Val Loss = 2.1247 | Acc = 0.9730 | AUC = undefined | Perplexity = 8.3707\n",
      "Epoch 18: Train Loss = 2.1524 | Val Loss = 2.0804 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.0074\n",
      "Epoch 19: Train Loss = 2.0838 | Val Loss = 2.0404 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.6936\n",
      "Epoch 20: Train Loss = 2.0335 | Val Loss = 1.9994 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.3850\n",
      "Epoch 21: Train Loss = 1.9917 | Val Loss = 1.9545 | Acc = 1.0000 | AUC = undefined | Perplexity = 7.0604\n",
      "Epoch 22: Train Loss = 1.9539 | Val Loss = 1.9098 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.7519\n",
      "Epoch 23: Train Loss = 1.9156 | Val Loss = 1.8660 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.4626\n",
      "Epoch 24: Train Loss = 1.8666 | Val Loss = 1.8244 | Acc = 1.0000 | AUC = undefined | Perplexity = 6.1994\n",
      "Epoch 25: Train Loss = 1.8466 | Val Loss = 1.7863 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.9675\n",
      "Epoch 26: Train Loss = 1.7953 | Val Loss = 1.7559 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.7889\n",
      "Epoch 27: Train Loss = 1.7471 | Val Loss = 1.7213 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.5918\n",
      "Epoch 28: Train Loss = 1.7070 | Val Loss = 1.6751 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.3394\n",
      "Epoch 29: Train Loss = 1.6796 | Val Loss = 1.6279 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.0930\n",
      "Epoch 30: Train Loss = 1.6269 | Val Loss = 1.5875 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.8916\n",
      "Epoch 31: Train Loss = 1.5901 | Val Loss = 1.5444 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.6852\n",
      "Epoch 32: Train Loss = 1.5474 | Val Loss = 1.5103 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.5283\n",
      "Epoch 33: Train Loss = 1.5056 | Val Loss = 1.4719 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.3575\n",
      "Epoch 34: Train Loss = 1.4806 | Val Loss = 1.4487 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.2577\n",
      "Epoch 35: Train Loss = 1.4423 | Val Loss = 1.4100 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.0961\n",
      "Epoch 36: Train Loss = 1.4029 | Val Loss = 1.3767 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.9620\n",
      "Epoch 37: Train Loss = 1.3914 | Val Loss = 1.3449 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.8379\n",
      "Epoch 38: Train Loss = 1.3395 | Val Loss = 1.3105 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.7082\n",
      "Epoch 39: Train Loss = 1.3064 | Val Loss = 1.2742 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.5760\n",
      "Epoch 40: Train Loss = 1.2745 | Val Loss = 1.2412 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.4598\n",
      "Epoch 41: Train Loss = 1.2389 | Val Loss = 1.2160 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.3738\n",
      "Epoch 42: Train Loss = 1.2109 | Val Loss = 1.1860 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.2739\n",
      "Epoch 43: Train Loss = 1.1733 | Val Loss = 1.1535 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.1693\n",
      "Epoch 44: Train Loss = 1.1488 | Val Loss = 1.1178 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.0580\n",
      "Epoch 45: Train Loss = 1.1295 | Val Loss = 1.0888 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.9706\n",
      "Epoch 46: Train Loss = 1.0871 | Val Loss = 1.0677 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.9088\n",
      "Epoch 47: Train Loss = 1.1260 | Val Loss = 1.0616 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.8911\n",
      "Epoch 48: Train Loss = 1.0521 | Val Loss = 1.0389 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.8262\n",
      "Epoch 49: Train Loss = 1.0430 | Val Loss = 1.0010 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.7210\n",
      "Epoch 50: Train Loss = 1.0156 | Val Loss = 0.9736 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.6473\n",
      "Epoch 51: Train Loss = 0.9660 | Val Loss = 0.9474 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.5791\n",
      "Epoch 52: Train Loss = 0.9416 | Val Loss = 0.9232 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.5174\n",
      "Epoch 53: Train Loss = 0.9260 | Val Loss = 0.8980 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.4547\n",
      "Epoch 54: Train Loss = 0.8862 | Val Loss = 0.8711 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.3895\n",
      "Epoch 55: Train Loss = 0.8660 | Val Loss = 0.8420 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.3210\n",
      "Epoch 56: Train Loss = 0.8817 | Val Loss = 0.8240 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.2795\n",
      "Epoch 57: Train Loss = 0.8366 | Val Loss = 0.8127 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.2539\n",
      "Epoch 58: Train Loss = 0.8143 | Val Loss = 0.7964 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.2176\n",
      "Epoch 59: Train Loss = 0.8033 | Val Loss = 0.7744 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.1692\n",
      "Epoch 60: Train Loss = 0.7644 | Val Loss = 0.7534 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.1242\n",
      "Epoch 61: Train Loss = 0.7637 | Val Loss = 0.7353 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0861\n",
      "Epoch 62: Train Loss = 0.7398 | Val Loss = 0.7193 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0531\n",
      "Epoch 63: Train Loss = 0.7151 | Val Loss = 0.7063 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0265\n",
      "Epoch 64: Train Loss = 0.7075 | Val Loss = 0.6894 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9925\n",
      "Epoch 65: Train Loss = 0.7043 | Val Loss = 0.6732 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9604\n",
      "Epoch 66: Train Loss = 0.6689 | Val Loss = 0.6562 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.9275\n",
      "Epoch 67: Train Loss = 0.6632 | Val Loss = 0.6365 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.8898\n",
      "Epoch 68: Train Loss = 0.6494 | Val Loss = 0.6232 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.8648\n",
      "Epoch 69: Train Loss = 0.6277 | Val Loss = 0.6146 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.8490\n",
      "Epoch 70: Train Loss = 0.6041 | Val Loss = 0.6022 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.8260\n",
      "Epoch 71: Train Loss = 0.5891 | Val Loss = 0.5885 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.8013\n",
      "Epoch 72: Train Loss = 0.5867 | Val Loss = 0.5731 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.7737\n",
      "Epoch 73: Train Loss = 0.5654 | Val Loss = 0.5582 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.7475\n",
      "Epoch 74: Train Loss = 0.5528 | Val Loss = 0.5382 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.7129\n",
      "Epoch 75: Train Loss = 0.5369 | Val Loss = 0.5252 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.6908\n",
      "Epoch 76: Train Loss = 0.5659 | Val Loss = 0.5149 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.6735\n",
      "Epoch 77: Train Loss = 0.5235 | Val Loss = 0.5047 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.6565\n",
      "Epoch 78: Train Loss = 0.5097 | Val Loss = 0.4914 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.6345\n",
      "Epoch 79: Train Loss = 0.5145 | Val Loss = 0.4883 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.6295\n",
      "Epoch 80: Train Loss = 0.4933 | Val Loss = 0.4740 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.6064\n",
      "Epoch 81: Train Loss = 0.4782 | Val Loss = 0.4613 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.5862\n",
      "Epoch 82: Train Loss = 0.4844 | Val Loss = 0.4510 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.5699\n",
      "Epoch 83: Train Loss = 0.4638 | Val Loss = 0.4444 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.5596\n",
      "Epoch 84: Train Loss = 0.4573 | Val Loss = 0.4397 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.5522\n",
      "Epoch 85: Train Loss = 0.4383 | Val Loss = 0.4321 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.5404\n",
      "Epoch 86: Train Loss = 0.4278 | Val Loss = 0.4245 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.5288\n",
      "Epoch 87: Train Loss = 0.4235 | Val Loss = 0.4157 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.5155\n",
      "Epoch 88: Train Loss = 0.4347 | Val Loss = 0.4063 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.5013\n",
      "Epoch 89: Train Loss = 0.4095 | Val Loss = 0.3983 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.4893\n",
      "Epoch 90: Train Loss = 0.3939 | Val Loss = 0.3848 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.4693\n",
      "Epoch 91: Train Loss = 0.3919 | Val Loss = 0.3770 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.4579\n",
      "Epoch 92: Train Loss = 0.3853 | Val Loss = 0.3731 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.4522\n",
      "Epoch 93: Train Loss = 0.3699 | Val Loss = 0.3688 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.4460\n",
      "Epoch 94: Train Loss = 0.3616 | Val Loss = 0.3603 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.4338\n",
      "Epoch 95: Train Loss = 0.3532 | Val Loss = 0.3523 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.4223\n",
      "Epoch 96: Train Loss = 0.3571 | Val Loss = 0.3509 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.4203\n",
      "Epoch 97: Train Loss = 0.3507 | Val Loss = 0.3410 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.4063\n",
      "Epoch 98: Train Loss = 0.3661 | Val Loss = 0.3363 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.3998\n",
      "Epoch 99: Train Loss = 0.3328 | Val Loss = 0.3300 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.3910\n",
      "Epoch 100: Train Loss = 0.3654 | Val Loss = 0.3291 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.3897\n",
      "Epoch 101: Train Loss = 0.3358 | Val Loss = 0.3236 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.3821\n",
      "Epoch 102: Train Loss = 0.3441 | Val Loss = 0.3184 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3749\n",
      "Epoch 103: Train Loss = 0.3150 | Val Loss = 0.3172 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3733\n",
      "Epoch 104: Train Loss = 0.3176 | Val Loss = 0.3142 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3691\n",
      "Epoch 105: Train Loss = 0.3095 | Val Loss = 0.3098 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3631\n",
      "Epoch 106: Train Loss = 0.3033 | Val Loss = 0.3060 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3580\n",
      "Epoch 107: Train Loss = 0.2982 | Val Loss = 0.2996 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3493\n",
      "Epoch 108: Train Loss = 0.3086 | Val Loss = 0.2920 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3391\n",
      "Epoch 109: Train Loss = 0.2943 | Val Loss = 0.2867 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3320\n",
      "Epoch 110: Train Loss = 0.3036 | Val Loss = 0.2834 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3277\n",
      "Epoch 111: Train Loss = 0.2898 | Val Loss = 0.2864 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3316\n",
      "Epoch 112: Train Loss = 0.2920 | Val Loss = 0.2892 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3354\n",
      "Epoch 113: Train Loss = 0.3297 | Val Loss = 0.2772 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3195\n",
      "Epoch 114: Train Loss = 0.2699 | Val Loss = 0.2733 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3144\n",
      "Epoch 115: Train Loss = 0.2657 | Val Loss = 0.2659 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.3046\n",
      "Epoch 116: Train Loss = 0.2526 | Val Loss = 0.2577 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2940\n",
      "Epoch 117: Train Loss = 0.2642 | Val Loss = 0.2540 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2892\n",
      "Epoch 118: Train Loss = 0.2601 | Val Loss = 0.2534 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2885\n",
      "Epoch 119: Train Loss = 0.3966 | Val Loss = 0.2533 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2882\n",
      "Epoch 120: Train Loss = 0.3169 | Val Loss = 0.2672 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3063\n",
      "Epoch 121: Train Loss = 0.2754 | Val Loss = 0.2648 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.3032\n",
      "Epoch 122: Train Loss = 0.2489 | Val Loss = 0.2548 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2902\n",
      "Epoch 123: Train Loss = 0.2470 | Val Loss = 0.2440 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2764\n",
      "Epoch 124: Train Loss = 0.2445 | Val Loss = 0.2316 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2606\n",
      "Epoch 125: Train Loss = 0.2756 | Val Loss = 0.2268 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2546\n",
      "Epoch 126: Train Loss = 0.2404 | Val Loss = 0.2313 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2602\n",
      "Epoch 127: Train Loss = 0.2398 | Val Loss = 0.2316 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2607\n",
      "Epoch 128: Train Loss = 0.2235 | Val Loss = 0.2293 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2578\n",
      "Epoch 129: Train Loss = 0.2264 | Val Loss = 0.2287 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2570\n",
      "Epoch 130: Train Loss = 0.2301 | Val Loss = 0.2282 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2564\n",
      "Epoch 131: Train Loss = 0.2230 | Val Loss = 0.2179 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2435\n",
      "Epoch 132: Train Loss = 0.2298 | Val Loss = 0.2152 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2401\n",
      "Epoch 133: Train Loss = 0.2107 | Val Loss = 0.2096 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2332\n",
      "Epoch 134: Train Loss = 0.2271 | Val Loss = 0.2060 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2287\n",
      "Epoch 135: Train Loss = 0.2062 | Val Loss = 0.2023 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2242\n",
      "Epoch 136: Train Loss = 0.2098 | Val Loss = 0.2027 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2247\n",
      "Epoch 137: Train Loss = 0.2052 | Val Loss = 0.2059 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2286\n",
      "Epoch 138: Train Loss = 0.2259 | Val Loss = 0.2036 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2258\n",
      "Epoch 139: Train Loss = 0.1992 | Val Loss = 0.1962 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2168\n",
      "Epoch 140: Train Loss = 0.1975 | Val Loss = 0.1876 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2063\n",
      "Epoch 141: Train Loss = 0.2055 | Val Loss = 0.1881 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2070\n",
      "Epoch 142: Train Loss = 0.2003 | Val Loss = 0.1870 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2057\n",
      "Epoch 143: Train Loss = 0.2379 | Val Loss = 0.1867 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2052\n",
      "Epoch 144: Train Loss = 0.1935 | Val Loss = 0.1942 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2144\n",
      "Epoch 145: Train Loss = 0.2237 | Val Loss = 0.1892 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.2083\n",
      "Epoch 146: Train Loss = 0.2543 | Val Loss = 0.1892 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2083\n",
      "Epoch 147: Train Loss = 0.1880 | Val Loss = 0.1906 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2100\n",
      "Epoch 148: Train Loss = 0.2039 | Val Loss = 0.1911 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2105\n",
      "Epoch 149: Train Loss = 0.1858 | Val Loss = 0.1915 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2110\n",
      "Epoch 150: Train Loss = 0.1987 | Val Loss = 0.1915 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2110\n",
      "Epoch 151: Train Loss = 0.1813 | Val Loss = 0.1890 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2081\n",
      "Epoch 152: Train Loss = 0.1832 | Val Loss = 0.1881 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.2070\n",
      "Epoch 153: Train Loss = 0.1752 | Val Loss = 0.1820 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1996\n",
      "Epoch 154: Train Loss = 0.1656 | Val Loss = 0.1704 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1858\n",
      "Epoch 155: Train Loss = 0.1724 | Val Loss = 0.1653 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1797\n",
      "Epoch 156: Train Loss = 0.1708 | Val Loss = 0.1607 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1743\n",
      "Epoch 157: Train Loss = 0.1739 | Val Loss = 0.1559 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1687\n",
      "Epoch 158: Train Loss = 0.1984 | Val Loss = 0.1695 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1847\n",
      "Epoch 159: Train Loss = 0.1925 | Val Loss = 0.1692 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1844\n",
      "Epoch 160: Train Loss = 0.1742 | Val Loss = 0.1783 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1952\n",
      "Epoch 161: Train Loss = 0.1612 | Val Loss = 0.1658 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1804\n",
      "Epoch 162: Train Loss = 0.1530 | Val Loss = 0.1612 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1750\n",
      "Epoch 163: Train Loss = 0.1645 | Val Loss = 0.1600 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1735\n",
      "Epoch 164: Train Loss = 0.2619 | Val Loss = 0.1606 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1742\n",
      "Epoch 165: Train Loss = 0.1551 | Val Loss = 0.1592 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1726\n",
      "Epoch 166: Train Loss = 0.1647 | Val Loss = 0.1585 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1717\n",
      "Epoch 167: Train Loss = 0.1818 | Val Loss = 0.1584 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1717\n",
      "Epoch 168: Train Loss = 0.1856 | Val Loss = 0.1560 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1688\n",
      "Epoch 169: Train Loss = 0.1519 | Val Loss = 0.1554 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.1681\n",
      "Epoch 170: Train Loss = 0.1635 | Val Loss = 0.1586 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1719\n",
      "Epoch 171: Train Loss = 0.1607 | Val Loss = 0.1515 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1635\n",
      "Epoch 172: Train Loss = 0.1546 | Val Loss = 0.1475 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1589\n",
      "Epoch 173: Train Loss = 0.1420 | Val Loss = 0.1493 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1610\n",
      "Epoch 174: Train Loss = 0.1716 | Val Loss = 0.1517 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1638\n",
      "Epoch 175: Train Loss = 0.1924 | Val Loss = 0.1602 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1738\n",
      "Epoch 176: Train Loss = 0.1696 | Val Loss = 0.1569 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1699\n",
      "Epoch 177: Train Loss = 0.1469 | Val Loss = 0.1540 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1665\n",
      "Epoch 178: Train Loss = 0.1567 | Val Loss = 0.1523 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1645\n",
      "Epoch 179: Train Loss = 0.1356 | Val Loss = 0.1496 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1614\n",
      "Epoch 180: Train Loss = 0.1532 | Val Loss = 0.1464 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1576\n",
      "Epoch 181: Train Loss = 0.1493 | Val Loss = 0.1475 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1590\n",
      "Epoch 182: Train Loss = 0.1532 | Val Loss = 0.1477 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1592\n",
      "Epoch 183: Train Loss = 0.1285 | Val Loss = 0.1498 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1616\n",
      "Epoch 184: Train Loss = 0.1366 | Val Loss = 0.1425 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1531\n",
      "Epoch 185: Train Loss = 0.1291 | Val Loss = 0.1425 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1531\n",
      "Epoch 186: Train Loss = 0.1320 | Val Loss = 0.1423 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1529\n",
      "Epoch 187: Train Loss = 0.1291 | Val Loss = 0.1442 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1551\n",
      "Epoch 188: Train Loss = 0.1326 | Val Loss = 0.1451 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1562\n",
      "Epoch 189: Train Loss = 0.1481 | Val Loss = 0.1451 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1562\n",
      "Epoch 190: Train Loss = 0.1329 | Val Loss = 0.1431 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1538\n",
      "Epoch 191: Train Loss = 0.1276 | Val Loss = 0.1440 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1549\n",
      "Epoch 192: Train Loss = 0.1453 | Val Loss = 0.1459 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1571\n",
      "Epoch 193: Train Loss = 0.1563 | Val Loss = 0.1448 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1558\n",
      "Epoch 194: Train Loss = 0.1476 | Val Loss = 0.1457 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1568\n",
      "Epoch 195: Train Loss = 0.1296 | Val Loss = 0.1437 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1546\n",
      "Epoch 196: Train Loss = 0.1194 | Val Loss = 0.1445 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1555\n",
      "Epoch 197: Train Loss = 0.1211 | Val Loss = 0.1467 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1580\n",
      "Epoch 198: Train Loss = 0.1204 | Val Loss = 0.1461 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1573\n",
      "Epoch 199: Train Loss = 0.1232 | Val Loss = 0.1395 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1497\n",
      "Epoch 200: Train Loss = 0.1202 | Val Loss = 0.1417 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.1522\n",
      " Final Test Metrics:\n",
      "Loss = 0.1417, Accuracy = 0.9730, AUC = undefined, Perplexity = 1.1522\n",
      "Model saved to final_amp_generator_lstm.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_best_params_tb[\"lr\"], weight_decay=lstm_gen_best_params_tb[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "# lstm_gen_best_params = {'hidden_dim': 133, 'num_layers': 2}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params_tb[\"dropout\"],\n",
    "    # weights_decay=lstm_gen_best_params_tb[\"weight_decay\"],\n",
    "    # lr=lstm_gen_best_params_tb[\"lr\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=200)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\" Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n",
    "# Save model weights\n",
    "torch.save(trained_model.state_dict(), \"final_amp_generator_lstm.pt\")\n",
    "print(\"Model saved to final_amp_generator_lstm.pt\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Recreate your amino acid vocab\n",
    "aa_vocab = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aa_to_idx = {aa: i for i, aa in enumerate(aa_vocab)}\n",
    "idx_to_aa = {i: aa for aa, i in aa_to_idx.items()}\n",
    "\n",
    "def one_hot_encode_amino_acid(aa, vocab=aa_vocab):\n",
    "    vec = torch.zeros(len(vocab))\n",
    "    vec[aa_to_idx[aa]] = 1.0\n",
    "    return vec\n",
    "\n",
    "def generate_sequence_from_seed(model, seed, max_length=30, temperature=1.0, device='cpu'):\n",
    "    model.eval()\n",
    "    input_seq = [one_hot_encode_amino_acid(aa).to(device) for aa in seed]\n",
    "    input_tensor = torch.stack(input_seq).unsqueeze(0)  # [1, L, 20]\n",
    "\n",
    "    generated = seed.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(seed)):\n",
    "            output = model(input_tensor)  # [1, L, vocab]\n",
    "            logits = output[0, -1, :]  # Last time step → [vocab]\n",
    "\n",
    "            # Apply temperature and sample\n",
    "            probs = F.softmax(logits / temperature, dim=-1).cpu().numpy()\n",
    "            next_idx = np.random.choice(len(aa_vocab), p=probs)\n",
    "            next_aa = idx_to_aa[next_idx]\n",
    "\n",
    "            # Update sequence\n",
    "            next_aa_vec = one_hot_encode_amino_acid(next_aa).to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, 20]\n",
    "            input_tensor = torch.cat([input_tensor, next_aa_vec], dim=1)\n",
    "            generated.append(next_aa)\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "class LengthSampler:\n",
    "    def __init__(self, sequence_lengths):\n",
    "        \"\"\"\n",
    "        Initialize sampler from observed sequence lengths.\n",
    "        \n",
    "        Args:\n",
    "            sequence_lengths (list[int]): List of sequence lengths (e.g., [20, 21, 20, 23, ...])\n",
    "        \"\"\"\n",
    "        self.length_counts = Counter(sequence_lengths)\n",
    "        self.lengths = np.array(sorted(self.length_counts.keys()))\n",
    "        counts = np.array([self.length_counts[l] for l in self.lengths])\n",
    "        self.probs = counts / counts.sum()  # Empirical probabilities\n",
    "\n",
    "    def sample(self, n=1):\n",
    "        \"\"\"\n",
    "        Sample one or more lengths based on the learned distribution.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray of sampled lengths\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.lengths, size=n, p=self.probs)\n",
    "length_sampler = LengthSampler([len(seq) for seq in df.loc[df['AMP'] == 1, :]['Sequences']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EPAAAAAYPRAYYGGG\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    # dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "gen_model.eval()\n",
    "# Define a seed and generate a sequence\n",
    "sampled_length = length_sampler.sample()[0]\n",
    "start_aa = sample_start_amino_acid()\n",
    "seed_sequence = list(start_aa)  # start with valine\n",
    "generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1.0, device=device)\n",
    "\n",
    "print(\"Generated AMP sequence:\", generated_peptide)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### batch gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    # dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "gen_model.eval()\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1.2, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 15:11:24,898] A new study created in memory with name: no-name-cf68f5aa-42bb-4b5b-8b48-a267cfc12f6f\n",
      "[I 2025-04-22 15:11:31,811] Trial 0 finished with value: 2.1874172687530518 and parameters: {'dropout': 0.26927805101609836, 'lr': 0.006607107825528057, 'weight_decay': 0.0007828517656279971}. Best is trial 0 with value: 2.1874172687530518.\n",
      "[I 2025-04-22 15:11:37,863] Trial 1 finished with value: 2.287254810333252 and parameters: {'dropout': 0.44973959975333355, 'lr': 0.005780681311997045, 'weight_decay': 0.0007892633251980037}. Best is trial 0 with value: 2.1874172687530518.\n",
      "[I 2025-04-22 15:11:43,597] Trial 2 finished with value: 2.50271737575531 and parameters: {'dropout': 0.4481068863127341, 'lr': 0.0038894908204979723, 'weight_decay': 0.0007341287032190064}. Best is trial 0 with value: 2.1874172687530518.\n",
      "[I 2025-04-22 15:11:49,360] Trial 3 finished with value: 2.6780993938446045 and parameters: {'dropout': 0.22541801700207792, 'lr': 0.002533643023326825, 'weight_decay': 0.0006279418723843857}. Best is trial 0 with value: 2.1874172687530518.\n",
      "[I 2025-04-22 15:11:55,575] Trial 4 finished with value: 2.5472041368484497 and parameters: {'dropout': 0.30960227576436417, 'lr': 0.0036488610536654096, 'weight_decay': 0.0007208093051585932}. Best is trial 0 with value: 2.1874172687530518.\n",
      "[I 2025-04-22 15:12:01,477] Trial 5 finished with value: 2.1212894916534424 and parameters: {'dropout': 0.3402136218943675, 'lr': 0.007180611606871151, 'weight_decay': 0.0008128745410642789}. Best is trial 5 with value: 2.1212894916534424.\n",
      "[I 2025-04-22 15:12:07,365] Trial 6 finished with value: 2.2637096643447876 and parameters: {'dropout': 0.3773133121621458, 'lr': 0.005894116143663885, 'weight_decay': 0.00046294983078255374}. Best is trial 5 with value: 2.1212894916534424.\n",
      "[I 2025-04-22 15:12:13,194] Trial 7 finished with value: 2.1938241720199585 and parameters: {'dropout': 0.30035032368207376, 'lr': 0.006556541608108955, 'weight_decay': 0.0006525479708065609}. Best is trial 5 with value: 2.1212894916534424.\n",
      "[I 2025-04-22 15:12:19,116] Trial 8 finished with value: 2.3647643327713013 and parameters: {'dropout': 0.46496364621100184, 'lr': 0.005114409334560946, 'weight_decay': 0.000739264077142385}. Best is trial 5 with value: 2.1212894916534424.\n",
      "[I 2025-04-22 15:12:25,622] Trial 9 finished with value: 2.943788170814514 and parameters: {'dropout': 0.17749310644435556, 'lr': 0.000420868738424753, 'weight_decay': 0.00017001176542300563}. Best is trial 5 with value: 2.1212894916534424.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best transfer learning hyperparameters: {'dropout': 0.3402136218943675, 'lr': 0.007180611606871151, 'weight_decay': 0.0008128745410642789}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Criterion\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "# Transfer Learning Loader\n",
    "def load_pretrained_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model.load_state_dict(pretrained_dict, strict=False)\n",
    "    return model\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    last_token_logits = output[:, -1, :]\n",
    "    last_token_targets = target_seq[:, -1, :]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-frozen-gen-tb/AMP_LSTM_GEN_TRANSFER_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # if train:\n",
    "            #     torch.save(model.state_dict(), 'best_model_lstm_transfer.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=-1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "    \n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Optuna objective for fine-tuning\n",
    "def objective_generation(trial):\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(    \n",
    "                hidden_dim=156,\n",
    "                num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "                dropout=lstm_gen_best_params[\"dropout\"])\n",
    "    model = load_pretrained_weights(model, 'best_model_lstm_generator.pt')  # path to the general AMP model\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "# Run Optuna\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=10)\n",
    "lstm_gen_frozen_best_params_tb = study.best_trial.params\n",
    "print(\"Best transfer learning hyperparameters:\", lstm_gen_frozen_best_params_tb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dropout': 0.3402136218943675,\n",
       " 'lr': 0.007180611606871151,\n",
       " 'weight_decay': 0.0008128745410642789}"
      ]
     },
     "execution_count": 237,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gen_frozen_best_params_tb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.9842 | Val Loss = 2.9296 | Acc = 0.9688 | AUC = undefined | Perplexity = 18.7204\n",
      "Epoch 2: Train Loss = 2.8892 | Val Loss = 2.8361 | Acc = 0.9688 | AUC = undefined | Perplexity = 17.0489\n",
      "Epoch 3: Train Loss = 2.7940 | Val Loss = 2.7438 | Acc = 0.9688 | AUC = undefined | Perplexity = 15.5452\n",
      "Epoch 4: Train Loss = 2.7025 | Val Loss = 2.6530 | Acc = 0.9688 | AUC = undefined | Perplexity = 14.1966\n",
      "Epoch 5: Train Loss = 2.6142 | Val Loss = 2.5641 | Acc = 0.9688 | AUC = undefined | Perplexity = 12.9887\n",
      "Epoch 6: Train Loss = 2.5228 | Val Loss = 2.4753 | Acc = 0.9688 | AUC = undefined | Perplexity = 11.8852\n",
      "Epoch 7: Train Loss = 2.4312 | Val Loss = 2.3884 | Acc = 0.9688 | AUC = undefined | Perplexity = 10.8957\n",
      "Epoch 8: Train Loss = 2.3420 | Val Loss = 2.3028 | Acc = 0.9688 | AUC = undefined | Perplexity = 10.0026\n",
      "Epoch 9: Train Loss = 2.2545 | Val Loss = 2.2179 | Acc = 0.9688 | AUC = undefined | Perplexity = 9.1885\n",
      "Epoch 10: Train Loss = 2.1691 | Val Loss = 2.1347 | Acc = 0.9688 | AUC = undefined | Perplexity = 8.4541\n",
      "Epoch 11: Train Loss = 2.0845 | Val Loss = 2.0533 | Acc = 0.9688 | AUC = undefined | Perplexity = 7.7938\n",
      "Epoch 12: Train Loss = 2.0048 | Val Loss = 1.9719 | Acc = 0.9688 | AUC = undefined | Perplexity = 7.1841\n",
      "Epoch 13: Train Loss = 1.9203 | Val Loss = 1.8933 | Acc = 0.9688 | AUC = undefined | Perplexity = 6.6412\n",
      "Epoch 14: Train Loss = 1.8445 | Val Loss = 1.8161 | Acc = 0.9688 | AUC = undefined | Perplexity = 6.1476\n",
      "Epoch 15: Train Loss = 1.7647 | Val Loss = 1.7421 | Acc = 0.9688 | AUC = undefined | Perplexity = 5.7096\n",
      "Epoch 16: Train Loss = 1.7006 | Val Loss = 1.6682 | Acc = 0.9688 | AUC = undefined | Perplexity = 5.3024\n",
      "Epoch 17: Train Loss = 1.6203 | Val Loss = 1.5970 | Acc = 0.9688 | AUC = undefined | Perplexity = 4.9383\n",
      "Epoch 18: Train Loss = 1.5499 | Val Loss = 1.5290 | Acc = 0.9688 | AUC = undefined | Perplexity = 4.6137\n",
      "Epoch 19: Train Loss = 1.4770 | Val Loss = 1.4635 | Acc = 0.9688 | AUC = undefined | Perplexity = 4.3212\n",
      "Epoch 20: Train Loss = 1.4074 | Val Loss = 1.4014 | Acc = 0.9688 | AUC = undefined | Perplexity = 4.0609\n",
      "Epoch 21: Train Loss = 1.3507 | Val Loss = 1.3406 | Acc = 0.9688 | AUC = undefined | Perplexity = 3.8215\n",
      "Epoch 22: Train Loss = 1.2839 | Val Loss = 1.2810 | Acc = 0.9688 | AUC = undefined | Perplexity = 3.6002\n",
      "Epoch 23: Train Loss = 1.2403 | Val Loss = 1.2237 | Acc = 0.9688 | AUC = undefined | Perplexity = 3.3998\n",
      "Epoch 24: Train Loss = 1.1674 | Val Loss = 1.1688 | Acc = 0.9688 | AUC = undefined | Perplexity = 3.2181\n",
      "Epoch 25: Train Loss = 1.1466 | Val Loss = 1.1161 | Acc = 0.9688 | AUC = undefined | Perplexity = 3.0528\n",
      "Epoch 26: Train Loss = 1.0897 | Val Loss = 1.0649 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.9004\n",
      "Epoch 27: Train Loss = 1.0195 | Val Loss = 1.0184 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.7688\n",
      "Epoch 28: Train Loss = 0.9740 | Val Loss = 0.9750 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.6511\n",
      "Epoch 29: Train Loss = 0.9307 | Val Loss = 0.9321 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.5399\n",
      "Epoch 30: Train Loss = 0.8885 | Val Loss = 0.8918 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.4394\n",
      "Epoch 31: Train Loss = 0.8509 | Val Loss = 0.8534 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.3477\n",
      "Epoch 32: Train Loss = 0.8085 | Val Loss = 0.8178 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.2656\n",
      "Epoch 33: Train Loss = 0.7865 | Val Loss = 0.7847 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.1918\n",
      "Epoch 34: Train Loss = 0.7388 | Val Loss = 0.7527 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.1228\n",
      "Epoch 35: Train Loss = 0.7034 | Val Loss = 0.7234 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.0614\n",
      "Epoch 36: Train Loss = 0.6729 | Val Loss = 0.6961 | Acc = 0.9688 | AUC = undefined | Perplexity = 2.0060\n",
      "Epoch 37: Train Loss = 0.6497 | Val Loss = 0.6694 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.9531\n",
      "Epoch 38: Train Loss = 0.6301 | Val Loss = 0.6439 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.9038\n",
      "Epoch 39: Train Loss = 0.5941 | Val Loss = 0.6196 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.8581\n",
      "Epoch 40: Train Loss = 0.5859 | Val Loss = 0.5966 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.8160\n",
      "Epoch 41: Train Loss = 0.5534 | Val Loss = 0.5750 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.7771\n",
      "Epoch 42: Train Loss = 0.5332 | Val Loss = 0.5538 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.7398\n",
      "Epoch 43: Train Loss = 0.5071 | Val Loss = 0.5352 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.7078\n",
      "Epoch 44: Train Loss = 0.5008 | Val Loss = 0.5183 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.6792\n",
      "Epoch 45: Train Loss = 0.4740 | Val Loss = 0.5013 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.6508\n",
      "Epoch 46: Train Loss = 0.4480 | Val Loss = 0.4857 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.6253\n",
      "Epoch 47: Train Loss = 0.4528 | Val Loss = 0.4711 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.6018\n",
      "Epoch 48: Train Loss = 0.4243 | Val Loss = 0.4542 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5749\n",
      "Epoch 49: Train Loss = 0.4084 | Val Loss = 0.4401 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5529\n",
      "Epoch 50: Train Loss = 0.3924 | Val Loss = 0.4289 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5355\n",
      "Epoch 51: Train Loss = 0.4049 | Val Loss = 0.4167 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5169\n",
      "Epoch 52: Train Loss = 0.3711 | Val Loss = 0.4065 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.5016\n",
      "Epoch 53: Train Loss = 0.3764 | Val Loss = 0.3948 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4840\n",
      "Epoch 54: Train Loss = 0.3440 | Val Loss = 0.3851 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4698\n",
      "Epoch 55: Train Loss = 0.3338 | Val Loss = 0.3776 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4588\n",
      "Epoch 56: Train Loss = 0.3242 | Val Loss = 0.3703 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4482\n",
      "Epoch 57: Train Loss = 0.3150 | Val Loss = 0.3624 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4368\n",
      "Epoch 58: Train Loss = 0.3123 | Val Loss = 0.3543 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4252\n",
      "Epoch 59: Train Loss = 0.3390 | Val Loss = 0.3439 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4105\n",
      "Epoch 60: Train Loss = 0.2983 | Val Loss = 0.3366 | Acc = 0.9688 | AUC = undefined | Perplexity = 1.4001\n",
      " Final Test Metrics:\n",
      "Loss = 0.3366, Accuracy = 0.9688, AUC = undefined, Perplexity = 1.4001\n",
      "Model saved to final_amp_generator_lstm.pt\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_frozen_best_params_tb[\"lr\"], weight_decay=lstm_gen_frozen_best_params_tb[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-frozen-gen-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "# lstm_gen_best_params = {'hidden_dim': 133, 'num_layers': 2, 'dropout': 0.10063270147175422, 'lr': 0.003237280156212186, 'weight_decay': 2.2594437829479466e-05}\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_frozen_best_params_tb[\"dropout\"],\n",
    "    # weights_decay=lstm_gen_frozen_best_params_tb[\"weight_decay\"],\n",
    "    # lr=lstm_gen_frozen_best_params_tb[\"lr\"]\n",
    ")\n",
    "freeze_encoder(final_model)\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\" Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n",
    "# Save model weights\n",
    "torch.save(trained_model.state_dict(), \"final_amp_frozen_generator_lstm.pt\")\n",
    "print(\"Model saved to final_amp_generator_lstm.pt\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EAAAAAAYRRDYYKGG\n",
      "Generated AMP sequence: AAAANAKNNYINRG\n",
      "Generated AMP sequence: KAAWYSKGRNGNGYLRNNPG\n",
      "Generated AMP sequence: IAPRNYDGFNNNYNNPGYGYYKDYRRYGNGYRNGNNRRY\n",
      "Generated AMP sequence: IAAKKRNNNDGFRNNYKNSK\n",
      "Generated AMP sequence: FAASRPYYGY\n",
      "Generated AMP sequence: EAHAAGNYYANNKGNYNNRN\n",
      "Generated AMP sequence: VGAAAGARNGNYKGNYKRYKRNRRPGYNGGRRDNKRGRN\n",
      "Generated AMP sequence: DAAAYYHRYPNKGYYRNNRYYYRGGYRDGRDGPRRK\n",
      "Generated AMP sequence: WAAINYRPGNKKYNYRYNPN\n",
      "Generated AMP sequence: QAAANDYYYNDY\n",
      "Generated AMP sequence: CIRPAKYNGPYRPGRYGN\n",
      "Generated AMP sequence: AAAGAGYYYYNNYRRYYNNGPGNPNRGGYNG\n",
      "Generated AMP sequence: DAAAAANNRRYNNYNNGFYY\n",
      "Generated AMP sequence: HAAAANRRKYRPRNKNVDGG\n",
      "Generated AMP sequence: IAAAANNGNN\n",
      "Generated AMP sequence: TAAANNYRGGRFRYPNRNPY\n",
      "Generated AMP sequence: YGKAAAAGRGNYFYNGR\n",
      "Generated AMP sequence: AAANADRYYNNYNYYNSSGY\n",
      "Generated AMP sequence: VAARDAYGNYYPRNNNRSYY\n",
      "Generated AMP sequence: HAAAGYMGGSR\n",
      "Generated AMP sequence: VAAARRDGYNN\n",
      "Generated AMP sequence: QAYKCASPNYGNDNGGGRSRYNNYKY\n",
      "Generated AMP sequence: IKARKYDPYN\n",
      "Generated AMP sequence: RAAALYAKYYNRNGNNRGYY\n",
      "Generated AMP sequence: WAANNAYYYRRNYYGFNYYG\n",
      "Generated AMP sequence: KASNRNNKGYYYYPYYYKNG\n",
      "Generated AMP sequence: AAADNKFRNWNYPPYNGGSRRKGDNRNNYNNYNRYYG\n",
      "Generated AMP sequence: GAAAYNKRKGGGGGRGNYNRGKGGNGGGNKNNRGY\n",
      "Generated AMP sequence: QAESAGWRGKNNRNNSGKRY\n",
      "Generated AMP sequence: MAAAKGKNDNKNGYRRYNGP\n",
      "Generated AMP sequence: KAAAAKRPNYRKGGKGGNGY\n",
      "Generated AMP sequence: FAAYAKYYYK\n",
      "Generated AMP sequence: HANAKGRGNNNK\n",
      "Generated AMP sequence: MAAAANNDNGG\n",
      "Generated AMP sequence: EAAANRPNNNYYYGRYGGSPYGYKGGYRNNYNYNNNNSNK\n",
      "Generated AMP sequence: DAARNAYRNGYRPYYGNKSGPYYNYGYGNYGKRR\n",
      "Generated AMP sequence: PAAAADYPNNYGRNRDYYPR\n",
      "Generated AMP sequence: EAAANLRYYNGYYGYYNRNGNYDNNPYNNRNKNGG\n",
      "Generated AMP sequence: NALALPARGYGNGYNRGSKY\n",
      "Generated AMP sequence: NAAAKASNNNNP\n",
      "Generated AMP sequence: YACWCGRNDGRR\n",
      "Generated AMP sequence: KAADAATGRK\n",
      "Generated AMP sequence: CAAPRKRGNYD\n",
      "Generated AMP sequence: RAAALLYYNKN\n",
      "Generated AMP sequence: VAAAKNKYSGYNYYNFNPRL\n",
      "Generated AMP sequence: EAYAAGAYGPNP\n",
      "Generated AMP sequence: PAAAAYYPRRGYNNNGGRGR\n",
      "Generated AMP sequence: DAAAPGPNRYRK\n",
      "Generated AMP sequence: VAAAGNKSGG\n",
      "Generated AMP sequence: LAAAASTRKQNGYGYNGPYRYR\n",
      "Generated AMP sequence: YAAAAYYDRYGPYGRNYYYN\n",
      "Generated AMP sequence: NAANANGGGRNRGNPYNRYRKFYFYPYYYN\n",
      "Generated AMP sequence: WAAASYRNNRKGNNNKGEYNNRKY\n",
      "Generated AMP sequence: HAARYNPGYKYRRNKGKGYNGYYYY\n",
      "Generated AMP sequence: DAAGASGHPRPG\n",
      "Generated AMP sequence: CAARYYYRPGNNKRNGKRGYNNRYNRPNYYR\n",
      "Generated AMP sequence: IAANNRPRGYYFYGNSGYYNDNRYRNRPY\n",
      "Generated AMP sequence: LAAYAPYGYRG\n",
      "Generated AMP sequence: DAAAAPAYYYSGSF\n",
      "Generated AMP sequence: IAAKNNKYNN\n",
      "Generated AMP sequence: EAAAARRKYGNY\n",
      "Generated AMP sequence: PAAAYKYIIFRNYNYGYY\n",
      "Generated AMP sequence: KAAAAAYRYNGGRNKNGRNYNNDG\n",
      "Generated AMP sequence: RAAANYSDNGKYGNRRP\n",
      "Generated AMP sequence: NAAAAADNGP\n",
      "Generated AMP sequence: GAAAAPRCYGPRYRYNYGYP\n",
      "Generated AMP sequence: NAAAPAAKGYKYNGYRNPRKYYRKRG\n",
      "Generated AMP sequence: NAAPPRNNNY\n",
      "Generated AMP sequence: HAAAPNYYNL\n",
      "Generated AMP sequence: KAAANRGRYGKYGYNKYNNN\n",
      "Generated AMP sequence: DAAYAPKSGYNKGGNNYNKR\n",
      "Generated AMP sequence: YAAAPPNYYPYKGNGPYGGN\n",
      "Generated AMP sequence: GAAAAYYAYRYNNNRGGNRKRYGRNYGRYYRNRYPG\n",
      "Generated AMP sequence: VAAANKRGRYGYNYYPRYNNRYYYNYGPDNNNRNN\n",
      "Generated AMP sequence: IAAANQPNNYNN\n",
      "Generated AMP sequence: GAAALNPSYR\n",
      "Generated AMP sequence: RAAANYGYGYNGNKNNYPNRRNNGGRGNRGPLYNYPG\n",
      "Generated AMP sequence: PAANVGGRNYYNKYNYNRGR\n",
      "Generated AMP sequence: KAAHSFEARRPKNRNYGRN\n",
      "Generated AMP sequence: VAAYAARPKGGYHNKNGN\n",
      "Generated AMP sequence: IAAATPNNNR\n",
      "Generated AMP sequence: MAAGGANYGRKPGNSNRYYKNGYYPYNRGTGYYRGNNNGY\n",
      "Generated AMP sequence: CAAANNYNYYKYGYNYYYYN\n",
      "Generated AMP sequence: IAAGAYRYGY\n",
      "Generated AMP sequence: CAAKANNGNGFGYPYNNNNGRKRRGGYNGYGRYKG\n",
      "Generated AMP sequence: MAAFANYYRNR\n",
      "Generated AMP sequence: PAARGENNYRYDNNELYGPK\n",
      "Generated AMP sequence: KATNYNFPNY\n",
      "Generated AMP sequence: DAAAPNNNNYY\n",
      "Generated AMP sequence: HAAFGGYDYNRRYRRGNGRNRGGRGGDYRNNNYRGGG\n",
      "Generated AMP sequence: WAAAAYKSRNKGRYRPGYYN\n",
      "Generated AMP sequence: MAAIRPGNYKNNNRYKGKYRNYYYYKSNYRYRGPYYGGG\n",
      "Generated AMP sequence: HAAARRNPNNPRYRYGNDKRYG\n",
      "Generated AMP sequence: SAAAPPGNYPASGSKYN\n",
      "Generated AMP sequence: PAARYKNRYRNNRA\n",
      "Generated AMP sequence: RAAAMNRFPPPRRYNYNGGN\n",
      "Generated AMP sequence: FAAAGAKNGKRRYYRNRYGNYNPRG\n",
      "Generated AMP sequence: KAAARGNKYKNYYGYN\n",
      "Generated AMP sequence: FAAGGNRSGRGSRYGGNNKY\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_best_params[\"dropout\"]\n",
    ")\n",
    "gen_model.load_state_dict(torch.load(\"final_amp_generator_lstm.pt\"))\n",
    "gen_model.to(device)\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=0.7, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TB no transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 12:35:22,470] A new study created in memory with name: no-name-6f159489-f081-4a98-a9ee-c88b00a56ada\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-25 12:35:25,425] Trial 0 finished with value: 2.824002265930176 and parameters: {'hidden_dim': 206, 'num_layers': 2, 'dropout': 0.4839790848775959, 'lr': 0.003644623413660024, 'weight_decay': 0.0008083199139350111}. Best is trial 0 with value: 2.824002265930176.\n",
      "[I 2025-04-25 12:35:28,158] Trial 1 finished with value: 2.824535846710205 and parameters: {'hidden_dim': 139, 'num_layers': 2, 'dropout': 0.4424040668201631, 'lr': 0.003142478903807537, 'weight_decay': 3.501252661638913e-05}. Best is trial 0 with value: 2.824002265930176.\n",
      "[I 2025-04-25 12:35:31,051] Trial 2 finished with value: 2.6291379928588867 and parameters: {'hidden_dim': 192, 'num_layers': 2, 'dropout': 0.4309368762499072, 'lr': 0.008735760571752748, 'weight_decay': 0.0005650464565859144}. Best is trial 2 with value: 2.6291379928588867.\n",
      "[I 2025-04-25 12:35:33,743] Trial 3 finished with value: 2.846959352493286 and parameters: {'hidden_dim': 111, 'num_layers': 2, 'dropout': 0.44656884698028143, 'lr': 0.0020570531201686334, 'weight_decay': 0.00019980314622698256}. Best is trial 2 with value: 2.6291379928588867.\n",
      "[I 2025-04-25 12:35:36,501] Trial 4 finished with value: 2.7734904289245605 and parameters: {'hidden_dim': 113, 'num_layers': 3, 'dropout': 0.35572893893804447, 'lr': 0.003743004488735959, 'weight_decay': 0.0005260209445012384}. Best is trial 2 with value: 2.6291379928588867.\n",
      "[I 2025-04-25 12:35:39,542] Trial 5 finished with value: 2.7845301628112793 and parameters: {'hidden_dim': 194, 'num_layers': 3, 'dropout': 0.19940470623386772, 'lr': 0.0072975370029563546, 'weight_decay': 2.8575750342093583e-05}. Best is trial 2 with value: 2.6291379928588867.\n",
      "[I 2025-04-25 12:35:42,021] Trial 6 finished with value: 2.7541563510894775 and parameters: {'hidden_dim': 109, 'num_layers': 1, 'dropout': 0.2807338122794584, 'lr': 0.005566605958238007, 'weight_decay': 0.000168939173369988}. Best is trial 2 with value: 2.6291379928588867.\n",
      "[I 2025-04-25 12:35:44,553] Trial 7 finished with value: 2.867727279663086 and parameters: {'hidden_dim': 68, 'num_layers': 3, 'dropout': 0.3296887053221075, 'lr': 0.004605065325947262, 'weight_decay': 0.00013617954517842682}. Best is trial 2 with value: 2.6291379928588867.\n",
      "[I 2025-04-25 12:35:47,054] Trial 8 finished with value: 2.8623595237731934 and parameters: {'hidden_dim': 94, 'num_layers': 2, 'dropout': 0.43845189236514426, 'lr': 0.003982162302059522, 'weight_decay': 0.0006599207153576244}. Best is trial 2 with value: 2.6291379928588867.\n",
      "[I 2025-04-25 12:35:49,907] Trial 9 finished with value: 2.9029340744018555 and parameters: {'hidden_dim': 216, 'num_layers': 3, 'dropout': 0.38867980186869977, 'lr': 0.001504535384149213, 'weight_decay': 0.000693646191837061}. Best is trial 2 with value: 2.6291379928588867.\n",
      "[I 2025-04-25 12:35:52,353] Trial 10 finished with value: 2.528346061706543 and parameters: {'hidden_dim': 256, 'num_layers': 1, 'dropout': 0.11354691652364896, 'lr': 0.009885371511466463, 'weight_decay': 0.0009726454568040441}. Best is trial 10 with value: 2.528346061706543.\n",
      "[I 2025-04-25 12:35:54,811] Trial 11 finished with value: 2.4473812580108643 and parameters: {'hidden_dim': 254, 'num_layers': 1, 'dropout': 0.10699047044012125, 'lr': 0.009935448382765784, 'weight_decay': 0.0009603867104781691}. Best is trial 11 with value: 2.4473812580108643.\n",
      "[I 2025-04-25 12:35:57,266] Trial 12 finished with value: 2.551492691040039 and parameters: {'hidden_dim': 255, 'num_layers': 1, 'dropout': 0.13493206124613735, 'lr': 0.009343835593430189, 'weight_decay': 0.0009987985012733151}. Best is trial 11 with value: 2.4473812580108643.\n",
      "[I 2025-04-25 12:35:59,653] Trial 13 finished with value: 2.711575984954834 and parameters: {'hidden_dim': 255, 'num_layers': 1, 'dropout': 0.11053615595128366, 'lr': 0.007205915473885915, 'weight_decay': 0.0009508515736638091}. Best is trial 11 with value: 2.4473812580108643.\n",
      "[I 2025-04-25 12:36:02,083] Trial 14 finished with value: 2.5204432010650635 and parameters: {'hidden_dim': 230, 'num_layers': 1, 'dropout': 0.1978487921518922, 'lr': 0.009912495738990975, 'weight_decay': 0.0008246449409286636}. Best is trial 11 with value: 2.4473812580108643.\n",
      "[I 2025-04-25 12:36:04,504] Trial 15 finished with value: 2.5728604793548584 and parameters: {'hidden_dim': 226, 'num_layers': 1, 'dropout': 0.19484936536901812, 'lr': 0.007886774918130003, 'weight_decay': 0.0008209264750526715}. Best is trial 11 with value: 2.4473812580108643.\n",
      "[I 2025-04-25 12:36:06,964] Trial 16 finished with value: 2.7314813137054443 and parameters: {'hidden_dim': 171, 'num_layers': 1, 'dropout': 0.18888648595923632, 'lr': 0.005975534175439968, 'weight_decay': 0.00035869182200273656}. Best is trial 11 with value: 2.4473812580108643.\n",
      "[I 2025-04-25 12:36:09,351] Trial 17 finished with value: 2.555279016494751 and parameters: {'hidden_dim': 228, 'num_layers': 1, 'dropout': 0.2563517910520424, 'lr': 0.00848179078634833, 'weight_decay': 0.0007965895541137221}. Best is trial 11 with value: 2.4473812580108643.\n",
      "[I 2025-04-25 12:36:11,748] Trial 18 finished with value: 2.916038990020752 and parameters: {'hidden_dim': 168, 'num_layers': 1, 'dropout': 0.1662305145730527, 'lr': 0.00033011500470085404, 'weight_decay': 0.0004188140813040899}. Best is trial 11 with value: 2.4473812580108643.\n",
      "[I 2025-04-25 12:36:14,263] Trial 19 finished with value: 2.7070348262786865 and parameters: {'hidden_dim': 237, 'num_layers': 1, 'dropout': 0.2364249862300482, 'lr': 0.006613530558527095, 'weight_decay': 0.0008798162481108218}. Best is trial 11 with value: 2.4473812580108643.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dim': 254, 'num_layers': 1, 'dropout': 0.10699047044012125, 'lr': 0.009935448382765784, 'weight_decay': 0.0009603867104781691}\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary packages after reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "\n",
    "# Criterion\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss on the last time step of each sequence.\n",
    "    \n",
    "    Args:\n",
    "        output: Tensor of shape [B, L, vocab_size]\n",
    "        target_seq: Tensor of shape [B, L] containing target class indices\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss computed only on the last token of each sequence\n",
    "    \"\"\"\n",
    "    # Get last time step for each sequence\n",
    "    last_token_logits = output[:, -1, :]        # [B, vocab_size]\n",
    "    last_token_targets = target_seq[:, -1, :]      # [B]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "    # print('last_token_logits',last_token_logits.shape)\n",
    "    # print('last_token_targets',last_token_targets.shape)\n",
    "\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen-notrans-tb/AMP_LSTM_GEN_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])      # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            # print('target_shape before reshape',target_seq.shape)\n",
    "            # target_seq = target_seq.reshape(-1)\n",
    "            # print(f\"Output shape: {output.shape}, Target shape: {target_seq.shape}\")\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_generator-notrans-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])  # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # # target_seq = target_seq.reshape(-1)\n",
    "            # # target_seq = target_seq.reshape(-1, target_seq.shape[-1])\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            # assert output.size(0) == target_seq.size(0), f\"Mismatch: {output.size(0)} vs {target_seq.size(0)}\"\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            # print('loss done')\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            preds = output[:, -1, :]        # shape: [B, vocab_size]\n",
    "            preds = torch.argmax(preds, dim=1)  # shape: [B]\n",
    "\n",
    "            targets = target_seq[:, -1, :]      # shape: [B, vocab_size]\n",
    "            targets = torch.argmax(targets, dim=-1)  # shape: [B]\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Objective for Optuna tuning\n",
    "def objective_generation(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "\n",
    "lstm_gen_notrans_tb_best_params = study.best_trial.params\n",
    "print(lstm_gen_notrans_tb_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_dim': 254,\n",
       " 'num_layers': 1,\n",
       " 'dropout': 0.10699047044012125,\n",
       " 'lr': 0.009935448382765784,\n",
       " 'weight_decay': 0.0009603867104781691}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_gen_notrans_tb_best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 2.9754 | Val Loss = 2.9310 | Acc = 1.0000 | AUC = undefined | Perplexity = 18.7457\n",
      "Epoch 2: Train Loss = 2.9193 | Val Loss = 2.8671 | Acc = 1.0000 | AUC = undefined | Perplexity = 17.5852\n",
      "Epoch 3: Train Loss = 2.8558 | Val Loss = 2.7980 | Acc = 1.0000 | AUC = undefined | Perplexity = 16.4116\n",
      "Epoch 4: Train Loss = 2.7848 | Val Loss = 2.7141 | Acc = 1.0000 | AUC = undefined | Perplexity = 15.0915\n",
      "Epoch 5: Train Loss = 2.7334 | Val Loss = 2.6830 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.6288\n",
      "Epoch 6: Train Loss = 2.6741 | Val Loss = 2.6266 | Acc = 1.0000 | AUC = undefined | Perplexity = 13.8272\n",
      "Epoch 7: Train Loss = 2.6196 | Val Loss = 2.5721 | Acc = 0.9730 | AUC = undefined | Perplexity = 13.0932\n",
      "Epoch 8: Train Loss = 2.5938 | Val Loss = 2.5299 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.5527\n",
      "Epoch 9: Train Loss = 2.5239 | Val Loss = 2.4879 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.0358\n",
      "Epoch 10: Train Loss = 2.4682 | Val Loss = 2.4253 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.3056\n",
      "Epoch 11: Train Loss = 2.5137 | Val Loss = 2.3703 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.7007\n",
      "Epoch 12: Train Loss = 2.3601 | Val Loss = 2.3276 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.2534\n",
      "Epoch 13: Train Loss = 2.3129 | Val Loss = 2.2734 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.7123\n",
      "Epoch 14: Train Loss = 2.2707 | Val Loss = 2.2191 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.1988\n",
      "Epoch 15: Train Loss = 2.2254 | Val Loss = 2.1678 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.7386\n",
      "Epoch 16: Train Loss = 2.1594 | Val Loss = 2.1180 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.3147\n",
      "Epoch 17: Train Loss = 2.1066 | Val Loss = 2.0632 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.8712\n",
      "Epoch 18: Train Loss = 2.0491 | Val Loss = 2.0115 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.4748\n",
      "Epoch 19: Train Loss = 2.0369 | Val Loss = 1.9638 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.1265\n",
      "Epoch 20: Train Loss = 1.9615 | Val Loss = 1.9270 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.8690\n",
      "Epoch 21: Train Loss = 1.9286 | Val Loss = 1.8778 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.5392\n",
      "Epoch 22: Train Loss = 1.8662 | Val Loss = 1.8269 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.2148\n",
      "Epoch 23: Train Loss = 1.8274 | Val Loss = 1.7767 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.9102\n",
      "Epoch 24: Train Loss = 1.7824 | Val Loss = 1.7301 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.6414\n",
      "Epoch 25: Train Loss = 1.7484 | Val Loss = 1.6890 | Acc = 1.0000 | AUC = undefined | Perplexity = 5.4139\n",
      "Epoch 26: Train Loss = 1.6833 | Val Loss = 1.6506 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.2099\n",
      "Epoch 27: Train Loss = 1.6370 | Val Loss = 1.6095 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.0001\n",
      "Epoch 28: Train Loss = 1.5974 | Val Loss = 1.5613 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.7652\n",
      "Epoch 29: Train Loss = 1.5816 | Val Loss = 1.5232 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.5868\n",
      "Epoch 30: Train Loss = 1.5215 | Val Loss = 1.4859 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.4188\n",
      "Epoch 31: Train Loss = 1.4692 | Val Loss = 1.4301 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.1790\n",
      "Epoch 32: Train Loss = 1.4541 | Val Loss = 1.3823 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.9840\n",
      "Epoch 33: Train Loss = 1.3944 | Val Loss = 1.3502 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.8583\n",
      "Epoch 34: Train Loss = 1.3665 | Val Loss = 1.3243 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.7596\n",
      "Epoch 35: Train Loss = 1.3197 | Val Loss = 1.2908 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.6355\n",
      "Epoch 36: Train Loss = 1.2876 | Val Loss = 1.2576 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.5169\n",
      "Epoch 37: Train Loss = 1.2741 | Val Loss = 1.2289 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.4174\n",
      "Epoch 38: Train Loss = 1.2074 | Val Loss = 1.1916 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.2922\n",
      "Epoch 39: Train Loss = 1.1770 | Val Loss = 1.1490 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.1549\n",
      "Epoch 40: Train Loss = 1.1448 | Val Loss = 1.1112 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.0380\n",
      "Epoch 41: Train Loss = 1.1255 | Val Loss = 1.0757 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.9321\n",
      "Epoch 42: Train Loss = 1.0969 | Val Loss = 1.0478 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.8515\n",
      "Epoch 43: Train Loss = 1.0669 | Val Loss = 1.0278 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.7950\n",
      "Epoch 44: Train Loss = 1.0209 | Val Loss = 1.0059 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.7345\n",
      "Epoch 45: Train Loss = 0.9921 | Val Loss = 0.9655 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.6262\n",
      "Epoch 46: Train Loss = 0.9667 | Val Loss = 0.9310 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.5371\n",
      "Epoch 47: Train Loss = 0.9450 | Val Loss = 0.9149 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.4964\n",
      "Epoch 48: Train Loss = 0.8995 | Val Loss = 0.8863 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.4262\n",
      "Epoch 49: Train Loss = 0.8931 | Val Loss = 0.8597 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.3625\n",
      "Epoch 50: Train Loss = 0.8520 | Val Loss = 0.8364 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.3080\n",
      "Epoch 51: Train Loss = 0.8289 | Val Loss = 0.8089 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.2454\n",
      "Epoch 52: Train Loss = 0.8097 | Val Loss = 0.7892 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.2017\n",
      "Epoch 53: Train Loss = 0.7856 | Val Loss = 0.7606 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.1395\n",
      "Epoch 54: Train Loss = 0.7937 | Val Loss = 0.7596 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.1375\n",
      "Epoch 55: Train Loss = 0.7740 | Val Loss = 0.7309 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0770\n",
      "Epoch 56: Train Loss = 0.7287 | Val Loss = 0.7129 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0399\n",
      "Epoch 57: Train Loss = 0.7050 | Val Loss = 0.6948 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0033\n",
      "Epoch 58: Train Loss = 0.7014 | Val Loss = 0.6720 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.9581\n",
      "Epoch 59: Train Loss = 0.7394 | Val Loss = 0.6577 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9304\n",
      "Epoch 60: Train Loss = 0.6508 | Val Loss = 0.6474 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9105\n",
      "\n",
      "✅ Final Test Metrics:\n",
      "Loss = 0.6474, Accuracy = 0.9730, AUC = undefined, Perplexity = 1.9105\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_notrans_tb_best_params[\"lr\"], weight_decay=lstm_gen_notrans_tb_best_params[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen-notrans-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_notrans_tb_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_notrans_tb_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_notrans_tb_best_params[\"dropout\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\"\\n✅ Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EVNKAFAYPRAYWGGG\n",
      "Generated AMP sequence: AACCLDGKKSGKPA\n",
      "Generated AMP sequence: KAAYYSHDPKGKAYGPHKKG\n",
      "Generated AMP sequence: IIWWLYAGAHKHVKHKGSEYSGATRSSEKGYPIDHHSPY\n",
      "Generated AMP sequence: IALQKSKKKAGAPHKYGKS\n",
      "Generated AMP sequence: FAAAYSPYSGYK\n",
      "Generated AMP sequence: ERAAGKSYAKKGGKYHKRKYYGKHHA\n",
      "Generated AMP sequence: VAAEYGGKYGPSGSKPPKFW\n",
      "Generated AMP sequence: DAAKPAKGPGQKYG\n",
      "Generated AMP sequence: WAWTGPSKKGFYYP\n",
      "Generated AMP sequence: QAMWYSPCGYPAGP\n",
      "Generated AMP sequence: CAGPPGRGHS\n",
      "Generated AMP sequence: APKKAHGGYKYPSKNKGSHA\n",
      "Generated AMP sequence: DAYYYHAYKYYYHKYHGKYQ\n",
      "Generated AMP sequence: HAHYAKYSQQKHSSYYKKSP\n",
      "Generated AMP sequence: IKSFHALAKKHPAAVKGKSG\n",
      "Generated AMP sequence: TAAKKPRYKHSHKEAYYQKG\n",
      "Generated AMP sequence: YAGPPGYSKPKG\n",
      "Generated AMP sequence: AHAAAAYQKFKKGKK\n",
      "Generated AMP sequence: VAAGPKYPGDPAPYNKPKKY\n",
      "Generated AMP sequence: HVTAAAAEPDHYASHG\n",
      "Generated AMP sequence: VASPSGGSSYKKSKYYKSSG\n",
      "Generated AMP sequence: QARGYKAYEHYYMPKHHPSSSFKCKKYKGGSPGF\n",
      "Generated AMP sequence: IAQPAAYKKSYYSKFSLKYG\n",
      "Generated AMP sequence: RAFAADPSNYKHYGYAYAYK\n",
      "Generated AMP sequence: WAGYKPQKPPYAHYYKPHGKKPEYYQKHSPGYVYSPKYYA\n",
      "Generated AMP sequence: KAQYAMKYYY\n",
      "Generated AMP sequence: AAAAWSYYKSYYGKGYPGP\n",
      "Generated AMP sequence: GAAPKSKYKLYKGASPRGGA\n",
      "Generated AMP sequence: QAAHYHKSKPYYGYK\n",
      "Generated AMP sequence: MAYHGPGCGGGGP\n",
      "Generated AMP sequence: KATHPGGAGHGF\n",
      "Generated AMP sequence: FAAGKPASPEY\n",
      "Generated AMP sequence: HAASRGGKKPKKSAGRYKKGKKGHKAHGHGYPPSK\n",
      "Generated AMP sequence: MAHPKDGKPMK\n",
      "Generated AMP sequence: EAAAAGGGHGYEKKYGKYYTGGPYKMHSGHKKGGPHNGK\n",
      "Generated AMP sequence: DAAAAYHSGPSPKKKYWYGS\n",
      "Generated AMP sequence: PAAQKVGSGGGTPKKYKTKKKHSKGYKKYPGYPIGS\n",
      "Generated AMP sequence: EASRCGGSALSYKTGYGKSG\n",
      "Generated AMP sequence: NGMMPKGIGYNK\n",
      "Generated AMP sequence: NVAKKPAYYLQYRGMPKSY\n",
      "Generated AMP sequence: YAAYSCYYKPKAISAIKKYKKSKGKGGKKYKNPAP\n",
      "Generated AMP sequence: KWAGAYKPASGY\n",
      "Generated AMP sequence: CAAHKASKKKKL\n",
      "Generated AMP sequence: RASYHGPKAGRP\n",
      "Generated AMP sequence: VAAPAASGPG\n",
      "Generated AMP sequence: EAMSSKPGHYA\n",
      "Generated AMP sequence: PAAAKKYSHG\n",
      "Generated AMP sequence: DAAKHKKHYSGYKYSK\n",
      "Generated AMP sequence: VAGLGFTYKG\n",
      "Generated AMP sequence: LAVAKGKPVGAGY\n",
      "Generated AMP sequence: YAHNDYKKKAGSFPGKHKRHLKPYSGAHPAK\n",
      "Generated AMP sequence: NAPAARKPKGTSQGPKFYGY\n",
      "Generated AMP sequence: WAGVRSPQYGKGYYARYG\n",
      "Generated AMP sequence: HSAPGYYYKYYHVAPGGFQI\n",
      "Generated AMP sequence: DAAKSHPYPGAYAYKYSYKSKK\n",
      "Generated AMP sequence: CPYRKKSGGKHHGAAYKKPG\n",
      "Generated AMP sequence: IKAGYYKPGYGYPPKGGGGSKCYYYYGKSQGTGGKPP\n",
      "Generated AMP sequence: LPAAYYYSPKF\n",
      "Generated AMP sequence: DAAPKAGPDYKKPVKPL\n",
      "Generated AMP sequence: IPPNVYQVQPPSG\n",
      "Generated AMP sequence: EPASAHSGTWKAHPYPGPKSGSKYKPYGYQCH\n",
      "Generated AMP sequence: PAHGNAYYYSGSAAHKSPKHY\n",
      "Generated AMP sequence: KAASPGAPPHYGKYKPKH\n",
      "Generated AMP sequence: RATAGAPKYKYGYSSYSPGAYPSKGGQKGKDNHSHKAEK\n",
      "Generated AMP sequence: NAAPVSAKFGYGKPRLAHKF\n",
      "Generated AMP sequence: GAAGAKPSGGGPRAY\n",
      "Generated AMP sequence: NALTQSHSDY\n",
      "Generated AMP sequence: NMDLKRCAGGYGYIGSPKKR\n",
      "Generated AMP sequence: HYYLDPAAGKYS\n",
      "Generated AMP sequence: KAAGYAYMKSKYYKHPYGPPSGR\n",
      "Generated AMP sequence: DAAYDWKGYKKKPGKYGPHSGYKGEGKKWKGPRGG\n",
      "Generated AMP sequence: YGKHTSMYGG\n",
      "Generated AMP sequence: GAGYADKYHKAGY\n",
      "Generated AMP sequence: VAYLWKKKPGAHRGPYFPKYGQYYPKPYMAYQPGPKPG\n",
      "Generated AMP sequence: IYAYHYSLSYKHPSSSKTGK\n",
      "Generated AMP sequence: GAAGPKKGKG\n",
      "Generated AMP sequence: RAKKKIYKKA\n",
      "Generated AMP sequence: PAAKKPSSRYAGAP\n",
      "Generated AMP sequence: KAWAYHCKGHHSKHPRHKGGPGKRGLGYKSKAPY\n",
      "Generated AMP sequence: VMSAFPKWYKGSHVKPGPKP\n",
      "Generated AMP sequence: IGSAAAPPKGKP\n",
      "Generated AMP sequence: MYAPHKRQYGGSNHEFYGHG\n",
      "Generated AMP sequence: CAGASGKYPKHHPYP\n",
      "Generated AMP sequence: IGDAKYGQGKFKSKPSYGKG\n",
      "Generated AMP sequence: CSGSKPASGVSRAHGKFYKHKKPKYKYYGYGTKYYSSHEK\n",
      "Generated AMP sequence: MEAYPSFYYKPS\n",
      "Generated AMP sequence: PAAAHAAGYKYK\n",
      "Generated AMP sequence: KACAPGPPGCSKAY\n",
      "Generated AMP sequence: DEYAADHEQA\n",
      "Generated AMP sequence: HYQKGPKKGYKGKKWSSAKK\n",
      "Generated AMP sequence: WAPAKGAGYS\n",
      "Generated AMP sequence: MAAKKVDKGHSKKKKSYYSHQKGYAYKPPYPPDKGSKP\n",
      "Generated AMP sequence: HALAAAYSKH\n",
      "Generated AMP sequence: SIKACAPSGAFYHSP\n",
      "Generated AMP sequence: PAAKSPKAYSHYPPSS\n",
      "Generated AMP sequence: RAAYGHKKPYGGGYPHTYYY\n",
      "Generated AMP sequence: FHCSQSPGKYYA\n",
      "Generated AMP sequence: KAMSFKVSKPKH\n",
      "Generated AMP sequence: FESLSFKAGSYFKSGKSPGK\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# final_model = GenerativeLSTM(\n",
    "#     input_dim=20,\n",
    "#     hidden_dim=lstm_gen_notrans_tb_best_params[\"hidden_dim\"],\n",
    "#     num_layers=lstm_gen_notrans_tb_best_params[\"num_layers\"],\n",
    "#     dropout=lstm_gen_notrans_tb_best_params[\"dropout\"]\n",
    "# )\n",
    "# trained_model = GenerativeLSTM(\n",
    "#     input_dim=20,\n",
    "#     hidden_dim=176,\n",
    "#     num_layers=lstm_gen_notrans_tb_best_params[\"num_layers\"],\n",
    "#     dropout=lstm_gen_notrans_tb_best_params[\"dropout\"]\n",
    "# )\n",
    "\n",
    "\n",
    "# final_model.load_state_dict(torch.load(\"best_model_lstm_generator-notrans-tb.pt\"))\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "class LengthSampler:\n",
    "    def __init__(self, sequence_lengths):\n",
    "        \"\"\"\n",
    "        Initialize sampler from observed sequence lengths.\n",
    "        \n",
    "        Args:\n",
    "            sequence_lengths (list[int]): List of sequence lengths (e.g., [20, 21, 20, 23, ...])\n",
    "        \"\"\"\n",
    "        self.length_counts = Counter(sequence_lengths)\n",
    "        self.lengths = np.array(sorted(self.length_counts.keys()))\n",
    "        counts = np.array([self.length_counts[l] for l in self.lengths])\n",
    "        self.probs = counts / counts.sum()  # Empirical probabilities\n",
    "\n",
    "    def sample(self, n=1):\n",
    "        \"\"\"\n",
    "        Sample one or more lengths based on the learned distribution.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray of sampled lengths\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.lengths, size=n, p=self.probs)\n",
    "length_sampler = LengthSampler([len(seq) for seq in df.loc[df['AMP'] == 1, :]['Sequences']])\n",
    "\n",
    "\n",
    "# Recreate your amino acid vocab\n",
    "aa_vocab = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aa_to_idx = {aa: i for i, aa in enumerate(aa_vocab)}\n",
    "idx_to_aa = {i: aa for aa, i in aa_to_idx.items()}\n",
    "\n",
    "def one_hot_encode_amino_acid(aa, vocab=aa_vocab):\n",
    "    vec = torch.zeros(len(vocab))\n",
    "    vec[aa_to_idx[aa]] = 1.0\n",
    "    return vec\n",
    "\n",
    "def generate_sequence_from_seed(model, seed, max_length=30, temperature=1.0, device='cpu'):\n",
    "    model.eval()\n",
    "    input_seq = [one_hot_encode_amino_acid(aa).to(device) for aa in seed]\n",
    "    input_tensor = torch.stack(input_seq).unsqueeze(0)  # [1, L, 20]\n",
    "\n",
    "    generated = seed.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(seed)):\n",
    "            output = model(input_tensor)  # [1, L, vocab]\n",
    "            logits = output[0, -1, :]  # Last time step → [vocab]\n",
    "\n",
    "            # Apply temperature and sample\n",
    "            probs = F.softmax(logits / temperature, dim=-1).cpu().numpy()\n",
    "            next_idx = np.random.choice(len(aa_vocab), p=probs)\n",
    "            next_aa = idx_to_aa[next_idx]\n",
    "\n",
    "            # Update sequence\n",
    "            next_aa_vec = one_hot_encode_amino_acid(next_aa).to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, 20]\n",
    "            input_tensor = torch.cat([input_tensor, next_aa_vec], dim=1)\n",
    "            generated.append(next_aa)\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = trained_model\n",
    "gen_model.to(device)\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=0.7, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides-notrans.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
