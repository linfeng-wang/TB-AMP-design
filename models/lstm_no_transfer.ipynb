{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env: new-ml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'W', 'X', 'D', 'L', 'T', 'G', 'H', 'F', 'I', 'M', 'N', 'V', 'Q', 'Y', 'C', 'R', 'E', 'K', 'A', 'S', 'P'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define One-Hot Encoding Function for DNA Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        length = len(seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        return one_hot_torch(seq, dtype=self.one_hot_dtype), torch.tensor(label, dtype=torch.float32), length\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def collate_and_pack(batch):\n",
    "    # batch = list of (tensor_seq, label, length)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "\n",
    "    # lengths as tensor\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # Sort by descending length (required by pack_padded_sequence)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    labels = torch.tensor([labels[i] for i in sorted_indices])\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    # Stack to shape: (batch_size, 20, seq_len) and transpose for LSTM input\n",
    "    # LSTM expects input of shape (seq_len, batch_size, features)\n",
    "    sequences = [seq.T for seq in sequences]  # Transpose each [20, L] to [L, 20]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)  # shape: [max_len, batch, features]\n",
    "\n",
    "    # Pack the sequence\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 269\n",
      "Validation: 90\n",
      "Test: 90\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "train_model() got an unexpected keyword argument 'verbose'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 114\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m avg_loss, acc, auc\n\u001b[1;32m    113\u001b[0m model \u001b[38;5;241m=\u001b[39m LSTMClassifier(hidden_dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m64\u001b[39m)\n\u001b[0;32m--> 114\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_epochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mTypeError\u001b[0m: train_model() got an unexpected keyword argument 'verbose'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "    # Set up TensorBoard writer\n",
    "    log_dir = f\"runs/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Run evaluation\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # handle corner cases\n",
    "\n",
    "    # Sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        # Print metrics\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "model = LSTMClassifier(hidden_dim=64)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling on general AMP data (bayesian optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 14:51:35,946] A new study created in memory with name: no-name-bef62f1d-d275-4b1f-841d-b54a0307578b\n",
      "[I 2025-04-22 14:54:49,146] Trial 0 finished with value: 0.6888018647829691 and parameters: {'hidden_dim': 36, 'num_layers': 3, 'dropout': 0.420542252686966, 'lr': 0.003353794076239294, 'weight_decay': 0.005388556593014484}. Best is trial 0 with value: 0.6888018647829691.\n",
      "[I 2025-04-22 15:01:25,653] Trial 1 finished with value: 0.6888130903244019 and parameters: {'hidden_dim': 81, 'num_layers': 3, 'dropout': 0.21054404223086773, 'lr': 0.006815397418964112, 'weight_decay': 0.005770857820001904}. Best is trial 0 with value: 0.6888018647829691.\n",
      "[I 2025-04-22 15:02:50,616] Trial 2 finished with value: 0.4847722351551056 and parameters: {'hidden_dim': 55, 'num_layers': 1, 'dropout': 0.24545958383670438, 'lr': 0.001592601401958308, 'weight_decay': 0.007730927844405101}. Best is trial 2 with value: 0.4847722351551056.\n",
      "[I 2025-04-22 15:03:17,725] Trial 3 finished with value: 0.4335531989733378 and parameters: {'hidden_dim': 89, 'num_layers': 1, 'dropout': 0.4162751197261255, 'lr': 0.005169218927600478, 'weight_decay': 0.009887359158827783}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:04:02,213] Trial 4 finished with value: 0.48633267482121784 and parameters: {'hidden_dim': 38, 'num_layers': 3, 'dropout': 0.35338098154627096, 'lr': 0.004855356399000411, 'weight_decay': 0.0003410019947662174}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:04:28,762] Trial 5 finished with value: 0.4391142427921295 and parameters: {'hidden_dim': 78, 'num_layers': 2, 'dropout': 0.2232272088800018, 'lr': 0.007011641267023337, 'weight_decay': 0.004263715051851624}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:04:53,321] Trial 6 finished with value: 0.47375838955243427 and parameters: {'hidden_dim': 80, 'num_layers': 2, 'dropout': 0.34182798671875153, 'lr': 0.004233942526001529, 'weight_decay': 0.002969223442650807}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:05:28,054] Trial 7 finished with value: 0.688925584157308 and parameters: {'hidden_dim': 63, 'num_layers': 3, 'dropout': 0.1058060943100815, 'lr': 0.0012297546768589524, 'weight_decay': 0.00886650057496204}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:06:02,089] Trial 8 finished with value: 0.6887526114781698 and parameters: {'hidden_dim': 82, 'num_layers': 3, 'dropout': 0.10568129061215262, 'lr': 0.0035756980060812193, 'weight_decay': 0.0017955906356071584}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:06:28,017] Trial 9 finished with value: 0.5468355317910513 and parameters: {'hidden_dim': 124, 'num_layers': 2, 'dropout': 0.23872507129465304, 'lr': 0.006759416773348787, 'weight_decay': 0.0014465527439275598}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:06:41,770] Trial 10 finished with value: 0.5860173106193542 and parameters: {'hidden_dim': 112, 'num_layers': 1, 'dropout': 0.47926615549084034, 'lr': 0.009932245943568195, 'weight_decay': 0.009655748802334317}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:06:55,779] Trial 11 finished with value: 0.4890888035297394 and parameters: {'hidden_dim': 100, 'num_layers': 1, 'dropout': 0.4198932908284402, 'lr': 0.007306574271648667, 'weight_decay': 0.0036592167177757345}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:07:20,797] Trial 12 finished with value: 0.6887409488360087 and parameters: {'hidden_dim': 98, 'num_layers': 2, 'dropout': 0.1738315073072851, 'lr': 0.008701936000207248, 'weight_decay': 0.0073223776737443135}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:07:33,101] Trial 13 finished with value: 0.43752261996269226 and parameters: {'hidden_dim': 66, 'num_layers': 1, 'dropout': 0.3026775306681369, 'lr': 0.005452583806507377, 'weight_decay': 0.004072430365560115}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:07:45,725] Trial 14 finished with value: 0.5427278876304626 and parameters: {'hidden_dim': 69, 'num_layers': 1, 'dropout': 0.3101394649342369, 'lr': 0.005647180988600303, 'weight_decay': 0.006578668780858114}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:07:57,799] Trial 15 finished with value: 0.4519205888112386 and parameters: {'hidden_dim': 51, 'num_layers': 1, 'dropout': 0.4025933993970493, 'lr': 0.0027629223501162074, 'weight_decay': 0.004564831729243547}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:08:11,563] Trial 16 finished with value: 0.43463801344235736 and parameters: {'hidden_dim': 94, 'num_layers': 1, 'dropout': 0.4740416209281767, 'lr': 0.005657844313540852, 'weight_decay': 0.0030643575854700893}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:08:25,271] Trial 17 finished with value: 0.4652961591879527 and parameters: {'hidden_dim': 96, 'num_layers': 1, 'dropout': 0.49081310544219275, 'lr': 0.002433765942052264, 'weight_decay': 0.0025639400905861196}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:08:50,902] Trial 18 finished with value: 0.688635508219401 and parameters: {'hidden_dim': 109, 'num_layers': 2, 'dropout': 0.4446998425993047, 'lr': 0.005503260043683913, 'weight_decay': 0.008481210399654701}. Best is trial 3 with value: 0.4335531989733378.\n",
      "[I 2025-04-22 15:09:05,844] Trial 19 finished with value: 0.6846411426862081 and parameters: {'hidden_dim': 92, 'num_layers': 1, 'dropout': 0.37611517135358474, 'lr': 0.0002578691939832201, 'weight_decay': 0.006356072476090783}. Best is trial 3 with value: 0.4335531989733378.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 89, 'num_layers': 1, 'dropout': 0.4162751197261255, 'lr': 0.005169218927600478, 'weight_decay': 0.009887359158827783}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-tb/no_transf-AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_auc = train_model(model, train_loader, val_loader, num_epochs=20, lr=lr,\n",
    "                          weight_decay=weight_decay, verbose=False)\n",
    "    return val_auc\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/19] - Train Loss: 0.6886, Val Loss: 0.6796, Val Acc: 0.5444, Val AUC: 0.7551\n",
      "\n",
      "Confusion Matrix:\n",
      "[[23 26]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.4694\n",
      "Epoch [2/19] - Train Loss: 0.6715, Val Loss: 0.6695, Val Acc: 0.5889, Val AUC: 0.7337\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39 10]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.7959\n",
      "Epoch [3/19] - Train Loss: 0.5777, Val Loss: 0.6197, Val Acc: 0.7444, Val AUC: 0.7606\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  2]\n",
      " [28 13]]\n",
      "Sensitivity: 0.3171, Specificity: 0.9592\n",
      "Epoch [4/19] - Train Loss: 0.6805, Val Loss: 0.6951, Val Acc: 0.6667, Val AUC: 0.7616\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [34  7]]\n",
      "Sensitivity: 0.1707, Specificity: 1.0000\n",
      "Epoch [5/19] - Train Loss: 0.6662, Val Loss: 0.6633, Val Acc: 0.6222, Val AUC: 0.6546\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [34  7]]\n",
      "Sensitivity: 0.1707, Specificity: 1.0000\n",
      "Epoch [6/19] - Train Loss: 0.6619, Val Loss: 0.6539, Val Acc: 0.6222, Val AUC: 0.6446\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  2]\n",
      " [33  8]]\n",
      "Sensitivity: 0.1951, Specificity: 0.9592\n",
      "Epoch [7/19] - Train Loss: 0.6483, Val Loss: 0.6525, Val Acc: 0.6111, Val AUC: 0.6386\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  2]\n",
      " [33  8]]\n",
      "Sensitivity: 0.1951, Specificity: 0.9592\n",
      "Epoch [8/19] - Train Loss: 0.6290, Val Loss: 0.6956, Val Acc: 0.6111, Val AUC: 0.6570\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  2]\n",
      " [32  9]]\n",
      "Sensitivity: 0.2195, Specificity: 0.9592\n",
      "Epoch [9/19] - Train Loss: 0.6566, Val Loss: 0.6541, Val Acc: 0.6222, Val AUC: 0.6297\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  2]\n",
      " [33  8]]\n",
      "Sensitivity: 0.1951, Specificity: 0.9592\n",
      "Epoch [10/19] - Train Loss: 0.6094, Val Loss: 0.6450, Val Acc: 0.6111, Val AUC: 0.6416\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [20 21]]\n",
      "Sensitivity: 0.5122, Specificity: 0.8776\n",
      "Epoch [11/19] - Train Loss: 0.5735, Val Loss: 0.6254, Val Acc: 0.7111, Val AUC: 0.6849\n",
      "\n",
      "Confusion Matrix:\n",
      "[[24 25]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.4898\n",
      "Epoch [12/19] - Train Loss: 0.5156, Val Loss: 0.6783, Val Acc: 0.6111, Val AUC: 0.7108\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  9]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.8163\n",
      "Epoch [13/19] - Train Loss: 0.5273, Val Loss: 0.6381, Val Acc: 0.7000, Val AUC: 0.7003\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  4]\n",
      " [24 17]]\n",
      "Sensitivity: 0.4146, Specificity: 0.9184\n",
      "Epoch [14/19] - Train Loss: 0.5148, Val Loss: 0.6546, Val Acc: 0.6889, Val AUC: 0.6849\n",
      "\n",
      "Confusion Matrix:\n",
      "[[20 29]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.4082\n",
      "Epoch [15/19] - Train Loss: 0.5422, Val Loss: 0.6961, Val Acc: 0.5556, Val AUC: 0.6685\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.8980\n",
      "Epoch [16/19] - Train Loss: 0.4868, Val Loss: 0.6931, Val Acc: 0.7000, Val AUC: 0.6909\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25 24]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.5102\n",
      "Epoch [17/19] - Train Loss: 0.4478, Val Loss: 0.7024, Val Acc: 0.6444, Val AUC: 0.7227\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25 24]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.5102\n",
      "Epoch [18/19] - Train Loss: 0.4871, Val Loss: 0.6812, Val Acc: 0.5889, Val AUC: 0.6874\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 14]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.7143\n",
      "Epoch [19/19] - Train Loss: 0.4707, Val Loss: 0.6473, Val Acc: 0.6667, Val AUC: 0.7093\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  9]\n",
      " [15 26]]\n",
      "Sensitivity: 0.6341, Specificity: 0.8163\n",
      "Test Loss: 0.5747, Test Accuracy: 0.7333, Test AUC: 0.7944\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "                      weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "criterion = nn.BCELoss()    \n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 15:58:11,366] A new study created in memory with name: no-name-e3a298c8-ed4d-4abd-8dd4-a83b937e3a15\n",
      "[I 2025-04-22 16:00:25,242] Trial 0 finished with value: 0.38735408584276837 and parameters: {'hidden_dim': 122, 'num_layers': 2, 'dropout': 0.2886500848001634, 'lr': 0.009924554328904543, 'weight_decay': 0.0053016000518520245}. Best is trial 0 with value: 0.38735408584276837.\n",
      "[I 2025-04-22 16:00:40,523] Trial 1 finished with value: 0.6324633757273356 and parameters: {'hidden_dim': 127, 'num_layers': 2, 'dropout': 0.35097633653554405, 'lr': 0.0003299035472825921, 'weight_decay': 0.0008491000197816713}. Best is trial 0 with value: 0.38735408584276837.\n",
      "[I 2025-04-22 16:00:48,758] Trial 2 finished with value: 0.291768158475558 and parameters: {'hidden_dim': 109, 'num_layers': 1, 'dropout': 0.2704783447659399, 'lr': 0.005578652852973335, 'weight_decay': 0.0020777440609733447}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:00:54,342] Trial 3 finished with value: 0.32117875417073566 and parameters: {'hidden_dim': 90, 'num_layers': 1, 'dropout': 0.36394833241566715, 'lr': 0.009424861821902211, 'weight_decay': 0.0030353921805030847}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:00:59,054] Trial 4 finished with value: 0.36277882754802704 and parameters: {'hidden_dim': 47, 'num_layers': 1, 'dropout': 0.43701477157861646, 'lr': 0.004400135405281565, 'weight_decay': 7.237836245027909e-06}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:01:14,044] Trial 5 finished with value: 0.686767597993215 and parameters: {'hidden_dim': 90, 'num_layers': 3, 'dropout': 0.33940478683937025, 'lr': 0.005208424649907395, 'weight_decay': 0.006346399176256977}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:01:19,171] Trial 6 finished with value: 0.29869137207667035 and parameters: {'hidden_dim': 60, 'num_layers': 1, 'dropout': 0.10975963641204084, 'lr': 0.008348881013074835, 'weight_decay': 0.004863340569492031}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:01:33,519] Trial 7 finished with value: 0.31261348227659863 and parameters: {'hidden_dim': 121, 'num_layers': 2, 'dropout': 0.18283579137089037, 'lr': 0.007640720501592202, 'weight_decay': 0.0013534163351551015}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:01:42,871] Trial 8 finished with value: 0.4107075035572052 and parameters: {'hidden_dim': 75, 'num_layers': 2, 'dropout': 0.2879777723455859, 'lr': 0.0058296608859701735, 'weight_decay': 0.007879617488751602}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:01:57,149] Trial 9 finished with value: 0.6834608117739359 and parameters: {'hidden_dim': 89, 'num_layers': 3, 'dropout': 0.2701932328328119, 'lr': 0.0022530272556248814, 'weight_decay': 0.0036646846079015226}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:02:03,641] Trial 10 finished with value: 0.3823953370253245 and parameters: {'hidden_dim': 107, 'num_layers': 1, 'dropout': 0.49638167249266774, 'lr': 0.004093005011217652, 'weight_decay': 0.009958482367162479}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:02:08,668] Trial 11 finished with value: 0.3235341211160024 and parameters: {'hidden_dim': 58, 'num_layers': 1, 'dropout': 0.11151712642484966, 'lr': 0.007536025018646164, 'weight_decay': 0.003412362326812796}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:02:14,229] Trial 12 finished with value: 0.2937765121459961 and parameters: {'hidden_dim': 33, 'num_layers': 1, 'dropout': 0.19259182055743315, 'lr': 0.007346099782905444, 'weight_decay': 0.002333968694752974}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:02:19,090] Trial 13 finished with value: 0.33433715502421063 and parameters: {'hidden_dim': 41, 'num_layers': 1, 'dropout': 0.21576371501166702, 'lr': 0.0061815774529841196, 'weight_decay': 0.0021084758343467487}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:02:26,478] Trial 14 finished with value: 0.337978074947993 and parameters: {'hidden_dim': 104, 'num_layers': 1, 'dropout': 0.20818490622136532, 'lr': 0.00302538028538857, 'weight_decay': 0.0021194465894112256}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:02:37,708] Trial 15 finished with value: 0.3818471034367879 and parameters: {'hidden_dim': 72, 'num_layers': 2, 'dropout': 0.168726024630573, 'lr': 0.006612518662110102, 'weight_decay': 0.004510783624449814}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:02:43,072] Trial 16 finished with value: 0.3295351465543111 and parameters: {'hidden_dim': 33, 'num_layers': 1, 'dropout': 0.24174542440254088, 'lr': 0.008434229468889888, 'weight_decay': 0.002079104310596187}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:03:04,532] Trial 17 finished with value: 0.6845725377400717 and parameters: {'hidden_dim': 108, 'num_layers': 3, 'dropout': 0.1530965255801204, 'lr': 0.006800047339537037, 'weight_decay': 0.006711544799109024}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:03:17,042] Trial 18 finished with value: 0.34016873439153034 and parameters: {'hidden_dim': 63, 'num_layers': 2, 'dropout': 0.24609635696375043, 'lr': 0.002772249710268143, 'weight_decay': 0.000378306943505235}. Best is trial 2 with value: 0.291768158475558.\n",
      "[I 2025-04-22 16:03:24,558] Trial 19 finished with value: 0.33269455035527545 and parameters: {'hidden_dim': 99, 'num_layers': 1, 'dropout': 0.4158708724898662, 'lr': 0.0048925557116251146, 'weight_decay': 0.0028896034629731023}. Best is trial 2 with value: 0.291768158475558.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 109, 'num_layers': 1, 'dropout': 0.2704783447659399, 'lr': 0.005578652852973335, 'weight_decay': 0.0020777440609733447}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# Updated BiLSTM with flatten layer as previously defined\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-BiLSTM_Flatten-tb/BiLSTM_Flatten_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "    max_seq_len = 100  # fixed for now; match your padding/truncation\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage (uncomment and run in your local environment):\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_dim': 42,\n",
       " 'num_layers': 3,\n",
       " 'dropout': 0.10155439630141562,\n",
       " 'lr': 0.0002623070179740925,\n",
       " 'weight_decay': 0.007688399807285689}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# study.best_trial.params['num_layers'] = 2\n",
    "# study.best_trial.params['dropout'] \n",
    "# study.best_trial.params['lr'] \n",
    "# study.best_trial.params['weight_decay'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[37 12]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.7551\n",
      "Epoch [1/20] - Train Loss: 0.7507, Val Loss: 0.6472, Val Acc: 0.7667, Val AUC: 0.8656\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  9]\n",
      " [14 27]]\n",
      "Sensitivity: 0.6585, Specificity: 0.8163\n",
      "Epoch [2/20] - Train Loss: 0.5640, Val Loss: 0.5302, Val Acc: 0.7444, Val AUC: 0.7785\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  2]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.9592\n",
      "Epoch [3/20] - Train Loss: 0.3739, Val Loss: 0.4188, Val Acc: 0.8444, Val AUC: 0.9029\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 12]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.7551\n",
      "Epoch [4/20] - Train Loss: 0.2965, Val Loss: 0.4059, Val Acc: 0.7556, Val AUC: 0.8915\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8776\n",
      "Epoch [5/20] - Train Loss: 0.2596, Val Loss: 0.3335, Val Acc: 0.8333, Val AUC: 0.9243\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  3]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.9388\n",
      "Epoch [6/20] - Train Loss: 0.1940, Val Loss: 0.3313, Val Acc: 0.8444, Val AUC: 0.9413\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  4]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.9184\n",
      "Epoch [7/20] - Train Loss: 0.1736, Val Loss: 0.3049, Val Acc: 0.8667, Val AUC: 0.9393\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8980\n",
      "Epoch [8/20] - Train Loss: 0.1474, Val Loss: 0.3099, Val Acc: 0.8778, Val AUC: 0.9343\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8980\n",
      "Epoch [9/20] - Train Loss: 0.1290, Val Loss: 0.3364, Val Acc: 0.8778, Val AUC: 0.9253\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  3]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.9388\n",
      "Epoch [10/20] - Train Loss: 0.1347, Val Loss: 0.3209, Val Acc: 0.9000, Val AUC: 0.9423\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  4]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.9184\n",
      "Epoch [11/20] - Train Loss: 0.1270, Val Loss: 0.3694, Val Acc: 0.8333, Val AUC: 0.9303\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8980\n",
      "Epoch [12/20] - Train Loss: 0.1292, Val Loss: 0.3879, Val Acc: 0.8444, Val AUC: 0.9189\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  4]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.9184\n",
      "Epoch [13/20] - Train Loss: 0.2240, Val Loss: 0.3899, Val Acc: 0.8222, Val AUC: 0.9248\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  2]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.9592\n",
      "Epoch [14/20] - Train Loss: 0.1387, Val Loss: 0.3425, Val Acc: 0.8556, Val AUC: 0.9388\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8980\n",
      "Epoch [15/20] - Train Loss: 0.1464, Val Loss: 0.3313, Val Acc: 0.8778, Val AUC: 0.9268\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8980\n",
      "Epoch [16/20] - Train Loss: 0.1224, Val Loss: 0.3640, Val Acc: 0.8556, Val AUC: 0.9189\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  3]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.9388\n",
      "Epoch [17/20] - Train Loss: 0.1152, Val Loss: 0.3425, Val Acc: 0.8667, Val AUC: 0.9398\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  8]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.8367\n",
      "Epoch [18/20] - Train Loss: 0.1107, Val Loss: 0.3229, Val Acc: 0.8667, Val AUC: 0.9467\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  4]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.9184\n",
      "Epoch [19/20] - Train Loss: 0.0950, Val Loss: 0.3496, Val Acc: 0.8444, Val AUC: 0.9353\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.8571\n",
      "Epoch [20/20] - Train Loss: 0.0655, Val Loss: 0.3866, Val Acc: 0.8667, Val AUC: 0.9258\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.8776\n",
      "Test Loss: 0.4647, Test Accuracy: 0.8778, Test AUC: 0.9119\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model =BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=study.best_trial.params['hidden_dim'], num_layers=study.best_trial.params['num_layers'], dropout= study.best_trial.params['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=study.best_trial.params['lr'],\n",
    "                      weight_decay=study.best_trial.params['weight_decay'] , verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 16:51:59,450] A new study created in memory with name: no-name-6b16c334-51f9-4da5-877c-8f4914f47de4\n",
      "[I 2025-04-22 16:52:07,695] Trial 0 finished with value: 0.6133725245793661 and parameters: {'hidden_dim': 112, 'num_layers': 2, 'dropout': 0.16073049180550192, 'lr': 0.0017843971226956222, 'weight_decay': 6.293410587329562e-05}. Best is trial 0 with value: 0.6133725245793661.\n",
      "[I 2025-04-22 16:52:12,390] Trial 1 finished with value: 0.6217828392982483 and parameters: {'hidden_dim': 77, 'num_layers': 1, 'dropout': 0.48177477299465343, 'lr': 0.0015776360450987475, 'weight_decay': 0.00018952122846290054}. Best is trial 0 with value: 0.6133725245793661.\n",
      "[I 2025-04-22 16:52:18,581] Trial 2 finished with value: 0.542195200920105 and parameters: {'hidden_dim': 43, 'num_layers': 2, 'dropout': 0.23873664891675786, 'lr': 0.006872691844280665, 'weight_decay': 0.00019035387573207053}. Best is trial 2 with value: 0.542195200920105.\n",
      "[I 2025-04-22 16:52:25,239] Trial 3 finished with value: 0.6847502390543619 and parameters: {'hidden_dim': 57, 'num_layers': 2, 'dropout': 0.21278365816222733, 'lr': 0.0005189565522213781, 'weight_decay': 1.5159789624412853e-05}. Best is trial 2 with value: 0.542195200920105.\n",
      "[I 2025-04-22 16:52:31,360] Trial 4 finished with value: 0.6659947633743286 and parameters: {'hidden_dim': 80, 'num_layers': 2, 'dropout': 0.13302204706030651, 'lr': 0.0010742214979588185, 'weight_decay': 0.00015605803384597525}. Best is trial 2 with value: 0.542195200920105.\n",
      "[I 2025-04-22 16:52:35,601] Trial 5 finished with value: 0.5066727002461752 and parameters: {'hidden_dim': 65, 'num_layers': 1, 'dropout': 0.20597747541236902, 'lr': 0.001732621428173287, 'weight_decay': 1.1310695660001912e-06}. Best is trial 5 with value: 0.5066727002461752.\n",
      "[I 2025-04-22 16:52:44,276] Trial 6 finished with value: 0.49704669912656146 and parameters: {'hidden_dim': 58, 'num_layers': 3, 'dropout': 0.40207791519069536, 'lr': 0.004196627508039472, 'weight_decay': 5.546282441716361e-06}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:52:51,827] Trial 7 finished with value: 0.6887639959653219 and parameters: {'hidden_dim': 76, 'num_layers': 3, 'dropout': 0.26821609000593627, 'lr': 0.0006893022982793126, 'weight_decay': 0.0019977486486558826}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:53:00,980] Trial 8 finished with value: 0.6873261332511902 and parameters: {'hidden_dim': 106, 'num_layers': 3, 'dropout': 0.4615154045124604, 'lr': 0.0005721261183451237, 'weight_decay': 1.2764750756149956e-05}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:53:09,521] Trial 9 finished with value: 0.6374010642369589 and parameters: {'hidden_dim': 76, 'num_layers': 3, 'dropout': 0.42426074264846025, 'lr': 0.0006884833183967775, 'weight_decay': 3.691822432534157e-06}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:53:15,541] Trial 10 finished with value: 0.6887759367624918 and parameters: {'hidden_dim': 33, 'num_layers': 3, 'dropout': 0.3600003241351016, 'lr': 0.008669744990126401, 'weight_decay': 0.006702962350396952}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:53:19,369] Trial 11 finished with value: 0.6350713968276978 and parameters: {'hidden_dim': 56, 'num_layers': 1, 'dropout': 0.340475202505212, 'lr': 0.0034140720368712046, 'weight_decay': 1.344187620948303e-06}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:53:23,809] Trial 12 finished with value: 0.6919340292612711 and parameters: {'hidden_dim': 60, 'num_layers': 1, 'dropout': 0.37765893925112504, 'lr': 0.00014880942445285564, 'weight_decay': 1.4916582836216148e-06}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:53:28,207] Trial 13 finished with value: 0.529809832572937 and parameters: {'hidden_dim': 94, 'num_layers': 1, 'dropout': 0.28923210623957013, 'lr': 0.004111478033669814, 'weight_decay': 8.383637729861182e-06}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:53:35,477] Trial 14 finished with value: 0.6831155816713969 and parameters: {'hidden_dim': 127, 'num_layers': 2, 'dropout': 0.17748847976079735, 'lr': 0.0030075335546561814, 'weight_decay': 3.3086236910632715e-05}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:53:42,664] Trial 15 finished with value: 0.689665695031484 and parameters: {'hidden_dim': 63, 'num_layers': 3, 'dropout': 0.10263633891707964, 'lr': 0.00014109221221118168, 'weight_decay': 3.607134534250376e-06}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:53:46,275] Trial 16 finished with value: 0.6884350577990214 and parameters: {'hidden_dim': 43, 'num_layers': 1, 'dropout': 0.32146886935712776, 'lr': 0.0002846998440353511, 'weight_decay': 1.205753763196149e-06}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:53:52,466] Trial 17 finished with value: 0.6039043466250101 and parameters: {'hidden_dim': 93, 'num_layers': 2, 'dropout': 0.4056965392319087, 'lr': 0.0020760339409897433, 'weight_decay': 3.3493316021625413e-06}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:53:58,331] Trial 18 finished with value: 0.5148659149805704 and parameters: {'hidden_dim': 68, 'num_layers': 2, 'dropout': 0.218400062840784, 'lr': 0.0050694687496634526, 'weight_decay': 2.519413941983025e-05}. Best is trial 6 with value: 0.49704669912656146.\n",
      "[I 2025-04-22 16:54:02,877] Trial 19 finished with value: 0.6855231126149496 and parameters: {'hidden_dim': 48, 'num_layers': 1, 'dropout': 0.2777820866709946, 'lr': 0.0011657762382851075, 'weight_decay': 0.0008850824533989693}. Best is trial 6 with value: 0.49704669912656146.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 58, 'num_layers': 3, 'dropout': 0.40207791519069536, 'lr': 0.004196627508039472, 'weight_decay': 5.546282441716361e-06}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# LSTM with Attention classifier\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)  # shape: [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)  # shape: [batch, seq_len]\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)  # normalize\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # shape: [batch, hidden_dim]\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-lstm-attn-tb/LSTM_Attn_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-lstm_attention-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/20] - Train Loss: 0.6959, Val Loss: 0.6894, Val Acc: 0.5444, Val AUC: 0.6526\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/20] - Train Loss: 0.6951, Val Loss: 0.6899, Val Acc: 0.5444, Val AUC: 0.6939\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/20] - Train Loss: 0.6908, Val Loss: 0.6893, Val Acc: 0.5444, Val AUC: 0.7282\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/20] - Train Loss: 0.7011, Val Loss: 0.6870, Val Acc: 0.5444, Val AUC: 0.7835\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48  1]\n",
      " [28 13]]\n",
      "Sensitivity: 0.3171, Specificity: 0.9796\n",
      "Epoch [5/20] - Train Loss: 0.6799, Val Loss: 0.6436, Val Acc: 0.6778, Val AUC: 0.8044\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 15]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.6939\n",
      "Epoch [6/20] - Train Loss: 0.5872, Val Loss: 0.5558, Val Acc: 0.7556, Val AUC: 0.8099\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  2]\n",
      " [25 16]]\n",
      "Sensitivity: 0.3902, Specificity: 0.9592\n",
      "Epoch [7/20] - Train Loss: 0.6101, Val Loss: 0.5981, Val Acc: 0.7000, Val AUC: 0.8342\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  4]\n",
      " [20 21]]\n",
      "Sensitivity: 0.5122, Specificity: 0.9184\n",
      "Epoch [8/20] - Train Loss: 0.5082, Val Loss: 0.5596, Val Acc: 0.7333, Val AUC: 0.8397\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [17 24]]\n",
      "Sensitivity: 0.5854, Specificity: 0.8980\n",
      "Epoch [9/20] - Train Loss: 0.4886, Val Loss: 0.5201, Val Acc: 0.7556, Val AUC: 0.8407\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  5]\n",
      " [14 27]]\n",
      "Sensitivity: 0.6585, Specificity: 0.8980\n",
      "Epoch [10/20] - Train Loss: 0.3969, Val Loss: 0.5423, Val Acc: 0.7889, Val AUC: 0.8507\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  3]\n",
      " [15 26]]\n",
      "Sensitivity: 0.6341, Specificity: 0.9388\n",
      "Epoch [11/20] - Train Loss: 0.5067, Val Loss: 0.4431, Val Acc: 0.8000, Val AUC: 0.8741\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  3]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.9388\n",
      "Epoch [12/20] - Train Loss: 0.4304, Val Loss: 0.4170, Val Acc: 0.8222, Val AUC: 0.8890\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.8776\n",
      "Epoch [13/20] - Train Loss: 0.3975, Val Loss: 0.4731, Val Acc: 0.8111, Val AUC: 0.8731\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39 10]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.7959\n",
      "Epoch [14/20] - Train Loss: 0.4148, Val Loss: 0.4868, Val Acc: 0.8000, Val AUC: 0.8532\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  6]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.8776\n",
      "Epoch [15/20] - Train Loss: 0.3806, Val Loss: 0.4927, Val Acc: 0.7889, Val AUC: 0.8542\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  7]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.8571\n",
      "Epoch [16/20] - Train Loss: 0.4062, Val Loss: 0.4743, Val Acc: 0.8111, Val AUC: 0.8532\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 13]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.7347\n",
      "Epoch [17/20] - Train Loss: 0.4200, Val Loss: 0.5290, Val Acc: 0.7778, Val AUC: 0.8123\n",
      "\n",
      "Confusion Matrix:\n",
      "[[14 35]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.2857\n",
      "Epoch [18/20] - Train Loss: 0.6214, Val Loss: 0.6201, Val Acc: 0.6111, Val AUC: 0.7352\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [19/20] - Train Loss: 0.6360, Val Loss: 0.6648, Val Acc: 0.5444, Val AUC: 0.4928\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 7 42]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.1429\n",
      "Epoch [20/20] - Train Loss: 0.6340, Val Loss: 0.6590, Val Acc: 0.5333, Val AUC: 0.7999\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 8 41]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.1633\n",
      "Test Loss: 0.6662, Test Accuracy: 0.5333, Test AUC: 0.8151\n"
     ]
    }
   ],
   "source": [
    "model =LSTMWithAttentionClassifier(input_dim=20, hidden_dim=study.best_trial.params['hidden_dim'], num_layers=study.best_trial.params['num_layers'], dropout= study.best_trial.params['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=study.best_trial.params['lr'],\n",
    "                      weight_decay=study.best_trial.params['weight_decay'] , verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 17:12:36,810] A new study created in memory with name: no-name-e3231b0d-62cb-4a7c-b8ef-1640d016ba21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-22 17:12:43,032] Trial 0 finished with value: 0.6865692734718323 and parameters: {'hidden_dim': 113, 'num_layers': 1, 'dropout': 0.428203926017391, 'lr': 0.002726967991889536, 'weight_decay': 0.00212510670445846}. Best is trial 0 with value: 0.6865692734718323.\n",
      "[I 2025-04-22 17:12:48,181] Trial 1 finished with value: 0.6876096129417419 and parameters: {'hidden_dim': 84, 'num_layers': 1, 'dropout': 0.2264035884789055, 'lr': 0.008432492607871383, 'weight_decay': 0.0030354631762287776}. Best is trial 0 with value: 0.6865692734718323.\n",
      "[I 2025-04-22 17:12:57,195] Trial 2 finished with value: 0.687524418036143 and parameters: {'hidden_dim': 56, 'num_layers': 3, 'dropout': 0.1758903056839194, 'lr': 0.0005143867104184558, 'weight_decay': 3.14775052245622e-05}. Best is trial 0 with value: 0.6865692734718323.\n",
      "[I 2025-04-22 17:13:14,378] Trial 3 finished with value: 0.6887698570887247 and parameters: {'hidden_dim': 127, 'num_layers': 3, 'dropout': 0.41705211882405635, 'lr': 0.008893274691492009, 'weight_decay': 0.009786675578491427}. Best is trial 0 with value: 0.6865692734718323.\n",
      "[I 2025-04-22 17:13:22,448] Trial 4 finished with value: 0.6887510220209757 and parameters: {'hidden_dim': 77, 'num_layers': 2, 'dropout': 0.46465803349843815, 'lr': 0.008716646116572163, 'weight_decay': 0.003982321022305563}. Best is trial 0 with value: 0.6865692734718323.\n",
      "[I 2025-04-22 17:13:35,866] Trial 5 finished with value: 0.6888854503631592 and parameters: {'hidden_dim': 94, 'num_layers': 3, 'dropout': 0.24171646939908287, 'lr': 0.00799404737966276, 'weight_decay': 0.009800489147490958}. Best is trial 0 with value: 0.6865692734718323.\n",
      "[I 2025-04-22 17:13:46,440] Trial 6 finished with value: 0.6888696551322937 and parameters: {'hidden_dim': 101, 'num_layers': 2, 'dropout': 0.49188787818384005, 'lr': 0.0028496518090553916, 'weight_decay': 0.008412053717086825}. Best is trial 0 with value: 0.6865692734718323.\n",
      "[I 2025-04-22 17:13:51,656] Trial 7 finished with value: 0.6887338757514954 and parameters: {'hidden_dim': 87, 'num_layers': 1, 'dropout': 0.3320315596332971, 'lr': 0.000962533951737828, 'weight_decay': 0.006639865741870566}. Best is trial 0 with value: 0.6865692734718323.\n",
      "[I 2025-04-22 17:14:04,564] Trial 8 finished with value: 0.6887641747792562 and parameters: {'hidden_dim': 127, 'num_layers': 2, 'dropout': 0.12186578011024607, 'lr': 0.005650389041772219, 'weight_decay': 0.004032816551225118}. Best is trial 0 with value: 0.6865692734718323.\n",
      "[I 2025-04-22 17:14:14,565] Trial 9 finished with value: 0.6887580156326294 and parameters: {'hidden_dim': 59, 'num_layers': 3, 'dropout': 0.41608579959954384, 'lr': 0.0071785535467308184, 'weight_decay': 0.007370078025511227}. Best is trial 0 with value: 0.6865692734718323.\n",
      "[I 2025-04-22 17:14:18,302] Trial 10 finished with value: 0.6680151621500651 and parameters: {'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.3278117115480891, 'lr': 0.0035433232728716516, 'weight_decay': 0.001308134958077957}. Best is trial 10 with value: 0.6680151621500651.\n",
      "[I 2025-04-22 17:14:24,213] Trial 11 finished with value: 0.6799405415852865 and parameters: {'hidden_dim': 109, 'num_layers': 1, 'dropout': 0.34320875551856656, 'lr': 0.0034009604290525922, 'weight_decay': 0.0013657522040651919}. Best is trial 10 with value: 0.6680151621500651.\n",
      "[I 2025-04-22 17:14:27,751] Trial 12 finished with value: 0.5704735020796458 and parameters: {'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.33205198824671533, 'lr': 0.004001290339909132, 'weight_decay': 0.0003696884319197779}. Best is trial 12 with value: 0.5704735020796458.\n",
      "[I 2025-04-22 17:14:31,602] Trial 13 finished with value: 0.528934488693873 and parameters: {'hidden_dim': 38, 'num_layers': 1, 'dropout': 0.2865606205038835, 'lr': 0.004707902126110177, 'weight_decay': 2.536302348966473e-05}. Best is trial 13 with value: 0.528934488693873.\n",
      "[I 2025-04-22 17:14:35,475] Trial 14 finished with value: 0.6004924972852071 and parameters: {'hidden_dim': 32, 'num_layers': 1, 'dropout': 0.2693773147771402, 'lr': 0.005198151425583509, 'weight_decay': 2.1110433328905022e-05}. Best is trial 13 with value: 0.528934488693873.\n",
      "[I 2025-04-22 17:14:41,111] Trial 15 finished with value: 0.6887520949045817 and parameters: {'hidden_dim': 48, 'num_layers': 2, 'dropout': 0.28831452411426434, 'lr': 0.0063877960256324764, 'weight_decay': 0.0056104226920703675}. Best is trial 13 with value: 0.528934488693873.\n",
      "[I 2025-04-22 17:14:45,505] Trial 16 finished with value: 0.554818461338679 and parameters: {'hidden_dim': 45, 'num_layers': 1, 'dropout': 0.3744672934076368, 'lr': 0.00444370144566267, 'weight_decay': 0.001173450609249727}. Best is trial 13 with value: 0.528934488693873.\n",
      "[I 2025-04-22 17:14:51,720] Trial 17 finished with value: 0.6887276967366537 and parameters: {'hidden_dim': 45, 'num_layers': 2, 'dropout': 0.37621035068144076, 'lr': 0.009958633170656535, 'weight_decay': 0.0025335615899935945}. Best is trial 13 with value: 0.528934488693873.\n",
      "[I 2025-04-22 17:14:56,437] Trial 18 finished with value: 0.6830907464027405 and parameters: {'hidden_dim': 70, 'num_layers': 1, 'dropout': 0.20657289751088434, 'lr': 0.0020532527329788462, 'weight_decay': 0.0012045147865439696}. Best is trial 13 with value: 0.528934488693873.\n",
      "[I 2025-04-22 17:15:00,297] Trial 19 finished with value: 0.6877365907033285 and parameters: {'hidden_dim': 45, 'num_layers': 1, 'dropout': 0.36468555846095074, 'lr': 0.0046116503093564165, 'weight_decay': 0.003629491030036682}. Best is trial 13 with value: 0.528934488693873.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 38, 'num_layers': 1, 'dropout': 0.2865606205038835, 'lr': 0.004707902126110177, 'weight_decay': 2.536302348966473e-05}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# BiLSTM with Attention Classifier\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention_tb/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm_attention_tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning to TB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'R', 'G', 'X', 'H', 'W', 'N', 'S', 'E', 'Y', 'V', 'A', 'I', 'F', 'M', 'K', 'T', 'D', 'C', 'P', 'Q', 'L'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 422\n",
      "Validation: 141\n",
      "Test: 141\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
