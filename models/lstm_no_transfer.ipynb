{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env: new-ml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'W', 'X', 'D', 'L', 'T', 'G', 'H', 'F', 'I', 'M', 'N', 'V', 'Q', 'Y', 'C', 'R', 'E', 'K', 'A', 'S', 'P'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define One-Hot Encoding Function for DNA Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        length = len(seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        return one_hot_torch(seq, dtype=self.one_hot_dtype), torch.tensor(label, dtype=torch.float32), length\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def collate_and_pack(batch):\n",
    "    # batch = list of (tensor_seq, label, length)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "\n",
    "    # lengths as tensor\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # Sort by descending length (required by pack_padded_sequence)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    labels = torch.tensor([labels[i] for i in sorted_indices])\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    # Stack to shape: (batch_size, 20, seq_len) and transpose for LSTM input\n",
    "    # LSTM expects input of shape (seq_len, batch_size, features)\n",
    "    sequences = [seq.T for seq in sequences]  # Transpose each [20, L] to [L, 20]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)  # shape: [max_len, batch, features]\n",
    "\n",
    "    # Pack the sequence\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 422\n",
      "Validation: 141\n",
      "Test: 141\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "    # Set up TensorBoard writer\n",
    "    log_dir = f\"runs/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Run evaluation\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # handle corner cases\n",
    "\n",
    "    # Sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        # Print metrics\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "model = LSTMClassifier(hidden_dim=64)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling on general AMP data (bayesian optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-19 16:57:09,581] A new study created in memory with name: no-name-b1d7fc52-7a7f-472b-8489-699c95dac1c8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-19 16:57:41,869] Trial 0 finished with value: 0.3789378583431244 and parameters: {'hidden_dim': 37, 'num_layers': 2, 'dropout': 0.21188933208133742, 'lr': 0.004064902221357742, 'weight_decay': 0.0018312092309791343}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 16:58:28,804] Trial 1 finished with value: 0.6929503917694092 and parameters: {'hidden_dim': 88, 'num_layers': 3, 'dropout': 0.2381157359710827, 'lr': 0.008930901362473249, 'weight_decay': 0.005791885317966466}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 16:59:17,823] Trial 2 finished with value: 0.6926333546638489 and parameters: {'hidden_dim': 95, 'num_layers': 3, 'dropout': 0.3427746029829881, 'lr': 0.007119599848461331, 'weight_decay': 0.0014186846782358266}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 16:59:34,921] Trial 3 finished with value: 0.41613681316375734 and parameters: {'hidden_dim': 66, 'num_layers': 1, 'dropout': 0.4759049359176608, 'lr': 0.005758962072575368, 'weight_decay': 0.006132287798056343}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:00:11,322] Trial 4 finished with value: 0.693131399154663 and parameters: {'hidden_dim': 105, 'num_layers': 2, 'dropout': 0.4159593577692501, 'lr': 0.002840930636200921, 'weight_decay': 0.009577890549859034}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:00:27,541] Trial 5 finished with value: 0.40703978538513186 and parameters: {'hidden_dim': 44, 'num_layers': 1, 'dropout': 0.23469829274266563, 'lr': 0.002684842876420897, 'weight_decay': 0.0014051225265246593}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:00:45,950] Trial 6 finished with value: 0.4397534608840942 and parameters: {'hidden_dim': 96, 'num_layers': 1, 'dropout': 0.2974010713377253, 'lr': 0.00996649913312798, 'weight_decay': 0.006655186888480811}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:01:24,116] Trial 7 finished with value: 0.6929257273674011 and parameters: {'hidden_dim': 36, 'num_layers': 3, 'dropout': 0.20169013196243268, 'lr': 0.009073253223712446, 'weight_decay': 0.008360615498118621}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:01:51,741] Trial 8 finished with value: 0.4224193036556244 and parameters: {'hidden_dim': 33, 'num_layers': 2, 'dropout': 0.36064342302421615, 'lr': 0.0006158374041767947, 'weight_decay': 0.0006512489384653178}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:02:33,239] Trial 9 finished with value: 0.6921634912490845 and parameters: {'hidden_dim': 34, 'num_layers': 3, 'dropout': 0.3399022404188956, 'lr': 0.009391053651259085, 'weight_decay': 0.0011187018919844222}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:03:19,624] Trial 10 finished with value: 0.6920947074890137 and parameters: {'hidden_dim': 124, 'num_layers': 2, 'dropout': 0.11161332692828735, 'lr': 0.0035912515515826985, 'weight_decay': 0.003550258523524641}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:03:41,781] Trial 11 finished with value: 0.4021967887878418 and parameters: {'hidden_dim': 58, 'num_layers': 1, 'dropout': 0.18750379075314288, 'lr': 0.002554376371415748, 'weight_decay': 0.002779426752659718}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:04:03,808] Trial 12 finished with value: 0.6604269742965698 and parameters: {'hidden_dim': 61, 'num_layers': 1, 'dropout': 0.14512711592515798, 'lr': 0.0002966130818317105, 'weight_decay': 0.0034447238816506924}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:04:44,216] Trial 13 finished with value: 0.47009786367416384 and parameters: {'hidden_dim': 57, 'num_layers': 2, 'dropout': 0.18634047894677536, 'lr': 0.0044279960943369775, 'weight_decay': 0.0032113720900201065}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:05:06,874] Trial 14 finished with value: 0.4252236545085907 and parameters: {'hidden_dim': 72, 'num_layers': 1, 'dropout': 0.2733673332938486, 'lr': 0.0018869903147318472, 'weight_decay': 0.0026226077359346378}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:05:42,721] Trial 15 finished with value: 0.6917196750640869 and parameters: {'hidden_dim': 51, 'num_layers': 2, 'dropout': 0.16767352093263072, 'lr': 0.005973352544440995, 'weight_decay': 0.004564340131577238}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:06:04,761] Trial 16 finished with value: 0.38964992165565493 and parameters: {'hidden_dim': 78, 'num_layers': 1, 'dropout': 0.11387168739281306, 'lr': 0.004382389185560638, 'weight_decay': 0.0001681676715228795}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:06:43,224] Trial 17 finished with value: 0.39356863498687744 and parameters: {'hidden_dim': 78, 'num_layers': 2, 'dropout': 0.10206814685214033, 'lr': 0.004568919258754266, 'weight_decay': 0.00019830091843507931}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:07:08,165] Trial 18 finished with value: 0.40304614305496217 and parameters: {'hidden_dim': 117, 'num_layers': 1, 'dropout': 0.14483886756501627, 'lr': 0.006265009516941629, 'weight_decay': 0.0019524832836031367}. Best is trial 0 with value: 0.3789378583431244.\n",
      "[I 2025-04-19 17:07:50,964] Trial 19 finished with value: 0.42966631054878235 and parameters: {'hidden_dim': 83, 'num_layers': 2, 'dropout': 0.24109874057600592, 'lr': 0.007616972146556399, 'weight_decay': 0.00011999518903676749}. Best is trial 0 with value: 0.3789378583431244.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 37, 'num_layers': 2, 'dropout': 0.21188933208133742, 'lr': 0.004064902221357742, 'weight_decay': 0.0018312092309791343}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-tb/no_transf-AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_auc = train_model(model, train_loader, val_loader, num_epochs=20, lr=lr,\n",
    "                          weight_decay=weight_decay, verbose=False)\n",
    "    return val_auc\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[70  1]\n",
      " [46 24]]\n",
      "Sensitivity: 0.3429, Specificity: 0.9859\n",
      "Epoch [1/19] - Train Loss: 0.6962, Val Loss: 0.6717, Val Acc: 0.6667, Val AUC: 0.8833\n",
      "\n",
      "Confusion Matrix:\n",
      "[[68  3]\n",
      " [29 41]]\n",
      "Sensitivity: 0.5857, Specificity: 0.9577\n",
      "Epoch [2/19] - Train Loss: 0.6300, Val Loss: 0.5355, Val Acc: 0.7730, Val AUC: 0.8785\n",
      "\n",
      "Confusion Matrix:\n",
      "[[70  1]\n",
      " [41 29]]\n",
      "Sensitivity: 0.4143, Specificity: 0.9859\n",
      "Epoch [3/19] - Train Loss: 0.5745, Val Loss: 0.5658, Val Acc: 0.7021, Val AUC: 0.8807\n",
      "\n",
      "Confusion Matrix:\n",
      "[[63  8]\n",
      " [17 53]]\n",
      "Sensitivity: 0.7571, Specificity: 0.8873\n",
      "Epoch [4/19] - Train Loss: 0.5166, Val Loss: 0.4256, Val Acc: 0.8227, Val AUC: 0.8801\n",
      "\n",
      "Confusion Matrix:\n",
      "[[68  3]\n",
      " [27 43]]\n",
      "Sensitivity: 0.6143, Specificity: 0.9577\n",
      "Epoch [5/19] - Train Loss: 0.4795, Val Loss: 0.4617, Val Acc: 0.7872, Val AUC: 0.8646\n",
      "\n",
      "Confusion Matrix:\n",
      "[[66  5]\n",
      " [21 49]]\n",
      "Sensitivity: 0.7000, Specificity: 0.9296\n",
      "Epoch [6/19] - Train Loss: 0.5048, Val Loss: 0.4238, Val Acc: 0.8156, Val AUC: 0.8799\n",
      "\n",
      "Confusion Matrix:\n",
      "[[69  2]\n",
      " [26 44]]\n",
      "Sensitivity: 0.6286, Specificity: 0.9718\n",
      "Epoch [7/19] - Train Loss: 0.4822, Val Loss: 0.4355, Val Acc: 0.8014, Val AUC: 0.8702\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50 21]\n",
      " [15 55]]\n",
      "Sensitivity: 0.7857, Specificity: 0.7042\n",
      "Epoch [8/19] - Train Loss: 0.5452, Val Loss: 0.5668, Val Acc: 0.7447, Val AUC: 0.8276\n",
      "\n",
      "Confusion Matrix:\n",
      "[[56 15]\n",
      " [16 54]]\n",
      "Sensitivity: 0.7714, Specificity: 0.7887\n",
      "Epoch [9/19] - Train Loss: 0.5204, Val Loss: 0.4752, Val Acc: 0.7801, Val AUC: 0.8457\n",
      "\n",
      "Confusion Matrix:\n",
      "[[57 14]\n",
      " [20 50]]\n",
      "Sensitivity: 0.7143, Specificity: 0.8028\n",
      "Epoch [10/19] - Train Loss: 0.4325, Val Loss: 0.5066, Val Acc: 0.7589, Val AUC: 0.8481\n",
      "\n",
      "Confusion Matrix:\n",
      "[[57 14]\n",
      " [19 51]]\n",
      "Sensitivity: 0.7286, Specificity: 0.8028\n",
      "Epoch [11/19] - Train Loss: 0.4349, Val Loss: 0.5204, Val Acc: 0.7660, Val AUC: 0.8523\n",
      "\n",
      "Confusion Matrix:\n",
      "[[57 14]\n",
      " [19 51]]\n",
      "Sensitivity: 0.7286, Specificity: 0.8028\n",
      "Epoch [12/19] - Train Loss: 0.4055, Val Loss: 0.4918, Val Acc: 0.7660, Val AUC: 0.8559\n",
      "\n",
      "Confusion Matrix:\n",
      "[[54 17]\n",
      " [13 57]]\n",
      "Sensitivity: 0.8143, Specificity: 0.7606\n",
      "Epoch [13/19] - Train Loss: 0.4047, Val Loss: 0.4787, Val Acc: 0.7872, Val AUC: 0.8803\n",
      "\n",
      "Confusion Matrix:\n",
      "[[63  8]\n",
      " [20 50]]\n",
      "Sensitivity: 0.7143, Specificity: 0.8873\n",
      "Epoch [14/19] - Train Loss: 0.3706, Val Loss: 0.3964, Val Acc: 0.8014, Val AUC: 0.8634\n",
      "\n",
      "Confusion Matrix:\n",
      "[[58 13]\n",
      " [18 52]]\n",
      "Sensitivity: 0.7429, Specificity: 0.8169\n",
      "Epoch [15/19] - Train Loss: 0.4180, Val Loss: 0.4390, Val Acc: 0.7801, Val AUC: 0.8710\n",
      "\n",
      "Confusion Matrix:\n",
      "[[63  8]\n",
      " [21 49]]\n",
      "Sensitivity: 0.7000, Specificity: 0.8873\n",
      "Epoch [16/19] - Train Loss: 0.4420, Val Loss: 0.4157, Val Acc: 0.7943, Val AUC: 0.8795\n",
      "\n",
      "Confusion Matrix:\n",
      "[[61 10]\n",
      " [24 46]]\n",
      "Sensitivity: 0.6571, Specificity: 0.8592\n",
      "Epoch [17/19] - Train Loss: 0.4044, Val Loss: 0.4739, Val Acc: 0.7589, Val AUC: 0.8586\n",
      "\n",
      "Confusion Matrix:\n",
      "[[53 18]\n",
      " [12 58]]\n",
      "Sensitivity: 0.8286, Specificity: 0.7465\n",
      "Epoch [18/19] - Train Loss: 0.4647, Val Loss: 0.4376, Val Acc: 0.7872, Val AUC: 0.8724\n",
      "\n",
      "Confusion Matrix:\n",
      "[[60 11]\n",
      " [17 53]]\n",
      "Sensitivity: 0.7571, Specificity: 0.8451\n",
      "Epoch [19/19] - Train Loss: 0.3910, Val Loss: 0.3976, Val Acc: 0.8014, Val AUC: 0.8813\n",
      "\n",
      "Confusion Matrix:\n",
      "[[55 15]\n",
      " [29 42]]\n",
      "Sensitivity: 0.5915, Specificity: 0.7857\n",
      "Test Loss: 0.6121, Test Accuracy: 0.6879, Test AUC: 0.8219\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "                      weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "criterion = nn.BCELoss()    \n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# Updated BiLSTM with flatten layer as previously defined\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-BiLSTM_Flatten-tb/BiLSTM_Flatten_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "    max_seq_len = 100  # fixed for now; match your padding/truncation\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage (uncomment and run in your local environment):\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hidden_dim': 42,\n",
       " 'num_layers': 3,\n",
       " 'dropout': 0.10155439630141562,\n",
       " 'lr': 0.0002623070179740925,\n",
       " 'weight_decay': 0.007688399807285689}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "study.best_trial.params['num_layers'] = 2\n",
    "study.best_trial.params['dropout'] \n",
    "study.best_trial.params['lr'] \n",
    "study.best_trial.params['weight_decay'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 71]\n",
      " [ 0 70]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [1/20] - Train Loss: 0.6976, Val Loss: 0.6924, Val Acc: 0.4965, Val AUC: 0.6067\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33 38]\n",
      " [46 24]]\n",
      "Sensitivity: 0.3429, Specificity: 0.4648\n",
      "Epoch [2/20] - Train Loss: 0.6937, Val Loss: 0.6970, Val Acc: 0.4043, Val AUC: 0.4398\n",
      "\n",
      "Confusion Matrix:\n",
      "[[52 19]\n",
      " [54 16]]\n",
      "Sensitivity: 0.2286, Specificity: 0.7324\n",
      "Epoch [3/20] - Train Loss: 0.6949, Val Loss: 0.6959, Val Acc: 0.4823, Val AUC: 0.4584\n",
      "\n",
      "Confusion Matrix:\n",
      "[[71  0]\n",
      " [70  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/20] - Train Loss: 0.6956, Val Loss: 0.6947, Val Acc: 0.5035, Val AUC: 0.4716\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 71]\n",
      " [ 0 70]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [5/20] - Train Loss: 0.6932, Val Loss: 0.6964, Val Acc: 0.4965, Val AUC: 0.4734\n",
      "\n",
      "Confusion Matrix:\n",
      "[[13 58]\n",
      " [13 57]]\n",
      "Sensitivity: 0.8143, Specificity: 0.1831\n",
      "Epoch [6/20] - Train Loss: 0.6940, Val Loss: 0.6942, Val Acc: 0.4965, Val AUC: 0.4800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[22 49]\n",
      " [10 60]]\n",
      "Sensitivity: 0.8571, Specificity: 0.3099\n",
      "Epoch [7/20] - Train Loss: 0.6933, Val Loss: 0.6923, Val Acc: 0.5816, Val AUC: 0.6143\n",
      "\n",
      "Confusion Matrix:\n",
      "[[22 49]\n",
      " [10 60]]\n",
      "Sensitivity: 0.8571, Specificity: 0.3099\n",
      "Epoch [8/20] - Train Loss: 0.6928, Val Loss: 0.6907, Val Acc: 0.5816, Val AUC: 0.7018\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40 31]\n",
      " [24 46]]\n",
      "Sensitivity: 0.6571, Specificity: 0.5634\n",
      "Epoch [9/20] - Train Loss: 0.6920, Val Loss: 0.6855, Val Acc: 0.6099, Val AUC: 0.6950\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 71]\n",
      " [ 0 70]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [10/20] - Train Loss: 0.6940, Val Loss: 0.6882, Val Acc: 0.4965, Val AUC: 0.6952\n",
      "\n",
      "Confusion Matrix:\n",
      "[[22 49]\n",
      " [10 60]]\n",
      "Sensitivity: 0.8571, Specificity: 0.3099\n",
      "Epoch [11/20] - Train Loss: 0.6949, Val Loss: 0.6902, Val Acc: 0.5816, Val AUC: 0.6636\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 71]\n",
      " [ 0 70]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [12/20] - Train Loss: 0.6941, Val Loss: 0.7016, Val Acc: 0.4965, Val AUC: 0.4740\n",
      "\n",
      "Confusion Matrix:\n",
      "[[53 18]\n",
      " [56 14]]\n",
      "Sensitivity: 0.2000, Specificity: 0.7465\n",
      "Epoch [13/20] - Train Loss: 0.6944, Val Loss: 0.6968, Val Acc: 0.4752, Val AUC: 0.4755\n",
      "\n",
      "Confusion Matrix:\n",
      "[[26 45]\n",
      " [38 32]]\n",
      "Sensitivity: 0.4571, Specificity: 0.3662\n",
      "Epoch [14/20] - Train Loss: 0.6930, Val Loss: 0.6967, Val Acc: 0.4113, Val AUC: 0.4763\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 71]\n",
      " [ 0 70]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [15/20] - Train Loss: 0.6926, Val Loss: 0.6954, Val Acc: 0.4965, Val AUC: 0.5020\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48 23]\n",
      " [48 22]]\n",
      "Sensitivity: 0.3143, Specificity: 0.6761\n",
      "Epoch [16/20] - Train Loss: 0.6945, Val Loss: 0.6938, Val Acc: 0.4965, Val AUC: 0.5012\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 71]\n",
      " [ 0 70]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [17/20] - Train Loss: 0.6937, Val Loss: 0.6926, Val Acc: 0.4965, Val AUC: 0.6161\n",
      "\n",
      "Confusion Matrix:\n",
      "[[22 49]\n",
      " [10 60]]\n",
      "Sensitivity: 0.8571, Specificity: 0.3099\n",
      "Epoch [18/20] - Train Loss: 0.6926, Val Loss: 0.6927, Val Acc: 0.5816, Val AUC: 0.6168\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48 23]\n",
      " [48 22]]\n",
      "Sensitivity: 0.3143, Specificity: 0.6761\n",
      "Epoch [19/20] - Train Loss: 0.6945, Val Loss: 0.6951, Val Acc: 0.4965, Val AUC: 0.4995\n",
      "\n",
      "Confusion Matrix:\n",
      "[[31 40]\n",
      " [46 24]]\n",
      "Sensitivity: 0.3429, Specificity: 0.4366\n",
      "Epoch [20/20] - Train Loss: 0.6927, Val Loss: 0.6961, Val Acc: 0.3901, Val AUC: 0.4740\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 70]\n",
      " [ 0 71]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Test Loss: 0.6942, Test Accuracy: 0.5035, Test AUC: 0.5572\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model =BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=study.best_trial.params['hidden_dim'], num_layers=study.best_trial.params['num_layers'], dropout= study.best_trial.params['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=study.best_trial.params['lr'],\n",
    "                      weight_decay=study.best_trial.params['weight_decay'] , verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-20 10:09:12,426] A new study created in memory with name: no-name-bcfb46a5-e4f9-4d86-9318-250aab5e9067\n",
      "[I 2025-04-20 10:09:17,366] Trial 0 finished with value: 0.6912369847297668 and parameters: {'hidden_dim': 53, 'num_layers': 1, 'dropout': 0.13010686831969567, 'lr': 0.00044725525893274376, 'weight_decay': 0.0007876233675597268}. Best is trial 0 with value: 0.6912369847297668.\n",
      "[I 2025-04-20 10:09:26,140] Trial 1 finished with value: 0.6929696202278137 and parameters: {'hidden_dim': 106, 'num_layers': 2, 'dropout': 0.3369009991397751, 'lr': 0.002057802982362913, 'weight_decay': 0.009629923003955806}. Best is trial 0 with value: 0.6912369847297668.\n",
      "[I 2025-04-20 10:09:32,417] Trial 2 finished with value: 0.4803470253944397 and parameters: {'hidden_dim': 52, 'num_layers': 2, 'dropout': 0.4364083448948711, 'lr': 0.0021263742180869945, 'weight_decay': 0.00033128635508326897}. Best is trial 2 with value: 0.4803470253944397.\n",
      "[I 2025-04-20 10:09:39,498] Trial 3 finished with value: 0.6928992629051208 and parameters: {'hidden_dim': 96, 'num_layers': 2, 'dropout': 0.4245173541377817, 'lr': 0.003768319126106186, 'weight_decay': 0.005461918894079395}. Best is trial 2 with value: 0.4803470253944397.\n",
      "[I 2025-04-20 10:09:44,539] Trial 4 finished with value: 0.47925782203674316 and parameters: {'hidden_dim': 80, 'num_layers': 1, 'dropout': 0.49636049564604423, 'lr': 0.004201309662253526, 'weight_decay': 1.1718254559607489e-06}. Best is trial 4 with value: 0.47925782203674316.\n",
      "[I 2025-04-20 10:09:51,949] Trial 5 finished with value: 0.6928471446037292 and parameters: {'hidden_dim': 82, 'num_layers': 2, 'dropout': 0.20877303726519425, 'lr': 0.0019234652795083976, 'weight_decay': 0.0007473325095366166}. Best is trial 4 with value: 0.47925782203674316.\n",
      "[I 2025-04-20 10:09:56,283] Trial 6 finished with value: 0.491410094499588 and parameters: {'hidden_dim': 59, 'num_layers': 1, 'dropout': 0.4762438207126841, 'lr': 0.003113992673251983, 'weight_decay': 0.002090324851939597}. Best is trial 4 with value: 0.47925782203674316.\n",
      "[I 2025-04-20 10:10:00,506] Trial 7 finished with value: 0.6906549453735351 and parameters: {'hidden_dim': 59, 'num_layers': 1, 'dropout': 0.3744800685843954, 'lr': 0.0012676795025387498, 'weight_decay': 0.0024290641353576885}. Best is trial 4 with value: 0.47925782203674316.\n",
      "[I 2025-04-20 10:10:05,416] Trial 8 finished with value: 0.45561290979385377 and parameters: {'hidden_dim': 84, 'num_layers': 1, 'dropout': 0.4728553280201707, 'lr': 0.007580993762581957, 'weight_decay': 8.933302363723691e-05}. Best is trial 8 with value: 0.45561290979385377.\n",
      "[I 2025-04-20 10:10:10,332] Trial 9 finished with value: 0.6911420583724975 and parameters: {'hidden_dim': 91, 'num_layers': 1, 'dropout': 0.44310800205174006, 'lr': 0.00030841420355572275, 'weight_decay': 6.074704524976604e-05}. Best is trial 8 with value: 0.45561290979385377.\n",
      "[I 2025-04-20 10:10:21,495] Trial 10 finished with value: 0.5304336547851562 and parameters: {'hidden_dim': 128, 'num_layers': 3, 'dropout': 0.24582844437823984, 'lr': 0.008906637337771034, 'weight_decay': 2.859973523397256e-05}. Best is trial 8 with value: 0.45561290979385377.\n",
      "[I 2025-04-20 10:10:25,902] Trial 11 finished with value: 0.38065372705459594 and parameters: {'hidden_dim': 33, 'num_layers': 1, 'dropout': 0.49823163163720807, 'lr': 0.008983223512199831, 'weight_decay': 1.4979953189487456e-06}. Best is trial 11 with value: 0.38065372705459594.\n",
      "[I 2025-04-20 10:10:32,183] Trial 12 finished with value: 0.4479083776473999 and parameters: {'hidden_dim': 40, 'num_layers': 3, 'dropout': 0.3649251813999904, 'lr': 0.009835323486737846, 'weight_decay': 5.817313309615907e-06}. Best is trial 11 with value: 0.38065372705459594.\n",
      "[I 2025-04-20 10:10:38,460] Trial 13 finished with value: 0.6940607786178589 and parameters: {'hidden_dim': 40, 'num_layers': 3, 'dropout': 0.2976232013839753, 'lr': 0.00015041441294259914, 'weight_decay': 1.2793262051562956e-06}. Best is trial 11 with value: 0.38065372705459594.\n",
      "[I 2025-04-20 10:10:44,847] Trial 14 finished with value: 0.49998430609703065 and parameters: {'hidden_dim': 38, 'num_layers': 3, 'dropout': 0.38679705834948286, 'lr': 0.009818711578602316, 'weight_decay': 6.252219295651803e-06}. Best is trial 11 with value: 0.38065372705459594.\n",
      "[I 2025-04-20 10:10:51,057] Trial 15 finished with value: 0.6745824575424194 and parameters: {'hidden_dim': 33, 'num_layers': 3, 'dropout': 0.29105158967236594, 'lr': 0.0007156340676198519, 'weight_decay': 6.7063723949364e-06}. Best is trial 11 with value: 0.38065372705459594.\n",
      "[I 2025-04-20 10:10:57,718] Trial 16 finished with value: 0.5120675742626191 and parameters: {'hidden_dim': 68, 'num_layers': 2, 'dropout': 0.3661032626034156, 'lr': 0.0054162743705613, 'weight_decay': 5.720598820546857e-06}. Best is trial 11 with value: 0.38065372705459594.\n",
      "[I 2025-04-20 10:11:04,197] Trial 17 finished with value: 0.6305171132087708 and parameters: {'hidden_dim': 32, 'num_layers': 3, 'dropout': 0.17612743220653093, 'lr': 0.005310645551777113, 'weight_decay': 2.1899819494597668e-05}. Best is trial 11 with value: 0.38065372705459594.\n",
      "[I 2025-04-20 10:11:09,819] Trial 18 finished with value: 0.46235463619232176 and parameters: {'hidden_dim': 47, 'num_layers': 2, 'dropout': 0.3302532004506704, 'lr': 0.0010434440124658772, 'weight_decay': 3.5373154478821677e-06}. Best is trial 11 with value: 0.38065372705459594.\n",
      "[I 2025-04-20 10:11:17,719] Trial 19 finished with value: 0.6929426312446594 and parameters: {'hidden_dim': 69, 'num_layers': 3, 'dropout': 0.2476218204328989, 'lr': 0.00011812055309059772, 'weight_decay': 1.5944383255646425e-05}. Best is trial 11 with value: 0.38065372705459594.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 33, 'num_layers': 1, 'dropout': 0.49823163163720807, 'lr': 0.008983223512199831, 'weight_decay': 1.4979953189487456e-06}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# LSTM with Attention classifier\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)  # shape: [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)  # shape: [batch, seq_len]\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)  # normalize\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # shape: [batch, hidden_dim]\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-lstm-attn-tb/LSTM_Attn_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-lstm_attention-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 71]\n",
      " [ 0 70]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [1/20] - Train Loss: 0.6971, Val Loss: 0.6909, Val Acc: 0.4965, Val AUC: 0.8004\n",
      "\n",
      "Confusion Matrix:\n",
      "[[23 48]\n",
      " [ 7 63]]\n",
      "Sensitivity: 0.9000, Specificity: 0.3239\n",
      "Epoch [2/20] - Train Loss: 0.6908, Val Loss: 0.6852, Val Acc: 0.6099, Val AUC: 0.8046\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 34]\n",
      " [ 6 64]]\n",
      "Sensitivity: 0.9143, Specificity: 0.5211\n",
      "Epoch [3/20] - Train Loss: 0.6352, Val Loss: 0.5582, Val Acc: 0.7163, Val AUC: 0.8439\n",
      "\n",
      "Confusion Matrix:\n",
      "[[65  6]\n",
      " [17 53]]\n",
      "Sensitivity: 0.7571, Specificity: 0.9155\n",
      "Epoch [4/20] - Train Loss: 0.5200, Val Loss: 0.4664, Val Acc: 0.8369, Val AUC: 0.8559\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50 21]\n",
      " [13 57]]\n",
      "Sensitivity: 0.8143, Specificity: 0.7042\n",
      "Epoch [5/20] - Train Loss: 0.4669, Val Loss: 0.4958, Val Acc: 0.7589, Val AUC: 0.8437\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33 38]\n",
      " [ 5 65]]\n",
      "Sensitivity: 0.9286, Specificity: 0.4648\n",
      "Epoch [6/20] - Train Loss: 0.4593, Val Loss: 0.6002, Val Acc: 0.6950, Val AUC: 0.8370\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49 22]\n",
      " [13 57]]\n",
      "Sensitivity: 0.8143, Specificity: 0.6901\n",
      "Epoch [7/20] - Train Loss: 0.5492, Val Loss: 0.5528, Val Acc: 0.7518, Val AUC: 0.8300\n",
      "\n",
      "Confusion Matrix:\n",
      "[[49 22]\n",
      " [13 57]]\n",
      "Sensitivity: 0.8143, Specificity: 0.6901\n",
      "Epoch [8/20] - Train Loss: 0.4854, Val Loss: 0.5327, Val Acc: 0.7518, Val AUC: 0.8286\n",
      "\n",
      "Confusion Matrix:\n",
      "[[57 14]\n",
      " [16 54]]\n",
      "Sensitivity: 0.7714, Specificity: 0.8028\n",
      "Epoch [9/20] - Train Loss: 0.4470, Val Loss: 0.5025, Val Acc: 0.7872, Val AUC: 0.8406\n",
      "\n",
      "Confusion Matrix:\n",
      "[[54 17]\n",
      " [13 57]]\n",
      "Sensitivity: 0.8143, Specificity: 0.7606\n",
      "Epoch [10/20] - Train Loss: 0.4438, Val Loss: 0.5107, Val Acc: 0.7872, Val AUC: 0.8294\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50 21]\n",
      " [11 59]]\n",
      "Sensitivity: 0.8429, Specificity: 0.7042\n",
      "Epoch [11/20] - Train Loss: 0.4118, Val Loss: 0.5330, Val Acc: 0.7730, Val AUC: 0.8262\n",
      "\n",
      "Confusion Matrix:\n",
      "[[48 23]\n",
      " [10 60]]\n",
      "Sensitivity: 0.8571, Specificity: 0.6761\n",
      "Epoch [12/20] - Train Loss: 0.4312, Val Loss: 0.5449, Val Acc: 0.7660, Val AUC: 0.8241\n",
      "\n",
      "Confusion Matrix:\n",
      "[[51 20]\n",
      " [10 60]]\n",
      "Sensitivity: 0.8571, Specificity: 0.7183\n",
      "Epoch [13/20] - Train Loss: 0.4505, Val Loss: 0.5168, Val Acc: 0.7872, Val AUC: 0.8292\n",
      "\n",
      "Confusion Matrix:\n",
      "[[57 14]\n",
      " [16 54]]\n",
      "Sensitivity: 0.7714, Specificity: 0.8028\n",
      "Epoch [14/20] - Train Loss: 0.4069, Val Loss: 0.4768, Val Acc: 0.7872, Val AUC: 0.8449\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43 28]\n",
      " [ 9 61]]\n",
      "Sensitivity: 0.8714, Specificity: 0.6056\n",
      "Epoch [15/20] - Train Loss: 0.4038, Val Loss: 0.5506, Val Acc: 0.7376, Val AUC: 0.8461\n",
      "\n",
      "Confusion Matrix:\n",
      "[[53 18]\n",
      " [14 56]]\n",
      "Sensitivity: 0.8000, Specificity: 0.7465\n",
      "Epoch [16/20] - Train Loss: 0.4156, Val Loss: 0.4803, Val Acc: 0.7730, Val AUC: 0.8431\n",
      "\n",
      "Confusion Matrix:\n",
      "[[50 21]\n",
      " [ 9 61]]\n",
      "Sensitivity: 0.8714, Specificity: 0.7042\n",
      "Epoch [17/20] - Train Loss: 0.3673, Val Loss: 0.5247, Val Acc: 0.7872, Val AUC: 0.8374\n",
      "\n",
      "Confusion Matrix:\n",
      "[[52 19]\n",
      " [12 58]]\n",
      "Sensitivity: 0.8286, Specificity: 0.7324\n",
      "Epoch [18/20] - Train Loss: 0.3796, Val Loss: 0.4545, Val Acc: 0.7801, Val AUC: 0.8672\n",
      "\n",
      "Confusion Matrix:\n",
      "[[55 16]\n",
      " [11 59]]\n",
      "Sensitivity: 0.8429, Specificity: 0.7746\n",
      "Epoch [19/20] - Train Loss: 0.3630, Val Loss: 0.4738, Val Acc: 0.8085, Val AUC: 0.8652\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38 33]\n",
      " [ 6 64]]\n",
      "Sensitivity: 0.9143, Specificity: 0.5352\n",
      "Epoch [20/20] - Train Loss: 0.3506, Val Loss: 0.7177, Val Acc: 0.7234, Val AUC: 0.8364\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41 29]\n",
      " [10 61]]\n",
      "Sensitivity: 0.8592, Specificity: 0.5857\n",
      "Test Loss: 0.7760, Test Accuracy: 0.7234, Test AUC: 0.7913\n"
     ]
    }
   ],
   "source": [
    "model =LSTMWithAttentionClassifier(input_dim=20, hidden_dim=study.best_trial.params['hidden_dim'], num_layers=study.best_trial.params['num_layers'], dropout= study.best_trial.params['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=study.best_trial.params['lr'],\n",
    "                      weight_decay=study.best_trial.params['weight_decay'] , verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning to TB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'R', 'G', 'X', 'H', 'W', 'N', 'S', 'E', 'Y', 'V', 'A', 'I', 'F', 'M', 'K', 'T', 'D', 'C', 'P', 'Q', 'L'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 422\n",
      "Validation: 141\n",
      "Test: 141\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 14:14:00,051] A new study created in memory with name: no-name-a43f6fbc-3fbd-4d2d-bf56-b69a8804a812\n",
      "[I 2025-04-15 14:14:28,668] Trial 0 finished with value: 0.7588652482269503 and parameters: {'lr': 0.00277516130197162, 'weight_decay': 0.00022260917887110485, 'dropout': 0.4581709483021509}. Best is trial 0 with value: 0.7588652482269503.\n",
      "[I 2025-04-15 14:14:56,974] Trial 1 finished with value: 0.7659574468085106 and parameters: {'lr': 0.003723976300324295, 'weight_decay': 0.00026099537314924434, 'dropout': 0.3318712272269039}. Best is trial 1 with value: 0.7659574468085106.\n",
      "[I 2025-04-15 14:15:25,510] Trial 2 finished with value: 0.7446808510638298 and parameters: {'lr': 2.4046134343634348e-05, 'weight_decay': 1.1228762504119307e-06, 'dropout': 0.27547624436114104}. Best is trial 1 with value: 0.7659574468085106.\n",
      "[I 2025-04-15 14:15:53,515] Trial 3 finished with value: 0.7446808510638298 and parameters: {'lr': 7.750240894399979e-05, 'weight_decay': 1.2977778086658241e-05, 'dropout': 0.3237185436981318}. Best is trial 1 with value: 0.7659574468085106.\n",
      "[I 2025-04-15 14:16:21,569] Trial 4 finished with value: 0.7446808510638298 and parameters: {'lr': 2.635751563612766e-05, 'weight_decay': 2.846756847355514e-06, 'dropout': 0.42138070735688926}. Best is trial 1 with value: 0.7659574468085106.\n",
      "[I 2025-04-15 14:16:49,509] Trial 5 finished with value: 0.7446808510638298 and parameters: {'lr': 5.4293187469359335e-05, 'weight_decay': 1.8633861522624314e-05, 'dropout': 0.2676647249125371}. Best is trial 1 with value: 0.7659574468085106.\n",
      "[I 2025-04-15 14:17:18,597] Trial 6 finished with value: 0.7446808510638298 and parameters: {'lr': 0.0007901499802818936, 'weight_decay': 1.8016914011055435e-05, 'dropout': 0.3299543411947675}. Best is trial 1 with value: 0.7659574468085106.\n",
      "[I 2025-04-15 14:17:47,489] Trial 7 finished with value: 0.7446808510638298 and parameters: {'lr': 0.0009780488857193902, 'weight_decay': 2.767096306711357e-05, 'dropout': 0.38789616103935975}. Best is trial 1 with value: 0.7659574468085106.\n",
      "[I 2025-04-15 14:18:16,025] Trial 8 finished with value: 0.7446808510638298 and parameters: {'lr': 0.0003799707810823722, 'weight_decay': 0.0047255972031581064, 'dropout': 0.3612369507867206}. Best is trial 1 with value: 0.7659574468085106.\n",
      "[I 2025-04-15 14:18:44,975] Trial 9 finished with value: 0.7588652482269503 and parameters: {'lr': 0.008168520415922765, 'weight_decay': 0.0014466895554739688, 'dropout': 0.46165669229507433}. Best is trial 1 with value: 0.7659574468085106.\n",
      "[I 2025-04-15 14:19:13,153] Trial 10 finished with value: 0.7801418439716312 and parameters: {'lr': 0.008118092140416524, 'weight_decay': 0.0003394713705332309, 'dropout': 0.14990718786944318}. Best is trial 10 with value: 0.7801418439716312.\n",
      "[I 2025-04-15 14:19:42,729] Trial 11 finished with value: 0.7801418439716312 and parameters: {'lr': 0.00985404128082692, 'weight_decay': 0.00034859441683466794, 'dropout': 0.14060813123715538}. Best is trial 10 with value: 0.7801418439716312.\n",
      "[I 2025-04-15 14:20:12,278] Trial 12 finished with value: 0.7801418439716312 and parameters: {'lr': 0.008346102395294144, 'weight_decay': 0.0008492224654896646, 'dropout': 0.14039446989319046}. Best is trial 10 with value: 0.7801418439716312.\n",
      "[I 2025-04-15 14:20:41,046] Trial 13 finished with value: 0.75177304964539 and parameters: {'lr': 0.002640005056193605, 'weight_decay': 0.00010567859469358316, 'dropout': 0.1158488474275496}. Best is trial 10 with value: 0.7801418439716312.\n",
      "[I 2025-04-15 14:21:11,414] Trial 14 finished with value: 0.7446808510638298 and parameters: {'lr': 0.0014086099921029396, 'weight_decay': 0.001235853679791362, 'dropout': 0.19241601163029606}. Best is trial 10 with value: 0.7801418439716312.\n",
      "[I 2025-04-15 14:21:43,287] Trial 15 finished with value: 0.7446808510638298 and parameters: {'lr': 0.0001524134669365303, 'weight_decay': 0.008700820825316423, 'dropout': 0.1992430086564586}. Best is trial 10 with value: 0.7801418439716312.\n",
      "[I 2025-04-15 14:22:13,840] Trial 16 finished with value: 0.7730496453900709 and parameters: {'lr': 0.009744971857335066, 'weight_decay': 0.0004311480342630996, 'dropout': 0.18297914116089803}. Best is trial 10 with value: 0.7801418439716312.\n",
      "[I 2025-04-15 14:22:43,481] Trial 17 finished with value: 0.7446808510638298 and parameters: {'lr': 0.0003832683907320405, 'weight_decay': 9.360370733189694e-05, 'dropout': 0.240419290645831}. Best is trial 10 with value: 0.7801418439716312.\n",
      "[I 2025-04-15 14:23:12,956] Trial 18 finished with value: 0.7730496453900709 and parameters: {'lr': 0.00443575598102718, 'weight_decay': 6.770934957951129e-05, 'dropout': 0.10777793626263871}. Best is trial 10 with value: 0.7801418439716312.\n",
      "[I 2025-04-15 14:23:41,945] Trial 19 finished with value: 0.7446808510638298 and parameters: {'lr': 0.0012500107521604303, 'weight_decay': 0.003295940709522396, 'dropout': 0.15246670180877078}. Best is trial 10 with value: 0.7801418439716312.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.008118092140416524, 'weight_decay': 0.0003394713705332309, 'dropout': 0.14990718786944318}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation function with detailed output\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    # print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    # print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    # print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function for frozen encoder\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    log_dir = f\"runs/FrozenEncoder_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        # if val_auc > best_val_auc:\n",
    "        #     best_val_auc = val_auc\n",
    "        #     torch.save(model.state_dict(), 'best_model_frozen.pt')\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Load the best pretrained model and fine-tune\n",
    "def finetune_with_frozen_encoder(pretrained_model_path, train_loader, val_loader, hidden_dim, num_layers, dropout):\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    best_auc = train_finetune_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model_frozen.pt'))\n",
    "    evaluate_model(model, val_loader, nn.BCELoss())\n",
    "\n",
    "    return model, best_auc\n",
    "\n",
    "model, best_auc = finetune_with_frozen_encoder(\n",
    "    pretrained_model_path='best_model-lstm.pt',\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    hidden_dim=47,  # or from Optuna\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=dropout)\n",
    "    model.load_state_dict(torch.load('best_model-lstm.pt'))\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7589, AUC: 0.8310\n",
      "Sensitivity: 0.8143, Specificity: 0.7042\n",
      "Confusion Matrix:\n",
      "[[50 21]\n",
      " [13 57]]\n",
      "Epoch [1/19] - Train Loss: 0.6946, Val Loss: 0.6854, Val Acc: 0.7589, Val AUC: 0.8310\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8227, AUC: 0.8968\n",
      "Sensitivity: 0.7286, Specificity: 0.9155\n",
      "Confusion Matrix:\n",
      "[[65  6]\n",
      " [19 51]]\n",
      "Epoch [2/19] - Train Loss: 0.6371, Val Loss: 0.4688, Val Acc: 0.8227, Val AUC: 0.8968\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7943, AUC: 0.8708\n",
      "Sensitivity: 0.8286, Specificity: 0.7606\n",
      "Confusion Matrix:\n",
      "[[54 17]\n",
      " [12 58]]\n",
      "Epoch [3/19] - Train Loss: 0.5430, Val Loss: 0.5214, Val Acc: 0.7943, Val AUC: 0.8708\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7943, AUC: 0.8600\n",
      "Sensitivity: 0.8286, Specificity: 0.7606\n",
      "Confusion Matrix:\n",
      "[[54 17]\n",
      " [12 58]]\n",
      "Epoch [4/19] - Train Loss: 0.5321, Val Loss: 0.4383, Val Acc: 0.7943, Val AUC: 0.8600\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8156, AUC: 0.8922\n",
      "Sensitivity: 0.7429, Specificity: 0.8873\n",
      "Confusion Matrix:\n",
      "[[63  8]\n",
      " [18 52]]\n",
      "Epoch [5/19] - Train Loss: 0.5344, Val Loss: 0.4551, Val Acc: 0.8156, Val AUC: 0.8922\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8014, AUC: 0.8628\n",
      "Sensitivity: 0.8286, Specificity: 0.7746\n",
      "Confusion Matrix:\n",
      "[[55 16]\n",
      " [12 58]]\n",
      "Epoch [6/19] - Train Loss: 0.4956, Val Loss: 0.4543, Val Acc: 0.8014, Val AUC: 0.8628\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8227, AUC: 0.8813\n",
      "Sensitivity: 0.7571, Specificity: 0.8873\n",
      "Confusion Matrix:\n",
      "[[63  8]\n",
      " [17 53]]\n",
      "Epoch [7/19] - Train Loss: 0.5114, Val Loss: 0.4798, Val Acc: 0.8227, Val AUC: 0.8813\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8511, AUC: 0.8972\n",
      "Sensitivity: 0.8429, Specificity: 0.8592\n",
      "Confusion Matrix:\n",
      "[[61 10]\n",
      " [11 59]]\n",
      "Epoch [8/19] - Train Loss: 0.4392, Val Loss: 0.3758, Val Acc: 0.8511, Val AUC: 0.8972\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7660, AUC: 0.7958\n",
      "Sensitivity: 0.5286, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[71  0]\n",
      " [33 37]]\n",
      "Epoch [9/19] - Train Loss: 0.4561, Val Loss: 0.5474, Val Acc: 0.7660, Val AUC: 0.7958\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7163, AUC: 0.7658\n",
      "Sensitivity: 0.4286, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[71  0]\n",
      " [40 30]]\n",
      "Epoch [10/19] - Train Loss: 0.5718, Val Loss: 0.5507, Val Acc: 0.7163, Val AUC: 0.7658\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7518, AUC: 0.7690\n",
      "Sensitivity: 0.5000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[71  0]\n",
      " [35 35]]\n",
      "Epoch [11/19] - Train Loss: 0.5697, Val Loss: 0.5022, Val Acc: 0.7518, Val AUC: 0.7690\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8014, AUC: 0.8384\n",
      "Sensitivity: 0.6000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[71  0]\n",
      " [28 42]]\n",
      "Epoch [12/19] - Train Loss: 0.5177, Val Loss: 0.4598, Val Acc: 0.8014, Val AUC: 0.8384\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7943, AUC: 0.8666\n",
      "Sensitivity: 0.5857, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[71  0]\n",
      " [29 41]]\n",
      "Epoch [13/19] - Train Loss: 0.5220, Val Loss: 0.4819, Val Acc: 0.7943, Val AUC: 0.8666\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7872, AUC: 0.8499\n",
      "Sensitivity: 0.6000, Specificity: 0.9718\n",
      "Confusion Matrix:\n",
      "[[69  2]\n",
      " [28 42]]\n",
      "Epoch [14/19] - Train Loss: 0.5075, Val Loss: 0.4746, Val Acc: 0.7872, Val AUC: 0.8499\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8085, AUC: 0.8268\n",
      "Sensitivity: 0.6429, Specificity: 0.9718\n",
      "Confusion Matrix:\n",
      "[[69  2]\n",
      " [25 45]]\n",
      "Epoch [15/19] - Train Loss: 0.4825, Val Loss: 0.4664, Val Acc: 0.8085, Val AUC: 0.8268\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8014, AUC: 0.8227\n",
      "Sensitivity: 0.6571, Specificity: 0.9437\n",
      "Confusion Matrix:\n",
      "[[67  4]\n",
      " [24 46]]\n",
      "Epoch [16/19] - Train Loss: 0.4914, Val Loss: 0.4590, Val Acc: 0.8014, Val AUC: 0.8227\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8085, AUC: 0.8070\n",
      "Sensitivity: 0.6429, Specificity: 0.9718\n",
      "Confusion Matrix:\n",
      "[[69  2]\n",
      " [25 45]]\n",
      "Epoch [17/19] - Train Loss: 0.4588, Val Loss: 0.4510, Val Acc: 0.8085, Val AUC: 0.8070\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7943, AUC: 0.7901\n",
      "Sensitivity: 0.6286, Specificity: 0.9577\n",
      "Confusion Matrix:\n",
      "[[68  3]\n",
      " [26 44]]\n",
      "Epoch [18/19] - Train Loss: 0.4673, Val Loss: 0.4720, Val Acc: 0.7943, Val AUC: 0.7901\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7943, AUC: 0.7988\n",
      "Sensitivity: 0.6429, Specificity: 0.9437\n",
      "Confusion Matrix:\n",
      "[[67  4]\n",
      " [25 45]]\n",
      "Epoch [19/19] - Train Loss: 0.4637, Val Loss: 0.4909, Val Acc: 0.7943, Val AUC: 0.7988\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7234, AUC: 0.7891\n",
      "Sensitivity: 0.5775, Specificity: 0.8714\n",
      "Confusion Matrix:\n",
      "[[61  9]\n",
      " [30 41]]\n",
      "Test Loss: 0.6321, Test Accuracy: 0.7234, Test AUC: 0.7891\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.14990718786944318)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.008118092140416524,\n",
    "                      weight_decay=0.0003394713705332309, verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-15 14:38:24,960] A new study created in memory with name: no-name-a90db21b-dea6-40fa-8370-8609af61fbe2\n",
      "[I 2025-04-15 14:39:09,664] Trial 0 finished with value: 0.7446808510638298 and parameters: {'lr': 4.137516195940895e-05, 'weight_decay': 5.377753930801714e-06, 'dropout': 0.4295429443692579}. Best is trial 0 with value: 0.7446808510638298.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation function with detailed output\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    # print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    # print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    # print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function for frozen encoder\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    log_dir = f\"runs/fullbackprop_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        # if val_auc > best_val_auc:\n",
    "        #     best_val_auc = val_auc\n",
    "        #     torch.save(model.state_dict(), 'best_model_frozen.pt')\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Load the best pretrained model and fine-tune\n",
    "def finetune_with_frozen_encoder(pretrained_model_path, train_loader, val_loader, hidden_dim, num_layers, dropout):\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    model.load_state_dict(torch.load(pretrained_model_path))\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    best_auc = train_finetune_model(\n",
    "        model=model,\n",
    "        train_loader=train_loader,\n",
    "        val_loader=val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=1e-3,\n",
    "        weight_decay=1e-4\n",
    "    )\n",
    "\n",
    "    model.load_state_dict(torch.load('best_model_frozen.pt'))\n",
    "    evaluate_model(model, val_loader, nn.BCELoss())\n",
    "\n",
    "    return model, best_auc\n",
    "\n",
    "model, best_auc = finetune_with_frozen_encoder(\n",
    "    pretrained_model_path='best_model_lstm.pt',\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    hidden_dim=47,  # or from Optuna\n",
    "    num_layers=2,\n",
    "    dropout=0.3\n",
    ")\n",
    "\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=dropout)\n",
    "    model.load_state_dict(torch.load('best_model-lstm.pt'))\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.14990718786944318)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.008118092140416524,\n",
    "                      weight_decay=0.0003394713705332309, verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
