{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "#env: new-ml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'R', 'Q', 'Y', 'H', 'G', 'A', 'X', 'T', 'C', 'D', 'K', 'I', 'W', 'V', 'E', 'L', 'N', 'F', 'S', 'M', 'P'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define One-Hot Encoding Function for DNA Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        length = len(seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        return one_hot_torch(seq, dtype=self.one_hot_dtype), torch.tensor(label, dtype=torch.float32), length\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def collate_and_pack(batch):\n",
    "    # batch = list of (tensor_seq, label, length)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "\n",
    "    # lengths as tensor\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # Sort by descending length (required by pack_padded_sequence)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    labels = torch.tensor([labels[i] for i in sorted_indices])\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    # Stack to shape: (batch_size, 20, seq_len) and transpose for LSTM input\n",
    "    # LSTM expects input of shape (seq_len, batch_size, features)\n",
    "    sequences = [seq.T for seq in sequences]  # Transpose each [20, L] to [L, 20]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)  # shape: [max_len, batch, features]\n",
    "\n",
    "    # Pack the sequence\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 264\n",
      "Validation: 88\n",
      "Test: 88\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling on general AMP data (bayesian optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-04-23 14:13:14,541] A new study created in memory with name: no-name-8c17b7f3-9294-416a-aa55-ac8bb32a7fa7\n",
      "[I 2025-04-23 14:13:24,107] Trial 0 finished with value: 0.4524758656819661 and parameters: {'hidden_dim': 88, 'num_layers': 1, 'dropout': 0.3586810715959584, 'lr': 0.003979532212863667, 'weight_decay': 0.00400991889461858}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:13:30,718] Trial 1 finished with value: 0.5116890668869019 and parameters: {'hidden_dim': 102, 'num_layers': 2, 'dropout': 0.38003939667644493, 'lr': 0.006178534587837021, 'weight_decay': 0.002554148108188809}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:13:36,888] Trial 2 finished with value: 0.6911927858988444 and parameters: {'hidden_dim': 121, 'num_layers': 3, 'dropout': 0.14367723846663905, 'lr': 0.00355876699028647, 'weight_decay': 0.0034955287187644034}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:13:43,851] Trial 3 finished with value: 0.680489182472229 and parameters: {'hidden_dim': 34, 'num_layers': 2, 'dropout': 0.2019044278404985, 'lr': 0.0008397292872390457, 'weight_decay': 0.003067078610859056}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:13:50,481] Trial 4 finished with value: 0.4772167503833771 and parameters: {'hidden_dim': 100, 'num_layers': 2, 'dropout': 0.2797737323484838, 'lr': 0.007782523269944145, 'weight_decay': 0.0011310620731997895}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:13:56,599] Trial 5 finished with value: 0.6911928653717041 and parameters: {'hidden_dim': 74, 'num_layers': 3, 'dropout': 0.13497551209425596, 'lr': 0.007110583546565158, 'weight_decay': 0.0062557279759138835}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:14:01,364] Trial 6 finished with value: 0.4534739851951599 and parameters: {'hidden_dim': 43, 'num_layers': 1, 'dropout': 0.13535179763453997, 'lr': 0.005960930046703553, 'weight_decay': 0.004752994204404001}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:14:07,615] Trial 7 finished with value: 0.6914531787236532 and parameters: {'hidden_dim': 74, 'num_layers': 3, 'dropout': 0.20520298694401926, 'lr': 0.00031967215633861676, 'weight_decay': 0.005279517110230566}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:14:12,233] Trial 8 finished with value: 0.4755384822686513 and parameters: {'hidden_dim': 80, 'num_layers': 1, 'dropout': 0.12732587211169402, 'lr': 0.005225365139402217, 'weight_decay': 0.0016667862821118636}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:14:18,224] Trial 9 finished with value: 0.4584047297636668 and parameters: {'hidden_dim': 63, 'num_layers': 2, 'dropout': 0.2132141835823299, 'lr': 0.007325805627426618, 'weight_decay': 0.0010483185314951586}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:14:23,392] Trial 10 finished with value: 0.48473148544629413 and parameters: {'hidden_dim': 101, 'num_layers': 1, 'dropout': 0.487780072286762, 'lr': 0.00960569228940339, 'weight_decay': 0.009804526813723423}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:14:28,475] Trial 11 finished with value: 0.4887073238690694 and parameters: {'hidden_dim': 46, 'num_layers': 1, 'dropout': 0.3723550286573427, 'lr': 0.0032676891806348364, 'weight_decay': 0.007055130599125286}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:14:33,652] Trial 12 finished with value: 0.516128679116567 and parameters: {'hidden_dim': 55, 'num_layers': 1, 'dropout': 0.3371424947144512, 'lr': 0.0037966263252578597, 'weight_decay': 0.004332756642176608}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:14:39,305] Trial 13 finished with value: 0.5137079755465189 and parameters: {'hidden_dim': 91, 'num_layers': 1, 'dropout': 0.43399350352278, 'lr': 0.0020622161789452436, 'weight_decay': 0.007640823279078249}. Best is trial 0 with value: 0.4524758656819661.\n",
      "[I 2025-04-23 14:14:44,799] Trial 14 finished with value: 0.4434473117192586 and parameters: {'hidden_dim': 34, 'num_layers': 1, 'dropout': 0.28063682899085873, 'lr': 0.0047614892612208495, 'weight_decay': 0.004999772803334963}. Best is trial 14 with value: 0.4434473117192586.\n",
      "[I 2025-04-23 14:14:50,270] Trial 15 finished with value: 0.46641016006469727 and parameters: {'hidden_dim': 115, 'num_layers': 1, 'dropout': 0.28561817279527535, 'lr': 0.004377481419549259, 'weight_decay': 0.008709451497407554}. Best is trial 14 with value: 0.4434473117192586.\n",
      "[I 2025-04-23 14:14:56,715] Trial 16 finished with value: 0.6904857357343038 and parameters: {'hidden_dim': 63, 'num_layers': 2, 'dropout': 0.33189796417869455, 'lr': 0.0027526246870689056, 'weight_decay': 0.0059209416349454}. Best is trial 14 with value: 0.4434473117192586.\n",
      "[I 2025-04-23 14:15:05,710] Trial 17 finished with value: 0.48164331912994385 and parameters: {'hidden_dim': 89, 'num_layers': 1, 'dropout': 0.2582223062729881, 'lr': 0.0017452248677537092, 'weight_decay': 0.0034698420020771105}. Best is trial 14 with value: 0.4434473117192586.\n",
      "[I 2025-04-23 14:15:11,375] Trial 18 finished with value: 0.4566703935464223 and parameters: {'hidden_dim': 35, 'num_layers': 1, 'dropout': 0.42758425511099174, 'lr': 0.004720939260217813, 'weight_decay': 0.0042512117390276685}. Best is trial 14 with value: 0.4434473117192586.\n",
      "[I 2025-04-23 14:15:16,992] Trial 19 finished with value: 0.543809692064921 and parameters: {'hidden_dim': 63, 'num_layers': 2, 'dropout': 0.24454960761837324, 'lr': 0.008768249975927882, 'weight_decay': 0.00024430621258524163}. Best is trial 14 with value: 0.4434473117192586.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 34, 'num_layers': 1, 'dropout': 0.28063682899085873, 'lr': 0.0047614892612208495, 'weight_decay': 0.004999772803334963}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-tb/no_transf-AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_auc = train_model(model, train_loader, val_loader, num_epochs=20, lr=lr,\n",
    "                          weight_decay=weight_decay, verbose=False)\n",
    "    return val_auc\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/19] - Train Loss: 0.6899, Val Loss: 0.6878, Val Acc: 0.5341, Val AUC: 0.7255\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/19] - Train Loss: 0.6894, Val Loss: 0.6851, Val Acc: 0.5341, Val AUC: 0.7452\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [40  1]]\n",
      "Sensitivity: 0.0244, Specificity: 1.0000\n",
      "Epoch [3/19] - Train Loss: 0.6812, Val Loss: 0.6813, Val Acc: 0.5455, Val AUC: 0.7587\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [39  2]]\n",
      "Sensitivity: 0.0488, Specificity: 0.9787\n",
      "Epoch [4/19] - Train Loss: 0.6758, Val Loss: 0.6754, Val Acc: 0.5455, Val AUC: 0.7623\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [23 18]]\n",
      "Sensitivity: 0.4390, Specificity: 0.8936\n",
      "Epoch [5/19] - Train Loss: 0.6620, Val Loss: 0.6585, Val Acc: 0.6818, Val AUC: 0.7955\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.7021\n",
      "Epoch [6/19] - Train Loss: 0.6101, Val Loss: 0.5614, Val Acc: 0.7386, Val AUC: 0.7852\n",
      "\n",
      "Confusion Matrix:\n",
      "[[31 16]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.6596\n",
      "Epoch [7/19] - Train Loss: 0.5482, Val Loss: 0.5630, Val Acc: 0.7614, Val AUC: 0.8469\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [20 21]]\n",
      "Sensitivity: 0.5122, Specificity: 0.8723\n",
      "Epoch [8/19] - Train Loss: 0.5466, Val Loss: 0.5838, Val Acc: 0.7045, Val AUC: 0.8168\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.7021\n",
      "Epoch [9/19] - Train Loss: 0.4970, Val Loss: 0.4867, Val Acc: 0.7727, Val AUC: 0.8583\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32 15]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.6809\n",
      "Epoch [10/19] - Train Loss: 0.4782, Val Loss: 0.4745, Val Acc: 0.7955, Val AUC: 0.8796\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8298\n",
      "Epoch [11/19] - Train Loss: 0.4667, Val Loss: 0.5566, Val Acc: 0.7727, Val AUC: 0.8184\n",
      "\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.7021\n",
      "Epoch [12/19] - Train Loss: 0.4534, Val Loss: 0.4847, Val Acc: 0.7955, Val AUC: 0.8708\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [21 20]]\n",
      "Sensitivity: 0.4878, Specificity: 0.9362\n",
      "Epoch [13/19] - Train Loss: 0.5844, Val Loss: 0.5984, Val Acc: 0.7273, Val AUC: 0.8251\n",
      "\n",
      "Confusion Matrix:\n",
      "[[26 21]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.5532\n",
      "Epoch [14/19] - Train Loss: 0.5322, Val Loss: 0.5751, Val Acc: 0.7273, Val AUC: 0.8474\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.7872\n",
      "Epoch [15/19] - Train Loss: 0.5224, Val Loss: 0.5300, Val Acc: 0.7955, Val AUC: 0.8505\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8085\n",
      "Epoch [16/19] - Train Loss: 0.4635, Val Loss: 0.5251, Val Acc: 0.7955, Val AUC: 0.8480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.7660\n",
      "Epoch [17/19] - Train Loss: 0.4503, Val Loss: 0.4910, Val Acc: 0.7955, Val AUC: 0.8578\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.8298\n",
      "Epoch [18/19] - Train Loss: 0.5111, Val Loss: 0.5381, Val Acc: 0.7614, Val AUC: 0.8412\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32 15]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.6809\n",
      "Epoch [19/19] - Train Loss: 0.4835, Val Loss: 0.5077, Val Acc: 0.7727, Val AUC: 0.8723\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.7447\n",
      "Test Loss: 0.4865, Test Accuracy: 0.7841, Test AUC: 0.8620\n"
     ]
    }
   ],
   "source": [
    "model = LSTMClassifier(input_dim=20, hidden_dim=study.best_trial.params['hidden_dim'], num_layers=study.best_trial.params['num_layers'], dropout=study.best_trial.params['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=19, lr=study.best_trial.params['lr'],\n",
    "                      weight_decay=study.best_trial.params['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()    \n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:22:31,237] A new study created in memory with name: no-name-5f67f5a5-ce20-49bf-b055-50ab371f2e26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:22:34,036] Trial 0 finished with value: 0.690111517906189 and parameters: {'hidden_dim': 116, 'num_layers': 3, 'dropout': 0.25258307972346056, 'lr': 0.0005304142349666803, 'weight_decay': 0.004016797377099064}. Best is trial 0 with value: 0.690111517906189.\n",
      "[I 2025-04-23 14:22:36,138] Trial 1 finished with value: 0.6903796195983887 and parameters: {'hidden_dim': 80, 'num_layers': 2, 'dropout': 0.25044667656815445, 'lr': 0.0034451957793400294, 'weight_decay': 0.009807251896448557}. Best is trial 0 with value: 0.690111517906189.\n",
      "[I 2025-04-23 14:22:40,156] Trial 2 finished with value: 0.5397475759188334 and parameters: {'hidden_dim': 83, 'num_layers': 3, 'dropout': 0.48645137535023564, 'lr': 0.009021803521459539, 'weight_decay': 0.0016924911964907518}. Best is trial 2 with value: 0.5397475759188334.\n",
      "[I 2025-04-23 14:22:42,926] Trial 3 finished with value: 0.6893950502077738 and parameters: {'hidden_dim': 84, 'num_layers': 2, 'dropout': 0.3201875150088126, 'lr': 0.0007772363806509276, 'weight_decay': 0.0086257173674918}. Best is trial 2 with value: 0.5397475759188334.\n",
      "[I 2025-04-23 14:22:45,219] Trial 4 finished with value: 0.6911047101020813 and parameters: {'hidden_dim': 36, 'num_layers': 3, 'dropout': 0.15247893943931246, 'lr': 0.004898923007187439, 'weight_decay': 0.005928215784081679}. Best is trial 2 with value: 0.5397475759188334.\n",
      "[I 2025-04-23 14:22:47,156] Trial 5 finished with value: 0.26144447922706604 and parameters: {'hidden_dim': 65, 'num_layers': 1, 'dropout': 0.19951588353174318, 'lr': 0.007314348660175334, 'weight_decay': 0.003544637818042783}. Best is trial 5 with value: 0.26144447922706604.\n",
      "[I 2025-04-23 14:22:49,130] Trial 6 finished with value: 0.27650706966718036 and parameters: {'hidden_dim': 39, 'num_layers': 1, 'dropout': 0.4479876242680455, 'lr': 0.007581138476665763, 'weight_decay': 0.003052372776251381}. Best is trial 5 with value: 0.26144447922706604.\n",
      "[I 2025-04-23 14:22:51,481] Trial 7 finished with value: 0.3350559522708257 and parameters: {'hidden_dim': 115, 'num_layers': 3, 'dropout': 0.46433066974219983, 'lr': 0.00929372632776125, 'weight_decay': 0.0031947320167653097}. Best is trial 5 with value: 0.26144447922706604.\n",
      "[I 2025-04-23 14:22:53,658] Trial 8 finished with value: 0.24960354963938394 and parameters: {'hidden_dim': 107, 'num_layers': 1, 'dropout': 0.19806521222540285, 'lr': 0.009123017511848838, 'weight_decay': 0.0037280585848603427}. Best is trial 8 with value: 0.24960354963938394.\n",
      "[I 2025-04-23 14:22:55,982] Trial 9 finished with value: 0.4020088315010071 and parameters: {'hidden_dim': 70, 'num_layers': 2, 'dropout': 0.1470606381195214, 'lr': 0.0016119141331610797, 'weight_decay': 0.0023368363004964776}. Best is trial 8 with value: 0.24960354963938394.\n",
      "[I 2025-04-23 14:22:57,937] Trial 10 finished with value: 0.25889847179253894 and parameters: {'hidden_dim': 98, 'num_layers': 1, 'dropout': 0.3452655960081562, 'lr': 0.006436076767923719, 'weight_decay': 0.005852258420514881}. Best is trial 8 with value: 0.24960354963938394.\n",
      "[I 2025-04-23 14:22:59,859] Trial 11 finished with value: 0.3179491460323334 and parameters: {'hidden_dim': 102, 'num_layers': 1, 'dropout': 0.3556766735982576, 'lr': 0.006584495922753713, 'weight_decay': 0.006225124081337529}. Best is trial 8 with value: 0.24960354963938394.\n",
      "[I 2025-04-23 14:23:01,772] Trial 12 finished with value: 0.2995210587978363 and parameters: {'hidden_dim': 100, 'num_layers': 1, 'dropout': 0.3841977974962656, 'lr': 0.009999944705032335, 'weight_decay': 0.0005446037147124431}. Best is trial 8 with value: 0.24960354963938394.\n",
      "[I 2025-04-23 14:23:03,792] Trial 13 finished with value: 0.27910203735033673 and parameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.2509834748715018, 'lr': 0.005576739189033793, 'weight_decay': 0.005666584526129232}. Best is trial 8 with value: 0.24960354963938394.\n",
      "[I 2025-04-23 14:23:05,762] Trial 14 finished with value: 0.30094969272613525 and parameters: {'hidden_dim': 100, 'num_layers': 1, 'dropout': 0.10458423809233441, 'lr': 0.008389663672850413, 'weight_decay': 0.0074167466389213}. Best is trial 8 with value: 0.24960354963938394.\n",
      "[I 2025-04-23 14:23:07,992] Trial 15 finished with value: 0.39194435874621075 and parameters: {'hidden_dim': 112, 'num_layers': 2, 'dropout': 0.3993073342533242, 'lr': 0.004013666752568868, 'weight_decay': 0.0046461205061137665}. Best is trial 8 with value: 0.24960354963938394.\n",
      "[I 2025-04-23 14:23:09,914] Trial 16 finished with value: 0.2778936227162679 and parameters: {'hidden_dim': 94, 'num_layers': 1, 'dropout': 0.3036854706924983, 'lr': 0.0060107773779951475, 'weight_decay': 0.007094895062746865}. Best is trial 8 with value: 0.24960354963938394.\n",
      "[I 2025-04-23 14:23:12,205] Trial 17 finished with value: 0.3228999475638072 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.2066927819867644, 'lr': 0.007903730664952799, 'weight_decay': 0.004883194157567087}. Best is trial 8 with value: 0.24960354963938394.\n",
      "[I 2025-04-23 14:23:14,134] Trial 18 finished with value: 0.2682045375307401 and parameters: {'hidden_dim': 63, 'num_layers': 1, 'dropout': 0.329832608480479, 'lr': 0.006708407655759941, 'weight_decay': 7.463942044332646e-05}. Best is trial 8 with value: 0.24960354963938394.\n",
      "[I 2025-04-23 14:23:16,463] Trial 19 finished with value: 0.4659919540087382 and parameters: {'hidden_dim': 91, 'num_layers': 2, 'dropout': 0.20909969885419633, 'lr': 0.003142237695177031, 'weight_decay': 0.007137477407051307}. Best is trial 8 with value: 0.24960354963938394.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 107, 'num_layers': 1, 'dropout': 0.19806521222540285, 'lr': 0.009123017511848838, 'weight_decay': 0.0037280585848603427}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# Updated BiLSTM with flatten layer as previously defined\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-BiLSTM_Flatten-tb/BiLSTM_Flatten_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "    max_seq_len = 100  # fixed for now; match your padding/truncation\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage (uncomment and run in your local environment):\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study.best_trial.params['num_layers'] = 2\n",
    "# study.best_trial.params['dropout'] \n",
    "# study.best_trial.params['lr'] \n",
    "# study.best_trial.params['weight_decay'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [1/20] - Train Loss: 0.9870, Val Loss: 0.8510, Val Acc: 0.4659, Val AUC: 0.7172\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.8085\n",
      "Epoch [2/20] - Train Loss: 0.6916, Val Loss: 0.6120, Val Acc: 0.7500, Val AUC: 0.7883\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [18 23]]\n",
      "Sensitivity: 0.5610, Specificity: 0.9787\n",
      "Epoch [3/20] - Train Loss: 0.5393, Val Loss: 0.4954, Val Acc: 0.7841, Val AUC: 0.8651\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [14 27]]\n",
      "Sensitivity: 0.6585, Specificity: 0.8936\n",
      "Epoch [4/20] - Train Loss: 0.3899, Val Loss: 0.3919, Val Acc: 0.7841, Val AUC: 0.9097\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.9149\n",
      "Epoch [5/20] - Train Loss: 0.2944, Val Loss: 0.3338, Val Acc: 0.8409, Val AUC: 0.9336\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8936\n",
      "Epoch [6/20] - Train Loss: 0.2458, Val Loss: 0.2900, Val Acc: 0.8636, Val AUC: 0.9533\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.7872\n",
      "Epoch [7/20] - Train Loss: 0.2030, Val Loss: 0.3179, Val Acc: 0.8523, Val AUC: 0.9543\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.8298\n",
      "Epoch [8/20] - Train Loss: 0.2308, Val Loss: 0.3027, Val Acc: 0.8523, Val AUC: 0.9491\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.7660\n",
      "Epoch [9/20] - Train Loss: 0.2535, Val Loss: 0.3382, Val Acc: 0.8409, Val AUC: 0.9491\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [14 27]]\n",
      "Sensitivity: 0.6585, Specificity: 0.9362\n",
      "Epoch [10/20] - Train Loss: 0.2501, Val Loss: 0.3860, Val Acc: 0.8068, Val AUC: 0.9351\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.9149\n",
      "Epoch [11/20] - Train Loss: 0.2108, Val Loss: 0.3008, Val Acc: 0.8636, Val AUC: 0.9517\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.7872\n",
      "Epoch [12/20] - Train Loss: 0.1941, Val Loss: 0.3409, Val Acc: 0.8409, Val AUC: 0.9440\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.9362\n",
      "Epoch [13/20] - Train Loss: 0.1881, Val Loss: 0.3175, Val Acc: 0.8750, Val AUC: 0.9465\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.9149\n",
      "Epoch [14/20] - Train Loss: 0.1551, Val Loss: 0.2929, Val Acc: 0.9091, Val AUC: 0.9465\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.8723\n",
      "Epoch [15/20] - Train Loss: 0.1835, Val Loss: 0.3252, Val Acc: 0.8750, Val AUC: 0.9367\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.8511\n",
      "Epoch [16/20] - Train Loss: 0.1527, Val Loss: 0.3053, Val Acc: 0.8636, Val AUC: 0.9465\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.9149\n",
      "Epoch [17/20] - Train Loss: 0.1415, Val Loss: 0.2841, Val Acc: 0.8977, Val AUC: 0.9523\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.9362\n",
      "Epoch [18/20] - Train Loss: 0.1366, Val Loss: 0.3142, Val Acc: 0.8750, Val AUC: 0.9497\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Epoch [19/20] - Train Loss: 0.1235, Val Loss: 0.3084, Val Acc: 0.8977, Val AUC: 0.9642\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.7872\n",
      "Epoch [20/20] - Train Loss: 0.1404, Val Loss: 0.4101, Val Acc: 0.8636, Val AUC: 0.9222\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8723\n",
      "Test Loss: 0.3197, Test Accuracy: 0.8523, Test AUC: 0.9445\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model =BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=study.best_trial.params['hidden_dim'], num_layers=study.best_trial.params['num_layers'], dropout= study.best_trial.params['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=study.best_trial.params['lr'],\n",
    "                      weight_decay=study.best_trial.params['weight_decay'] , verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:24:23,948] A new study created in memory with name: no-name-64e70f0c-b5d4-4022-8ec7-b5d1233ea505\n",
      "[I 2025-04-23 14:24:26,168] Trial 0 finished with value: 0.6759146054585775 and parameters: {'hidden_dim': 73, 'num_layers': 1, 'dropout': 0.3681145283752768, 'lr': 0.0018186323984398377, 'weight_decay': 2.472032335096137e-05}. Best is trial 0 with value: 0.6759146054585775.\n",
      "[I 2025-04-23 14:24:28,215] Trial 1 finished with value: 0.6441958944002787 and parameters: {'hidden_dim': 89, 'num_layers': 2, 'dropout': 0.1089237469376418, 'lr': 0.0047430791885970346, 'weight_decay': 1.5687789542981325e-05}. Best is trial 1 with value: 0.6441958944002787.\n",
      "[I 2025-04-23 14:24:30,291] Trial 2 finished with value: 0.6815311113993326 and parameters: {'hidden_dim': 34, 'num_layers': 3, 'dropout': 0.23348708970719845, 'lr': 0.0019527001334488183, 'weight_decay': 2.4257634685509786e-06}. Best is trial 1 with value: 0.6441958944002787.\n",
      "[I 2025-04-23 14:24:32,504] Trial 3 finished with value: 0.6912922064463297 and parameters: {'hidden_dim': 111, 'num_layers': 1, 'dropout': 0.1387123941468502, 'lr': 0.0002390845222194128, 'weight_decay': 0.0010425844615285517}. Best is trial 1 with value: 0.6441958944002787.\n",
      "[I 2025-04-23 14:24:34,618] Trial 4 finished with value: 0.6912027597427368 and parameters: {'hidden_dim': 90, 'num_layers': 3, 'dropout': 0.405186615460874, 'lr': 0.000904100084651555, 'weight_decay': 0.0017889504939259332}. Best is trial 1 with value: 0.6441958944002787.\n",
      "[I 2025-04-23 14:24:36,860] Trial 5 finished with value: 0.48085056742032367 and parameters: {'hidden_dim': 67, 'num_layers': 2, 'dropout': 0.24861977043678604, 'lr': 0.008570377188845762, 'weight_decay': 1.7799398175219582e-05}. Best is trial 5 with value: 0.48085056742032367.\n",
      "[I 2025-04-23 14:24:39,170] Trial 6 finished with value: 0.691197951634725 and parameters: {'hidden_dim': 107, 'num_layers': 3, 'dropout': 0.11839487406840701, 'lr': 0.00993267220996426, 'weight_decay': 0.0012001671642781382}. Best is trial 5 with value: 0.48085056742032367.\n",
      "[I 2025-04-23 14:24:41,406] Trial 7 finished with value: 0.686784029006958 and parameters: {'hidden_dim': 91, 'num_layers': 2, 'dropout': 0.48380460254849034, 'lr': 0.0029869200478881215, 'weight_decay': 4.42199106001813e-06}. Best is trial 5 with value: 0.48085056742032367.\n",
      "[I 2025-04-23 14:24:43,702] Trial 8 finished with value: 0.45700626571973163 and parameters: {'hidden_dim': 66, 'num_layers': 3, 'dropout': 0.4032543693594386, 'lr': 0.0025577297745675806, 'weight_decay': 2.855370672264669e-06}. Best is trial 8 with value: 0.45700626571973163.\n",
      "[I 2025-04-23 14:24:45,968] Trial 9 finished with value: 0.6911510427792867 and parameters: {'hidden_dim': 34, 'num_layers': 3, 'dropout': 0.27661255210859936, 'lr': 0.00036247565014218625, 'weight_decay': 2.364882740051309e-06}. Best is trial 8 with value: 0.45700626571973163.\n",
      "[I 2025-04-23 14:24:48,274] Trial 10 finished with value: 0.6911828716595968 and parameters: {'hidden_dim': 57, 'num_layers': 3, 'dropout': 0.49908817199563577, 'lr': 0.0005922518370250644, 'weight_decay': 0.00022494530155146928}. Best is trial 8 with value: 0.45700626571973163.\n",
      "[I 2025-04-23 14:24:50,528] Trial 11 finished with value: 0.49372004469235736 and parameters: {'hidden_dim': 61, 'num_layers': 2, 'dropout': 0.3421724036014463, 'lr': 0.008037390224333923, 'weight_decay': 3.062240226640756e-05}. Best is trial 8 with value: 0.45700626571973163.\n",
      "[I 2025-04-23 14:24:52,810] Trial 12 finished with value: 0.5112567146619161 and parameters: {'hidden_dim': 55, 'num_layers': 2, 'dropout': 0.21986762459362583, 'lr': 0.004467425771457703, 'weight_decay': 1.1080373713735084e-06}. Best is trial 8 with value: 0.45700626571973163.\n",
      "[I 2025-04-23 14:24:54,886] Trial 13 finished with value: 0.6863397359848022 and parameters: {'hidden_dim': 75, 'num_layers': 1, 'dropout': 0.4256814072711626, 'lr': 0.001437756721614918, 'weight_decay': 0.0001345193301733891}. Best is trial 8 with value: 0.45700626571973163.\n",
      "[I 2025-04-23 14:24:56,882] Trial 14 finished with value: 0.694766898949941 and parameters: {'hidden_dim': 47, 'num_layers': 2, 'dropout': 0.3027113572900106, 'lr': 0.00010301870751692727, 'weight_decay': 0.009571008335750215}. Best is trial 8 with value: 0.45700626571973163.\n",
      "[I 2025-04-23 14:24:58,879] Trial 15 finished with value: 0.5232426027456919 and parameters: {'hidden_dim': 69, 'num_layers': 2, 'dropout': 0.20310688666662075, 'lr': 0.004791386289137139, 'weight_decay': 8.675380190877194e-06}. Best is trial 8 with value: 0.45700626571973163.\n",
      "[I 2025-04-23 14:25:01,203] Trial 16 finished with value: 0.5921480456988016 and parameters: {'hidden_dim': 123, 'num_layers': 3, 'dropout': 0.3064644879521936, 'lr': 0.0029540535743319577, 'weight_decay': 3.20393498268276e-05}. Best is trial 8 with value: 0.45700626571973163.\n",
      "[I 2025-04-23 14:25:03,387] Trial 17 finished with value: 0.6184086600939432 and parameters: {'hidden_dim': 45, 'num_layers': 1, 'dropout': 0.4338282479420108, 'lr': 0.006610377272848628, 'weight_decay': 8.944854122305991e-06}. Best is trial 8 with value: 0.45700626571973163.\n",
      "[I 2025-04-23 14:25:05,640] Trial 18 finished with value: 0.6414191524187723 and parameters: {'hidden_dim': 84, 'num_layers': 2, 'dropout': 0.1801853184778568, 'lr': 0.0029011261987675796, 'weight_decay': 6.286323424332048e-05}. Best is trial 8 with value: 0.45700626571973163.\n",
      "[I 2025-04-23 14:25:07,951] Trial 19 finished with value: 0.6667313575744629 and parameters: {'hidden_dim': 65, 'num_layers': 2, 'dropout': 0.25566922412911736, 'lr': 0.000979299560682997, 'weight_decay': 5.026560859981217e-06}. Best is trial 8 with value: 0.45700626571973163.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 66, 'num_layers': 3, 'dropout': 0.4032543693594386, 'lr': 0.0025577297745675806, 'weight_decay': 2.855370672264669e-06}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# LSTM with Attention classifier\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)  # shape: [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)  # shape: [batch, seq_len]\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)  # normalize\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # shape: [batch, hidden_dim]\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-lstm-attn-tb/LSTM_Attn_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-lstm_attention-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/20] - Train Loss: 0.6972, Val Loss: 0.6918, Val Acc: 0.5341, Val AUC: 0.5812\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/20] - Train Loss: 0.6922, Val Loss: 0.6914, Val Acc: 0.5341, Val AUC: 0.8124\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/20] - Train Loss: 0.6908, Val Loss: 0.6916, Val Acc: 0.5341, Val AUC: 0.7992\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/20] - Train Loss: 0.6903, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/20] - Train Loss: 0.6891, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.7706\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/20] - Train Loss: 0.6859, Val Loss: 0.6858, Val Acc: 0.5341, Val AUC: 0.7426\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/20] - Train Loss: 0.6747, Val Loss: 0.6733, Val Acc: 0.5341, Val AUC: 0.7997\n",
      "\n",
      "Confusion Matrix:\n",
      "[[31 16]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.6596\n",
      "Epoch [8/20] - Train Loss: 0.6576, Val Loss: 0.5854, Val Acc: 0.7841, Val AUC: 0.8272\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 3 44]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0638\n",
      "Epoch [9/20] - Train Loss: 0.5858, Val Loss: 0.7098, Val Acc: 0.5000, Val AUC: 0.8801\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 1 46]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0213\n",
      "Epoch [10/20] - Train Loss: 0.7023, Val Loss: 0.6961, Val Acc: 0.4773, Val AUC: 0.8775\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.7234\n",
      "Epoch [11/20] - Train Loss: 0.6886, Val Loss: 0.6802, Val Acc: 0.7841, Val AUC: 0.8542\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [12/20] - Train Loss: 0.6587, Val Loss: 0.6135, Val Acc: 0.5341, Val AUC: 0.8142\n",
      "\n",
      "Confusion Matrix:\n",
      "[[31 16]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.6596\n",
      "Epoch [13/20] - Train Loss: 0.5843, Val Loss: 0.5179, Val Acc: 0.7727, Val AUC: 0.8625\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.8085\n",
      "Epoch [14/20] - Train Loss: 0.5487, Val Loss: 0.5793, Val Acc: 0.7841, Val AUC: 0.8677\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32 15]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.6809\n",
      "Epoch [15/20] - Train Loss: 0.5175, Val Loss: 0.4836, Val Acc: 0.7955, Val AUC: 0.8557\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8511\n",
      "Epoch [16/20] - Train Loss: 0.4795, Val Loss: 0.4357, Val Acc: 0.8409, Val AUC: 0.8827\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.7872\n",
      "Epoch [17/20] - Train Loss: 0.4819, Val Loss: 0.4223, Val Acc: 0.8295, Val AUC: 0.8910\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.8511\n",
      "Epoch [18/20] - Train Loss: 0.4543, Val Loss: 0.4918, Val Acc: 0.7955, Val AUC: 0.8843\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.7660\n",
      "Epoch [19/20] - Train Loss: 0.4798, Val Loss: 0.4628, Val Acc: 0.7841, Val AUC: 0.8931\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.8723\n",
      "Epoch [20/20] - Train Loss: 0.4446, Val Loss: 0.4532, Val Acc: 0.8182, Val AUC: 0.8957\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.9362\n",
      "Test Loss: 0.4099, Test Accuracy: 0.8636, Test AUC: 0.8967\n"
     ]
    }
   ],
   "source": [
    "model =LSTMWithAttentionClassifier(input_dim=20, hidden_dim=study.best_trial.params['hidden_dim'], num_layers=study.best_trial.params['num_layers'], dropout= study.best_trial.params['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=study.best_trial.params['lr'],\n",
    "                      weight_decay=study.best_trial.params['weight_decay'] , verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-23 14:54:20,189] A new study created in memory with name: no-name-f6689b0f-4da0-4a4f-b0c3-d6804aa6150f\n",
      "[I 2025-04-23 14:54:23,066] Trial 0 finished with value: 0.6912049452463785 and parameters: {'hidden_dim': 87, 'num_layers': 3, 'dropout': 0.3906205948895597, 'lr': 0.00652691514454541, 'weight_decay': 0.0025512145517473865}. Best is trial 0 with value: 0.6912049452463785.\n",
      "[I 2025-04-23 14:54:25,507] Trial 1 finished with value: 0.6901354193687439 and parameters: {'hidden_dim': 110, 'num_layers': 1, 'dropout': 0.3188667910594709, 'lr': 0.0038761277502797173, 'weight_decay': 0.001759483786807672}. Best is trial 1 with value: 0.6901354193687439.\n",
      "[I 2025-04-23 14:54:28,217] Trial 2 finished with value: 0.6913604537645975 and parameters: {'hidden_dim': 62, 'num_layers': 3, 'dropout': 0.2990792032015541, 'lr': 0.00419393554778037, 'weight_decay': 0.00033468116976440536}. Best is trial 1 with value: 0.6901354193687439.\n",
      "[I 2025-04-23 14:54:30,775] Trial 3 finished with value: 0.6912091771761576 and parameters: {'hidden_dim': 36, 'num_layers': 3, 'dropout': 0.4878634355619319, 'lr': 0.008429096287856097, 'weight_decay': 0.00814055555867691}. Best is trial 1 with value: 0.6901354193687439.\n",
      "[I 2025-04-23 14:54:33,579] Trial 4 finished with value: 0.691214402516683 and parameters: {'hidden_dim': 118, 'num_layers': 3, 'dropout': 0.45463858014614833, 'lr': 0.008305653890220888, 'weight_decay': 0.0019622254759618977}. Best is trial 1 with value: 0.6901354193687439.\n",
      "[I 2025-04-23 14:54:35,846] Trial 5 finished with value: 0.6909451285998026 and parameters: {'hidden_dim': 51, 'num_layers': 1, 'dropout': 0.4009181719429802, 'lr': 0.0026323118256960524, 'weight_decay': 0.00541436992198642}. Best is trial 1 with value: 0.6901354193687439.\n",
      "[I 2025-04-23 14:54:38,088] Trial 6 finished with value: 0.6908335487047831 and parameters: {'hidden_dim': 124, 'num_layers': 1, 'dropout': 0.35770229200715364, 'lr': 0.001129607724528289, 'weight_decay': 0.0040978380675475825}. Best is trial 1 with value: 0.6901354193687439.\n",
      "[I 2025-04-23 14:54:40,509] Trial 7 finished with value: 0.6911920309066772 and parameters: {'hidden_dim': 80, 'num_layers': 2, 'dropout': 0.48732934133360606, 'lr': 0.008315389267382413, 'weight_decay': 0.005830978233261477}. Best is trial 1 with value: 0.6901354193687439.\n",
      "[I 2025-04-23 14:54:42,774] Trial 8 finished with value: 0.6909638245900472 and parameters: {'hidden_dim': 87, 'num_layers': 1, 'dropout': 0.47900335298399965, 'lr': 0.002753383388226118, 'weight_decay': 0.006681219574730054}. Best is trial 1 with value: 0.6901354193687439.\n",
      "[I 2025-04-23 14:54:45,137] Trial 9 finished with value: 0.6911991039911906 and parameters: {'hidden_dim': 87, 'num_layers': 2, 'dropout': 0.14537677731933482, 'lr': 0.0061633072614422765, 'weight_decay': 0.00716673747265811}. Best is trial 1 with value: 0.6901354193687439.\n",
      "[I 2025-04-23 14:54:47,461] Trial 10 finished with value: 0.6910353104273478 and parameters: {'hidden_dim': 107, 'num_layers': 1, 'dropout': 0.22804175385138506, 'lr': 0.004404285695574805, 'weight_decay': 0.009703579755427627}. Best is trial 1 with value: 0.6901354193687439.\n",
      "[I 2025-04-23 14:54:49,791] Trial 11 finished with value: 0.6910184224446615 and parameters: {'hidden_dim': 123, 'num_layers': 1, 'dropout': 0.310023470184749, 'lr': 0.00034710393201698364, 'weight_decay': 0.003424372340307292}. Best is trial 1 with value: 0.6901354193687439.\n",
      "[I 2025-04-23 14:54:52,367] Trial 12 finished with value: 0.6912645498911539 and parameters: {'hidden_dim': 106, 'num_layers': 2, 'dropout': 0.3125955463859067, 'lr': 0.00027110326675307175, 'weight_decay': 0.004223359587264023}. Best is trial 1 with value: 0.6901354193687439.\n",
      "[I 2025-04-23 14:54:54,698] Trial 13 finished with value: 0.5761226018269857 and parameters: {'hidden_dim': 108, 'num_layers': 1, 'dropout': 0.3681666800683058, 'lr': 0.0018530603322363874, 'weight_decay': 0.0004851114714641879}. Best is trial 13 with value: 0.5761226018269857.\n",
      "[I 2025-04-23 14:55:00,388] Trial 14 finished with value: 0.6550329923629761 and parameters: {'hidden_dim': 106, 'num_layers': 1, 'dropout': 0.20649407684437232, 'lr': 0.0018711154468799814, 'weight_decay': 0.0002604533093296154}. Best is trial 13 with value: 0.5761226018269857.\n",
      "[I 2025-04-23 14:55:04,458] Trial 15 finished with value: 0.5140862464904785 and parameters: {'hidden_dim': 98, 'num_layers': 2, 'dropout': 0.20688912166464254, 'lr': 0.0019679070827060474, 'weight_decay': 0.0001894278483221083}. Best is trial 15 with value: 0.5140862464904785.\n",
      "[I 2025-04-23 14:55:06,938] Trial 16 finished with value: 0.6911113262176514 and parameters: {'hidden_dim': 97, 'num_layers': 2, 'dropout': 0.11269792563554334, 'lr': 0.002889024489154864, 'weight_decay': 0.0011128197541813913}. Best is trial 15 with value: 0.5140862464904785.\n",
      "[I 2025-04-23 14:55:09,378] Trial 17 finished with value: 0.6911930044492086 and parameters: {'hidden_dim': 66, 'num_layers': 2, 'dropout': 0.2416588418042511, 'lr': 0.005800618849237165, 'weight_decay': 0.002882951748490303}. Best is trial 15 with value: 0.5140862464904785.\n",
      "[I 2025-04-23 14:55:11,872] Trial 18 finished with value: 0.5112300117810568 and parameters: {'hidden_dim': 96, 'num_layers': 2, 'dropout': 0.17289227743392294, 'lr': 0.0015283514363898724, 'weight_decay': 6.738358289965483e-05}. Best is trial 18 with value: 0.5112300117810568.\n",
      "[I 2025-04-23 14:55:14,291] Trial 19 finished with value: 0.6911584536234537 and parameters: {'hidden_dim': 70, 'num_layers': 2, 'dropout': 0.18272628231557106, 'lr': 0.0014497197762663406, 'weight_decay': 0.0015171439392779105}. Best is trial 18 with value: 0.5112300117810568.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 96, 'num_layers': 2, 'dropout': 0.17289227743392294, 'lr': 0.0015283514363898724, 'weight_decay': 6.738358289965483e-05}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# BiLSTM with Attention Classifier\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention_tb/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm_attention_tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/20] - Train Loss: 0.7145, Val Loss: 0.6995, Val Acc: 0.5341, Val AUC: 0.5184\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/20] - Train Loss: 0.6968, Val Loss: 0.6917, Val Acc: 0.5341, Val AUC: 0.7307\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/20] - Train Loss: 0.6940, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.6777\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/20] - Train Loss: 0.6797, Val Loss: 0.7698, Val Acc: 0.5341, Val AUC: 0.7743\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [5/20] - Train Loss: 0.7287, Val Loss: 0.6805, Val Acc: 0.4659, Val AUC: 0.6295\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [28 13]]\n",
      "Sensitivity: 0.3171, Specificity: 0.9574\n",
      "Epoch [6/20] - Train Loss: 0.6770, Val Loss: 0.6243, Val Acc: 0.6591, Val AUC: 0.8075\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [23 18]]\n",
      "Sensitivity: 0.4390, Specificity: 0.9362\n",
      "Epoch [7/20] - Train Loss: 0.5984, Val Loss: 0.5891, Val Acc: 0.7045, Val AUC: 0.8085\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [17 24]]\n",
      "Sensitivity: 0.5854, Specificity: 0.7872\n",
      "Epoch [8/20] - Train Loss: 0.5984, Val Loss: 0.6469, Val Acc: 0.6932, Val AUC: 0.8012\n",
      "\n",
      "Confusion Matrix:\n",
      "[[28 19]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.5957\n",
      "Epoch [9/20] - Train Loss: 0.5495, Val Loss: 0.5481, Val Acc: 0.7500, Val AUC: 0.7981\n",
      "\n",
      "Confusion Matrix:\n",
      "[[28 19]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.5957\n",
      "Epoch [10/20] - Train Loss: 0.5771, Val Loss: 0.5386, Val Acc: 0.7500, Val AUC: 0.8064\n",
      "\n",
      "Confusion Matrix:\n",
      "[[28 19]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.5957\n",
      "Epoch [11/20] - Train Loss: 0.5292, Val Loss: 0.5395, Val Acc: 0.7500, Val AUC: 0.8054\n",
      "\n",
      "Confusion Matrix:\n",
      "[[29 18]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.6170\n",
      "Epoch [12/20] - Train Loss: 0.4941, Val Loss: 0.5411, Val Acc: 0.7614, Val AUC: 0.7966\n",
      "\n",
      "Confusion Matrix:\n",
      "[[32 15]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.6809\n",
      "Epoch [13/20] - Train Loss: 0.4759, Val Loss: 0.5345, Val Acc: 0.7614, Val AUC: 0.8015\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [10 31]]\n",
      "Sensitivity: 0.7561, Specificity: 0.7447\n",
      "Epoch [14/20] - Train Loss: 0.4827, Val Loss: 0.5494, Val Acc: 0.7500, Val AUC: 0.8023\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.7872\n",
      "Epoch [15/20] - Train Loss: 0.4524, Val Loss: 0.5682, Val Acc: 0.7386, Val AUC: 0.8018\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.7447\n",
      "Epoch [16/20] - Train Loss: 0.4612, Val Loss: 0.5419, Val Acc: 0.7614, Val AUC: 0.8106\n",
      "\n",
      "Confusion Matrix:\n",
      "[[30 17]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.6383\n",
      "Epoch [17/20] - Train Loss: 0.4905, Val Loss: 0.5571, Val Acc: 0.7500, Val AUC: 0.8012\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.7447\n",
      "Epoch [18/20] - Train Loss: 0.4525, Val Loss: 0.5497, Val Acc: 0.7614, Val AUC: 0.8023\n",
      "\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.7660\n",
      "Epoch [19/20] - Train Loss: 0.4144, Val Loss: 0.5696, Val Acc: 0.7500, Val AUC: 0.7966\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.7447\n",
      "Epoch [20/20] - Train Loss: 0.5097, Val Loss: 0.5979, Val Acc: 0.7614, Val AUC: 0.7971\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.8723\n",
      "Test Loss: 0.3699, Test Accuracy: 0.8750, Test AUC: 0.9263\n"
     ]
    }
   ],
   "source": [
    "model =LSTMWithAttentionClassifier(input_dim=20, hidden_dim=study.best_trial.params['hidden_dim'], num_layers=study.best_trial.params['num_layers'], dropout= study.best_trial.params['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=study.best_trial.params['lr'],\n",
    "                      weight_decay=study.best_trial.params['weight_decay'] , verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
