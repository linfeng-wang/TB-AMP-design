{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env: esm1\n",
    "import torch\n",
    "import esm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "# model.eval().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:01<00:00, 11.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ ESM-1b embeddings generated and saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load ESM-1b model and alphabet\n",
    "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval().cuda()  # Use GPU if available\n",
    "\n",
    "# Example: sequences to embed\n",
    "# Replace this with your DataFrame or sequence list\n",
    "# df = pd.read_csv(\"your_sequences.csv\")\n",
    "# sequences = df[\"Sequences\"].tolist()\n",
    "sequences = [str(record.seq) for record in SeqIO.parse(\"/mnt/storageG1/lwang/Projects/TB-AMP-design/models/generated_peptides.fasta\", \"fasta\")]\n",
    "\n",
    "# Prepare data: ESM expects list of (name, sequence) tuples\n",
    "data = [(f\"seq{i}\", seq) for i, seq in enumerate(sequences)]\n",
    "\n",
    "# Batch encode and extract embeddings\n",
    "all_embeddings = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in tqdm(range(0, len(data), 8)):  # small batches if memory constrained\n",
    "        batch_data = data[i:i+8]\n",
    "        labels, strs, tokens = batch_converter(batch_data)\n",
    "        tokens = tokens.cuda()\n",
    "\n",
    "        # Extract representations from the final layer\n",
    "        results = model(tokens, repr_layers=[33], return_contacts=False)\n",
    "        token_representations = results[\"representations\"][33]\n",
    "\n",
    "        for j, (_, seq) in enumerate(batch_data):\n",
    "            # Extract per-sequence embedding: mean of all token embeddings (excluding special tokens)\n",
    "            emb = token_representations[j, 1:len(seq)+1].mean(0)  # shape: (1280,)\n",
    "            all_embeddings.append(emb.cpu().numpy())\n",
    "\n",
    "# Convert to DataFrame\n",
    "embedding_df = pd.DataFrame(all_embeddings)\n",
    "embedding_df[\"sequence\"] = sequences\n",
    "\n",
    "# Save if needed\n",
    "# embedding_df.to_csv(\"esm1b_embeddings.csv\", index=False)\n",
    "\n",
    "print(\"✅ ESM-1b embeddings generated and saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import esm\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from Bio import SeqIO\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load ESM-1b model\n",
    "model, alphabet = esm.pretrained.esm1b_t33_650M_UR50S()\n",
    "batch_converter = alphabet.get_batch_converter()\n",
    "model.eval()\n",
    "\n",
    "def extract_esm_embeddings(sequence_list, label_prefix, batch_size=32):\n",
    "    data = [(f\"{label_prefix}{i}\", seq) for i, seq in enumerate(sequence_list)]\n",
    "    embeddings = []\n",
    "    labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(data), batch_size)):\n",
    "            batch_data = data[i:i+batch_size]\n",
    "            labels_batch, strs, tokens = batch_converter(batch_data)\n",
    "            results = model(tokens, repr_layers=[33], return_contacts=False)\n",
    "            token_representations = results[\"representations\"][33]\n",
    "\n",
    "            for j, (_, seq) in enumerate(batch_data):\n",
    "                emb = token_representations[j, 1:len(seq)+1].mean(0)\n",
    "                embeddings.append(emb.cpu().numpy())\n",
    "                labels.append(label_prefix)\n",
    "\n",
    "    return embeddings, labels\n",
    "\n",
    "# Load generated sequences\n",
    "generated_sequences = [str(record.seq) for record in SeqIO.parse(\n",
    "    \"/mnt/storageG1/lwang/Projects/TB-AMP-design/models/1745346340796_AMPCandidates0.7-pos.fa\", \"fasta\")]\n",
    "gen_embeddings_pos, gen_labels_pos = extract_esm_embeddings(generated_sequences, \"Generated-AMP\")\n",
    "\n",
    "generated_sequences = [str(record.seq) for record in SeqIO.parse(\n",
    "    \"/mnt/storageG1/lwang/Projects/TB-AMP-design/models/1745346340796_AMPCandidates0.7-neg.fa\", \"fasta\")]\n",
    "gen_embeddings_neg, gen_labels_neg = extract_esm_embeddings(generated_sequences, \"Generated-Non_AMP\")\n",
    "\n",
    "\n",
    "# Load reference AMP sequences\n",
    "ref_df = pd.read_csv(\"../data/all_seq702.csv\")\n",
    "df_filtered = ref_df\n",
    "\n",
    "def split_sequence(seq, chunk_size=20):\n",
    "    return [seq[i:i+chunk_size] for i in range(0, len(seq), chunk_size)]\n",
    "\n",
    "new_rows = []\n",
    "for _, row in df_filtered.iterrows():\n",
    "    seq = row['Sequences']\n",
    "    amp_label = row['AMP']\n",
    "    if len(seq) > 40:\n",
    "        for chunk in split_sequence(seq, 20):\n",
    "            new_rows.append({'Sequences': chunk, 'AMP': amp_label})\n",
    "    else:\n",
    "        new_rows.append({'Sequences': seq, 'AMP': amp_label})\n",
    "\n",
    "df_filtered = pd.DataFrame(new_rows)\n",
    "ref_df = df_filtered\n",
    "ref_df = ref_df[\n",
    "    (ref_df['Sequences'].str.len() >= 10) &\n",
    "    (ref_df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~ref_df['Sequences'].str.contains('X'))\n",
    "]\n",
    "ref_df = ref_df.loc[ref_df['AMP'] == 1, :]\n",
    "reference_sequences = ref_df[\"Sequences\"].tolist()\n",
    "ref_embeddings, ref_labels = extract_esm_embeddings(reference_sequences, \"TB-AMP\")\n",
    "\n",
    "################\n",
    "\n",
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "adam_df.columns = ['Peptide ID', 'Sequences']\n",
    "filtered_adam_df = adam_df[\n",
    "    (adam_df['Sequences'].str.len() >= 10) &\n",
    "    (adam_df['Sequences'].str.len() <= 40)\n",
    "].reset_index(drop=True)\n",
    "\n",
    "adam_df = filtered_adam_df[:1200]\n",
    "reference_sequences = adam_df[\"Sequences\"].tolist()\n",
    "general_embeddings, general_labels = extract_esm_embeddings(reference_sequences, \"General-AMP\")\n",
    "\n",
    "\n",
    "# Combine and reduce\n",
    "all_embeddings = gen_embeddings_pos + gen_embeddings_neg +ref_embeddings + general_embeddings\n",
    "all_labels = gen_labels_pos + gen_labels_neg +ref_labels + general_labels\n",
    "\n",
    "pca_model = PCA(n_components=50)\n",
    "pca_result = pca_model.fit_transform(all_embeddings)\n",
    "\n",
    "# Calculate how much variance is explained by the first 50 components\n",
    "explained_variance_50d = np.sum(pca_model.explained_variance_ratio_)\n",
    "print(f\"Explained variance by first 50 components: {explained_variance_50d:.2f}\")\n",
    "# Apply t-SNE on the PCA result\n",
    "tsne = TSNE(n_components=2, perplexity=30, n_iter=3000, random_state=42).fit_transform(pca_result)\n",
    "\n",
    "# Plot using matplotlib\n",
    "vis_df = pd.DataFrame(tsne, columns=[\"x\", \"y\"])\n",
    "vis_df[\"Source\"] = all_labels\n",
    "\n",
    "colors = {\n",
    "    \"Generated-AMP\": \"red\",\n",
    "    \"Generated-Non_AMP\": \"orange\",\n",
    "    \"TB-AMP\": \"cyan\",\n",
    "    \"General-AMP\": \"blue\"\n",
    "}\n",
    "plt.figure(figsize=(10, 8))\n",
    "for label in vis_df[\"Source\"].unique():\n",
    "    subset = vis_df[vis_df[\"Source\"] == label]\n",
    "    plt.scatter(subset[\"x\"], subset[\"y\"], label=label, alpha=0.5, s=60, c=colors[label])\n",
    "\n",
    "# plt.title(\"t-SNE of ESM-1b Embeddings: Generated vs Reference AMPs\")\n",
    "plt.xlabel(\"t-SNE 1\")\n",
    "plt.ylabel(\"t-SNE 2\")\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.9306798917264013)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explained_variance_50d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "esm1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
