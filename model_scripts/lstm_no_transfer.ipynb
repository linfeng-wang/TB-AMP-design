{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env: new-ml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "\n",
    "# Set seed for base Python random\n",
    "random.seed(42)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set seed for PyTorch (CPU and GPU)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)  # if using multi-GPU\n",
    "\n",
    "# Force deterministic behavior in PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "random.seed(42)\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'V', 'T', 'R', 'X', 'P', 'K', 'E', 'Y', 'A', 'H', 'D', 'G', 'I', 'F', 'L', 'W', 'C', 'S', 'M', 'N', 'Q'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define One-Hot Encoding Function for DNA Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        length = len(seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        return one_hot_torch(seq, dtype=self.one_hot_dtype), torch.tensor(label, dtype=torch.float32), length\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def collate_and_pack(batch):\n",
    "    # batch = list of (tensor_seq, label, length)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "\n",
    "    # lengths as tensor\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # Sort by descending length (required by pack_padded_sequence)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    labels = torch.tensor([labels[i] for i in sorted_indices])\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    # Stack to shape: (batch_size, 20, seq_len) and transpose for LSTM input\n",
    "    # LSTM expects input of shape (seq_len, batch_size, features)\n",
    "    sequences = [seq.T for seq in sequences]  # Transpose each [20, L] to [L, 20]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)  # shape: [max_len, batch, features]\n",
    "\n",
    "    # Pack the sequence\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 264\n",
      "Validation: 88\n",
      "Test: 88\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling on amp specific"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-04-30 18:00:26,274] A new study created in memory with name: no-name-8afcec48-8069-4e80-a865-eedd0a82527f\n",
      "[I 2025-04-30 18:00:41,332] Trial 0 finished with value: 0.46738434831301373 and parameters: {'hidden_dim': 69, 'num_layers': 1, 'dropout': 0.41587129117220656, 'lr': 0.00469461605906771, 'weight_decay': 0.0033085755767768526}. Best is trial 0 with value: 0.46738434831301373.\n",
      "[I 2025-04-30 18:00:52,949] Trial 1 finished with value: 0.5592302878697714 and parameters: {'hidden_dim': 117, 'num_layers': 3, 'dropout': 0.4171361618979218, 'lr': 0.006627064090695855, 'weight_decay': 0.0010422314612976223}. Best is trial 0 with value: 0.46738434831301373.\n",
      "[I 2025-04-30 18:01:02,337] Trial 2 finished with value: 0.48084478576978046 and parameters: {'hidden_dim': 56, 'num_layers': 2, 'dropout': 0.4880981301881634, 'lr': 0.008118934595678374, 'weight_decay': 0.0032770662788157118}. Best is trial 0 with value: 0.46738434831301373.\n",
      "[I 2025-04-30 18:01:10,073] Trial 3 finished with value: 0.44969918330510456 and parameters: {'hidden_dim': 111, 'num_layers': 1, 'dropout': 0.3365093588544167, 'lr': 0.006445609242616727, 'weight_decay': 0.0015987273336345103}. Best is trial 3 with value: 0.44969918330510456.\n",
      "[I 2025-04-30 18:01:21,254] Trial 4 finished with value: 0.6911927858988444 and parameters: {'hidden_dim': 38, 'num_layers': 3, 'dropout': 0.17123169999158927, 'lr': 0.005512583387412042, 'weight_decay': 0.0073378672157149915}. Best is trial 3 with value: 0.44969918330510456.\n",
      "[I 2025-04-30 18:01:30,475] Trial 5 finished with value: 0.5517056286334991 and parameters: {'hidden_dim': 58, 'num_layers': 2, 'dropout': 0.3061698840492205, 'lr': 0.0015475691205028932, 'weight_decay': 0.004945419018282146}. Best is trial 3 with value: 0.44969918330510456.\n",
      "[I 2025-04-30 18:01:38,467] Trial 6 finished with value: 0.4496956964333852 and parameters: {'hidden_dim': 82, 'num_layers': 1, 'dropout': 0.4075076161136183, 'lr': 0.005152083756021303, 'weight_decay': 0.004491333660213208}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:01:47,870] Trial 7 finished with value: 0.5316848556200663 and parameters: {'hidden_dim': 109, 'num_layers': 2, 'dropout': 0.46397911666799607, 'lr': 0.007202183438285882, 'weight_decay': 0.003192631316534498}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:01:57,068] Trial 8 finished with value: 0.48661552866299945 and parameters: {'hidden_dim': 58, 'num_layers': 2, 'dropout': 0.2856190556462551, 'lr': 0.0062750792914601575, 'weight_decay': 0.0011615961997724167}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:02:04,169] Trial 9 finished with value: 0.47229868173599243 and parameters: {'hidden_dim': 48, 'num_layers': 1, 'dropout': 0.24137418945932324, 'lr': 0.008598663207080736, 'weight_decay': 0.0002876216238619494}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:02:11,538] Trial 10 finished with value: 0.4943547248840332 and parameters: {'hidden_dim': 86, 'num_layers': 1, 'dropout': 0.11806337747424098, 'lr': 0.0031281509680665225, 'weight_decay': 0.009912017284833579}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:02:18,746] Trial 11 finished with value: 0.47625744342803955 and parameters: {'hidden_dim': 94, 'num_layers': 1, 'dropout': 0.35641339435672065, 'lr': 0.00386356924013574, 'weight_decay': 0.006015000712197549}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:02:26,188] Trial 12 finished with value: 0.49526869257291156 and parameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.3583205977688951, 'lr': 0.009978881634187238, 'weight_decay': 0.0025015420385893546}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:02:34,124] Trial 13 finished with value: 0.6864157915115356 and parameters: {'hidden_dim': 100, 'num_layers': 1, 'dropout': 0.3709127675692329, 'lr': 0.00015958755670805215, 'weight_decay': 0.004999838821479265}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:02:41,440] Trial 14 finished with value: 0.4521328608194987 and parameters: {'hidden_dim': 79, 'num_layers': 1, 'dropout': 0.24578562408072405, 'lr': 0.0032867741991559557, 'weight_decay': 0.007271006784315574}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:02:51,039] Trial 15 finished with value: 0.5163095196088155 and parameters: {'hidden_dim': 106, 'num_layers': 2, 'dropout': 0.41628877275298526, 'lr': 0.005254527555640594, 'weight_decay': 0.001815453697327414}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:02:58,322] Trial 16 finished with value: 0.48401610056559247 and parameters: {'hidden_dim': 76, 'num_layers': 1, 'dropout': 0.31314930976388433, 'lr': 0.008173734199190387, 'weight_decay': 0.004212915338449105}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:03:09,757] Trial 17 finished with value: 0.6911932627360026 and parameters: {'hidden_dim': 128, 'num_layers': 3, 'dropout': 0.4402428452799707, 'lr': 0.004349057316037281, 'weight_decay': 0.006523112267921136}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:03:17,202] Trial 18 finished with value: 0.49745328227678937 and parameters: {'hidden_dim': 90, 'num_layers': 1, 'dropout': 0.38160421969067143, 'lr': 0.001922492435072514, 'weight_decay': 0.008858298797426074}. Best is trial 6 with value: 0.4496956964333852.\n",
      "[I 2025-04-30 18:03:26,241] Trial 19 finished with value: 0.6907195051511129 and parameters: {'hidden_dim': 116, 'num_layers': 2, 'dropout': 0.2419962594911106, 'lr': 0.005794808318063604, 'weight_decay': 0.004384594080579919}. Best is trial 6 with value: 0.4496956964333852.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 82, 'num_layers': 1, 'dropout': 0.4075076161136183, 'lr': 0.005152083756021303, 'weight_decay': 0.004491333660213208}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-tb/no_transf-AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_auc = train_model(model, train_loader, val_loader, num_epochs=20, lr=lr,\n",
    "                          weight_decay=weight_decay, verbose=False)\n",
    "    return val_auc\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = {'hidden_dim': 82, 'num_layers': 1, 'dropout': 0.4075076161136183, 'lr': 0.005152083756021303, 'weight_decay': 0.004491333660213208}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/20] - Train Loss: 0.6919, Val Loss: 0.6831, Val Acc: 0.5341, Val AUC: 0.7462\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [36  5]]\n",
      "Sensitivity: 0.1220, Specificity: 0.9362\n",
      "Epoch [2/20] - Train Loss: 0.6796, Val Loss: 0.6743, Val Acc: 0.5568, Val AUC: 0.7732\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.8085\n",
      "Epoch [3/20] - Train Loss: 0.6682, Val Loss: 0.6502, Val Acc: 0.7159, Val AUC: 0.8023\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [29 12]]\n",
      "Sensitivity: 0.2927, Specificity: 0.9149\n",
      "Epoch [4/20] - Train Loss: 0.6021, Val Loss: 0.7380, Val Acc: 0.6250, Val AUC: 0.6892\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [11 30]]\n",
      "Sensitivity: 0.7317, Specificity: 0.8085\n",
      "Epoch [5/20] - Train Loss: 0.5873, Val Loss: 0.5562, Val Acc: 0.7727, Val AUC: 0.8184\n",
      "\n",
      "Confusion Matrix:\n",
      "[[30 17]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.6383\n",
      "Epoch [6/20] - Train Loss: 0.4771, Val Loss: 0.5294, Val Acc: 0.7614, Val AUC: 0.8474\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.7872\n",
      "Epoch [7/20] - Train Loss: 0.5066, Val Loss: 0.5121, Val Acc: 0.8068, Val AUC: 0.8635\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [15 26]]\n",
      "Sensitivity: 0.6341, Specificity: 0.8723\n",
      "Epoch [8/20] - Train Loss: 0.4761, Val Loss: 0.5870, Val Acc: 0.7614, Val AUC: 0.8106\n",
      "\n",
      "Confusion Matrix:\n",
      "[[24 23]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.5106\n",
      "Epoch [9/20] - Train Loss: 0.4649, Val Loss: 0.5691, Val Acc: 0.7273, Val AUC: 0.8801\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.8936\n",
      "Epoch [10/20] - Train Loss: 0.4996, Val Loss: 0.5527, Val Acc: 0.7955, Val AUC: 0.8298\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.7234\n",
      "Epoch [11/20] - Train Loss: 0.4950, Val Loss: 0.4742, Val Acc: 0.7841, Val AUC: 0.8697\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8085\n",
      "Epoch [12/20] - Train Loss: 0.4686, Val Loss: 0.4787, Val Acc: 0.7955, Val AUC: 0.8729\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.7447\n",
      "Epoch [13/20] - Train Loss: 0.4443, Val Loss: 0.4819, Val Acc: 0.8182, Val AUC: 0.8755\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.8723\n",
      "Epoch [14/20] - Train Loss: 0.5317, Val Loss: 0.5418, Val Acc: 0.7841, Val AUC: 0.8573\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.9149\n",
      "Epoch [15/20] - Train Loss: 0.5029, Val Loss: 0.5461, Val Acc: 0.7727, Val AUC: 0.8552\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [23 18]]\n",
      "Sensitivity: 0.4390, Specificity: 1.0000\n",
      "Epoch [16/20] - Train Loss: 0.5382, Val Loss: 0.5657, Val Acc: 0.7386, Val AUC: 0.8313\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8298\n",
      "Epoch [17/20] - Train Loss: 0.4920, Val Loss: 0.5466, Val Acc: 0.8182, Val AUC: 0.8630\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.8298\n",
      "Epoch [18/20] - Train Loss: 0.4726, Val Loss: 0.4851, Val Acc: 0.8068, Val AUC: 0.8661\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8936\n",
      "Epoch [19/20] - Train Loss: 0.4076, Val Loss: 0.4845, Val Acc: 0.8068, Val AUC: 0.8433\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.7447\n",
      "Epoch [20/20] - Train Loss: 0.4054, Val Loss: 0.5045, Val Acc: 0.7727, Val AUC: 0.8521\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.9149\n",
      "Test Loss: 0.3766, Test Accuracy: 0.8864, Test AUC: 0.9196\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-tb/no_transf-AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=best_trial['hidden_dim'], num_layers=best_trial['num_layers'], dropout=best_trial['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=best_trial['lr'], weight_decay=best_trial['weight_decay'], verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()    \n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Run 1/10\n",
      "\n",
      "🔁 Run 2/10\n",
      "\n",
      "🔁 Run 3/10\n",
      "\n",
      "🔁 Run 4/10\n",
      "\n",
      "🔁 Run 5/10\n",
      "\n",
      "🔁 Run 6/10\n",
      "\n",
      "🔁 Run 7/10\n",
      "\n",
      "🔁 Run 8/10\n",
      "\n",
      "🔁 Run 9/10\n",
      "\n",
      "🔁 Run 10/10\n",
      "\n",
      "📊 Summary across 10 runs (Vanilla LSTM from Optuna):\n",
      "Accuracy:       0.8284 ± 0.0375\n",
      "AUC:            0.8861 ± 0.0218\n",
      "Sensitivity:    0.7902 ± 0.0569\n",
      "Specificity:    0.8617 ± 0.0803\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Helper: suppress stdout\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Number of repeated runs\n",
    "n_runs = 10\n",
    "\n",
    "# Metric containers\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = LSTMClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=best_trial['hidden_dim'],\n",
    "        num_layers=best_trial['num_layers'],\n",
    "        dropout=best_trial['dropout']\n",
    "    )\n",
    "\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=25,\n",
    "            lr=best_trial['lr'],\n",
    "            weight_decay=best_trial['weight_decay'],\n",
    "            verbose=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Manual calculation of sensitivity and specificity\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "    # Save final model\n",
    "    if run == n_runs - 1:\n",
    "        torch.save(model.state_dict(), 'best_model_lstm_from_optuna.pt')\n",
    "\n",
    "# Summary output\n",
    "print(\"\\n📊 Summary across 10 runs (Vanilla LSTM from Optuna):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 18:24:48,350] A new study created in memory with name: no-name-b07b76b3-c7a0-4023-9790-1d56337451ee\n",
      "[I 2025-04-30 18:24:52,921] Trial 0 finished with value: 0.6898207465807596 and parameters: {'hidden_dim': 97, 'num_layers': 3, 'dropout': 0.24528985730325956, 'lr': 0.005063212714011772, 'weight_decay': 0.0071862512422995635}. Best is trial 0 with value: 0.6898207465807596.\n",
      "[I 2025-04-30 18:24:55,955] Trial 1 finished with value: 0.28785648941993713 and parameters: {'hidden_dim': 36, 'num_layers': 1, 'dropout': 0.30210585305732385, 'lr': 0.007809465764820014, 'weight_decay': 0.004529087941172045}. Best is trial 1 with value: 0.28785648941993713.\n",
      "[I 2025-04-30 18:24:59,185] Trial 2 finished with value: 0.24870430926481882 and parameters: {'hidden_dim': 37, 'num_layers': 2, 'dropout': 0.3975261965733413, 'lr': 0.0044937686896288995, 'weight_decay': 0.0007746518722486764}. Best is trial 2 with value: 0.24870430926481882.\n",
      "[I 2025-04-30 18:25:02,280] Trial 3 finished with value: 0.28843483080466586 and parameters: {'hidden_dim': 64, 'num_layers': 1, 'dropout': 0.1586901089921885, 'lr': 0.003399216123197099, 'weight_decay': 0.0004613618316240959}. Best is trial 2 with value: 0.24870430926481882.\n",
      "[I 2025-04-30 18:25:05,708] Trial 4 finished with value: 0.501004566748937 and parameters: {'hidden_dim': 96, 'num_layers': 2, 'dropout': 0.38722678600148586, 'lr': 0.00423455243758553, 'weight_decay': 0.007803428146601998}. Best is trial 2 with value: 0.24870430926481882.\n",
      "[I 2025-04-30 18:25:09,202] Trial 5 finished with value: 0.6932457089424133 and parameters: {'hidden_dim': 89, 'num_layers': 3, 'dropout': 0.40713820294604874, 'lr': 0.0009227235498178957, 'weight_decay': 0.0041487488852532525}. Best is trial 2 with value: 0.24870430926481882.\n",
      "[I 2025-04-30 18:25:14,379] Trial 6 finished with value: 0.6915470163027445 and parameters: {'hidden_dim': 124, 'num_layers': 3, 'dropout': 0.4915861471481242, 'lr': 0.0008808945758732868, 'weight_decay': 0.004132426615247025}. Best is trial 2 with value: 0.24870430926481882.\n",
      "[I 2025-04-30 18:25:17,843] Trial 7 finished with value: 0.6899998784065247 and parameters: {'hidden_dim': 87, 'num_layers': 2, 'dropout': 0.13820394309743975, 'lr': 0.0024222101463687172, 'weight_decay': 0.009958969068595997}. Best is trial 2 with value: 0.24870430926481882.\n",
      "[I 2025-04-30 18:25:21,393] Trial 8 finished with value: 0.33404899140199024 and parameters: {'hidden_dim': 85, 'num_layers': 2, 'dropout': 0.3100875206373509, 'lr': 0.008824959475049481, 'weight_decay': 0.0007974678099956048}. Best is trial 2 with value: 0.24870430926481882.\n",
      "[I 2025-04-30 18:25:25,566] Trial 9 finished with value: 0.40193567673365277 and parameters: {'hidden_dim': 33, 'num_layers': 2, 'dropout': 0.2263385562000405, 'lr': 0.004909818749897756, 'weight_decay': 0.004209090242144025}. Best is trial 2 with value: 0.24870430926481882.\n",
      "[I 2025-04-30 18:25:29,433] Trial 10 finished with value: 0.24787494043509165 and parameters: {'hidden_dim': 57, 'num_layers': 1, 'dropout': 0.48414723586367936, 'lr': 0.007637568745916616, 'weight_decay': 0.0022829825602779224}. Best is trial 10 with value: 0.24787494043509165.\n",
      "[I 2025-04-30 18:25:33,532] Trial 11 finished with value: 0.3073986768722534 and parameters: {'hidden_dim': 55, 'num_layers': 1, 'dropout': 0.47924009129978395, 'lr': 0.006706419615807419, 'weight_decay': 0.002032293158307521}. Best is trial 10 with value: 0.24787494043509165.\n",
      "[I 2025-04-30 18:25:37,578] Trial 12 finished with value: 0.2827240775028865 and parameters: {'hidden_dim': 54, 'num_layers': 1, 'dropout': 0.41471240237691964, 'lr': 0.009816168162751845, 'weight_decay': 0.002122078053673795}. Best is trial 10 with value: 0.24787494043509165.\n",
      "[I 2025-04-30 18:25:41,251] Trial 13 finished with value: 0.2540023624897003 and parameters: {'hidden_dim': 47, 'num_layers': 1, 'dropout': 0.3715496589141881, 'lr': 0.0065924721556600285, 'weight_decay': 0.0023782952528590856}. Best is trial 10 with value: 0.24787494043509165.\n",
      "[I 2025-04-30 18:25:46,897] Trial 14 finished with value: 0.25229262312253314 and parameters: {'hidden_dim': 68, 'num_layers': 2, 'dropout': 0.4461861253417477, 'lr': 0.00595882273378552, 'weight_decay': 5.5015398923071715e-06}. Best is trial 10 with value: 0.24787494043509165.\n",
      "[I 2025-04-30 18:25:51,535] Trial 15 finished with value: 0.2929808522264163 and parameters: {'hidden_dim': 69, 'num_layers': 1, 'dropout': 0.342970561584373, 'lr': 0.00786529358680138, 'weight_decay': 0.00210679042685341}. Best is trial 10 with value: 0.24787494043509165.\n",
      "[I 2025-04-30 18:25:56,789] Trial 16 finished with value: 0.6924379070599874 and parameters: {'hidden_dim': 44, 'num_layers': 3, 'dropout': 0.44410462632367065, 'lr': 0.0023077546098688785, 'weight_decay': 0.0061113482342570035}. Best is trial 10 with value: 0.24787494043509165.\n",
      "[I 2025-04-30 18:26:01,419] Trial 17 finished with value: 0.3502027789751689 and parameters: {'hidden_dim': 45, 'num_layers': 2, 'dropout': 0.4668991426087841, 'lr': 0.007807694070636263, 'weight_decay': 0.003207120357790397}. Best is trial 10 with value: 0.24787494043509165.\n",
      "[I 2025-04-30 18:26:07,344] Trial 18 finished with value: 0.2930847853422165 and parameters: {'hidden_dim': 76, 'num_layers': 2, 'dropout': 0.3454681686481188, 'lr': 0.0036092237844488426, 'weight_decay': 0.000945783445386995}. Best is trial 10 with value: 0.24787494043509165.\n",
      "[I 2025-04-30 18:26:12,698] Trial 19 finished with value: 0.26707395414511365 and parameters: {'hidden_dim': 58, 'num_layers': 1, 'dropout': 0.250393205671843, 'lr': 0.005752023922584393, 'weight_decay': 0.003102913968557575}. Best is trial 10 with value: 0.24787494043509165.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 57, 'num_layers': 1, 'dropout': 0.48414723586367936, 'lr': 0.007637568745916616, 'weight_decay': 0.0022829825602779224}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# Updated BiLSTM with flatten layer as previously defined\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-BiLSTM_Flatten-tb/BiLSTM_Flatten_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "    max_seq_len = 100  # fixed for now; match your padding/truncation\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage (uncomment and run in your local environment):\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# study.best_trial.params['num_layers'] = 2\n",
    "# study.best_trial.params['dropout'] \n",
    "# study.best_trial.params['lr'] \n",
    "# study.best_trial.params['weight_decay'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = {'hidden_dim': 57, 'num_layers': 1, 'dropout': 0.48414723586367936, 'lr': 0.007637568745916616, 'weight_decay': 0.0022829825602779224}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/20] - Train Loss: 0.8102, Val Loss: 0.7081, Val Acc: 0.5341, Val AUC: 0.7826\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [12 29]]\n",
      "Sensitivity: 0.7073, Specificity: 0.8936\n",
      "Epoch [2/20] - Train Loss: 0.6312, Val Loss: 0.5931, Val Acc: 0.8068, Val AUC: 0.8780\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [16 25]]\n",
      "Sensitivity: 0.6098, Specificity: 0.9362\n",
      "Epoch [3/20] - Train Loss: 0.5102, Val Loss: 0.4751, Val Acc: 0.7841, Val AUC: 0.8941\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.9787\n",
      "Epoch [4/20] - Train Loss: 0.3626, Val Loss: 0.3795, Val Acc: 0.8409, Val AUC: 0.9299\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8936\n",
      "Epoch [5/20] - Train Loss: 0.2889, Val Loss: 0.3092, Val Acc: 0.8636, Val AUC: 0.9440\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.8511\n",
      "Epoch [6/20] - Train Loss: 0.2479, Val Loss: 0.2894, Val Acc: 0.8636, Val AUC: 0.9543\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8936\n",
      "Epoch [7/20] - Train Loss: 0.2282, Val Loss: 0.2676, Val Acc: 0.8750, Val AUC: 0.9564\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.9149\n",
      "Epoch [8/20] - Train Loss: 0.2119, Val Loss: 0.2534, Val Acc: 0.8750, Val AUC: 0.9621\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.9149\n",
      "Epoch [9/20] - Train Loss: 0.2141, Val Loss: 0.2598, Val Acc: 0.9091, Val AUC: 0.9678\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.9149\n",
      "Epoch [10/20] - Train Loss: 0.1749, Val Loss: 0.2369, Val Acc: 0.8864, Val AUC: 0.9694\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.8936\n",
      "Epoch [11/20] - Train Loss: 0.1740, Val Loss: 0.2807, Val Acc: 0.9091, Val AUC: 0.9626\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.9362\n",
      "Epoch [12/20] - Train Loss: 0.2412, Val Loss: 0.2534, Val Acc: 0.9205, Val AUC: 0.9642\n",
      "\n",
      "Confusion Matrix:\n",
      "[[28 19]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.5957\n",
      "Epoch [13/20] - Train Loss: 0.2227, Val Loss: 0.5671, Val Acc: 0.7841, Val AUC: 0.9491\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.9362\n",
      "Epoch [14/20] - Train Loss: 0.2538, Val Loss: 0.2715, Val Acc: 0.8977, Val AUC: 0.9559\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.9362\n",
      "Epoch [15/20] - Train Loss: 0.1487, Val Loss: 0.3011, Val Acc: 0.8636, Val AUC: 0.9616\n",
      "\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.7234\n",
      "Epoch [16/20] - Train Loss: 0.1260, Val Loss: 0.3814, Val Acc: 0.8295, Val AUC: 0.9611\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.8511\n",
      "Epoch [17/20] - Train Loss: 0.1670, Val Loss: 0.2670, Val Acc: 0.8864, Val AUC: 0.9621\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.9362\n",
      "Epoch [18/20] - Train Loss: 0.1762, Val Loss: 0.2760, Val Acc: 0.8977, Val AUC: 0.9580\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.9362\n",
      "Epoch [19/20] - Train Loss: 0.1381, Val Loss: 0.2792, Val Acc: 0.9091, Val AUC: 0.9549\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.9362\n",
      "Epoch [20/20] - Train Loss: 0.1191, Val Loss: 0.2844, Val Acc: 0.8750, Val AUC: 0.9580\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.9574\n",
      "Test Loss: 0.3203, Test Accuracy: 0.8864, Test AUC: 0.9590\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Updated BiLSTM with flatten layer as previously defined\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-BiLSTM_Flatten-tb/BiLSTM_Flatten_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model =BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=best_trial['hidden_dim'], num_layers=best_trial['num_layers'], dropout= best_trial['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=best_trial['lr'],\n",
    "                      weight_decay=best_trial['weight_decay'] , verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Run 1/10\n",
      "\n",
      "🔁 Run 2/10\n",
      "\n",
      "🔁 Run 3/10\n",
      "\n",
      "🔁 Run 4/10\n",
      "\n",
      "🔁 Run 5/10\n",
      "\n",
      "🔁 Run 6/10\n",
      "\n",
      "🔁 Run 7/10\n",
      "\n",
      "🔁 Run 8/10\n",
      "\n",
      "🔁 Run 9/10\n",
      "\n",
      "🔁 Run 10/10\n",
      "\n",
      "📊 Summary across 10 runs (BiLSTM + Flatten):\n",
      "Accuracy:       0.8864 ± 0.0197\n",
      "AUC:            0.9553 ± 0.0085\n",
      "Sensitivity:    0.8317 ± 0.0442\n",
      "Specificity:    0.9340 ± 0.0242\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Suppress print helper\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Number of repeated runs\n",
    "n_runs = 10\n",
    "\n",
    "# Metric containers\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=best_trial['hidden_dim'],\n",
    "        num_layers=best_trial['num_layers'],\n",
    "        dropout=best_trial['dropout']\n",
    "    )\n",
    "\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=25,\n",
    "            lr=best_trial['lr'],\n",
    "            weight_decay=best_trial['weight_decay'],\n",
    "            verbose=False,\n",
    "            # train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Manually compute sensitivity and specificity\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Summary output\n",
    "print(\"\\n📊 Summary across 10 runs (BiLSTM + Flatten):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 18:10:09,162] A new study created in memory with name: no-name-5357a911-13e1-45bb-9841-39261fa96a37\n",
      "[I 2025-04-30 18:10:12,206] Trial 0 finished with value: 0.6910277009010315 and parameters: {'hidden_dim': 79, 'num_layers': 2, 'dropout': 0.37110173646279365, 'lr': 0.002263155597014851, 'weight_decay': 0.0008581363760045208}. Best is trial 0 with value: 0.6910277009010315.\n",
      "[I 2025-04-30 18:10:15,385] Trial 1 finished with value: 0.6906319657961527 and parameters: {'hidden_dim': 118, 'num_layers': 1, 'dropout': 0.3561217340013353, 'lr': 0.0025816578850674536, 'weight_decay': 0.004386425372395789}. Best is trial 1 with value: 0.6906319657961527.\n",
      "[I 2025-04-30 18:10:18,483] Trial 2 finished with value: 0.6671907107035319 and parameters: {'hidden_dim': 67, 'num_layers': 1, 'dropout': 0.3011428920399165, 'lr': 0.0011196241499856797, 'weight_decay': 0.0002575061509942688}. Best is trial 2 with value: 0.6671907107035319.\n",
      "[I 2025-04-30 18:10:21,891] Trial 3 finished with value: 0.6913137634595236 and parameters: {'hidden_dim': 90, 'num_layers': 3, 'dropout': 0.21320662133718615, 'lr': 0.0002849878481210072, 'weight_decay': 0.0008070378623215198}. Best is trial 2 with value: 0.6671907107035319.\n",
      "[I 2025-04-30 18:10:24,812] Trial 4 finished with value: 0.5755362113316854 and parameters: {'hidden_dim': 51, 'num_layers': 1, 'dropout': 0.2580642865586656, 'lr': 0.0027752300601556106, 'weight_decay': 0.0012526469005709703}. Best is trial 4 with value: 0.5755362113316854.\n",
      "[I 2025-04-30 18:10:28,352] Trial 5 finished with value: 0.5291903813680013 and parameters: {'hidden_dim': 103, 'num_layers': 3, 'dropout': 0.12562160827534488, 'lr': 0.0012889153722012918, 'weight_decay': 2.046224981295287e-06}. Best is trial 5 with value: 0.5291903813680013.\n",
      "[I 2025-04-30 18:10:31,598] Trial 6 finished with value: 0.6906265020370483 and parameters: {'hidden_dim': 72, 'num_layers': 2, 'dropout': 0.36602886650337985, 'lr': 0.0006721030828896391, 'weight_decay': 0.0002930019521451519}. Best is trial 5 with value: 0.5291903813680013.\n",
      "[I 2025-04-30 18:10:34,959] Trial 7 finished with value: 0.691159188747406 and parameters: {'hidden_dim': 74, 'num_layers': 3, 'dropout': 0.12177526572434344, 'lr': 0.0013402599416906186, 'weight_decay': 0.00029655586865011803}. Best is trial 5 with value: 0.5291903813680013.\n",
      "[I 2025-04-30 18:10:38,162] Trial 8 finished with value: 0.6910893519719442 and parameters: {'hidden_dim': 63, 'num_layers': 3, 'dropout': 0.4292841454221593, 'lr': 0.00022141245497188117, 'weight_decay': 2.1413521145847484e-06}. Best is trial 5 with value: 0.5291903813680013.\n",
      "[I 2025-04-30 18:10:41,375] Trial 9 finished with value: 0.6936765511830648 and parameters: {'hidden_dim': 71, 'num_layers': 3, 'dropout': 0.45821228539444736, 'lr': 0.0001783327159652901, 'weight_decay': 0.0015451842623227148}. Best is trial 5 with value: 0.5291903813680013.\n",
      "[I 2025-04-30 18:10:44,814] Trial 10 finished with value: 0.48753222823143005 and parameters: {'hidden_dim': 108, 'num_layers': 2, 'dropout': 0.10126263486326345, 'lr': 0.00637845477179278, 'weight_decay': 1.0401484022856134e-06}. Best is trial 10 with value: 0.48753222823143005.\n",
      "[I 2025-04-30 18:10:48,207] Trial 11 finished with value: 0.5056045254071554 and parameters: {'hidden_dim': 108, 'num_layers': 2, 'dropout': 0.10309305867917895, 'lr': 0.008239176430338885, 'weight_decay': 1.0425008562209474e-06}. Best is trial 10 with value: 0.48753222823143005.\n",
      "[I 2025-04-30 18:10:51,470] Trial 12 finished with value: 0.5313811898231506 and parameters: {'hidden_dim': 127, 'num_layers': 2, 'dropout': 0.1713931653480908, 'lr': 0.006787222036872818, 'weight_decay': 1.611936110256765e-05}. Best is trial 10 with value: 0.48753222823143005.\n",
      "[I 2025-04-30 18:10:54,467] Trial 13 finished with value: 0.5038144489129385 and parameters: {'hidden_dim': 104, 'num_layers': 2, 'dropout': 0.11285816055699896, 'lr': 0.009977394253711188, 'weight_decay': 1.3099034770921098e-05}. Best is trial 10 with value: 0.48753222823143005.\n",
      "[I 2025-04-30 18:10:57,143] Trial 14 finished with value: 0.5045018593470255 and parameters: {'hidden_dim': 95, 'num_layers': 2, 'dropout': 0.18686833835929978, 'lr': 0.009799393270237088, 'weight_decay': 1.796727247756232e-05}. Best is trial 10 with value: 0.48753222823143005.\n",
      "[I 2025-04-30 18:10:59,993] Trial 15 finished with value: 0.4372965296109517 and parameters: {'hidden_dim': 110, 'num_layers': 2, 'dropout': 0.24633551203936624, 'lr': 0.004563575834329992, 'weight_decay': 9.178474908999675e-06}. Best is trial 15 with value: 0.4372965296109517.\n",
      "[I 2025-04-30 18:11:02,572] Trial 16 finished with value: 0.48999417821566266 and parameters: {'hidden_dim': 37, 'num_layers': 1, 'dropout': 0.2582741738423946, 'lr': 0.003968193016784801, 'weight_decay': 5.587502597408954e-06}. Best is trial 15 with value: 0.4372965296109517.\n",
      "[I 2025-04-30 18:11:05,571] Trial 17 finished with value: 0.4681751529375712 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.49831577081068124, 'lr': 0.005186148094349387, 'weight_decay': 4.886419728486451e-05}. Best is trial 15 with value: 0.4372965296109517.\n",
      "[I 2025-04-30 18:11:08,555] Trial 18 finished with value: 0.6918193896611532 and parameters: {'hidden_dim': 127, 'num_layers': 2, 'dropout': 0.49944906764857494, 'lr': 0.00010077947453428663, 'weight_decay': 5.696643276773175e-05}. Best is trial 15 with value: 0.4372965296109517.\n",
      "[I 2025-04-30 18:11:11,450] Trial 19 finished with value: 0.49163302779197693 and parameters: {'hidden_dim': 118, 'num_layers': 1, 'dropout': 0.30981883189935217, 'lr': 0.004529389971256255, 'weight_decay': 6.742668135890811e-05}. Best is trial 15 with value: 0.4372965296109517.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 110, 'num_layers': 2, 'dropout': 0.24633551203936624, 'lr': 0.004563575834329992, 'weight_decay': 9.178474908999675e-06}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# LSTM with Attention classifier\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)  # shape: [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)  # shape: [batch, seq_len]\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)  # normalize\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # shape: [batch, hidden_dim]\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-lstm-attn-tb/LSTM_Attn_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-lstm_attention-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = {'hidden_dim': 110, 'num_layers': 2, 'dropout': 0.24633551203936624, 'lr': 0.004563575834329992, 'weight_decay': 9.178474908999675e-06}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# LSTM with Attention classifier\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)  # shape: [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)  # shape: [batch, seq_len]\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)  # normalize\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # shape: [batch, hidden_dim]\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-lstm-attn-tb/LSTM_Attn_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-lstm_attention-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model =LSTMWithAttentionClassifier(input_dim=20, hidden_dim=best_trial['hidden_dim'], num_layers=best_trial['num_layers'], dropout= best_trial['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=best_trial['lr'],\n",
    "                      weight_decay=best_trial['weight_decay'] , verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Run 1/10\n",
      "\n",
      "🔁 Run 2/10\n",
      "\n",
      "🔁 Run 3/10\n",
      "\n",
      "🔁 Run 4/10\n",
      "\n",
      "🔁 Run 5/10\n",
      "\n",
      "🔁 Run 6/10\n",
      "\n",
      "🔁 Run 7/10\n",
      "\n",
      "🔁 Run 8/10\n",
      "\n",
      "🔁 Run 9/10\n",
      "\n",
      "🔁 Run 10/10\n",
      "\n",
      "📊 Summary across 10 runs (LSTM + Attention):\n",
      "Accuracy:       0.8261 ± 0.0449\n",
      "AUC:            0.9197 ± 0.0119\n",
      "Sensitivity:    0.8098 ± 0.0937\n",
      "Specificity:    0.8404 ± 0.1166\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Suppress print helper\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Number of runs\n",
    "n_runs = 10\n",
    "\n",
    "# Metric storage\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=best_trial['hidden_dim'],\n",
    "        num_layers=best_trial['num_layers'],\n",
    "        dropout=best_trial['dropout']\n",
    "    )\n",
    "\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=25,\n",
    "            lr=best_trial['lr'],\n",
    "            weight_decay=best_trial['weight_decay'],\n",
    "            verbose=False,\n",
    "            # train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Compute confusion matrix metrics\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n📊 Summary across 10 runs (LSTM + Attention):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 18:11:18,003] A new study created in memory with name: no-name-ebb929fd-87a0-49dc-bb21-459b5828cb2a\n",
      "[I 2025-04-30 18:11:21,359] Trial 0 finished with value: 0.6866805156071981 and parameters: {'hidden_dim': 115, 'num_layers': 1, 'dropout': 0.26632588822023223, 'lr': 0.009666571319781478, 'weight_decay': 0.0010942533227846368}. Best is trial 0 with value: 0.6866805156071981.\n",
      "[I 2025-04-30 18:11:24,785] Trial 1 finished with value: 0.6912067333857218 and parameters: {'hidden_dim': 63, 'num_layers': 3, 'dropout': 0.1284694364538071, 'lr': 0.005152262818067029, 'weight_decay': 0.00904492003627939}. Best is trial 0 with value: 0.6866805156071981.\n",
      "[I 2025-04-30 18:11:27,914] Trial 2 finished with value: 0.6912070115407308 and parameters: {'hidden_dim': 76, 'num_layers': 2, 'dropout': 0.24669653004845418, 'lr': 0.00996895629892605, 'weight_decay': 0.005191934714821247}. Best is trial 0 with value: 0.6866805156071981.\n",
      "[I 2025-04-30 18:11:31,100] Trial 3 finished with value: 0.6907888054847717 and parameters: {'hidden_dim': 41, 'num_layers': 1, 'dropout': 0.19718872886978944, 'lr': 0.001269201927577386, 'weight_decay': 0.0043460080341064045}. Best is trial 0 with value: 0.6866805156071981.\n",
      "[I 2025-04-30 18:11:34,586] Trial 4 finished with value: 0.6912372708320618 and parameters: {'hidden_dim': 74, 'num_layers': 3, 'dropout': 0.2593466376874073, 'lr': 0.005176078505081275, 'weight_decay': 0.0032054549301490654}. Best is trial 0 with value: 0.6866805156071981.\n",
      "[I 2025-04-30 18:11:38,102] Trial 5 finished with value: 0.6912407676378886 and parameters: {'hidden_dim': 98, 'num_layers': 2, 'dropout': 0.43905311432531935, 'lr': 0.0009632523958390629, 'weight_decay': 0.005528243740429483}. Best is trial 0 with value: 0.6866805156071981.\n",
      "[I 2025-04-30 18:11:41,673] Trial 6 finished with value: 0.6911749839782715 and parameters: {'hidden_dim': 84, 'num_layers': 2, 'dropout': 0.4925169066831351, 'lr': 0.003944261255250082, 'weight_decay': 0.006354462029677799}. Best is trial 0 with value: 0.6866805156071981.\n",
      "[I 2025-04-30 18:11:45,294] Trial 7 finished with value: 0.6912036935488383 and parameters: {'hidden_dim': 53, 'num_layers': 3, 'dropout': 0.2158992953292231, 'lr': 0.0056268925203452926, 'weight_decay': 0.0021295896718259238}. Best is trial 0 with value: 0.6866805156071981.\n",
      "[I 2025-04-30 18:11:48,835] Trial 8 finished with value: 0.6912160714467367 and parameters: {'hidden_dim': 38, 'num_layers': 3, 'dropout': 0.3373549972966104, 'lr': 0.0026576720429836147, 'weight_decay': 0.008738945089842147}. Best is trial 0 with value: 0.6866805156071981.\n",
      "[I 2025-04-30 18:11:52,450] Trial 9 finished with value: 0.6911978125572205 and parameters: {'hidden_dim': 61, 'num_layers': 3, 'dropout': 0.22929293758963398, 'lr': 0.004852159960845556, 'weight_decay': 0.006701731355384489}. Best is trial 0 with value: 0.6866805156071981.\n",
      "[I 2025-04-30 18:11:55,795] Trial 10 finished with value: 0.499078889687856 and parameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.3580726815282417, 'lr': 0.009583812526017515, 'weight_decay': 0.00019522214167259889}. Best is trial 10 with value: 0.499078889687856.\n",
      "[I 2025-04-30 18:11:59,222] Trial 11 finished with value: 0.6229438583056132 and parameters: {'hidden_dim': 126, 'num_layers': 1, 'dropout': 0.3527596422279097, 'lr': 0.009604457040250898, 'weight_decay': 7.384191923489968e-06}. Best is trial 10 with value: 0.499078889687856.\n",
      "[I 2025-04-30 18:12:02,590] Trial 12 finished with value: 0.6852761308352152 and parameters: {'hidden_dim': 122, 'num_layers': 1, 'dropout': 0.35582480278696765, 'lr': 0.00778387745436696, 'weight_decay': 0.00033549212185156503}. Best is trial 10 with value: 0.499078889687856.\n",
      "[I 2025-04-30 18:12:05,898] Trial 13 finished with value: 0.44172481695810956 and parameters: {'hidden_dim': 105, 'num_layers': 1, 'dropout': 0.4022965052975248, 'lr': 0.007892181355545593, 'weight_decay': 5.110662477807865e-06}. Best is trial 13 with value: 0.44172481695810956.\n",
      "[I 2025-04-30 18:12:09,160] Trial 14 finished with value: 0.6897414922714233 and parameters: {'hidden_dim': 106, 'num_layers': 1, 'dropout': 0.41548872666227876, 'lr': 0.007186382450879053, 'weight_decay': 0.002245175367024105}. Best is trial 13 with value: 0.44172481695810956.\n",
      "[I 2025-04-30 18:12:11,960] Trial 15 finished with value: 0.6303258736928304 and parameters: {'hidden_dim': 96, 'num_layers': 1, 'dropout': 0.4126239296085749, 'lr': 0.007529556075641586, 'weight_decay': 0.0015028119805486236}. Best is trial 13 with value: 0.44172481695810956.\n",
      "[I 2025-04-30 18:12:15,572] Trial 16 finished with value: 0.6911937594413757 and parameters: {'hidden_dim': 110, 'num_layers': 2, 'dropout': 0.48572705561695045, 'lr': 0.008440434723847604, 'weight_decay': 0.0034990040433984667}. Best is trial 13 with value: 0.44172481695810956.\n",
      "[I 2025-04-30 18:12:18,310] Trial 17 finished with value: 0.5458269516626993 and parameters: {'hidden_dim': 91, 'num_layers': 1, 'dropout': 0.3861179693510105, 'lr': 0.0064440138923464235, 'weight_decay': 0.0008920278894691712}. Best is trial 13 with value: 0.44172481695810956.\n",
      "[I 2025-04-30 18:12:22,139] Trial 18 finished with value: 0.6912043889363607 and parameters: {'hidden_dim': 128, 'num_layers': 2, 'dropout': 0.30600918354096784, 'lr': 0.008670647393617503, 'weight_decay': 0.003025310897422855}. Best is trial 13 with value: 0.44172481695810956.\n",
      "[I 2025-04-30 18:12:25,523] Trial 19 finished with value: 0.6910808086395264 and parameters: {'hidden_dim': 116, 'num_layers': 1, 'dropout': 0.3061015659684418, 'lr': 0.008845853506463417, 'weight_decay': 0.007948837635813171}. Best is trial 13 with value: 0.44172481695810956.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 105, 'num_layers': 1, 'dropout': 0.4022965052975248, 'lr': 0.007892181355545593, 'weight_decay': 5.110662477807865e-06}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# BiLSTM with Attention Classifier\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention_tb/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm_attention_tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_trial = {'hidden_dim': 105, 'num_layers': 1, 'dropout': 0.4022965052975248, 'lr': 0.007892181355545593, 'weight_decay': 5.110662477807865e-06}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[10 37]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.2128\n",
      "Epoch [1/20] - Train Loss: 0.6907, Val Loss: 0.6914, Val Acc: 0.5568, Val AUC: 0.7686\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/20] - Train Loss: 0.6858, Val Loss: 0.6840, Val Acc: 0.5341, Val AUC: 0.7104\n",
      "\n",
      "Confusion Matrix:\n",
      "[[18 29]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.3830\n",
      "Epoch [3/20] - Train Loss: 0.6981, Val Loss: 0.6214, Val Acc: 0.6705, Val AUC: 0.8293\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [25 16]]\n",
      "Sensitivity: 0.3902, Specificity: 0.9787\n",
      "Epoch [4/20] - Train Loss: 0.6216, Val Loss: 0.5848, Val Acc: 0.7045, Val AUC: 0.7602\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [34  7]]\n",
      "Sensitivity: 0.1707, Specificity: 1.0000\n",
      "Epoch [5/20] - Train Loss: 0.6285, Val Loss: 0.6393, Val Acc: 0.6136, Val AUC: 0.7686\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/20] - Train Loss: 0.6474, Val Loss: 0.7001, Val Acc: 0.5341, Val AUC: 0.7182\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [39  2]]\n",
      "Sensitivity: 0.0488, Specificity: 1.0000\n",
      "Epoch [7/20] - Train Loss: 0.6817, Val Loss: 0.6719, Val Acc: 0.5568, Val AUC: 0.7291\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [36  5]]\n",
      "Sensitivity: 0.1220, Specificity: 1.0000\n",
      "Epoch [8/20] - Train Loss: 0.6487, Val Loss: 0.6489, Val Acc: 0.5909, Val AUC: 0.7457\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [32  9]]\n",
      "Sensitivity: 0.2195, Specificity: 0.9574\n",
      "Epoch [9/20] - Train Loss: 0.6041, Val Loss: 0.6745, Val Acc: 0.6136, Val AUC: 0.7592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [32  9]]\n",
      "Sensitivity: 0.2195, Specificity: 1.0000\n",
      "Epoch [10/20] - Train Loss: 0.5973, Val Loss: 0.6133, Val Acc: 0.6364, Val AUC: 0.7244\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [29 12]]\n",
      "Sensitivity: 0.2927, Specificity: 0.9574\n",
      "Epoch [11/20] - Train Loss: 0.5825, Val Loss: 0.6605, Val Acc: 0.6477, Val AUC: 0.7447\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [28 13]]\n",
      "Sensitivity: 0.3171, Specificity: 0.9574\n",
      "Epoch [12/20] - Train Loss: 0.5815, Val Loss: 0.6116, Val Acc: 0.6591, Val AUC: 0.7644\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [27 14]]\n",
      "Sensitivity: 0.3415, Specificity: 0.9574\n",
      "Epoch [13/20] - Train Loss: 0.5424, Val Loss: 0.6557, Val Acc: 0.6705, Val AUC: 0.6907\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [25 16]]\n",
      "Sensitivity: 0.3902, Specificity: 0.9149\n",
      "Epoch [14/20] - Train Loss: 0.5969, Val Loss: 0.7387, Val Acc: 0.6705, Val AUC: 0.7104\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [23 18]]\n",
      "Sensitivity: 0.4390, Specificity: 0.9149\n",
      "Epoch [15/20] - Train Loss: 0.5554, Val Loss: 0.6800, Val Acc: 0.6932, Val AUC: 0.7338\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.9149\n",
      "Epoch [16/20] - Train Loss: 0.5478, Val Loss: 0.6522, Val Acc: 0.7045, Val AUC: 0.7621\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.9149\n",
      "Epoch [17/20] - Train Loss: 0.5257, Val Loss: 0.6517, Val Acc: 0.7045, Val AUC: 0.7618\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.9149\n",
      "Epoch [18/20] - Train Loss: 0.5370, Val Loss: 0.6562, Val Acc: 0.7045, Val AUC: 0.7621\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.9149\n",
      "Epoch [19/20] - Train Loss: 0.5323, Val Loss: 0.6608, Val Acc: 0.7045, Val AUC: 0.7678\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [22 19]]\n",
      "Sensitivity: 0.4634, Specificity: 0.9149\n",
      "Epoch [20/20] - Train Loss: 0.5344, Val Loss: 0.6643, Val Acc: 0.7045, Val AUC: 0.7660\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [24 17]]\n",
      "Sensitivity: 0.4146, Specificity: 1.0000\n",
      "Test Loss: 0.5202, Test Accuracy: 0.7273, Test AUC: 0.8864\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# BiLSTM with Attention Classifier\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention_tb/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm_attention_tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model =BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=best_trial['hidden_dim'], num_layers=best_trial['num_layers'], dropout= best_trial['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=best_trial['lr'],\n",
    "                      weight_decay=best_trial['weight_decay'] , verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Run 1/10\n",
      "\n",
      "🔁 Run 2/10\n",
      "\n",
      "🔁 Run 3/10\n",
      "\n",
      "🔁 Run 4/10\n",
      "\n",
      "🔁 Run 5/10\n",
      "\n",
      "🔁 Run 6/10\n",
      "\n",
      "🔁 Run 7/10\n",
      "\n",
      "🔁 Run 8/10\n",
      "\n",
      "🔁 Run 9/10\n",
      "\n",
      "🔁 Run 10/10\n",
      "\n",
      "📊 Summary across 10 runs (BiLSTM + Attention):\n",
      "Accuracy:       0.8443 ± 0.1076\n",
      "AUC:            0.9256 ± 0.0487\n",
      "Sensitivity:    0.7537 ± 0.2540\n",
      "Specificity:    0.9234 ± 0.0701\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Helper: suppress output\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Number of repeated runs\n",
    "n_runs = 10\n",
    "\n",
    "# Metric storage\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=best_trial['hidden_dim'],\n",
    "        num_layers=best_trial['num_layers'],\n",
    "        dropout=best_trial['dropout']\n",
    "    )\n",
    "\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=25,\n",
    "            lr=best_trial['lr'],\n",
    "            weight_decay=best_trial['weight_decay'],\n",
    "            verbose=False,\n",
    "            # train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Collect labels and predictions\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Final summary\n",
    "print(\"\\n📊 Summary across 10 runs (BiLSTM + Attention):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning to TB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'R', 'G', 'X', 'H', 'W', 'N', 'S', 'E', 'Y', 'V', 'A', 'I', 'F', 'M', 'K', 'T', 'D', 'C', 'P', 'Q', 'L'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 422\n",
      "Validation: 141\n",
      "Test: 141\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tb_amp_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
