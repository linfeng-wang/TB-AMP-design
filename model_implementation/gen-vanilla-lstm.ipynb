{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# If using PyTorch\n",
    "import torch\n",
    "\n",
    "# If using TensorFlow\n",
    "\n",
    "# Optional: If using Python hash-based functions\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "\n",
    "# Set seed for base Python random\n",
    "random.seed(42)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set seed for PyTorch (CPU and GPU)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)  # if using multi-GPU\n",
    "\n",
    "# Force deterministic behavior in PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### raw data gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'G', 'F', 'L', 'Q', 'D', 'H', 'A', 'N', 'V', 'W', 'C', 'M', 'S', 'K', 'E', 'P', 'Y', 'I', 'R', 'T'}\n",
      "20\n",
      "Number of sequences after filtering: 3306\n",
      "Dataset sizes: {'Train': 2219, 'Validation': 475, 'Test': 476}\n",
      "Input shape: torch.Size([1793, 20])\n",
      "Target shape: torch.Size([1793, 20])\n",
      "Lengths: tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 64, 59, 57, 55, 53, 53, 50,\n",
      "        46, 45, 40, 40, 38, 35, 32, 31, 31, 30, 29, 29, 28, 26, 20, 19, 16, 14,\n",
      "        13, 10,  7,  5,  5,  4,  4,  4,  4,  3,  2,  2,  2,  2,  2,  2,  2,  2,\n",
      "         2,  2,  2,  2,  2,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
      "         1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load only positive (AMP) sequences\n",
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "\n",
    "unique_letters = set(''.join(adam_df[\"Sequence\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "print(f\"Number of sequences after filtering: {len(adam_df)}\")\n",
    "adam_df = adam_df.drop_duplicates(subset='Sequence')\n",
    "tb_df = pd.read_csv('../data/all_seq702.csv')\n",
    "adam_df = adam_df[~adam_df['Sequence'].isin(tb_df['Sequences'])]\n",
    "adam_df = adam_df[adam_df[\"Sequence\"].str.len() >= 10]\n",
    "generation_seqs = adam_df[\"Sequence\"].reset_index(drop=True)\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "class GenerativeSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        input_seq = seq[:-1]  # all residues except the last\n",
    "        target_seq = seq[1:]  # all residues except the first\n",
    "        length = len(input_seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        input_one_hot = one_hot_torch(input_seq, dtype=self.one_hot_dtype)\n",
    "        target_one_hot = one_hot_torch(target_seq, dtype=self.one_hot_dtype)\n",
    "        # target_indices = torch.tensor([\"ACDEFGHIKLMNPQRSTVWY\".index(res) for res in target_seq], dtype=torch.long)\n",
    "        return input_one_hot, target_one_hot, length\n",
    "\n",
    "def generative_collate_and_pack(batch):\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "\n",
    "    lengths = torch.tensor(lengths)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    targets = [targets[i] for i in sorted_indices]\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    sequences = [seq.T for seq in sequences]  # transpose to [seq_len, features]\n",
    "    targets = [tgt.T for tgt in targets]      # transpose targets as well\n",
    "\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)\n",
    "    padded_targets = pad_sequence(targets, batch_first=False)\n",
    "\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "    packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, packed_target, lengths\n",
    "\n",
    "\n",
    "# Train/val/test split\n",
    "train_seqs, test_seqs = train_test_split(generation_seqs, test_size=0.3, random_state=42)\n",
    "val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "\n",
    "train_dataset = GenerativeSequenceDataset(train_seqs)\n",
    "val_dataset = GenerativeSequenceDataset(val_seqs)\n",
    "test_dataset = GenerativeSequenceDataset(test_seqs)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=generative_collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "\n",
    "# Dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\", dataset_sizes)\n",
    "\n",
    "for x, y, l in train_loader:\n",
    "    print(\"Input shape:\", x.data.shape)  # [L, B, 20]\n",
    "    print(\"Target shape:\", y.data.shape)  # [L, B, 20]\n",
    "    print(\"Lengths:\", y.batch_sizes)  # Lengths of sequences in the batch\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tb amp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'G', 'F', 'L', 'Q', 'D', 'H', 'A', 'N', 'V', 'W', 'C', 'M', 'S', 'K', 'X', 'E', 'P', 'Y', 'I', 'R', 'T'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "# df_filtered = df[\n",
    "#     (df['Sequences'].str.len() >= 10) &\n",
    "#     (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "#     (~df['Sequences'].str.contains('X'))\n",
    "# ]\n",
    "df_filtered = df\n",
    "\n",
    "def split_sequence(seq, chunk_size=20):\n",
    "    return [seq[i:i+chunk_size] for i in range(0, len(seq), chunk_size)]\n",
    "\n",
    "new_rows = []\n",
    "for _, row in df_filtered.iterrows():\n",
    "    seq = row['Sequences']\n",
    "    amp_label = row['AMP']\n",
    "    if len(seq) > 40:\n",
    "        for chunk in split_sequence(seq, 20):\n",
    "            new_rows.append({'Sequences': chunk, 'AMP': amp_label})\n",
    "    else:\n",
    "        new_rows.append({'Sequences': seq, 'AMP': amp_label})\n",
    "\n",
    "df_filtered = pd.DataFrame(new_rows)\n",
    "\n",
    "\n",
    "df_filtered = df_filtered[\n",
    "    (df_filtered['Sequences'].str.len() >= 10) &\n",
    "    (df_filtered['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df_filtered['Sequences'].str.contains('X'))\n",
    "]\n",
    "df_filtered = df_filtered[df_filtered['AMP']==1]\n",
    "df_filtered = df_filtered.drop_duplicates(subset='Sequences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Input shape': torch.Size([1179, 20]),\n",
       " 'Target shape': torch.Size([1179, 20]),\n",
       " 'Lengths': tensor([64, 64, 64, 64, 64, 64, 64, 64, 64, 56, 54, 48, 44, 44, 42, 40, 38, 37,\n",
       "         35, 18, 17, 17, 16, 15, 11,  9,  9,  9,  9,  6,  6,  6,  6,  5,  2,  2,\n",
       "          1,  1])}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Re-import libraries after environment reset\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data\n",
    "df = df_filtered\n",
    "\n",
    "# Clean and inspect\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "# non_standard_amino_acids = unique_letters - amino_acids\n",
    "# df = df[~df[\"Sequences\"].str.contains('|'.join(non_standard_amino_acids))]\n",
    "\n",
    "# Extract sequences\n",
    "sequences = df[\"Sequences\"].reset_index(drop=True)\n",
    "\n",
    "# Define one-hot function\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "# Define dataset class\n",
    "class GenerativeSequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        input_seq = seq[:-1]\n",
    "        target_seq = seq[1:]\n",
    "        length = len(input_seq.replace(\"X\", \"\"))\n",
    "        input_one_hot = one_hot_torch(input_seq, dtype=self.one_hot_dtype)\n",
    "        target_one_hot = one_hot_torch(target_seq, dtype=self.one_hot_dtype)\n",
    "        return input_one_hot, target_one_hot, length\n",
    "\n",
    "# Define collate function\n",
    "def generative_collate_and_pack(batch):\n",
    "    sequences, targets, lengths = zip(*batch)\n",
    "    lengths = torch.tensor(lengths)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    targets = [targets[i] for i in sorted_indices]\n",
    "    lengths = lengths[sorted_indices]\n",
    "    sequences = [seq.T for seq in sequences]\n",
    "    targets = [tgt.T for tgt in targets]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)\n",
    "    padded_targets = pad_sequence(targets, batch_first=False)\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "    packed_target = pack_padded_sequence(padded_targets, lengths.cpu(), batch_first=False)\n",
    "    return packed_input, packed_target, lengths\n",
    "\n",
    "# Split and load data\n",
    "train_seqs, test_seqs = train_test_split(sequences, test_size=0.3, random_state=42)\n",
    "val_seqs, test_seqs = train_test_split(test_seqs, test_size=0.5, random_state=42)\n",
    "train_dataset = GenerativeSequenceDataset(train_seqs)\n",
    "val_dataset = GenerativeSequenceDataset(val_seqs)\n",
    "test_dataset = GenerativeSequenceDataset(test_seqs)\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True, collate_fn=generative_collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False, collate_fn=generative_collate_and_pack)\n",
    "\n",
    "# Preview batch\n",
    "batch_sample = next(iter(train_loader))\n",
    "batch_sample_shapes = {\n",
    "    \"Input shape\": batch_sample[0].data.shape,\n",
    "    \"Target shape\": batch_sample[1].data.shape,\n",
    "    \"Lengths\": batch_sample[0].batch_sizes\n",
    "}\n",
    "batch_sample_shapes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 171\n",
      "Validation: 37\n",
      "Test: 37\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TB no transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-02 10:55:14,561] A new study created in memory with name: no-name-f7a5cdc2-a106-4dcf-a41d-0c75ebdcb8f2\n",
      "[I 2025-05-02 10:55:22,868] Trial 0 finished with value: 2.7173891067504883 and parameters: {'hidden_dim': 144, 'num_layers': 3, 'dropout': 0.15437274023341982, 'lr': 0.005771881998865653, 'weight_decay': 0.0002682713506887346}. Best is trial 0 with value: 2.7173891067504883.\n",
      "[I 2025-05-02 10:55:25,214] Trial 1 finished with value: 2.7665016651153564 and parameters: {'hidden_dim': 227, 'num_layers': 2, 'dropout': 0.4424763441823395, 'lr': 0.005704011585947213, 'weight_decay': 0.00041701060111245934}. Best is trial 0 with value: 2.7173891067504883.\n",
      "[I 2025-05-02 10:55:27,423] Trial 2 finished with value: 2.9343535900115967 and parameters: {'hidden_dim': 250, 'num_layers': 1, 'dropout': 0.32673827998102084, 'lr': 0.00154077252086226, 'weight_decay': 0.0008423535539033275}. Best is trial 0 with value: 2.7173891067504883.\n",
      "[I 2025-05-02 10:55:29,530] Trial 3 finished with value: 2.921525478363037 and parameters: {'hidden_dim': 203, 'num_layers': 1, 'dropout': 0.2910025775992817, 'lr': 0.002777183171745712, 'weight_decay': 0.0003434969516472462}. Best is trial 0 with value: 2.7173891067504883.\n",
      "[I 2025-05-02 10:55:31,689] Trial 4 finished with value: 2.6613221168518066 and parameters: {'hidden_dim': 94, 'num_layers': 3, 'dropout': 0.26014567080645096, 'lr': 0.0070399821490159725, 'weight_decay': 0.00035452053677446617}. Best is trial 4 with value: 2.6613221168518066.\n",
      "[I 2025-05-02 10:55:33,703] Trial 5 finished with value: 2.4661667346954346 and parameters: {'hidden_dim': 149, 'num_layers': 2, 'dropout': 0.4350981856205326, 'lr': 0.009996913527093828, 'weight_decay': 0.00012991309919791892}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:36,002] Trial 6 finished with value: 2.5921413898468018 and parameters: {'hidden_dim': 156, 'num_layers': 2, 'dropout': 0.26233032429984915, 'lr': 0.007085314917207174, 'weight_decay': 0.0003734833010050261}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:38,023] Trial 7 finished with value: 2.6170382499694824 and parameters: {'hidden_dim': 198, 'num_layers': 1, 'dropout': 0.2992198925463133, 'lr': 0.007478531405196831, 'weight_decay': 0.00032844451955447204}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:40,070] Trial 8 finished with value: 2.840430498123169 and parameters: {'hidden_dim': 246, 'num_layers': 2, 'dropout': 0.2860246538822809, 'lr': 0.0028624639927731003, 'weight_decay': 0.0006479475216515398}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:41,973] Trial 9 finished with value: 2.7308855056762695 and parameters: {'hidden_dim': 251, 'num_layers': 1, 'dropout': 0.1253868148614699, 'lr': 0.0061950686700516615, 'weight_decay': 0.00037620391007206044}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:44,080] Trial 10 finished with value: 2.4994585514068604 and parameters: {'hidden_dim': 104, 'num_layers': 3, 'dropout': 0.4761814996060996, 'lr': 0.008950795438477129, 'weight_decay': 4.841308347150881e-05}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:46,228] Trial 11 finished with value: 2.473280429840088 and parameters: {'hidden_dim': 110, 'num_layers': 3, 'dropout': 0.493711162895993, 'lr': 0.009976896028417542, 'weight_decay': 3.8410958555953375e-05}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:48,456] Trial 12 finished with value: 2.5803210735321045 and parameters: {'hidden_dim': 128, 'num_layers': 3, 'dropout': 0.4030390884872124, 'lr': 0.009912634416465286, 'weight_decay': 1.4941241000134039e-05}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:50,833] Trial 13 finished with value: 2.5807318687438965 and parameters: {'hidden_dim': 119, 'num_layers': 2, 'dropout': 0.4955914901115842, 'lr': 0.008935117102889415, 'weight_decay': 0.0001581504847002647}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:53,191] Trial 14 finished with value: 2.508286952972412 and parameters: {'hidden_dim': 72, 'num_layers': 2, 'dropout': 0.3929849973560849, 'lr': 0.009956845066137525, 'weight_decay': 0.0006072891214864552}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:55,624] Trial 15 finished with value: 2.591632843017578 and parameters: {'hidden_dim': 173, 'num_layers': 3, 'dropout': 0.37166428354469105, 'lr': 0.008408121428336614, 'weight_decay': 0.00016500535403210457}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:55:57,749] Trial 16 finished with value: 2.700115919113159 and parameters: {'hidden_dim': 73, 'num_layers': 2, 'dropout': 0.44696270216232137, 'lr': 0.004629756215012773, 'weight_decay': 0.0001677033310943217}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:56:00,317] Trial 17 finished with value: 2.5816473960876465 and parameters: {'hidden_dim': 171, 'num_layers': 3, 'dropout': 0.1988244547342201, 'lr': 0.008242167192370142, 'weight_decay': 0.0009716342592541768}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:56:02,703] Trial 18 finished with value: 2.7583444118499756 and parameters: {'hidden_dim': 135, 'num_layers': 2, 'dropout': 0.35954400375975193, 'lr': 0.004800561503195884, 'weight_decay': 0.0005332670932732454}. Best is trial 5 with value: 2.4661667346954346.\n",
      "[I 2025-05-02 10:56:05,108] Trial 19 finished with value: 2.605470657348633 and parameters: {'hidden_dim': 99, 'num_layers': 2, 'dropout': 0.4359927569570083, 'lr': 0.009518826156893808, 'weight_decay': 0.00010606058205186216}. Best is trial 5 with value: 2.4661667346954346.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'hidden_dim': 149, 'num_layers': 2, 'dropout': 0.4350981856205326, 'lr': 0.009996913527093828, 'weight_decay': 0.00012991309919791892}\n"
     ]
    }
   ],
   "source": [
    "# Re-import necessary packages after reset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "\n",
    "\n",
    "class GenerativeLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=128, num_layers=1, dropout=0.3):\n",
    "        super(GenerativeLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_dim, hidden_dim, num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Handle packed input\n",
    "        if isinstance(x, torch.nn.utils.rnn.PackedSequence):\n",
    "            packed_output, _ = self.lstm(x)\n",
    "            unpacked_output, _ = pad_packed_sequence(packed_output, batch_first=True)\n",
    "            return self.fc(unpacked_output)\n",
    "        else:\n",
    "            out, _ = self.lstm(x)\n",
    "            return self.fc(out)\n",
    "\n",
    "# Criterion\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "\n",
    "\n",
    "def compute_last_token_loss(output, target_seq, criterion):\n",
    "    \"\"\"\n",
    "    Computes cross-entropy loss on the last time step of each sequence.\n",
    "    \n",
    "    Args:\n",
    "        output: Tensor of shape [B, L, vocab_size]\n",
    "        target_seq: Tensor of shape [B, L] containing target class indices\n",
    "    \n",
    "    Returns:\n",
    "        loss: Scalar loss computed only on the last token of each sequence\n",
    "    \"\"\"\n",
    "    # Get last time step for each sequence\n",
    "    last_token_logits = output[:, -1, :]        # [B, vocab_size]\n",
    "    last_token_targets = target_seq[:, -1, :]      # [B]\n",
    "    last_token_targets = torch.argmax(last_token_targets, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "    # print('last_token_logits',last_token_logits.shape)\n",
    "    # print('last_token_targets',last_token_targets.shape)\n",
    "\n",
    "    return criterion(last_token_logits, last_token_targets)\n",
    "\n",
    "# Training function\n",
    "def train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                           device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=True):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-gen-notrans-tb/AMP_LSTM_GEN_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])      # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            \n",
    "            # print('target_shape before reshape',target_seq.shape)\n",
    "            # target_seq = target_seq.reshape(-1)\n",
    "            # print(f\"Output shape: {output.shape}, Target shape: {target_seq.shape}\")\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, acc, auc = evaluate_model_generation(model, val_loader, criterion, device, verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Acc: {acc:.4f}, AUC: {auc}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_generator-notrans-tb.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model_generation(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq = input_seq.to(device)\n",
    "            target_seq = target_seq.to(device)\n",
    "\n",
    "            output = model(input_seq)\n",
    "            # output = output.view(-1, output.shape[-1])\n",
    "            # output = output.reshape(-1, output.shape[-1])  # [B*L, vocab]\n",
    "            # output = torch.argmax(output, dim=-1)  # now shape is [batch_size, seq_len]\n",
    "\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "            # # target_seq = target_seq.view(-1)\n",
    "            # # target_seq = target_seq.reshape(-1)\n",
    "            # # target_seq = target_seq.reshape(-1, target_seq.shape[-1])\n",
    "            # target_seq = torch.argmax(target_seq, dim=-1)  #  now shape is [batch_size, seq_len]\n",
    "\n",
    "            # assert output.size(0) == target_seq.size(0), f\"Mismatch: {output.size(0)} vs {target_seq.size(0)}\"\n",
    "\n",
    "            # loss = criterion(output, target_seq)\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            # print('loss done')\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            \n",
    "            preds = output[:, -1, :]        # shape: [B, vocab_size]\n",
    "            preds = torch.argmax(preds, dim=1)  # shape: [B]\n",
    "\n",
    "            targets = target_seq[:, -1, :]      # shape: [B, vocab_size]\n",
    "            targets = torch.argmax(targets, dim=-1)  # shape: [B]\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Objective for Optuna tuning\n",
    "def objective_generation(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 64, 256)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3)\n",
    "\n",
    "    model = GenerativeLSTM(hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_loss = train_model_generation(model, train_loader, val_loader, num_epochs=10, lr=lr, weight_decay=weight_decay, verbose=False)\n",
    "    return val_loss\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective_generation, n_trials=20)\n",
    "\n",
    "lstm_gen_notrans_tb_best_params = study.best_trial.params\n",
    "print(lstm_gen_notrans_tb_best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1: Train Loss = 3.0548 | Val Loss = 3.0090 | Acc = 0.0270 | AUC = undefined | Perplexity = 20.2662\n",
      "Epoch 2: Train Loss = 2.9965 | Val Loss = 2.9435 | Acc = 0.0270 | AUC = undefined | Perplexity = 18.9822\n",
      "Epoch 3: Train Loss = 2.9277 | Val Loss = 2.8642 | Acc = 0.9730 | AUC = undefined | Perplexity = 17.5345\n",
      "Epoch 4: Train Loss = 2.8628 | Val Loss = 2.7988 | Acc = 1.0000 | AUC = undefined | Perplexity = 16.4257\n",
      "Epoch 5: Train Loss = 2.8049 | Val Loss = 2.7393 | Acc = 1.0000 | AUC = undefined | Perplexity = 15.4760\n",
      "Epoch 6: Train Loss = 2.7479 | Val Loss = 2.6935 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.7829\n",
      "Epoch 7: Train Loss = 2.6925 | Val Loss = 2.6404 | Acc = 0.9730 | AUC = undefined | Perplexity = 14.0191\n",
      "Epoch 8: Train Loss = 2.6369 | Val Loss = 2.5883 | Acc = 0.9730 | AUC = undefined | Perplexity = 13.3075\n",
      "Epoch 9: Train Loss = 2.5930 | Val Loss = 2.5379 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.6535\n",
      "Epoch 10: Train Loss = 2.5679 | Val Loss = 2.4886 | Acc = 0.9730 | AUC = undefined | Perplexity = 12.0445\n",
      "Epoch 11: Train Loss = 2.4979 | Val Loss = 2.4429 | Acc = 0.9730 | AUC = undefined | Perplexity = 11.5069\n",
      "Epoch 12: Train Loss = 2.4344 | Val Loss = 2.3924 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.9399\n",
      "Epoch 13: Train Loss = 2.3910 | Val Loss = 2.3338 | Acc = 0.9730 | AUC = undefined | Perplexity = 10.3175\n",
      "Epoch 14: Train Loss = 2.3210 | Val Loss = 2.2806 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.7830\n",
      "Epoch 15: Train Loss = 2.2695 | Val Loss = 2.2254 | Acc = 0.9730 | AUC = undefined | Perplexity = 9.2571\n",
      "Epoch 16: Train Loss = 2.2189 | Val Loss = 2.1678 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.7391\n",
      "Epoch 17: Train Loss = 2.1710 | Val Loss = 2.1145 | Acc = 1.0000 | AUC = undefined | Perplexity = 8.2853\n",
      "Epoch 18: Train Loss = 2.1090 | Val Loss = 2.0647 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.8831\n",
      "Epoch 19: Train Loss = 2.0967 | Val Loss = 2.0239 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.5675\n",
      "Epoch 20: Train Loss = 2.0232 | Val Loss = 1.9821 | Acc = 0.9730 | AUC = undefined | Perplexity = 7.2579\n",
      "Epoch 21: Train Loss = 1.9692 | Val Loss = 1.9328 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.9090\n",
      "Epoch 22: Train Loss = 1.9208 | Val Loss = 1.8839 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.5791\n",
      "Epoch 23: Train Loss = 1.8725 | Val Loss = 1.8355 | Acc = 0.9730 | AUC = undefined | Perplexity = 6.2683\n",
      "Epoch 24: Train Loss = 1.8243 | Val Loss = 1.7873 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.9734\n",
      "Epoch 25: Train Loss = 1.7727 | Val Loss = 1.7418 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.7077\n",
      "Epoch 26: Train Loss = 1.7375 | Val Loss = 1.7069 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.5119\n",
      "Epoch 27: Train Loss = 1.6830 | Val Loss = 1.6516 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.2151\n",
      "Epoch 28: Train Loss = 1.6418 | Val Loss = 1.6205 | Acc = 0.9730 | AUC = undefined | Perplexity = 5.0554\n",
      "Epoch 29: Train Loss = 1.6265 | Val Loss = 1.5928 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.9173\n",
      "Epoch 30: Train Loss = 1.5590 | Val Loss = 1.5435 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.6811\n",
      "Epoch 31: Train Loss = 1.5096 | Val Loss = 1.4809 | Acc = 1.0000 | AUC = undefined | Perplexity = 4.3969\n",
      "Epoch 32: Train Loss = 1.4711 | Val Loss = 1.4596 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.3044\n",
      "Epoch 33: Train Loss = 1.4701 | Val Loss = 1.4189 | Acc = 0.9730 | AUC = undefined | Perplexity = 4.1324\n",
      "Epoch 34: Train Loss = 1.4025 | Val Loss = 1.3763 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.9602\n",
      "Epoch 35: Train Loss = 1.3666 | Val Loss = 1.3143 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.7222\n",
      "Epoch 36: Train Loss = 1.3193 | Val Loss = 1.2723 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.5691\n",
      "Epoch 37: Train Loss = 1.2764 | Val Loss = 1.2426 | Acc = 1.0000 | AUC = undefined | Perplexity = 3.4647\n",
      "Epoch 38: Train Loss = 1.2628 | Val Loss = 1.2157 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.3726\n",
      "Epoch 39: Train Loss = 1.2075 | Val Loss = 1.2056 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.3388\n",
      "Epoch 40: Train Loss = 1.1708 | Val Loss = 1.1870 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.2773\n",
      "Epoch 41: Train Loss = 1.1270 | Val Loss = 1.1746 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.2369\n",
      "Epoch 42: Train Loss = 1.1500 | Val Loss = 1.1117 | Acc = 0.9730 | AUC = undefined | Perplexity = 3.0397\n",
      "Epoch 43: Train Loss = 1.0546 | Val Loss = 1.0840 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.9566\n",
      "Epoch 44: Train Loss = 1.0816 | Val Loss = 1.0778 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.9382\n",
      "Epoch 45: Train Loss = 1.0346 | Val Loss = 1.0787 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.9408\n",
      "Epoch 46: Train Loss = 1.0063 | Val Loss = 0.9876 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.6849\n",
      "Epoch 47: Train Loss = 1.0096 | Val Loss = 0.9313 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.5379\n",
      "Epoch 48: Train Loss = 1.0302 | Val Loss = 0.9131 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.4920\n",
      "Epoch 49: Train Loss = 0.9184 | Val Loss = 0.8908 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.4371\n",
      "Epoch 50: Train Loss = 0.8861 | Val Loss = 0.8640 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.3727\n",
      "Epoch 51: Train Loss = 0.8515 | Val Loss = 0.8388 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.3136\n",
      "Epoch 52: Train Loss = 0.8409 | Val Loss = 0.8135 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.2557\n",
      "Epoch 53: Train Loss = 0.8157 | Val Loss = 0.7907 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.2050\n",
      "Epoch 54: Train Loss = 0.8092 | Val Loss = 0.7698 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.1594\n",
      "Epoch 55: Train Loss = 0.7674 | Val Loss = 0.7473 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.1114\n",
      "Epoch 56: Train Loss = 0.7351 | Val Loss = 0.7270 | Acc = 0.9730 | AUC = undefined | Perplexity = 2.0688\n",
      "Epoch 57: Train Loss = 0.7307 | Val Loss = 0.7027 | Acc = 1.0000 | AUC = undefined | Perplexity = 2.0193\n",
      "Epoch 58: Train Loss = 0.7172 | Val Loss = 0.6816 | Acc = 1.0000 | AUC = undefined | Perplexity = 1.9771\n",
      "Epoch 59: Train Loss = 0.6898 | Val Loss = 0.6711 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9564\n",
      "Epoch 60: Train Loss = 0.6594 | Val Loss = 0.6509 | Acc = 0.9730 | AUC = undefined | Perplexity = 1.9174\n",
      "\n",
      "✅ Final Test Metrics:\n",
      "Loss = 0.6509, Accuracy = 0.9730, AUC = undefined, Perplexity = 1.9174\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, recall_score, confusion_matrix, roc_auc_score\n",
    "import math\n",
    "\n",
    "# --- Assumes you already have these from your previous steps ---\n",
    "# lstm_gen_best_params\n",
    "# train_loader, val_loader, test_loader\n",
    "# GenerativeLSTM\n",
    "# compute_last_token_loss\n",
    "lstm_gen_notrans_tb_best_params = {'hidden_dim': 149, 'num_layers': 2, 'dropout': 0.4350981856205326, 'lr': 0.009996913527093828, 'weight_decay': 0.00012991309919791892}\n",
    "\n",
    "\n",
    "\n",
    "PAD_IDX = -100\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def train_final_model(model, train_loader, val_loader, num_epochs=20):\n",
    "    model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lstm_gen_notrans_tb_best_params[\"lr\"], weight_decay=lstm_gen_notrans_tb_best_params[\"weight_decay\"])\n",
    "    writer = SummaryWriter(log_dir=f\"runs-lstm-gen-notrans-tb/AMPGen_LSTM_final\")\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for input_seq, target_seq, _ in train_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output = model(input_seq)\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_loss / len(train_loader)\n",
    "        val_loss, acc, auc, perp = evaluate_final_model(model, test_loader)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', auc if auc != \"undefined\" else 0.0, epoch)\n",
    "\n",
    "        print(f\"Epoch {epoch}: Train Loss = {avg_train_loss:.4f} | Val Loss = {val_loss:.4f} | Acc = {acc:.4f} | AUC = {auc} | Perplexity = {perp:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), \"best_model_lstm_generator.pt\")\n",
    "\n",
    "    writer.close()\n",
    "    return model\n",
    "\n",
    "def evaluate_final_model(model, data_loader):\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_seq, target_seq, _ in data_loader:\n",
    "            input_seq, target_seq = input_seq.to(device), target_seq.to(device)\n",
    "            output = model(input_seq)\n",
    "\n",
    "            if isinstance(target_seq, torch.nn.utils.rnn.PackedSequence):\n",
    "                target_seq, _ = pad_packed_sequence(target_seq, batch_first=True)\n",
    "\n",
    "            loss = compute_last_token_loss(output, target_seq, criterion)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            preds = output[:, -1, :]  # [B, vocab]\n",
    "            preds = torch.argmax(preds, dim=1)\n",
    "            targets = target_seq[:, -1, :]\n",
    "            targets = torch.argmax(targets, dim=1)\n",
    "\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    acc = accuracy_score(all_labels, all_preds)\n",
    "    # try:\n",
    "    #     auc = roc_auc_score(\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_labels), num_classes=20),\n",
    "    #         torch.nn.functional.one_hot(torch.tensor(all_preds), num_classes=20),\n",
    "    #         multi_class='ovr', average='macro'\n",
    "    #     )\n",
    "    # except Exception:\n",
    "    auc = \"undefined\"\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    perplexity = math.exp(avg_loss)\n",
    "    return avg_loss, acc, auc, perplexity\n",
    "\n",
    "# --- Build and train final model using best parameters ---\n",
    "\n",
    "final_model = GenerativeLSTM(\n",
    "    input_dim=20,\n",
    "    hidden_dim=lstm_gen_notrans_tb_best_params[\"hidden_dim\"],\n",
    "    num_layers=lstm_gen_notrans_tb_best_params[\"num_layers\"],\n",
    "    dropout=lstm_gen_notrans_tb_best_params[\"dropout\"]\n",
    ")\n",
    "\n",
    "trained_model = train_final_model(final_model, train_loader, val_loader, num_epochs=60)\n",
    "\n",
    "# --- Evaluate on test set ---\n",
    "test_loss, test_acc, test_auc, perp = evaluate_final_model(trained_model, test_loader)\n",
    "print(f\"\\n✅ Final Test Metrics:\\nLoss = {test_loss:.4f}, Accuracy = {test_acc:.4f}, AUC = {test_auc}, Perplexity = {perp:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### batch gen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated AMP sequence: EYPPGGGYPPGYYGGG\n",
      "Generated AMP sequence: AHIHPGHKKSGLPG\n",
      "Generated AMP sequence: KAAYYYHGPKGKGYHPHPPG\n",
      "Generated AMP sequence: IQYYPYGGGHKHYIHPGYGYRGDYPPRGKGYPHGHHPPY\n",
      "Generated AMP sequence: IAPRPRKLKGGGPHLYHKR\n",
      "Generated AMP sequence: FAGGYYPYYGYP\n",
      "Generated AMP sequence: EVHGGKYYDLKGGHYHPPKYYHKHHG\n",
      "Generated AMP sequence: VHAHYHGKYHPRHPKPPPGY\n",
      "Generated AMP sequence: DAAPPGKGPGPKYG\n",
      "Generated AMP sequence: WAYYHPYPNHGYYP\n",
      "Generated AMP sequence: QEPYYRPGGYPFGP\n",
      "Generated AMP sequence: CAKPPGPHHR\n",
      "Generated AMP sequence: ASPLGIHHYKYPYKPKGPHG\n",
      "Generated AMP sequence: DAYYYKGYKYYYHKYHGPYP\n",
      "Generated AMP sequence: HANYGLYQPPHHYYYYLKYP\n",
      "Generated AMP sequence: IRYHKGPGKPHPGGYKGPRG\n",
      "Generated AMP sequence: TAAKLPPYLHYHKGGYYPKG\n",
      "Generated AMP sequence: YAKPPHYPPPKH\n",
      "Generated AMP sequence: APAGGGYPKGKKGKK\n",
      "Generated AMP sequence: VLAHPKYPGGPGPYPKPKPY\n",
      "Generated AMP sequence: HYYGGGGGPGHYGYHG\n",
      "Generated AMP sequence: VLVPSHGRYYKKRHYYKRRG\n",
      "Generated AMP sequence: QHSHYKFYGHYYPPKHHPRYWGKGPKYHGGRPGG\n",
      "Generated AMP sequence: IASPGGYKKYYYRKGRPKYG\n",
      "Generated AMP sequence: RAKGGGPRPYKHYGYGYGYP\n",
      "Generated AMP sequence: WAKYKPPKPPYGHYYKPHGKHPGYYPPHYPGYYYPPKYYG\n",
      "Generated AMP sequence: KGSYGPKYYY\n",
      "Generated AMP sequence: AGGFYYYYPRYYHKGYPHP\n",
      "Generated AMP sequence: GFEPLRLYPPYKGGRPPGGG\n",
      "Generated AMP sequence: QKHKYHLSKPYYGYK\n",
      "Generated AMP sequence: MGYKHPHGGGGGP\n",
      "Generated AMP sequence: KFYKPGGGGHGG\n",
      "Generated AMP sequence: FGGHKPGYPGY\n",
      "Generated AMP sequence: HAGSRGGKKPKKRGHPYLPGKPHHKGHGHGYPPWK\n",
      "Generated AMP sequence: MIMQKGHIPPH\n",
      "Generated AMP sequence: EKGGGHGGHGYGPKYGKYYYHGPYPPHRGHKLHGPHPGK\n",
      "Generated AMP sequence: DAHGGYHYHPRPKKHYYYGP\n",
      "Generated AMP sequence: PAAPPYGWGGGYPPKYKYKKKHRLHYKPYPGYPHGY\n",
      "Generated AMP sequence: EIYRGHHRGPRYHYGYGKYG\n",
      "Generated AMP sequence: NPPPPPHHGYPK\n",
      "Generated AMP sequence: NYFPKPGYYPPYPGPPKPY\n",
      "Generated AMP sequence: YGDYYGYYLPKGHYDHKPYHHQKGKGGKKYKPPGP\n",
      "Generated AMP sequence: KYGKGYKPGRGY\n",
      "Generated AMP sequence: CAAKPGRKLKKP\n",
      "Generated AMP sequence: RAVYKHPKGGPP\n",
      "Generated AMP sequence: VAGPEGWGPH\n",
      "Generated AMP sequence: EAPYYKPGHYG\n",
      "Generated AMP sequence: PAAGPKYYHH\n",
      "Generated AMP sequence: DKGPKPKHYRGYKYYK\n",
      "Generated AMP sequence: VAKPHGYYMG\n",
      "Generated AMP sequence: LAYGPHPPYGGGY\n",
      "Generated AMP sequence: YKMPGYKKLGGQGPHKHHPHPKPYPGGHPGK\n",
      "Generated AMP sequence: NDRGGPKPKHYYPHPKGYGY\n",
      "Generated AMP sequence: WAKYPWPPYHKGYYGPYG\n",
      "Generated AMP sequence: HWAPHYYYKYYHYGPHGGPH\n",
      "Generated AMP sequence: DAHLRHPYPHGYGYPYYYHRKK\n",
      "Generated AMP sequence: CTYRKKPHGHHHHGGYKKPG\n",
      "Generated AMP sequence: IQDKYYKPGYHYPPLHGGGRHGYYYYGKRPGYGGPPP\n",
      "Generated AMP sequence: LTGGYYYYPPG\n",
      "Generated AMP sequence: DFGPKGGPGYKKPYHPP\n",
      "Generated AMP sequence: ITRPYYPYPPPRG\n",
      "Generated AMP sequence: ETAVGHRGYYLDHPYPHPPRGRPYHPYGYPGH\n",
      "Generated AMP sequence: PALHPGYYYRGRGGHKRPKHY\n",
      "Generated AMP sequence: KHGSPGGPPHYGKYKPKH\n",
      "Generated AMP sequence: RGYGGGPKYKYGYRRYRPGGYPYKGGPKGKGPHYHKGGK\n",
      "Generated AMP sequence: NKGPYRGKGHYGKPPPFHNG\n",
      "Generated AMP sequence: GAAKGPPYGGGPPGY\n",
      "Generated AMP sequence: NIPYPYHYGY\n",
      "Generated AMP sequence: NRKPLPGGHGYGYHGWPKPP\n",
      "Generated AMP sequence: HYYPGPGGHKYP\n",
      "Generated AMP sequence: KGHIYGYPKPKYYKHPYGPPPGP\n",
      "Generated AMP sequence: DAGYGYKHYKLIPGKYHPHRGYKHGGPHYKHPPGG\n",
      "Generated AMP sequence: YPPKYYPYGG\n",
      "Generated AMP sequence: GAKYGGKYHKGGY\n",
      "Generated AMP sequence: VAYPYKLKPGGHPGPYGPKYGPYYPKPYPGYPPGPKPG\n",
      "Generated AMP sequence: IYAYKYRPPYKHPYRYKYGP\n",
      "Generated AMP sequence: GEHKPKKHKG\n",
      "Generated AMP sequence: RKPPKKYLKG\n",
      "Generated AMP sequence: PFHPLPRYPYGGEP\n",
      "Generated AMP sequence: KAYGYHGKHHHYPHPPHKGGPGPPGPHYKYPGPY\n",
      "Generated AMP sequence: VRWGGPKYYHHRHYKPGPKP\n",
      "Generated AMP sequence: IPWGGGPPPHKP\n",
      "Generated AMP sequence: MYEPKKPPYGGPPHGGYGHG\n",
      "Generated AMP sequence: CAKGSGPYPPHHPYP\n",
      "Generated AMP sequence: IPKGPYGPGPGKRHPYYHKG\n",
      "Generated AMP sequence: CWKYKPGRGYRPGHHKGYPHKKPMYKYYGYGYKYYYWHGK\n",
      "Generated AMP sequence: MNHYPYGYYKPR\n",
      "Generated AMP sequence: PHHGHGGGYPYK\n",
      "Generated AMP sequence: KHKGPGPPGGRKGY\n",
      "Generated AMP sequence: DPYGGGHGPG\n",
      "Generated AMP sequence: HYSPHPLKGYKGKKYPRGKK\n",
      "Generated AMP sequence: WAQGLGFHYY\n",
      "Generated AMP sequence: MHDLKYGKGHPKKKKSYYTHPKHYGYKPPYPPGPGQLP\n",
      "Generated AMP sequence: HAPGGFYPHH\n",
      "Generated AMP sequence: SQPGGGPRHGGYHRP\n",
      "Generated AMP sequence: PACPRPMGYYHYPPRY\n",
      "Generated AMP sequence: RAHYGKKKPYHGGYPHYYYY\n",
      "Generated AMP sequence: FPKYPRPGPYYG\n",
      "Generated AMP sequence: KAPWGKYPPPLH\n",
      "Generated AMP sequence: FNWPRGKFHPYGKYGPPPGH\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Set global seed for reproducibility\n",
    "SEED = 42\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "\n",
    "# Define the amino acid vocabulary\n",
    "amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "\n",
    "aa_vocab = 'ACDEFGHIKLMNPQRSTVWY'\n",
    "aa_to_idx = {aa: i for i, aa in enumerate(aa_vocab)}\n",
    "idx_to_aa = {i: aa for aa, i in aa_to_idx.items()}\n",
    "\n",
    "def sample_start_amino_acid():\n",
    "    return random.choice(amino_acids)\n",
    "\n",
    "def one_hot_encode_amino_acid(aa, vocab=aa_vocab):\n",
    "    vec = torch.zeros(len(vocab))\n",
    "    vec[aa_to_idx[aa]] = 1.0\n",
    "    return vec\n",
    "\n",
    "\n",
    "def generate_sequence_from_seed(model, seed, max_length=30, temperature=1.0, device='cpu'):\n",
    "    model.eval()\n",
    "    input_seq = [one_hot_encode_amino_acid(aa).to(device) for aa in seed]\n",
    "    input_tensor = torch.stack(input_seq).unsqueeze(0)  # [1, L, 20]\n",
    "\n",
    "    generated = seed.copy()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_length - len(seed)):\n",
    "            output = model(input_tensor)  # [1, L, vocab]\n",
    "            logits = output[0, -1, :]  # Last time step → [vocab]\n",
    "\n",
    "            # Apply temperature and sample\n",
    "            probs = F.softmax(logits / temperature, dim=-1).cpu().numpy()\n",
    "            next_idx = np.random.choice(len(aa_vocab), p=probs)\n",
    "            next_aa = idx_to_aa[next_idx]\n",
    "\n",
    "            # Update sequence\n",
    "            next_aa_vec = one_hot_encode_amino_acid(next_aa).to(device).unsqueeze(0).unsqueeze(0)  # [1, 1, 20]\n",
    "            input_tensor = torch.cat([input_tensor, next_aa_vec], dim=1)\n",
    "            generated.append(next_aa)\n",
    "\n",
    "    return ''.join(generated)\n",
    "\n",
    "\n",
    "class LengthSampler:\n",
    "    def __init__(self, sequence_lengths):\n",
    "        \"\"\"\n",
    "        Initialize sampler from observed sequence lengths.\n",
    "        \n",
    "        Args:\n",
    "            sequence_lengths (list[int]): List of sequence lengths (e.g., [20, 21, 20, 23, ...])\n",
    "        \"\"\"\n",
    "        self.length_counts = Counter(sequence_lengths)\n",
    "        self.lengths = np.array(sorted(self.length_counts.keys()))\n",
    "        counts = np.array([self.length_counts[l] for l in self.lengths])\n",
    "        self.probs = counts / counts.sum()  # Empirical probabilities\n",
    "\n",
    "    def sample(self, n=1):\n",
    "        \"\"\"\n",
    "        Sample one or more lengths based on the learned distribution.\n",
    "\n",
    "        Returns:\n",
    "            np.ndarray of sampled lengths\n",
    "        \"\"\"\n",
    "        return np.random.choice(self.lengths, size=n, p=self.probs)\n",
    "length_sampler = LengthSampler([len(seq) for seq in df.loc[df['AMP'] == 1, :]['Sequences']])\n",
    "\n",
    "# Reload the trained model\n",
    "gen_model = trained_model\n",
    "gen_model.to(device)\n",
    "\n",
    "generated_peptides = []\n",
    "# (Re-run the generation loop to collect sequences)\n",
    "for x in range(100):\n",
    "    sampled_length = length_sampler.sample()[0]\n",
    "    # sampled_length = 20\n",
    "    start_aa = sample_start_amino_acid()\n",
    "    seed_sequence = list(start_aa)\n",
    "    generated_peptide = generate_sequence_from_seed(gen_model, seed_sequence, max_length=sampled_length, temperature=1, device=device)\n",
    "    generated_peptides.append(generated_peptide)\n",
    "    print(\"Generated AMP sequence:\", generated_peptide)\n",
    "\n",
    "# Save all generated sequences into a text file\n",
    "with open(\"generated_peptides-notrans.fasta\", \"w\") as f:\n",
    "    for i, peptide in enumerate(generated_peptides):\n",
    "        f.write(f\">peptide{i}\\n\")\n",
    "        f.write(peptide + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
