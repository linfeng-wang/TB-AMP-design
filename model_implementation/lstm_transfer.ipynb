{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# new-ml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# If using PyTorch\n",
    "import torch\n",
    "\n",
    "# If using TensorFlow\n",
    "\n",
    "# Optional: If using Python hash-based functions\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "\n",
    "# Set seed for base Python random\n",
    "random.seed(42)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set seed for PyTorch (CPU and GPU)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)  # if using multi-GPU\n",
    "\n",
    "# Force deterministic behavior in PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define One-Hot Encoding Function for DNA Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        length = len(seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        return one_hot_torch(seq, dtype=self.one_hot_dtype), torch.tensor(label, dtype=torch.float32), length\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def collate_and_pack(batch):\n",
    "    # batch = list of (tensor_seq, label, length)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "\n",
    "    # Filter out sequences with zero length\n",
    "    filtered_batch = [(seq, lbl, l) for seq, lbl, l in zip(sequences, labels, lengths) if l > 0]\n",
    "\n",
    "    if len(filtered_batch) == 0:\n",
    "        raise ValueError(\"All sequences in the batch have zero length.\")\n",
    "\n",
    "    sequences, labels, lengths = zip(*filtered_batch)\n",
    "\n",
    "    # Convert lengths to tensor\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # Sort by descending length (required by pack_padded_sequence)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    labels = torch.tensor([labels[i] for i in sorted_indices])\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    # Stack to shape: (batch_size, 20, seq_len) and transpose for LSTM input\n",
    "    # LSTM expects input of shape (seq_len, batch_size, features)\n",
    "    sequences = [seq.T for seq in sequences]  # Transpose each [20, L] to [L, 20]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)  # shape: [max_len, batch, features]\n",
    "\n",
    "    # Pack the sequence\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning to TB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'V', 'Q', 'Y', 'A', 'R', 'X', 'F', 'S', 'W', 'M', 'T', 'E', 'N', 'I', 'H', 'C', 'P', 'D', 'L', 'K', 'G'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n",
      "Dataset sizes:\n",
      "Train: 264\n",
      "Validation: 88\n",
      "Test: 88\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8636, AUC: 0.9517\n",
      "Sensitivity: 0.9512, Specificity: 0.7872\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 2 39]]\n",
      "Epoch [1/20] - Train Loss: 0.4783, Val Loss: 0.3454, Val Acc: 0.8636, Val AUC: 0.9517\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9528\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [2/20] - Train Loss: 0.4203, Val Loss: 0.3135, Val Acc: 0.8864, Val AUC: 0.9528\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9554\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [3/20] - Train Loss: 0.4260, Val Loss: 0.2907, Val Acc: 0.8864, Val AUC: 0.9554\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9564\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [4/20] - Train Loss: 0.3498, Val Loss: 0.2766, Val Acc: 0.8977, Val AUC: 0.9564\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9585\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [5/20] - Train Loss: 0.3583, Val Loss: 0.2703, Val Acc: 0.8977, Val AUC: 0.9585\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9595\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [6/20] - Train Loss: 0.3796, Val Loss: 0.2648, Val Acc: 0.8977, Val AUC: 0.9595\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9621\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [7/20] - Train Loss: 0.3735, Val Loss: 0.2627, Val Acc: 0.8977, Val AUC: 0.9621\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9642\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [8/20] - Train Loss: 0.3510, Val Loss: 0.2593, Val Acc: 0.8977, Val AUC: 0.9642\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9673\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [9/20] - Train Loss: 0.3415, Val Loss: 0.2561, Val Acc: 0.9091, Val AUC: 0.9673\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9689\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [10/20] - Train Loss: 0.3203, Val Loss: 0.2564, Val Acc: 0.9091, Val AUC: 0.9689\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9709\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [11/20] - Train Loss: 0.3117, Val Loss: 0.2559, Val Acc: 0.9091, Val AUC: 0.9709\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9715\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [12/20] - Train Loss: 0.3236, Val Loss: 0.2528, Val Acc: 0.9091, Val AUC: 0.9715\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9725\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [13/20] - Train Loss: 0.3397, Val Loss: 0.2498, Val Acc: 0.9091, Val AUC: 0.9725\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9725\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [14/20] - Train Loss: 0.3492, Val Loss: 0.2494, Val Acc: 0.9091, Val AUC: 0.9725\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9730\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [15/20] - Train Loss: 0.3415, Val Loss: 0.2515, Val Acc: 0.8977, Val AUC: 0.9730\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9730\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [16/20] - Train Loss: 0.3010, Val Loss: 0.2518, Val Acc: 0.8864, Val AUC: 0.9730\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9720\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [17/20] - Train Loss: 0.3494, Val Loss: 0.2490, Val Acc: 0.8864, Val AUC: 0.9720\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9715\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [18/20] - Train Loss: 0.3123, Val Loss: 0.2466, Val Acc: 0.8864, Val AUC: 0.9715\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9715\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [19/20] - Train Loss: 0.2973, Val Loss: 0.2453, Val Acc: 0.8864, Val AUC: 0.9715\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9715\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [20/20] - Train Loss: 0.3159, Val Loss: 0.2438, Val Acc: 0.8864, Val AUC: 0.9715\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9678\n",
      "Sensitivity: 0.9268, Specificity: 0.8936\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 3 38]]\n",
      "Test Loss: 0.2450, Test Accuracy: 0.9091, Test AUC: 0.9678\n"
     ]
    }
   ],
   "source": [
    "lstm_best_param = {'hidden_dim': 95, 'num_layers': 1, 'dropout': 0.20287746211724011, 'lr': 0.004396992152527415, 'weight_decay': 3.999214064585909e-05}\n",
    "lstm_frozen_best_param = {'lr': 0.009721760525684369, 'weight_decay': 0.0017251614959305703, 'dropout': 0.273304750162406}\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "def load_partial_weights(model, checkpoint_path, max_layers=None):\n",
    "    \"\"\"\n",
    "    Load up to `max_layers` compatible layers from a checkpoint into the model.\n",
    "    If max_layers is None, load all compatible layers.\n",
    "    \"\"\"\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter compatible layers\n",
    "    compatible_items = [\n",
    "        (k, v) for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    ]\n",
    "\n",
    "    # Limit number of layers to load\n",
    "    if max_layers is not None:\n",
    "        compatible_items = compatible_items[:max_layers]\n",
    "\n",
    "    # Convert list of tuples back to dict\n",
    "    compatible_dict = dict(compatible_items)\n",
    "\n",
    "    # Update model state dict\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-trans-frozen/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss, model, optimizer\n",
    "\n",
    "# Best hyperparameters: {'lr': 0.009940295438316211, 'weight_decay': 1.4383289881186473e-05, 'dropout': 0.22563027249521914}\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=lstm_frozen_best_param['dropout'])\n",
    "# model.load_state_dict(torch.load('best_model_lstm_1.pt')) \n",
    "model = load_partial_weights(model, '/mnt/storageG1/lwang/Projects/TB-AMP-design/model_scripts/weights/best_model_lstm_1.pt')\n",
    "\n",
    "freeze_encoder(model)\n",
    "\n",
    "history, model, optimzer = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_frozen_best_param['lr'],\n",
    "                      weight_decay=lstm_frozen_best_param['weight_decay'], verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_path = \"./best_model_lstm_frozen.pt\"\n",
    "\n",
    "torch.save({\n",
    "    'model_state_dict': model.state_dict(),\n",
    "    'optimizer_state_dict': optimzer.state_dict(),\n",
    "    'best_val_loss': val_loss,\n",
    "    'best_val_acc': val_acc,\n",
    "    'best_val_auc': val_auc,\n",
    "}, save_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
