{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/xgboost/core.py:377: FutureWarning: Your system has an old version of glibc (< 2.28). We will stop supporting Linux distros with glibc older than 2.28 after **May 31, 2025**. Please upgrade to a recent Linux distro (with glibc >= 2.28) to use future versions of XGBoost.\n",
      "Note: You have installed the 'manylinux2014' variant of XGBoost. Certain features such as GPU algorithms or federated learning are not available. To use these features, please upgrade to a recent Linux distro with glibc 2.28+, and install the 'manylinux_2_28' variant.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from icecream import ic\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, confusion_matrix\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# If using PyTorch\n",
    "import torch\n",
    "\n",
    "# If using TensorFlow\n",
    "\n",
    "# Optional: If using Python hash-based functions\n",
    "os.environ[\"PYTHONHASHSEED\"] = \"42\"\n",
    "\n",
    "# Set seed for base Python random\n",
    "random.seed(42)\n",
    "\n",
    "# Set seed for NumPy\n",
    "np.random.seed(42)\n",
    "\n",
    "# Set seed for PyTorch (CPU and GPU)\n",
    "torch.manual_seed(42)\n",
    "torch.cuda.manual_seed(42)\n",
    "torch.cuda.manual_seed_all(42)  # if using multi-GPU\n",
    "\n",
    "# Force deterministic behavior in PyTorch\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manual FASTA parsing (without Biopython)\n",
    "# fasta_path = \"../data//naturalAMPs_APD2024a-ADAM.fasta.txt\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\", index=False)\n",
    "\n",
    "\n",
    "# Manual FASTA parsing (without Biopython)\n",
    "# fasta_path = \"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "# fasta_path = \"../data/uniprotkb_length_5_TO_30_NOT_antimicrob_2025_04_14.fasta (1)\"\n",
    "\n",
    "# # Read lines from the FASTA file\n",
    "# with open(fasta_path, \"r\") as f:\n",
    "#     lines = f.read().strip().splitlines()\n",
    "\n",
    "# # Parse into (ID, Sequence) pairs\n",
    "# records = []\n",
    "# current_id = None\n",
    "# current_seq = []\n",
    "# for line in lines:\n",
    "#     if line.startswith(\">\"):\n",
    "#         if current_id is not None:\n",
    "#             records.append([current_id, ''.join(current_seq)])\n",
    "#         current_id = line[1:].strip()\n",
    "#         current_seq = []\n",
    "#     else:\n",
    "#         current_seq.append(line.strip())\n",
    "\n",
    "# # Add the last record\n",
    "# if current_id is not None:\n",
    "#     records.append([current_id, ''.join(current_seq)])\n",
    "\n",
    "# # Convert to DataFrame\n",
    "# df_fasta_manual = pd.DataFrame(records, columns=[\"Peptide ID\", \"Sequence\"])\n",
    "# df_fasta_manual.to_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from Bio import SeqIO\n",
    "\n",
    "# def read_fasta_sequences(fasta_path):\n",
    "#     \"\"\"\n",
    "#     Reads sequences from a FASTA file and returns them as a list of strings.\n",
    "\n",
    "#     Args:\n",
    "#         fasta_path (str): Path to the FASTA file.\n",
    "\n",
    "#     Returns:\n",
    "#         List[str]: A list of amino acid or nucleotide sequences as strings.\n",
    "#     \"\"\"\n",
    "#     sequences = [str(record.seq) for record in SeqIO.parse(fasta_path, \"fasta\")]\n",
    "#     return sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dbbasp = pd.read_csv(\"../models1/database_check/DBAASP_peptides.csv\")\n",
    "dbbasp = dbbasp[dbbasp[\"SEQUENCE\"].str.len() >= 10]\n",
    "dbbasp = dbbasp[~dbbasp[\"TARGET GROUP\"].str.contains(\"Fungus\", na=False)]\n",
    "dbbasp = dbbasp[[\"ID\", \"SEQUENCE\"]]\n",
    "dbbasp.columns = [\"Peptide ID\", \"Sequence\"]\n",
    "adam_df = pd.read_csv(\"../data/naturalAMPs_APD2024a-ADAM.csv\")\n",
    "adam_df = pd.concat([adam_df, dbbasp], ignore_index=True)\n",
    "uniprot_df = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta.csv\")\n",
    "uniprot_df1 = pd.read_csv(\"../data/uniprotkb_length_10_TO_80_NOT_antimicro_2025_04_14.fasta1.csv\")\n",
    "uniprot_df2 = pd.read_csv(\"/mnt/storageG1/lwang/Projects/TB-AMP-design/data/uniprotkb_length_5_TO_30_NOT_antimicrob_2025_04_23 (1).tsv\", sep=\"\\t\")\n",
    "uniprot_df2\n",
    "uniprot_df2 = uniprot_df2[['Entry', 'Sequence']]\n",
    "uniprot_df2.columns = [\"Peptide ID\", \"Sequence\"]\n",
    "uniprot_df = pd.concat([uniprot_df, uniprot_df1, uniprot_df2], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Peptide ID</th>\n",
       "      <th>Sequence</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>tr|A0A009HCC0|A0A009HCC0_9GAMM Acyl carrier pr...</td>\n",
       "      <td>MSDIEQRVKQAVAEQLGLKAEEIKNEASFMDDLGADSLDLVELVMS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>tr|A0A009YF97|A0A009YF97_9GAMM Acyl carrier pr...</td>\n",
       "      <td>MSDIEQRVKQAVAEQLGMKVEEIKNEASFMDDLGADSLDLVELVMS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>tr|A0A010SZ96|A0A010SZ96_PSEFL Acyl carrier pr...</td>\n",
       "      <td>MSTIEERVKKIVAEQLGVKEEEVVNTASFVEDLGADSLDTVELVMA...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>tr|A0A011NAB1|A0A011NAB1_9PAST Acyl carrier pr...</td>\n",
       "      <td>MSIEERVKKIIVDQLGAKAEDVKPEASFIEDLGADSLDTVELVMAL...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>tr|A0A011NTH9|A0A011NTH9_9PROT DNA-directed RN...</td>\n",
       "      <td>MARVTVDDCLTRIPNRFQMTLAATYRARQITAGASPLIDANRDKPT...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104968</th>\n",
       "      <td>P9WEJ1</td>\n",
       "      <td>MPEQKANCSPNGNITVDSMIMSLGSS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105566</th>\n",
       "      <td>Q6LEB3</td>\n",
       "      <td>LFNKYISRPRRVELAVMLNLTERHIKI</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105568</th>\n",
       "      <td>Q6QLL8</td>\n",
       "      <td>VKGFSFKYGNGVWIGRTKSTNSRSGFQ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>105789</th>\n",
       "      <td>Q9DTV7</td>\n",
       "      <td>HFPGFGQSLLFGYPVYVFGDCVQGDWCR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106056</th>\n",
       "      <td>T0YYA3</td>\n",
       "      <td>EPGLPESFNVLIKEMQSLALDIELLKTREK</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38787 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Peptide ID  \\\n",
       "0       tr|A0A009HCC0|A0A009HCC0_9GAMM Acyl carrier pr...   \n",
       "6       tr|A0A009YF97|A0A009YF97_9GAMM Acyl carrier pr...   \n",
       "8       tr|A0A010SZ96|A0A010SZ96_PSEFL Acyl carrier pr...   \n",
       "9       tr|A0A011NAB1|A0A011NAB1_9PAST Acyl carrier pr...   \n",
       "10      tr|A0A011NTH9|A0A011NTH9_9PROT DNA-directed RN...   \n",
       "...                                                   ...   \n",
       "104968                                             P9WEJ1   \n",
       "105566                                             Q6LEB3   \n",
       "105568                                             Q6QLL8   \n",
       "105789                                             Q9DTV7   \n",
       "106056                                             T0YYA3   \n",
       "\n",
       "                                                 Sequence  \n",
       "0       MSDIEQRVKQAVAEQLGLKAEEIKNEASFMDDLGADSLDLVELVMS...  \n",
       "6       MSDIEQRVKQAVAEQLGMKVEEIKNEASFMDDLGADSLDLVELVMS...  \n",
       "8       MSTIEERVKKIVAEQLGVKEEEVVNTASFVEDLGADSLDTVELVMA...  \n",
       "9       MSIEERVKKIIVDQLGAKAEDVKPEASFIEDLGADSLDTVELVMAL...  \n",
       "10      MARVTVDDCLTRIPNRFQMTLAATYRARQITAGASPLIDANRDKPT...  \n",
       "...                                                   ...  \n",
       "104968                         MPEQKANCSPNGNITVDSMIMSLGSS  \n",
       "105566                        LFNKYISRPRRVELAVMLNLTERHIKI  \n",
       "105568                        VKGFSFKYGNGVWIGRTKSTNSRSGFQ  \n",
       "105789                       HFPGFGQSLLFGYPVYVFGDCVQGDWCR  \n",
       "106056                     EPGLPESFNVLIKEMQSLALDIELLKTREK  \n",
       "\n",
       "[38787 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniprot_df = uniprot_df[uniprot_df['Sequence'].apply(lambda s: isinstance(s, str) and len(s) >= 10)]\n",
    "uniprot_df = uniprot_df.drop_duplicates(subset=\"Sequence\")\n",
    "uniprot_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Remove rows where the Sequence is not a string or is shorter than 10 characters\n",
    "adam_df = adam_df[adam_df['Sequence'].apply(lambda x: isinstance(x, str) and len(x) >= 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190 10\n",
      "Range of sequence lengths: 180\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean sequence length: 23.215054249547922\n",
      "Median sequence length: 19.0\n",
      "Standard deviation of sequence lengths: 15.467236594869847\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Calculate sequence lengths\n",
    "sequence_lengths = adam_df['Sequence'].apply(len)\n",
    "\n",
    "# Calculate the range of sequence lengths\n",
    "length_range = sequence_lengths.max() - sequence_lengths.min()\n",
    "print(sequence_lengths.max(),sequence_lengths.min())\n",
    "print(f\"Range of sequence lengths: {length_range}\")\n",
    "\n",
    "# Draw a box plot\n",
    "plt.boxplot(sequence_lengths)\n",
    "plt.title(\"Box Plot of Sequence Lengths\")\n",
    "plt.ylabel(\"Sequence Length\")\n",
    "plt.show()\n",
    "\n",
    "# Calculate and display distribution statistics\n",
    "mean_length = sequence_lengths.mean()\n",
    "median_length = sequence_lengths.median()\n",
    "std_dev_length = sequence_lengths.std()\n",
    "\n",
    "print(f\"Mean sequence length: {mean_length}\")\n",
    "print(f\"Median sequence length: {median_length}\")\n",
    "print(f\"Standard deviation of sequence lengths: {std_dev_length}\")\n",
    "\n",
    "# adam_df = adam_df.drop(columns=['Sequence Length'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_58668/679251586.py:11: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_uniprot_df = uniprot_df.groupby('Sequence Length', group_keys=False).apply(\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Peptide ID  \\\n",
      "0      sp|C0HJE2|AMYG_BACLI Glucoamylase (Fragment) O...   \n",
      "1      sp|Q03367|PSBF_CAPAN Cytochrome b559 subunit b...   \n",
      "2      sp|Q10997|SPI_HALRO Serine proteinase inhibito...   \n",
      "3      sp|P85962|RLA2_PSEMZ Large ribosomal subunit p...   \n",
      "4      sp|P02728|GLEM_HUMAN Erythrocyte membrane glyc...   \n",
      "...                                                  ...   \n",
      "15376  tr|A0A497J933|A0A497J933_9ARCH DNA-directed RN...   \n",
      "15377  tr|Q6Q0P4|Q6Q0P4_POLCT Cytochrome c oxidase su...   \n",
      "15378  tr|A0A2J4G6H0|A0A2J4G6H0_9CREN DNA-directed RN...   \n",
      "15379  tr|A0A327J0Z7|A0A327J0Z7_9BACT ATP synthase su...   \n",
      "15380  tr|A0A0G2F2K3|A0A0G2F2K3_PHACM U6 snRNA-associ...   \n",
      "\n",
      "                                                Sequence  \n",
      "0                                             SSNKLTTSWG  \n",
      "1                                             SISAMQFIQR  \n",
      "2                                             TKKDGEEKVA  \n",
      "3                                             DITEVIAAGR  \n",
      "4                                             CEGHSHDHGA  \n",
      "...                                                  ...  \n",
      "15376  MPEEFKVGRHVLVPKHEILPKEKVEEVLKRYKIQPYHLPLIKSSDP...  \n",
      "15377  IKAFSWMATIYGSKLNFSPTMMWSMGFIFLFTMGGLTGIILSNSSL...  \n",
      "15378  MPRKINVLEHELVPKHILLTREEAKKILKLMGLRKSELPWIYATDP...  \n",
      "15379  MADVKTISDLAQLGAGVAIGFGAVGAGLGIGIATKGLLESISRQPE...  \n",
      "15380  MENGAPSEGKDPAAFLGEIIGVPVTVKLNSGVVYKGELQSVDGYMN...  \n",
      "\n",
      "[15381 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "# Calculate sequence lengths for uniprot_df\n",
    "uniprot_df['Sequence Length'] = uniprot_df['Sequence'].apply(len)\n",
    "\n",
    "# Calculate sequence lengths for adam_df\n",
    "adam_df['Sequence Length'] = adam_df['Sequence'].apply(len)\n",
    "\n",
    "# Perform stratified sampling to select more samples\n",
    "default_min_samples = 30\n",
    "\n",
    "# Perform lenient stratified sampling\n",
    "sampled_uniprot_df = uniprot_df.groupby('Sequence Length', group_keys=False).apply(\n",
    "    lambda x: x.sample(\n",
    "        n=min(\n",
    "            len(x), \n",
    "            int(14 * adam_df['Sequence Length'].value_counts().get(x.name, default_min_samples))\n",
    "        ),\n",
    "        random_state=42\n",
    "    )\n",
    ").reset_index(drop=True)\n",
    "# Drop the 'Sequence Length' column after sampling\n",
    "sampled_uniprot_df = sampled_uniprot_df.drop(columns=['Sequence Length'])\n",
    "adam_df = adam_df.drop(columns=['Sequence Length'])\n",
    "\n",
    "# Draw a box plot to visualize the distribution\n",
    "plt.boxplot(sampled_uniprot_df['Sequence'].apply(len))\n",
    "plt.title(\"Box Plot of Sequence Lengths (Sampled Negatives)\")\n",
    "plt.ylabel(\"Sequence Length\")\n",
    "plt.show()\n",
    "\n",
    "print(sampled_uniprot_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data organization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "190\n",
      "{'f', 'r', 'l', '9', ' ', 'h', 's', 'q', 'E', 'X', 'B', 'H', 'y', 'd', 'M', 'P', 'T', 'i', 'p', 'N', 'U', 'V', 'x', 'e', 'G', 'L', 'A', 'm', 'K', 'v', 'n', 'F', 'Q', 'Y', 'C', 'a', 'O', 'Z', 'c', 'w', 'W', 't', 'S', 'I', 'k', 'R', 'g', 'D'}\n",
      "48\n",
      "{'f', 'r', 'l', '9', ' ', 'h', 's', 'q', 'B', 'y', 'd', 'i', 'p', 'U', 'x', 'e', 'm', 'v', 'n', 'Z', 'a', 'O', 'c', 'w', 't', 'k', 'g'}\n",
      "Number of sequences after filtering: 30987\n",
      "label\n",
      "1    15839\n",
      "0    15148\n",
      "Name: count, dtype: int64\n",
      "{'E', 'X', 'H', 'M', 'P', 'T', 'N', 'V', 'G', 'L', 'A', 'K', 'F', 'Q', 'Y', 'C', 'W', 'S', 'I', 'R', 'D'}\n",
      "21\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "adam_df['label'] = 1\n",
    "sampled_uniprot_df['label'] = 0\n",
    "adam_df.columns = [\"Peptide ID\", \"Sequences\", 'label']\n",
    "sampled_uniprot_df.columns = [\"Peptide ID\", \"Sequences\" , 'label']\n",
    "df = pd.concat([adam_df, sampled_uniprot_df], ignore_index=True)\n",
    "\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWYX\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "\n",
    "# Filter out sequences containing non-standard amino acids\n",
    "df = df[~df['Sequences'].str.contains('|'.join(non_standard_amino_acids))]\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "tb_df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df[~df['Sequences'].isin(tb_df['Sequences'])]\n",
    "df = df[df['Sequences'].apply(lambda x: isinstance(x, str) and len(x) >= 10)]\n",
    "print(f\"Number of sequences after filtering: {len(df)}\")\n",
    "print(df['label'].value_counts())\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWYX\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"label\"]\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.3, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "# lengths = [len(seq) for seq in df['Sequences']]\n",
    "# print(lengths[lengths <= 0])\n",
    "\n",
    "lengths = np.array([len(seq) for seq in df['Sequences']])\n",
    "print(lengths[lengths <= 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define One-Hot Encoding Function for DNA Sequences in PyTorch\n",
    "def one_hot_torch(seq: str, dtype=torch.float32):\n",
    "    amino_acids = \"ACDEFGHIKLMNPQRSTVWY\"\n",
    "    seq_bytes = torch.ByteTensor(list(bytes(seq, \"utf-8\")))\n",
    "    aa_bytes = torch.ByteTensor(list(bytes(amino_acids, \"utf-8\")))\n",
    "    arr = torch.zeros(len(amino_acids), len(seq_bytes), dtype=dtype)\n",
    "    for i, aa in enumerate(aa_bytes):\n",
    "        arr[i, seq_bytes == aa] = 1\n",
    "    return arr\n",
    "\n",
    "\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, sequences, labels, one_hot_dtype=torch.float32):\n",
    "        self.sequences = sequences\n",
    "        self.labels = labels\n",
    "        self.one_hot_dtype = one_hot_dtype\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        seq = self.sequences.iloc[idx]\n",
    "        label = self.labels.iloc[idx]\n",
    "        length = len(seq.replace(\"X\", \"\"))  # unpadded length\n",
    "        return one_hot_torch(seq, dtype=self.one_hot_dtype), torch.tensor(label, dtype=torch.float32), length\n",
    "\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence\n",
    "\n",
    "def collate_and_pack(batch):\n",
    "    # batch = list of (tensor_seq, label, length)\n",
    "    sequences, labels, lengths = zip(*batch)\n",
    "\n",
    "    # Filter out sequences with zero length\n",
    "    filtered_batch = [(seq, lbl, l) for seq, lbl, l in zip(sequences, labels, lengths) if l > 0]\n",
    "\n",
    "    if len(filtered_batch) == 0:\n",
    "        raise ValueError(\"All sequences in the batch have zero length.\")\n",
    "\n",
    "    sequences, labels, lengths = zip(*filtered_batch)\n",
    "\n",
    "    # Convert lengths to tensor\n",
    "    lengths = torch.tensor(lengths)\n",
    "\n",
    "    # Sort by descending length (required by pack_padded_sequence)\n",
    "    sorted_indices = torch.argsort(lengths, descending=True)\n",
    "    sequences = [sequences[i] for i in sorted_indices]\n",
    "    labels = torch.tensor([labels[i] for i in sorted_indices])\n",
    "    lengths = lengths[sorted_indices]\n",
    "\n",
    "    # Stack to shape: (batch_size, 20, seq_len) and transpose for LSTM input\n",
    "    # LSTM expects input of shape (seq_len, batch_size, features)\n",
    "    sequences = [seq.T for seq in sequences]  # Transpose each [20, L] to [L, 20]\n",
    "    padded_seqs = pad_sequence(sequences, batch_first=False)  # shape: [max_len, batch, features]\n",
    "\n",
    "    # Pack the sequence\n",
    "    packed_input = pack_padded_sequence(padded_seqs, lengths.cpu(), batch_first=False)\n",
    "\n",
    "    return packed_input, labels\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes:\n",
      "Train: 21690\n",
      "Validation: 4649\n",
      "Test: 4648\n"
     ]
    }
   ],
   "source": [
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 64\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "\n",
    "        # Classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        # packed_input: PackedSequence\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # hn: [num_layers, batch_size, hidden_dim]\n",
    "        # We'll use the **last layer's** hidden state as feature\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(last_hidden)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function _get_module_lock.<locals>.cb at 0x7f99e0e175b0>\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen importlib._bootstrap>\", line 198, in cb\n",
      "KeyboardInterrupt: \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import datetime\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "    # Set up TensorBoard writer\n",
    "    log_dir = f\"runs/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Run evaluation\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device)\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - Train Loss: {avg_train_loss:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}, Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return history\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # handle corner cases\n",
    "\n",
    "    # Sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        # Print metrics\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "model = LSTMClassifier(hidden_dim=64)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### adding in regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Confusion Matrix:\n",
      "[[401 160]\n",
      " [ 89 407]]\n",
      "Sensitivity (Recall for Positive Class): 0.8206\n",
      "Specificity (Recall for Negative Class): 0.7148\n",
      "Epoch [1/10] - Train Loss: 0.6630, Val Loss: 0.5471, Val Acc: 0.7644, Val AUC: 0.8299\n",
      "\n",
      "Confusion Matrix:\n",
      "[[460 101]\n",
      " [101 395]]\n",
      "Sensitivity (Recall for Positive Class): 0.7964\n",
      "Specificity (Recall for Negative Class): 0.8200\n",
      "Epoch [2/10] - Train Loss: 0.5161, Val Loss: 0.4849, Val Acc: 0.8089, Val AUC: 0.8625\n",
      "\n",
      "Confusion Matrix:\n",
      "[[192 369]\n",
      " [ 29 467]]\n",
      "Sensitivity (Recall for Positive Class): 0.9415\n",
      "Specificity (Recall for Negative Class): 0.3422\n",
      "Epoch [3/10] - Train Loss: 0.6564, Val Loss: 0.6466, Val Acc: 0.6235, Val AUC: 0.8047\n",
      "\n",
      "Confusion Matrix:\n",
      "[[424 137]\n",
      " [119 377]]\n",
      "Sensitivity (Recall for Positive Class): 0.7601\n",
      "Specificity (Recall for Negative Class): 0.7558\n",
      "Epoch [4/10] - Train Loss: 0.5960, Val Loss: 0.5372, Val Acc: 0.7578, Val AUC: 0.8364\n",
      "\n",
      "Confusion Matrix:\n",
      "[[458 103]\n",
      " [ 77 419]]\n",
      "Sensitivity (Recall for Positive Class): 0.8448\n",
      "Specificity (Recall for Negative Class): 0.8164\n",
      "Epoch [5/10] - Train Loss: 0.4724, Val Loss: 0.3893, Val Acc: 0.8297, Val AUC: 0.9082\n",
      "\n",
      "Confusion Matrix:\n",
      "[[515  46]\n",
      " [121 375]]\n",
      "Sensitivity (Recall for Positive Class): 0.7560\n",
      "Specificity (Recall for Negative Class): 0.9180\n",
      "Epoch [6/10] - Train Loss: 0.3992, Val Loss: 0.3821, Val Acc: 0.8420, Val AUC: 0.9220\n",
      "\n",
      "Confusion Matrix:\n",
      "[[476  85]\n",
      " [ 67 429]]\n",
      "Sensitivity (Recall for Positive Class): 0.8649\n",
      "Specificity (Recall for Negative Class): 0.8485\n",
      "Epoch [7/10] - Train Loss: 0.3674, Val Loss: 0.3593, Val Acc: 0.8562, Val AUC: 0.9305\n",
      "\n",
      "Confusion Matrix:\n",
      "[[512  49]\n",
      " [ 91 405]]\n",
      "Sensitivity (Recall for Positive Class): 0.8165\n",
      "Specificity (Recall for Negative Class): 0.9127\n",
      "Epoch [8/10] - Train Loss: 0.3570, Val Loss: 0.3330, Val Acc: 0.8675, Val AUC: 0.9375\n",
      "\n",
      "Confusion Matrix:\n",
      "[[451 110]\n",
      " [ 55 441]]\n",
      "Sensitivity (Recall for Positive Class): 0.8891\n",
      "Specificity (Recall for Negative Class): 0.8039\n",
      "Epoch [9/10] - Train Loss: 0.4099, Val Loss: 0.3379, Val Acc: 0.8439, Val AUC: 0.9359\n",
      "\n",
      "Confusion Matrix:\n",
      "[[464  97]\n",
      " [ 43 453]]\n",
      "Sensitivity (Recall for Positive Class): 0.9133\n",
      "Specificity (Recall for Negative Class): 0.8271\n",
      "Epoch [10/10] - Train Loss: 0.3665, Val Loss: 0.3215, Val Acc: 0.8675, Val AUC: 0.9421\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)  # L2 regularization\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    history = {'train_loss': [], 'val_loss': [], 'val_acc': [], 'val_auc': []}\n",
    "\n",
    "    # Set up TensorBoard writer\n",
    "    log_dir = f\"runs/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)   \n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "\n",
    "        # Run evaluation\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        # Logging\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "              f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "              f\"Val Loss: {val_loss:.4f}, \"\n",
    "              f\"Val Acc: {val_acc:.4f}, \"\n",
    "              f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        # Save to history\n",
    "        history['train_loss'].append(avg_train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['val_acc'].append(val_acc)\n",
    "        history['val_auc'].append(val_auc)\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return history\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix, recall_score\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "\n",
    "    # Convert predicted probabilities to binary predictions\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "\n",
    "    # Metrics\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)  # handle corner cases\n",
    "\n",
    "    # Sensitivity and specificity\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        # Print metrics\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity (Recall for Positive Class): {sensitivity:.4f}\")\n",
    "        print(f\"Specificity (Recall for Negative Class): {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=64, dropout=0.5)\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3,\n",
    "                      weight_decay=1e-4, verbose=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling on general AMP data (bayesian optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/storageG1/lwang/miniconda3/envs/new-ml/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[I 2025-04-29 17:33:07,537] A new study created in memory with name: no-name-f870dc5a-cf33-4209-8d39-e30a7224b46c\n",
      "[I 2025-04-29 17:41:07,026] Trial 0 finished with value: 0.2616956039038423 and parameters: {'hidden_dim': 92, 'num_layers': 3, 'dropout': 0.14970413746911349, 'lr': 0.008962204396598826, 'weight_decay': 0.00038043632818235694}. Best is trial 0 with value: 0.2616956039038423.\n",
      "[I 2025-04-29 17:47:32,932] Trial 1 finished with value: 0.6929274160568029 and parameters: {'hidden_dim': 35, 'num_layers': 2, 'dropout': 0.1466157950111399, 'lr': 0.0028233687762241493, 'weight_decay': 0.009838017482456001}. Best is trial 0 with value: 0.2616956039038423.\n",
      "[I 2025-04-29 17:54:50,980] Trial 2 finished with value: 0.6929280276167883 and parameters: {'hidden_dim': 89, 'num_layers': 3, 'dropout': 0.2837499055270248, 'lr': 0.003322909650905964, 'weight_decay': 0.0044321580575719735}. Best is trial 0 with value: 0.2616956039038423.\n",
      "[I 2025-04-29 18:01:09,237] Trial 3 finished with value: 0.6929274364693524 and parameters: {'hidden_dim': 61, 'num_layers': 2, 'dropout': 0.355295023141857, 'lr': 0.0007916449842901848, 'weight_decay': 0.006446888698270418}. Best is trial 0 with value: 0.2616956039038423.\n",
      "[I 2025-04-29 18:07:33,907] Trial 4 finished with value: 0.1959346202138352 and parameters: {'hidden_dim': 119, 'num_layers': 2, 'dropout': 0.2624542496440573, 'lr': 0.005976578445700756, 'weight_decay': 0.001564444294004443}. Best is trial 4 with value: 0.1959346202138352.\n",
      "[I 2025-04-29 18:14:28,667] Trial 5 finished with value: 0.6929274454508743 and parameters: {'hidden_dim': 50, 'num_layers': 3, 'dropout': 0.35250986831080566, 'lr': 0.0014380914350676371, 'weight_decay': 0.00996108798357692}. Best is trial 4 with value: 0.1959346202138352.\n",
      "[I 2025-04-29 18:20:14,266] Trial 6 finished with value: 0.6929270723094679 and parameters: {'hidden_dim': 36, 'num_layers': 2, 'dropout': 0.4351315835189037, 'lr': 0.0021892055578405668, 'weight_decay': 0.007758985267440443}. Best is trial 4 with value: 0.1959346202138352.\n",
      "[I 2025-04-29 18:25:28,400] Trial 7 finished with value: 0.1739710488752143 and parameters: {'hidden_dim': 120, 'num_layers': 1, 'dropout': 0.37149755794253203, 'lr': 0.0015509477108869038, 'weight_decay': 0.0026433881680213945}. Best is trial 7 with value: 0.1739710488752143.\n",
      "[I 2025-04-29 18:31:17,280] Trial 8 finished with value: 0.19823517701397203 and parameters: {'hidden_dim': 37, 'num_layers': 2, 'dropout': 0.2764350026098326, 'lr': 0.008575115873108516, 'weight_decay': 0.0019693405056333945}. Best is trial 7 with value: 0.1739710488752143.\n",
      "[I 2025-04-29 18:36:45,917] Trial 9 finished with value: 0.261588233178609 and parameters: {'hidden_dim': 65, 'num_layers': 2, 'dropout': 0.28039108215210184, 'lr': 0.003400715335111028, 'weight_decay': 0.0013026271040616638}. Best is trial 7 with value: 0.1739710488752143.\n",
      "[I 2025-04-29 18:41:28,869] Trial 10 finished with value: 0.19341036478338175 and parameters: {'hidden_dim': 126, 'num_layers': 1, 'dropout': 0.49504223137648073, 'lr': 0.00592035881596013, 'weight_decay': 0.0038818374623931532}. Best is trial 7 with value: 0.1739710488752143.\n",
      "[I 2025-04-29 18:46:12,606] Trial 11 finished with value: 0.19135818604941238 and parameters: {'hidden_dim': 126, 'num_layers': 1, 'dropout': 0.49944018022142905, 'lr': 0.005198441861385667, 'weight_decay': 0.0037255496637640497}. Best is trial 7 with value: 0.1739710488752143.\n",
      "[I 2025-04-29 18:51:01,539] Trial 12 finished with value: 0.20149985017025307 and parameters: {'hidden_dim': 108, 'num_layers': 1, 'dropout': 0.48936822249310075, 'lr': 0.004773712025085829, 'weight_decay': 0.003189557026409896}. Best is trial 7 with value: 0.1739710488752143.\n",
      "[I 2025-04-29 18:55:50,523] Trial 13 finished with value: 0.2097585340068765 and parameters: {'hidden_dim': 106, 'num_layers': 1, 'dropout': 0.4156986560076826, 'lr': 0.007239579837251009, 'weight_decay': 0.005770239061156732}. Best is trial 7 with value: 0.1739710488752143.\n",
      "[I 2025-04-29 19:00:44,257] Trial 14 finished with value: 0.18357293062830624 and parameters: {'hidden_dim': 128, 'num_layers': 1, 'dropout': 0.40580644753427075, 'lr': 0.0044445499412075805, 'weight_decay': 0.002812472745262103}. Best is trial 7 with value: 0.1739710488752143.\n",
      "[I 2025-04-29 19:05:37,022] Trial 15 finished with value: 0.18740518743248835 and parameters: {'hidden_dim': 109, 'num_layers': 1, 'dropout': 0.3599370729427809, 'lr': 0.0005862481667163288, 'weight_decay': 0.0024660949218127974}. Best is trial 7 with value: 0.1739710488752143.\n",
      "[I 2025-04-29 19:10:27,697] Trial 16 finished with value: 0.10087734175054994 and parameters: {'hidden_dim': 95, 'num_layers': 1, 'dropout': 0.20287746211724011, 'lr': 0.004396992152527415, 'weight_decay': 3.999214064585909e-05}. Best is trial 16 with value: 0.10087734175054994.\n",
      "[I 2025-04-29 19:15:18,948] Trial 17 finished with value: 0.13763968335234955 and parameters: {'hidden_dim': 79, 'num_layers': 1, 'dropout': 0.19153539856732932, 'lr': 0.007125395270128784, 'weight_decay': 0.0001113248541240679}. Best is trial 16 with value: 0.10087734175054994.\n",
      "[I 2025-04-29 19:20:05,472] Trial 18 finished with value: 0.124394990503788 and parameters: {'hidden_dim': 74, 'num_layers': 1, 'dropout': 0.19855024013743808, 'lr': 0.007506212303458819, 'weight_decay': 0.0001590081088864142}. Best is trial 16 with value: 0.10087734175054994.\n",
      "[I 2025-04-29 19:24:54,676] Trial 19 finished with value: 0.1738811257257037 and parameters: {'hidden_dim': 77, 'num_layers': 1, 'dropout': 0.22265542342749226, 'lr': 0.007905198042231255, 'weight_decay': 0.0009000087961773865}. Best is trial 16 with value: 0.10087734175054994.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 95, 'num_layers': 1, 'dropout': 0.20287746211724011, 'lr': 0.004396992152527415, 'weight_decay': 3.999214064585909e-05}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "        last_hidden = hn[-1]\n",
    "        dropped = self.dropout(last_hidden)\n",
    "        out = self.fc(dropped)\n",
    "        out = self.sigmoid(out).squeeze(1)\n",
    "        return out\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    # if not train:\n",
    "    #     model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm_.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "    val_auc = train_model(model, train_loader, val_loader, num_epochs=20, lr=lr,\n",
    "                          weight_decay=weight_decay, verbose=False, train=True)\n",
    "    return val_auc\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_best_param = study.best_trial.params\n",
    "\n",
    "torch.onnx.export(model, input, \"lstm.onnx\", verbose=False, \n",
    "                 input_names=[\"input_names\"], output_names=[\"output_names\"],\n",
    "                 export_params=True,)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'W', 'V', 'E', 'Q', 'A', 'L', 'P', 'K', 'Y', 'G', 'S', 'D', 'T', 'X', 'H', 'C', 'I', 'N', 'R', 'M', 'F'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n",
      "Dataset sizes:\n",
      "Train: 21690\n",
      "Validation: 4649\n",
      "Test: 88\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Define DataLoaders\n",
    "\n",
    "\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x7fd15c5b3ee0>"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7806, AUC: 0.8455\n",
      "Sensitivity: 0.8224, Specificity: 0.7369\n",
      "Confusion Matrix:\n",
      "[[1675  598]\n",
      " [ 422 1954]]\n",
      "Epoch [1/20] - Train Loss: 0.5551, Val Loss: 0.5944, Val Acc: 0.7806, Val AUC: 0.8455\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7544, AUC: 0.9358\n",
      "Sensitivity: 0.9482, Specificity: 0.5517\n",
      "Confusion Matrix:\n",
      "[[1254 1019]\n",
      " [ 123 2253]]\n",
      "Epoch [2/20] - Train Loss: 0.5365, Val Loss: 0.4620, Val Acc: 0.7544, Val AUC: 0.9358\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9174, AUC: 0.9715\n",
      "Sensitivity: 0.9457, Specificity: 0.8878\n",
      "Confusion Matrix:\n",
      "[[2018  255]\n",
      " [ 129 2247]]\n",
      "Epoch [3/20] - Train Loss: 0.2528, Val Loss: 0.2414, Val Acc: 0.9174, Val AUC: 0.9715\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9215, AUC: 0.9761\n",
      "Sensitivity: 0.9146, Specificity: 0.9287\n",
      "Confusion Matrix:\n",
      "[[2111  162]\n",
      " [ 203 2173]]\n",
      "Epoch [4/20] - Train Loss: 0.2108, Val Loss: 0.2002, Val Acc: 0.9215, Val AUC: 0.9761\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9247, AUC: 0.9789\n",
      "Sensitivity: 0.9053, Specificity: 0.9450\n",
      "Confusion Matrix:\n",
      "[[2148  125]\n",
      " [ 225 2151]]\n",
      "Epoch [5/20] - Train Loss: 0.2002, Val Loss: 0.1941, Val Acc: 0.9247, Val AUC: 0.9789\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9271, AUC: 0.9818\n",
      "Sensitivity: 0.9150, Specificity: 0.9397\n",
      "Confusion Matrix:\n",
      "[[2136  137]\n",
      " [ 202 2174]]\n",
      "Epoch [6/20] - Train Loss: 0.1816, Val Loss: 0.1883, Val Acc: 0.9271, Val AUC: 0.9818\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9294, AUC: 0.9826\n",
      "Sensitivity: 0.9386, Specificity: 0.9199\n",
      "Confusion Matrix:\n",
      "[[2091  182]\n",
      " [ 146 2230]]\n",
      "Epoch [7/20] - Train Loss: 0.1763, Val Loss: 0.1756, Val Acc: 0.9294, Val AUC: 0.9826\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9329, AUC: 0.9845\n",
      "Sensitivity: 0.9474, Specificity: 0.9177\n",
      "Confusion Matrix:\n",
      "[[2086  187]\n",
      " [ 125 2251]]\n",
      "Epoch [8/20] - Train Loss: 0.1647, Val Loss: 0.1605, Val Acc: 0.9329, Val AUC: 0.9845\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9297, AUC: 0.9843\n",
      "Sensitivity: 0.9158, Specificity: 0.9441\n",
      "Confusion Matrix:\n",
      "[[2146  127]\n",
      " [ 200 2176]]\n",
      "Epoch [9/20] - Train Loss: 0.1580, Val Loss: 0.1732, Val Acc: 0.9297, Val AUC: 0.9843\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9355, AUC: 0.9859\n",
      "Sensitivity: 0.9411, Specificity: 0.9296\n",
      "Confusion Matrix:\n",
      "[[2113  160]\n",
      " [ 140 2236]]\n",
      "Epoch [10/20] - Train Loss: 0.1556, Val Loss: 0.1557, Val Acc: 0.9355, Val AUC: 0.9859\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9365, AUC: 0.9866\n",
      "Sensitivity: 0.9487, Specificity: 0.9239\n",
      "Confusion Matrix:\n",
      "[[2100  173]\n",
      " [ 122 2254]]\n",
      "Epoch [11/20] - Train Loss: 0.1504, Val Loss: 0.1488, Val Acc: 0.9365, Val AUC: 0.9866\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9411, AUC: 0.9878\n",
      "Sensitivity: 0.9268, Specificity: 0.9560\n",
      "Confusion Matrix:\n",
      "[[2173  100]\n",
      " [ 174 2202]]\n",
      "Epoch [12/20] - Train Loss: 0.1394, Val Loss: 0.1501, Val Acc: 0.9411, Val AUC: 0.9878\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9469, AUC: 0.9892\n",
      "Sensitivity: 0.9444, Specificity: 0.9494\n",
      "Confusion Matrix:\n",
      "[[2158  115]\n",
      " [ 132 2244]]\n",
      "Epoch [13/20] - Train Loss: 0.1314, Val Loss: 0.1331, Val Acc: 0.9469, Val AUC: 0.9892\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9391, AUC: 0.9904\n",
      "Sensitivity: 0.9752, Specificity: 0.9015\n",
      "Confusion Matrix:\n",
      "[[2049  224]\n",
      " [  59 2317]]\n",
      "Epoch [14/20] - Train Loss: 0.1210, Val Loss: 0.1387, Val Acc: 0.9391, Val AUC: 0.9904\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9475, AUC: 0.9913\n",
      "Sensitivity: 0.9764, Specificity: 0.9173\n",
      "Confusion Matrix:\n",
      "[[2085  188]\n",
      " [  56 2320]]\n",
      "Epoch [15/20] - Train Loss: 0.1137, Val Loss: 0.1240, Val Acc: 0.9475, Val AUC: 0.9913\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9525, AUC: 0.9902\n",
      "Sensitivity: 0.9524, Specificity: 0.9525\n",
      "Confusion Matrix:\n",
      "[[2165  108]\n",
      " [ 113 2263]]\n",
      "Epoch [16/20] - Train Loss: 0.1073, Val Loss: 0.1272, Val Acc: 0.9525, Val AUC: 0.9902\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9512, AUC: 0.9910\n",
      "Sensitivity: 0.9558, Specificity: 0.9463\n",
      "Confusion Matrix:\n",
      "[[2151  122]\n",
      " [ 105 2271]]\n",
      "Epoch [17/20] - Train Loss: 0.1040, Val Loss: 0.1221, Val Acc: 0.9512, Val AUC: 0.9910\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9563, AUC: 0.9928\n",
      "Sensitivity: 0.9415, Specificity: 0.9718\n",
      "Confusion Matrix:\n",
      "[[2209   64]\n",
      " [ 139 2237]]\n",
      "Epoch [18/20] - Train Loss: 0.0982, Val Loss: 0.1127, Val Acc: 0.9563, Val AUC: 0.9928\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9501, AUC: 0.9918\n",
      "Sensitivity: 0.9251, Specificity: 0.9762\n",
      "Confusion Matrix:\n",
      "[[2219   54]\n",
      " [ 178 2198]]\n",
      "Epoch [19/20] - Train Loss: 0.0972, Val Loss: 0.1264, Val Acc: 0.9501, Val AUC: 0.9918\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9501, AUC: 0.9923\n",
      "Sensitivity: 0.9255, Specificity: 0.9758\n",
      "Confusion Matrix:\n",
      "[[2218   55]\n",
      " [ 177 2199]]\n",
      "Epoch [20/20] - Train Loss: 0.0967, Val Loss: 0.1254, Val Acc: 0.9501, Val AUC: 0.9923\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8068, AUC: 0.9388\n",
      "Sensitivity: 0.9024, Specificity: 0.7234\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 4 37]]\n",
      "Test Loss: 0.4172, Test Accuracy: 0.8068, Test AUC: 0.9388\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout= lstm_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_best_param['lr'],\n",
    "                      weight_decay=lstm_best_param['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n",
    "\n",
    "torch.save(model.state_dict(), 'best_model_lstm_1.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Helper: suppress stdout\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Number of repeated runs\n",
    "n_runs = 10\n",
    "\n",
    "# Metric containers\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = LSTMClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=lstm_best_param['hidden_dim'],\n",
    "        num_layers=lstm_best_param['num_layers'],\n",
    "        dropout=lstm_best_param['dropout']\n",
    "    )\n",
    "\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=20,\n",
    "            lr=lstm_best_param['lr'],\n",
    "            weight_decay=lstm_best_param['weight_decay'],\n",
    "            verbose=False,\n",
    "            train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Manual calculation of sensitivity and specificity\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "    # Save model from the final run\n",
    "    if run == n_runs - 1:\n",
    "        torch.save(model.state_dict(), 'best_model_lstm_1.pt')\n",
    "\n",
    "# Summary output\n",
    "print(\"\\n📊 Summary across 10 runs (Vanilla LSTM):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### biLSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 19:29:40,503] A new study created in memory with name: no-name-d81b4d3d-c525-4f21-b95b-1d2a478f9d82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 19:32:01,708] Trial 0 finished with value: 0.6928907338887045 and parameters: {'hidden_dim': 75, 'num_layers': 3, 'dropout': 0.42693865944872267, 'lr': 0.0020261333483325484, 'weight_decay': 0.005952273227183691}. Best is trial 0 with value: 0.6928907338887045.\n",
      "[I 2025-04-29 19:34:36,224] Trial 1 finished with value: 0.20479340718625344 and parameters: {'hidden_dim': 128, 'num_layers': 3, 'dropout': 0.2182307095633875, 'lr': 0.005662073185742132, 'weight_decay': 0.0037388558223595586}. Best is trial 1 with value: 0.20479340718625344.\n",
      "[I 2025-04-29 19:36:54,036] Trial 2 finished with value: 0.2191017038405758 and parameters: {'hidden_dim': 56, 'num_layers': 2, 'dropout': 0.2420715654472932, 'lr': 0.008023568972467593, 'weight_decay': 0.009494177308951385}. Best is trial 1 with value: 0.20479340718625344.\n",
      "[I 2025-04-29 19:39:06,541] Trial 3 finished with value: 0.2220317884667279 and parameters: {'hidden_dim': 91, 'num_layers': 1, 'dropout': 0.25541069846604164, 'lr': 0.0001574549921771994, 'weight_decay': 0.007976588389834156}. Best is trial 1 with value: 0.20479340718625344.\n",
      "[I 2025-04-29 19:41:16,328] Trial 4 finished with value: 0.17887940870164193 and parameters: {'hidden_dim': 117, 'num_layers': 1, 'dropout': 0.39937018154966974, 'lr': 0.000907909071876225, 'weight_decay': 0.004913245310292082}. Best is trial 4 with value: 0.17887940870164193.\n",
      "[I 2025-04-29 19:43:34,433] Trial 5 finished with value: 0.2178462564128719 and parameters: {'hidden_dim': 46, 'num_layers': 2, 'dropout': 0.4427403243980138, 'lr': 0.003311995258014248, 'weight_decay': 0.007966142491219174}. Best is trial 4 with value: 0.17887940870164193.\n",
      "[I 2025-04-29 19:45:43,055] Trial 6 finished with value: 0.14055966044941995 and parameters: {'hidden_dim': 116, 'num_layers': 1, 'dropout': 0.23896432375824206, 'lr': 0.008453448770942253, 'weight_decay': 0.0004598675359067595}. Best is trial 6 with value: 0.14055966044941995.\n",
      "[I 2025-04-29 19:47:50,930] Trial 7 finished with value: 0.14466835879912116 and parameters: {'hidden_dim': 95, 'num_layers': 1, 'dropout': 0.2770320622725183, 'lr': 0.006689298427759187, 'weight_decay': 0.0010485418326487196}. Best is trial 6 with value: 0.14055966044941995.\n",
      "[I 2025-04-29 19:50:21,094] Trial 8 finished with value: 0.23065826847945173 and parameters: {'hidden_dim': 118, 'num_layers': 3, 'dropout': 0.1424366099182165, 'lr': 0.005491468923100309, 'weight_decay': 0.0066468523834015796}. Best is trial 6 with value: 0.14055966044941995.\n",
      "[I 2025-04-29 19:52:56,689] Trial 9 finished with value: 0.1847753506408979 and parameters: {'hidden_dim': 79, 'num_layers': 3, 'dropout': 0.4896106847024996, 'lr': 0.004426292559272102, 'weight_decay': 0.004318068912650752}. Best is trial 6 with value: 0.14055966044941995.\n",
      "[I 2025-04-29 19:55:19,064] Trial 10 finished with value: 0.14566494259115767 and parameters: {'hidden_dim': 104, 'num_layers': 1, 'dropout': 0.11124487525346877, 'lr': 0.009912419235840533, 'weight_decay': 0.0004762943513571824}. Best is trial 6 with value: 0.14055966044941995.\n",
      "[I 2025-04-29 19:57:39,522] Trial 11 finished with value: 0.13709879462441352 and parameters: {'hidden_dim': 96, 'num_layers': 1, 'dropout': 0.333894806909878, 'lr': 0.0077223771873992115, 'weight_decay': 0.000488940137385453}. Best is trial 11 with value: 0.13709879462441352.\n",
      "[I 2025-04-29 20:00:12,672] Trial 12 finished with value: 0.16151466067523174 and parameters: {'hidden_dim': 106, 'num_layers': 2, 'dropout': 0.349815338943058, 'lr': 0.008608678164081335, 'weight_decay': 0.0021103712778406457}. Best is trial 11 with value: 0.13709879462441352.\n",
      "[I 2025-04-29 20:02:36,352] Trial 13 finished with value: 0.16480144797122642 and parameters: {'hidden_dim': 92, 'num_layers': 1, 'dropout': 0.33321364227218536, 'lr': 0.0075715734571239115, 'weight_decay': 0.002641358906880422}. Best is trial 11 with value: 0.13709879462441352.\n",
      "[I 2025-04-29 20:04:56,172] Trial 14 finished with value: 0.12133898612826247 and parameters: {'hidden_dim': 59, 'num_layers': 1, 'dropout': 0.19595557432276017, 'lr': 0.009816503475479747, 'weight_decay': 2.600777275437832e-05}. Best is trial 14 with value: 0.12133898612826247.\n",
      "[I 2025-04-29 20:07:21,331] Trial 15 finished with value: 0.15996538975263294 and parameters: {'hidden_dim': 66, 'num_layers': 2, 'dropout': 0.16772466877161793, 'lr': 0.009407802463810016, 'weight_decay': 0.0019674468810842232}. Best is trial 14 with value: 0.12133898612826247.\n",
      "[I 2025-04-29 20:09:40,720] Trial 16 finished with value: 0.12464843333175737 and parameters: {'hidden_dim': 38, 'num_layers': 1, 'dropout': 0.18446208206714815, 'lr': 0.006755187507568671, 'weight_decay': 2.6627178322397367e-05}. Best is trial 14 with value: 0.12133898612826247.\n",
      "[I 2025-04-29 20:12:04,225] Trial 17 finished with value: 0.16881782224733535 and parameters: {'hidden_dim': 34, 'num_layers': 2, 'dropout': 0.1834652455979825, 'lr': 0.006618639309654464, 'weight_decay': 0.0032508884757191376}. Best is trial 14 with value: 0.12133898612826247.\n",
      "[I 2025-04-29 20:14:24,169] Trial 18 finished with value: 0.14738492208392653 and parameters: {'hidden_dim': 40, 'num_layers': 1, 'dropout': 0.19450025691406383, 'lr': 0.004236899021418661, 'weight_decay': 0.001630525851153829}. Best is trial 14 with value: 0.12133898612826247.\n",
      "[I 2025-04-29 20:16:41,991] Trial 19 finished with value: 0.16407070992744133 and parameters: {'hidden_dim': 54, 'num_layers': 1, 'dropout': 0.10493084767639381, 'lr': 0.006674282332694446, 'weight_decay': 0.002840248235252958}. Best is trial 14 with value: 0.12133898612826247.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 59, 'num_layers': 1, 'dropout': 0.19595557432276017, 'lr': 0.009816503475479747, 'weight_decay': 2.600777275437832e-05}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_flatten/BiLSTM_Flatten_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "    max_seq_len = 100  # fixed for now; match your padding/truncation\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout,\n",
    "        max_seq_len=max_seq_len\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage (uncomment and run in your local environment):\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_best_param = {'hidden_dim': 59, 'num_layers': 1, 'dropout': 0.19595557432276017, 'lr': 0.009816503475479747, 'weight_decay': 2.600777275437832e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Define DataLoaders\n",
    "\n",
    "\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9348, AUC: 0.9854\n",
      "Sensitivity: 0.9613, Specificity: 0.9072\n",
      "Confusion Matrix:\n",
      "[[2062  211]\n",
      " [  92 2284]]\n",
      "Epoch [1/30] - Train Loss: 0.2113, Val Loss: 0.1561, Val Acc: 0.9348, Val AUC: 0.9854\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9413, AUC: 0.9864\n",
      "Sensitivity: 0.9554, Specificity: 0.9265\n",
      "Confusion Matrix:\n",
      "[[2106  167]\n",
      " [ 106 2270]]\n",
      "Epoch [2/30] - Train Loss: 0.1473, Val Loss: 0.1477, Val Acc: 0.9413, Val AUC: 0.9864\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9417, AUC: 0.9879\n",
      "Sensitivity: 0.9604, Specificity: 0.9221\n",
      "Confusion Matrix:\n",
      "[[2096  177]\n",
      " [  94 2282]]\n",
      "Epoch [3/30] - Train Loss: 0.1297, Val Loss: 0.1417, Val Acc: 0.9417, Val AUC: 0.9879\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9458, AUC: 0.9881\n",
      "Sensitivity: 0.9621, Specificity: 0.9287\n",
      "Confusion Matrix:\n",
      "[[2111  162]\n",
      " [  90 2286]]\n",
      "Epoch [4/30] - Train Loss: 0.1204, Val Loss: 0.1390, Val Acc: 0.9458, Val AUC: 0.9881\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9432, AUC: 0.9882\n",
      "Sensitivity: 0.9440, Specificity: 0.9424\n",
      "Confusion Matrix:\n",
      "[[2142  131]\n",
      " [ 133 2243]]\n",
      "Epoch [5/30] - Train Loss: 0.1133, Val Loss: 0.1383, Val Acc: 0.9432, Val AUC: 0.9882\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9495, AUC: 0.9905\n",
      "Sensitivity: 0.9689, Specificity: 0.9292\n",
      "Confusion Matrix:\n",
      "[[2112  161]\n",
      " [  74 2302]]\n",
      "Epoch [6/30] - Train Loss: 0.1040, Val Loss: 0.1270, Val Acc: 0.9495, Val AUC: 0.9905\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9484, AUC: 0.9889\n",
      "Sensitivity: 0.9672, Specificity: 0.9287\n",
      "Confusion Matrix:\n",
      "[[2111  162]\n",
      " [  78 2298]]\n",
      "Epoch [7/30] - Train Loss: 0.0995, Val Loss: 0.1361, Val Acc: 0.9484, Val AUC: 0.9889\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9499, AUC: 0.9913\n",
      "Sensitivity: 0.9423, Specificity: 0.9578\n",
      "Confusion Matrix:\n",
      "[[2177   96]\n",
      " [ 137 2239]]\n",
      "Epoch [8/30] - Train Loss: 0.0923, Val Loss: 0.1258, Val Acc: 0.9499, Val AUC: 0.9913\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9531, AUC: 0.9915\n",
      "Sensitivity: 0.9474, Specificity: 0.9591\n",
      "Confusion Matrix:\n",
      "[[2180   93]\n",
      " [ 125 2251]]\n",
      "Epoch [9/30] - Train Loss: 0.0871, Val Loss: 0.1286, Val Acc: 0.9531, Val AUC: 0.9915\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9510, AUC: 0.9907\n",
      "Sensitivity: 0.9592, Specificity: 0.9424\n",
      "Confusion Matrix:\n",
      "[[2142  131]\n",
      " [  97 2279]]\n",
      "Epoch [10/30] - Train Loss: 0.0828, Val Loss: 0.1267, Val Acc: 0.9510, Val AUC: 0.9907\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9542, AUC: 0.9911\n",
      "Sensitivity: 0.9592, Specificity: 0.9490\n",
      "Confusion Matrix:\n",
      "[[2157  116]\n",
      " [  97 2279]]\n",
      "Epoch [11/30] - Train Loss: 0.0808, Val Loss: 0.1289, Val Acc: 0.9542, Val AUC: 0.9911\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9512, AUC: 0.9911\n",
      "Sensitivity: 0.9554, Specificity: 0.9468\n",
      "Confusion Matrix:\n",
      "[[2152  121]\n",
      " [ 106 2270]]\n",
      "Epoch [12/30] - Train Loss: 0.0755, Val Loss: 0.1286, Val Acc: 0.9512, Val AUC: 0.9911\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9538, AUC: 0.9906\n",
      "Sensitivity: 0.9571, Specificity: 0.9503\n",
      "Confusion Matrix:\n",
      "[[2160  113]\n",
      " [ 102 2274]]\n",
      "Epoch [13/30] - Train Loss: 0.0681, Val Loss: 0.1367, Val Acc: 0.9538, Val AUC: 0.9906\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9529, AUC: 0.9913\n",
      "Sensitivity: 0.9491, Specificity: 0.9569\n",
      "Confusion Matrix:\n",
      "[[2175   98]\n",
      " [ 121 2255]]\n",
      "Epoch [14/30] - Train Loss: 0.0653, Val Loss: 0.1350, Val Acc: 0.9529, Val AUC: 0.9913\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9529, AUC: 0.9906\n",
      "Sensitivity: 0.9407, Specificity: 0.9657\n",
      "Confusion Matrix:\n",
      "[[2195   78]\n",
      " [ 141 2235]]\n",
      "Epoch [15/30] - Train Loss: 0.0636, Val Loss: 0.1519, Val Acc: 0.9529, Val AUC: 0.9906\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9538, AUC: 0.9916\n",
      "Sensitivity: 0.9672, Specificity: 0.9397\n",
      "Confusion Matrix:\n",
      "[[2136  137]\n",
      " [  78 2298]]\n",
      "Epoch [16/30] - Train Loss: 0.0605, Val Loss: 0.1386, Val Acc: 0.9538, Val AUC: 0.9916\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9540, AUC: 0.9914\n",
      "Sensitivity: 0.9609, Specificity: 0.9468\n",
      "Confusion Matrix:\n",
      "[[2152  121]\n",
      " [  93 2283]]\n",
      "Epoch [17/30] - Train Loss: 0.0584, Val Loss: 0.1393, Val Acc: 0.9540, Val AUC: 0.9914\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9538, AUC: 0.9909\n",
      "Sensitivity: 0.9503, Specificity: 0.9573\n",
      "Confusion Matrix:\n",
      "[[2176   97]\n",
      " [ 118 2258]]\n",
      "Epoch [18/30] - Train Loss: 0.0565, Val Loss: 0.1401, Val Acc: 0.9538, Val AUC: 0.9909\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9563, AUC: 0.9912\n",
      "Sensitivity: 0.9651, Specificity: 0.9472\n",
      "Confusion Matrix:\n",
      "[[2153  120]\n",
      " [  83 2293]]\n",
      "Epoch [19/30] - Train Loss: 0.0542, Val Loss: 0.1557, Val Acc: 0.9563, Val AUC: 0.9912\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9570, AUC: 0.9918\n",
      "Sensitivity: 0.9541, Specificity: 0.9600\n",
      "Confusion Matrix:\n",
      "[[2182   91]\n",
      " [ 109 2267]]\n",
      "Epoch [20/30] - Train Loss: 0.0512, Val Loss: 0.1526, Val Acc: 0.9570, Val AUC: 0.9918\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9507, AUC: 0.9906\n",
      "Sensitivity: 0.9516, Specificity: 0.9498\n",
      "Confusion Matrix:\n",
      "[[2159  114]\n",
      " [ 115 2261]]\n",
      "Epoch [21/30] - Train Loss: 0.0468, Val Loss: 0.1526, Val Acc: 0.9507, Val AUC: 0.9906\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9486, AUC: 0.9894\n",
      "Sensitivity: 0.9428, Specificity: 0.9547\n",
      "Confusion Matrix:\n",
      "[[2170  103]\n",
      " [ 136 2240]]\n",
      "Epoch [22/30] - Train Loss: 0.0471, Val Loss: 0.1658, Val Acc: 0.9486, Val AUC: 0.9894\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9555, AUC: 0.9914\n",
      "Sensitivity: 0.9571, Specificity: 0.9538\n",
      "Confusion Matrix:\n",
      "[[2168  105]\n",
      " [ 102 2274]]\n",
      "Epoch [23/30] - Train Loss: 0.0443, Val Loss: 0.1469, Val Acc: 0.9555, Val AUC: 0.9914\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9538, AUC: 0.9911\n",
      "Sensitivity: 0.9693, Specificity: 0.9375\n",
      "Confusion Matrix:\n",
      "[[2131  142]\n",
      " [  73 2303]]\n",
      "Epoch [24/30] - Train Loss: 0.0441, Val Loss: 0.1572, Val Acc: 0.9538, Val AUC: 0.9911\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9538, AUC: 0.9916\n",
      "Sensitivity: 0.9457, Specificity: 0.9622\n",
      "Confusion Matrix:\n",
      "[[2187   86]\n",
      " [ 129 2247]]\n",
      "Epoch [25/30] - Train Loss: 0.0425, Val Loss: 0.1611, Val Acc: 0.9538, Val AUC: 0.9916\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9544, AUC: 0.9921\n",
      "Sensitivity: 0.9558, Specificity: 0.9529\n",
      "Confusion Matrix:\n",
      "[[2166  107]\n",
      " [ 105 2271]]\n",
      "Epoch [26/30] - Train Loss: 0.0426, Val Loss: 0.1551, Val Acc: 0.9544, Val AUC: 0.9921\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9525, AUC: 0.9908\n",
      "Sensitivity: 0.9461, Specificity: 0.9591\n",
      "Confusion Matrix:\n",
      "[[2180   93]\n",
      " [ 128 2248]]\n",
      "Epoch [27/30] - Train Loss: 0.0443, Val Loss: 0.1693, Val Acc: 0.9525, Val AUC: 0.9908\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9510, AUC: 0.9908\n",
      "Sensitivity: 0.9339, Specificity: 0.9688\n",
      "Confusion Matrix:\n",
      "[[2202   71]\n",
      " [ 157 2219]]\n",
      "Epoch [28/30] - Train Loss: 0.0442, Val Loss: 0.1751, Val Acc: 0.9510, Val AUC: 0.9908\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9486, AUC: 0.9894\n",
      "Sensitivity: 0.9432, Specificity: 0.9542\n",
      "Confusion Matrix:\n",
      "[[2169  104]\n",
      " [ 135 2241]]\n",
      "Epoch [29/30] - Train Loss: 0.0404, Val Loss: 0.1781, Val Acc: 0.9486, Val AUC: 0.9894\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9529, AUC: 0.9908\n",
      "Sensitivity: 0.9697, Specificity: 0.9353\n",
      "Confusion Matrix:\n",
      "[[2126  147]\n",
      " [  72 2304]]\n",
      "Epoch [30/30] - Train Loss: 0.0457, Val Loss: 0.1667, Val Acc: 0.9529, Val AUC: 0.9908\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7727, AUC: 0.9445\n",
      "Sensitivity: 0.9512, Specificity: 0.6170\n",
      "Confusion Matrix:\n",
      "[[29 18]\n",
      " [ 2 39]]\n",
      "Test Loss: 0.6426, Test Accuracy: 0.7727, Test AUC: 0.9445\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "# to rerun\n",
    "model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout= bilstm_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=30, lr=bilstm_best_param['lr'],\n",
    "                      weight_decay=bilstm_best_param['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "# Helper: suppress stdout\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Run count\n",
    "n_runs = 10\n",
    "\n",
    "# Metric trackers\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=bilstm_best_param['hidden_dim'],\n",
    "        num_layers=bilstm_best_param['num_layers'],\n",
    "        dropout=bilstm_best_param['dropout'],\n",
    "        max_seq_len=100\n",
    "    )\n",
    "\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=30,\n",
    "            lr=bilstm_best_param['lr'],\n",
    "            weight_decay=bilstm_best_param['weight_decay'],\n",
    "            verbose=False,\n",
    "            train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Manually calculate Sens/Spec\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n📊 Summary across 10 runs (BiLSTM with Flatten):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 10:56:45,905] A new study created in memory with name: no-name-cb044d44-94a6-43ad-b244-f0c29d5a8dea\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 10:59:19,670] Trial 0 finished with value: 0.1714380279897827 and parameters: {'hidden_dim': 117, 'num_layers': 3, 'dropout': 0.17378063568060603, 'lr': 0.0009571318934944739, 'weight_decay': 9.93078411095272e-06}. Best is trial 0 with value: 0.1714380279897827.\n",
      "[I 2025-04-30 11:01:43,899] Trial 1 finished with value: 0.2027659824449722 and parameters: {'hidden_dim': 115, 'num_layers': 1, 'dropout': 0.38266715758938774, 'lr': 0.0031185109363283145, 'weight_decay': 0.002949711204449274}. Best is trial 0 with value: 0.1714380279897827.\n",
      "[I 2025-04-30 11:04:09,339] Trial 2 finished with value: 0.20427552069703195 and parameters: {'hidden_dim': 57, 'num_layers': 3, 'dropout': 0.25587002914485535, 'lr': 0.00011630774752681913, 'weight_decay': 1.106919812481162e-06}. Best is trial 0 with value: 0.1714380279897827.\n",
      "[I 2025-04-30 11:06:53,693] Trial 3 finished with value: 0.19804924042665795 and parameters: {'hidden_dim': 103, 'num_layers': 3, 'dropout': 0.1479826371972119, 'lr': 0.00013844676704452238, 'weight_decay': 1.7980493584730404e-05}. Best is trial 0 with value: 0.1714380279897827.\n",
      "[I 2025-04-30 11:09:16,960] Trial 4 finished with value: 0.14755204876195893 and parameters: {'hidden_dim': 85, 'num_layers': 1, 'dropout': 0.4548453210586073, 'lr': 0.005881852254528488, 'weight_decay': 1.22922875003128e-06}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:11:42,720] Trial 5 finished with value: 0.24574539963513203 and parameters: {'hidden_dim': 113, 'num_layers': 1, 'dropout': 0.2772365726607158, 'lr': 0.0006617224563334824, 'weight_decay': 3.0376809557226924e-05}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:14:07,784] Trial 6 finished with value: 0.21181870629526164 and parameters: {'hidden_dim': 43, 'num_layers': 1, 'dropout': 0.3378481052511716, 'lr': 0.0005554645083959282, 'weight_decay': 1.9318264307267306e-05}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:16:31,885] Trial 7 finished with value: 0.18459065391184531 and parameters: {'hidden_dim': 32, 'num_layers': 3, 'dropout': 0.10256174347723507, 'lr': 0.0005718594766559277, 'weight_decay': 2.706079201749471e-06}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:18:55,377] Trial 8 finished with value: 0.1734144127328102 and parameters: {'hidden_dim': 34, 'num_layers': 1, 'dropout': 0.41507448331738905, 'lr': 0.0039501407306813145, 'weight_decay': 1.327602098112636e-05}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:21:27,554] Trial 9 finished with value: 0.6929276397783463 and parameters: {'hidden_dim': 71, 'num_layers': 2, 'dropout': 0.14009325118629012, 'lr': 0.0021377642471693816, 'weight_decay': 0.007808785119122121}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:24:04,051] Trial 10 finished with value: 0.28795328797542885 and parameters: {'hidden_dim': 89, 'num_layers': 2, 'dropout': 0.4818826226241306, 'lr': 0.009579339993277121, 'weight_decay': 0.0002157200578673514}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:26:41,883] Trial 11 finished with value: 0.1837963427684895 and parameters: {'hidden_dim': 126, 'num_layers': 2, 'dropout': 0.2166026426834073, 'lr': 0.0014764291376671097, 'weight_decay': 4.090881921337958e-06}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:29:16,690] Trial 12 finished with value: 0.2914352139381513 and parameters: {'hidden_dim': 94, 'num_layers': 3, 'dropout': 0.47736262827179426, 'lr': 0.009246323646700819, 'weight_decay': 0.00012860050488597941}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:31:49,632] Trial 13 finished with value: 0.1875353314696926 and parameters: {'hidden_dim': 77, 'num_layers': 2, 'dropout': 0.21790985255834067, 'lr': 0.00030054056184291753, 'weight_decay': 1.085304522940181e-06}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:34:21,469] Trial 14 finished with value: 0.17169438476023607 and parameters: {'hidden_dim': 66, 'num_layers': 2, 'dropout': 0.3565329972977989, 'lr': 0.0010029210809839168, 'weight_decay': 5.400937236021966e-06}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:36:48,037] Trial 15 finished with value: 0.20767636964582417 and parameters: {'hidden_dim': 127, 'num_layers': 1, 'dropout': 0.41743726696017563, 'lr': 0.005346693679570666, 'weight_decay': 0.0005600363912516941}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:39:22,544] Trial 16 finished with value: 0.20139448885640052 and parameters: {'hidden_dim': 89, 'num_layers': 3, 'dropout': 0.19652251237877782, 'lr': 0.000255634491532263, 'weight_decay': 4.3174941066474134e-05}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:41:59,646] Trial 17 finished with value: 0.1601775326345065 and parameters: {'hidden_dim': 105, 'num_layers': 2, 'dropout': 0.3096146454734321, 'lr': 0.001761602981366735, 'weight_decay': 5.093397872774934e-06}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:44:34,620] Trial 18 finished with value: 0.18029527937712736 and parameters: {'hidden_dim': 100, 'num_layers': 2, 'dropout': 0.3137412344185726, 'lr': 0.005530974954481241, 'weight_decay': 1.7677646503675963e-06}. Best is trial 4 with value: 0.14755204876195893.\n",
      "[I 2025-04-30 11:47:07,075] Trial 19 finished with value: 0.20121939837524336 and parameters: {'hidden_dim': 82, 'num_layers': 1, 'dropout': 0.4397668804510415, 'lr': 0.0020926810883819267, 'weight_decay': 5.421325066007768e-06}. Best is trial 4 with value: 0.14755204876195893.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 85, 'num_layers': 1, 'dropout': 0.4548453210586073, 'lr': 0.005881852254528488, 'weight_decay': 1.22922875003128e-06}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# LSTM with Attention classifier\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)  # shape: [batch, seq_len, hidden_dim]\n",
    "\n",
    "        # Compute attention weights\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)  # shape: [batch, seq_len]\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)  # normalize\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)  # shape: [batch, hidden_dim]\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-attn/LSTM_Attn_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-lstm_attention.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "# Usage\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_attn_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Define DataLoaders\n",
    "\n",
    "\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8892, AUC: 0.9422\n",
      "Sensitivity: 0.9566, Specificity: 0.8187\n",
      "Confusion Matrix:\n",
      "[[1861  412]\n",
      " [ 103 2273]]\n",
      "Epoch [1/20] - Train Loss: 0.4581, Val Loss: 0.2900, Val Acc: 0.8892, Val AUC: 0.9422\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8879, AUC: 0.9435\n",
      "Sensitivity: 0.9449, Specificity: 0.8284\n",
      "Confusion Matrix:\n",
      "[[1883  390]\n",
      " [ 131 2245]]\n",
      "Epoch [2/20] - Train Loss: 0.3422, Val Loss: 0.2905, Val Acc: 0.8879, Val AUC: 0.9435\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9262, AUC: 0.9754\n",
      "Sensitivity: 0.9360, Specificity: 0.9160\n",
      "Confusion Matrix:\n",
      "[[2082  191]\n",
      " [ 152 2224]]\n",
      "Epoch [3/20] - Train Loss: 0.2511, Val Loss: 0.2013, Val Acc: 0.9262, Val AUC: 0.9754\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9273, AUC: 0.9772\n",
      "Sensitivity: 0.9432, Specificity: 0.9107\n",
      "Confusion Matrix:\n",
      "[[2070  203]\n",
      " [ 135 2241]]\n",
      "Epoch [4/20] - Train Loss: 0.2097, Val Loss: 0.1933, Val Acc: 0.9273, Val AUC: 0.9772\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9256, AUC: 0.9794\n",
      "Sensitivity: 0.9070, Specificity: 0.9450\n",
      "Confusion Matrix:\n",
      "[[2148  125]\n",
      " [ 221 2155]]\n",
      "Epoch [5/20] - Train Loss: 0.2078, Val Loss: 0.1982, Val Acc: 0.9256, Val AUC: 0.9794\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9243, AUC: 0.9804\n",
      "Sensitivity: 0.9011, Specificity: 0.9485\n",
      "Confusion Matrix:\n",
      "[[2156  117]\n",
      " [ 235 2141]]\n",
      "Epoch [6/20] - Train Loss: 0.1970, Val Loss: 0.1965, Val Acc: 0.9243, Val AUC: 0.9804\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9322, AUC: 0.9826\n",
      "Sensitivity: 0.9465, Specificity: 0.9173\n",
      "Confusion Matrix:\n",
      "[[2085  188]\n",
      " [ 127 2249]]\n",
      "Epoch [7/20] - Train Loss: 0.1818, Val Loss: 0.1712, Val Acc: 0.9322, Val AUC: 0.9826\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9269, AUC: 0.9814\n",
      "Sensitivity: 0.9053, Specificity: 0.9494\n",
      "Confusion Matrix:\n",
      "[[2158  115]\n",
      " [ 225 2151]]\n",
      "Epoch [8/20] - Train Loss: 0.1941, Val Loss: 0.1839, Val Acc: 0.9269, Val AUC: 0.9814\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9325, AUC: 0.9827\n",
      "Sensitivity: 0.9562, Specificity: 0.9076\n",
      "Confusion Matrix:\n",
      "[[2063  210]\n",
      " [ 104 2272]]\n",
      "Epoch [9/20] - Train Loss: 0.1844, Val Loss: 0.1744, Val Acc: 0.9325, Val AUC: 0.9827\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9376, AUC: 0.9846\n",
      "Sensitivity: 0.9263, Specificity: 0.9494\n",
      "Confusion Matrix:\n",
      "[[2158  115]\n",
      " [ 175 2201]]\n",
      "Epoch [10/20] - Train Loss: 0.1667, Val Loss: 0.1679, Val Acc: 0.9376, Val AUC: 0.9846\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9348, AUC: 0.9858\n",
      "Sensitivity: 0.9196, Specificity: 0.9507\n",
      "Confusion Matrix:\n",
      "[[2161  112]\n",
      " [ 191 2185]]\n",
      "Epoch [11/20] - Train Loss: 0.1681, Val Loss: 0.1622, Val Acc: 0.9348, Val AUC: 0.9858\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9346, AUC: 0.9852\n",
      "Sensitivity: 0.9588, Specificity: 0.9094\n",
      "Confusion Matrix:\n",
      "[[2067  206]\n",
      " [  98 2278]]\n",
      "Epoch [12/20] - Train Loss: 0.1523, Val Loss: 0.1605, Val Acc: 0.9346, Val AUC: 0.9852\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9363, AUC: 0.9863\n",
      "Sensitivity: 0.9141, Specificity: 0.9595\n",
      "Confusion Matrix:\n",
      "[[2181   92]\n",
      " [ 204 2172]]\n",
      "Epoch [13/20] - Train Loss: 0.1415, Val Loss: 0.1605, Val Acc: 0.9363, Val AUC: 0.9863\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9340, AUC: 0.9827\n",
      "Sensitivity: 0.9259, Specificity: 0.9424\n",
      "Confusion Matrix:\n",
      "[[2142  131]\n",
      " [ 176 2200]]\n",
      "Epoch [14/20] - Train Loss: 0.1648, Val Loss: 0.1699, Val Acc: 0.9340, Val AUC: 0.9827\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9436, AUC: 0.9872\n",
      "Sensitivity: 0.9285, Specificity: 0.9595\n",
      "Confusion Matrix:\n",
      "[[2181   92]\n",
      " [ 170 2206]]\n",
      "Epoch [15/20] - Train Loss: 0.1453, Val Loss: 0.1471, Val Acc: 0.9436, Val AUC: 0.9872\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9454, AUC: 0.9886\n",
      "Sensitivity: 0.9621, Specificity: 0.9278\n",
      "Confusion Matrix:\n",
      "[[2109  164]\n",
      " [  90 2286]]\n",
      "Epoch [16/20] - Train Loss: 0.1291, Val Loss: 0.1390, Val Acc: 0.9454, Val AUC: 0.9886\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9445, AUC: 0.9874\n",
      "Sensitivity: 0.9318, Specificity: 0.9578\n",
      "Confusion Matrix:\n",
      "[[2177   96]\n",
      " [ 162 2214]]\n",
      "Epoch [17/20] - Train Loss: 0.1155, Val Loss: 0.1455, Val Acc: 0.9445, Val AUC: 0.9874\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9417, AUC: 0.9891\n",
      "Sensitivity: 0.9141, Specificity: 0.9705\n",
      "Confusion Matrix:\n",
      "[[2206   67]\n",
      " [ 204 2172]]\n",
      "Epoch [18/20] - Train Loss: 0.1043, Val Loss: 0.1423, Val Acc: 0.9417, Val AUC: 0.9891\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9507, AUC: 0.9898\n",
      "Sensitivity: 0.9419, Specificity: 0.9600\n",
      "Confusion Matrix:\n",
      "[[2182   91]\n",
      " [ 138 2238]]\n",
      "Epoch [19/20] - Train Loss: 0.0984, Val Loss: 0.1338, Val Acc: 0.9507, Val AUC: 0.9898\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9520, AUC: 0.9897\n",
      "Sensitivity: 0.9495, Specificity: 0.9547\n",
      "Confusion Matrix:\n",
      "[[2170  103]\n",
      " [ 120 2256]]\n",
      "Epoch [20/20] - Train Loss: 0.0889, Val Loss: 0.1337, Val Acc: 0.9520, Val AUC: 0.9897\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8295, AUC: 0.9289\n",
      "Sensitivity: 0.9024, Specificity: 0.7660\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 4 37]]\n",
      "Test Loss: 0.4240, Test Accuracy: 0.8295, Test AUC: 0.9289\n"
     ]
    }
   ],
   "source": [
    "# model = LSTMClassifier(input_dim=20, hidden_dim=47, num_layers=2, dropout=0.18950252633567022)\n",
    "# history = train_model(model, train_loader, val_loader, num_epochs=19, lr=0.009528266081905703,\n",
    "#                       weight_decay=1.1052415577383506e-05, verbose=True)\n",
    "\n",
    "# to rerun\n",
    "model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=lstm_attn_best_param['hidden_dim'], num_layers=lstm_attn_best_param['num_layers'], dropout= lstm_attn_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_attn_best_param['lr'],\n",
    "                      weight_decay=lstm_attn_best_param['weight_decay'], verbose=True)\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'])\n",
    "model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'], max_seq_len=100)\n",
    "\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt', 4)\n",
    "# freeze_encoder(model)\n",
    "freeze_encoder(model, 4)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=15, lr=bilstm_frozen_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8298\n",
      "Epoch [1/15] - Train Loss: 0.5835, Val Loss: 0.4087, Val Acc: 0.8182, Val AUC: 0.9030\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.9362\n",
      "Epoch [2/15] - Train Loss: 0.3249, Val Loss: 0.3755, Val Acc: 0.8182, Val AUC: 0.9377\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.7447\n",
      "Epoch [3/15] - Train Loss: 0.2763, Val Loss: 0.3600, Val Acc: 0.8523, Val AUC: 0.9481\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.8511\n",
      "Epoch [4/15] - Train Loss: 0.2220, Val Loss: 0.2900, Val Acc: 0.8750, Val AUC: 0.9538\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8936\n",
      "Epoch [5/15] - Train Loss: 0.1717, Val Loss: 0.2487, Val Acc: 0.8750, Val AUC: 0.9637\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.7872\n",
      "Epoch [6/15] - Train Loss: 0.1659, Val Loss: 0.3464, Val Acc: 0.8523, Val AUC: 0.9528\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8936\n",
      "Epoch [7/15] - Train Loss: 0.0936, Val Loss: 0.2837, Val Acc: 0.8636, Val AUC: 0.9689\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Epoch [8/15] - Train Loss: 0.0693, Val Loss: 0.2551, Val Acc: 0.9205, Val AUC: 0.9751\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.9362\n",
      "Epoch [9/15] - Train Loss: 0.0349, Val Loss: 0.5979, Val Acc: 0.8636, Val AUC: 0.9481\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.8936\n",
      "Epoch [10/15] - Train Loss: 0.0481, Val Loss: 0.2824, Val Acc: 0.8977, Val AUC: 0.9715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Epoch [11/15] - Train Loss: 0.0222, Val Loss: 0.3010, Val Acc: 0.9205, Val AUC: 0.9704\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Epoch [12/15] - Train Loss: 0.0204, Val Loss: 0.3112, Val Acc: 0.9205, Val AUC: 0.9730\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.9362\n",
      "Epoch [13/15] - Train Loss: 0.0087, Val Loss: 0.2676, Val Acc: 0.9205, Val AUC: 0.9761\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.8936\n",
      "Epoch [14/15] - Train Loss: 0.0108, Val Loss: 0.2732, Val Acc: 0.9091, Val AUC: 0.9751\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.9149\n",
      "Epoch [15/15] - Train Loss: 0.0060, Val Loss: 0.2447, Val Acc: 0.9091, Val AUC: 0.9772\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.9787\n",
      "Test Loss: 0.3509, Test Accuracy: 0.9091, Test AUC: 0.9580\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'])\n",
    "model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'], max_seq_len=100)\n",
    "\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt', 4)\n",
    "# freeze_encoder(model)\n",
    "freeze_encoder(model, 4)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=15, lr=bilstm_frozen_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.8298\n",
      "Epoch [1/15] - Train Loss: 0.5835, Val Loss: 0.4087, Val Acc: 0.8182, Val AUC: 0.9030\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [13 28]]\n",
      "Sensitivity: 0.6829, Specificity: 0.9362\n",
      "Epoch [2/15] - Train Loss: 0.3249, Val Loss: 0.3755, Val Acc: 0.8182, Val AUC: 0.9377\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 1 40]]\n",
      "Sensitivity: 0.9756, Specificity: 0.7447\n",
      "Epoch [3/15] - Train Loss: 0.2763, Val Loss: 0.3600, Val Acc: 0.8523, Val AUC: 0.9481\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.8511\n",
      "Epoch [4/15] - Train Loss: 0.2220, Val Loss: 0.2900, Val Acc: 0.8750, Val AUC: 0.9538\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8936\n",
      "Epoch [5/15] - Train Loss: 0.1717, Val Loss: 0.2487, Val Acc: 0.8750, Val AUC: 0.9637\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.7872\n",
      "Epoch [6/15] - Train Loss: 0.1659, Val Loss: 0.3464, Val Acc: 0.8523, Val AUC: 0.9528\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8936\n",
      "Epoch [7/15] - Train Loss: 0.0936, Val Loss: 0.2837, Val Acc: 0.8636, Val AUC: 0.9689\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Epoch [8/15] - Train Loss: 0.0693, Val Loss: 0.2551, Val Acc: 0.9205, Val AUC: 0.9751\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.9362\n",
      "Epoch [9/15] - Train Loss: 0.0349, Val Loss: 0.5979, Val Acc: 0.8636, Val AUC: 0.9481\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.8936\n",
      "Epoch [10/15] - Train Loss: 0.0481, Val Loss: 0.2824, Val Acc: 0.8977, Val AUC: 0.9715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Epoch [11/15] - Train Loss: 0.0222, Val Loss: 0.3010, Val Acc: 0.9205, Val AUC: 0.9704\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Epoch [12/15] - Train Loss: 0.0204, Val Loss: 0.3112, Val Acc: 0.9205, Val AUC: 0.9730\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.9362\n",
      "Epoch [13/15] - Train Loss: 0.0087, Val Loss: 0.2676, Val Acc: 0.9205, Val AUC: 0.9761\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.8936\n",
      "Epoch [14/15] - Train Loss: 0.0108, Val Loss: 0.2732, Val Acc: 0.9091, Val AUC: 0.9751\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.9149\n",
      "Epoch [15/15] - Train Loss: 0.0060, Val Loss: 0.2447, Val Acc: 0.9091, Val AUC: 0.9772\n",
      "\n",
      "Confusion Matrix:\n",
      "[[46  1]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.9787\n",
      "Test Loss: 0.3509, Test Accuracy: 0.9091, Test AUC: 0.9580\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'])\n",
    "model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'], max_seq_len=100)\n",
    "\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt', 4)\n",
    "# freeze_encoder(model)\n",
    "freeze_encoder(model, 4)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=15, lr=bilstm_frozen_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "n_runs = 10\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=lstm_attn_best_param['hidden_dim'],\n",
    "        num_layers=lstm_attn_best_param['num_layers'],\n",
    "        dropout=lstm_attn_best_param['dropout']\n",
    "    )\n",
    "\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=20,\n",
    "            lr=lstm_attn_best_param['lr'],\n",
    "            weight_decay=lstm_attn_best_param['weight_decay'],\n",
    "            verbose=False,\n",
    "            train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n📊 Summary across 10 runs (LSTM with Attention):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### bilstm + attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 11:52:19,509] A new study created in memory with name: no-name-612f63c9-bfc4-48e9-9c9a-ee69bf40edb9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 11:55:06,303] Trial 0 finished with value: 0.6929279345355622 and parameters: {'hidden_dim': 79, 'num_layers': 3, 'dropout': 0.221346511022789, 'lr': 0.00613814337153662, 'weight_decay': 0.005083703423901748}. Best is trial 0 with value: 0.6929279345355622.\n",
      "[I 2025-04-30 11:58:38,412] Trial 1 finished with value: 0.6929279761771633 and parameters: {'hidden_dim': 125, 'num_layers': 3, 'dropout': 0.35732306222079335, 'lr': 0.007084435342679313, 'weight_decay': 0.006686337905533278}. Best is trial 0 with value: 0.6929279345355622.\n",
      "[I 2025-04-30 12:02:37,922] Trial 2 finished with value: 0.6929273809472175 and parameters: {'hidden_dim': 120, 'num_layers': 3, 'dropout': 0.27109507174671965, 'lr': 0.0031315766198012442, 'weight_decay': 0.008719785260592349}. Best is trial 2 with value: 0.6929273809472175.\n",
      "[I 2025-04-30 12:05:16,529] Trial 3 finished with value: 0.2146620074771855 and parameters: {'hidden_dim': 56, 'num_layers': 1, 'dropout': 0.2796810265084073, 'lr': 0.0025584907992139308, 'weight_decay': 0.008182146441151398}. Best is trial 3 with value: 0.2146620074771855.\n",
      "[I 2025-04-30 12:07:55,774] Trial 4 finished with value: 0.1562362523009516 and parameters: {'hidden_dim': 34, 'num_layers': 1, 'dropout': 0.26583919516164967, 'lr': 0.003077066270958078, 'weight_decay': 0.0002320884662182122}. Best is trial 4 with value: 0.1562362523009516.\n",
      "[I 2025-04-30 12:10:54,175] Trial 5 finished with value: 0.25096481446533986 and parameters: {'hidden_dim': 121, 'num_layers': 1, 'dropout': 0.44502044916365346, 'lr': 0.00042691893291592304, 'weight_decay': 0.007796612422273516}. Best is trial 4 with value: 0.1562362523009516.\n",
      "[I 2025-04-30 12:14:18,233] Trial 6 finished with value: 0.6929286653048372 and parameters: {'hidden_dim': 118, 'num_layers': 2, 'dropout': 0.40504834245136334, 'lr': 0.007652127048121037, 'weight_decay': 0.00719946975946075}. Best is trial 4 with value: 0.1562362523009516.\n",
      "[I 2025-04-30 12:17:20,390] Trial 7 finished with value: 0.6929361599765412 and parameters: {'hidden_dim': 76, 'num_layers': 3, 'dropout': 0.29610647687622144, 'lr': 0.00473349247240355, 'weight_decay': 0.0038988542154769885}. Best is trial 4 with value: 0.1562362523009516.\n",
      "[I 2025-04-30 12:20:06,252] Trial 8 finished with value: 0.2092718443233673 and parameters: {'hidden_dim': 57, 'num_layers': 1, 'dropout': 0.42373765790767637, 'lr': 0.002945103678564821, 'weight_decay': 0.003307060169892262}. Best is trial 4 with value: 0.1562362523009516.\n",
      "[I 2025-04-30 12:23:11,776] Trial 9 finished with value: 0.6929351760916513 and parameters: {'hidden_dim': 66, 'num_layers': 3, 'dropout': 0.4166113805452145, 'lr': 0.007529758961459736, 'weight_decay': 0.001764899258565488}. Best is trial 4 with value: 0.1562362523009516.\n",
      "[I 2025-04-30 12:26:03,091] Trial 10 finished with value: 0.16821679171838172 and parameters: {'hidden_dim': 36, 'num_layers': 2, 'dropout': 0.10643483849151686, 'lr': 0.00954480102291156, 'weight_decay': 0.0003003860564806444}. Best is trial 4 with value: 0.1562362523009516.\n",
      "[I 2025-04-30 12:28:46,191] Trial 11 finished with value: 0.1510965875158571 and parameters: {'hidden_dim': 32, 'num_layers': 2, 'dropout': 0.10819732395590986, 'lr': 0.009706718965264208, 'weight_decay': 6.890137936656302e-05}. Best is trial 11 with value: 0.1510965875158571.\n",
      "[I 2025-04-30 12:31:33,654] Trial 12 finished with value: 0.12582546697087485 and parameters: {'hidden_dim': 33, 'num_layers': 2, 'dropout': 0.10065981308640495, 'lr': 0.009726729721310196, 'weight_decay': 1.9050323459033188e-05}. Best is trial 12 with value: 0.12582546697087485.\n",
      "[I 2025-04-30 12:34:59,266] Trial 13 finished with value: 0.18168594555495537 and parameters: {'hidden_dim': 98, 'num_layers': 2, 'dropout': 0.10711817802544588, 'lr': 0.009895766650407622, 'weight_decay': 0.0017577127626710585}. Best is trial 12 with value: 0.12582546697087485.\n",
      "[I 2025-04-30 12:37:49,135] Trial 14 finished with value: 0.17378416984048609 and parameters: {'hidden_dim': 43, 'num_layers': 2, 'dropout': 0.18651244256344024, 'lr': 0.00881593523889096, 'weight_decay': 0.0019520899285597325}. Best is trial 12 with value: 0.12582546697087485.\n",
      "[I 2025-04-30 12:40:35,330] Trial 15 finished with value: 0.1486096332130367 and parameters: {'hidden_dim': 46, 'num_layers': 2, 'dropout': 0.17022059186711475, 'lr': 0.008625571051547227, 'weight_decay': 0.00012803680934845337}. Best is trial 12 with value: 0.12582546697087485.\n",
      "[I 2025-04-30 12:43:26,019] Trial 16 finished with value: 0.6929275589446499 and parameters: {'hidden_dim': 49, 'num_layers': 2, 'dropout': 0.17795186326106113, 'lr': 0.008382188778984036, 'weight_decay': 0.00977080139217202}. Best is trial 12 with value: 0.12582546697087485.\n",
      "[I 2025-04-30 12:46:50,830] Trial 17 finished with value: 0.6929279737276574 and parameters: {'hidden_dim': 99, 'num_layers': 2, 'dropout': 0.1550793878865051, 'lr': 0.005754680079937988, 'weight_decay': 0.0029882061869352613}. Best is trial 12 with value: 0.12582546697087485.\n",
      "[I 2025-04-30 12:49:49,580] Trial 18 finished with value: 0.6929273417551224 and parameters: {'hidden_dim': 66, 'num_layers': 2, 'dropout': 0.49469636679653567, 'lr': 0.00846067503117423, 'weight_decay': 0.004764791080958572}. Best is trial 12 with value: 0.12582546697087485.\n",
      "[I 2025-04-30 12:52:30,215] Trial 19 finished with value: 0.17248856061941956 and parameters: {'hidden_dim': 44, 'num_layers': 1, 'dropout': 0.22468124554365623, 'lr': 0.006540474686226254, 'weight_decay': 0.0008706666175061463}. Best is trial 12 with value: 0.12582546697087485.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'hidden_dim': 33, 'num_layers': 2, 'dropout': 0.10065981308640495, 'lr': 0.009726729721310196, 'weight_decay': 1.9050323459033188e-05}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import optuna\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "\n",
    "# BiLSTM with Attention Classifier\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective function\n",
    "def objective(trial):\n",
    "    hidden_dim = trial.suggest_int(\"hidden_dim\", 32, 128)\n",
    "    num_layers = trial.suggest_int(\"num_layers\", 1, 3)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "        dropout=dropout\n",
    "    )\n",
    "\n",
    "    val_auc = train_model(\n",
    "        model,\n",
    "        train_loader,\n",
    "        val_loader,\n",
    "        num_epochs=10,\n",
    "        lr=lr,\n",
    "        weight_decay=weight_decay,\n",
    "        verbose=False\n",
    "    )\n",
    "    return val_auc\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Define DataLoaders\n",
    "\n",
    "\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9239, AUC: 0.9767\n",
      "Sensitivity: 0.9352, Specificity: 0.9120\n",
      "Confusion Matrix:\n",
      "[[2073  200]\n",
      " [ 154 2222]]\n",
      "Epoch [1/30] - Train Loss: 0.2778, Val Loss: 0.1969, Val Acc: 0.9239, Val AUC: 0.9767\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9305, AUC: 0.9805\n",
      "Sensitivity: 0.9352, Specificity: 0.9256\n",
      "Confusion Matrix:\n",
      "[[2104  169]\n",
      " [ 154 2222]]\n",
      "Epoch [2/30] - Train Loss: 0.1979, Val Loss: 0.1903, Val Acc: 0.9305, Val AUC: 0.9805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9271, AUC: 0.9805\n",
      "Sensitivity: 0.9684, Specificity: 0.8839\n",
      "Confusion Matrix:\n",
      "[[2009  264]\n",
      " [  75 2301]]\n",
      "Epoch [3/30] - Train Loss: 0.1808, Val Loss: 0.1917, Val Acc: 0.9271, Val AUC: 0.9805\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9348, AUC: 0.9850\n",
      "Sensitivity: 0.9377, Specificity: 0.9318\n",
      "Confusion Matrix:\n",
      "[[2118  155]\n",
      " [ 148 2228]]\n",
      "Epoch [4/30] - Train Loss: 0.1707, Val Loss: 0.1616, Val Acc: 0.9348, Val AUC: 0.9850\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9357, AUC: 0.9850\n",
      "Sensitivity: 0.9082, Specificity: 0.9644\n",
      "Confusion Matrix:\n",
      "[[2192   81]\n",
      " [ 218 2158]]\n",
      "Epoch [5/30] - Train Loss: 0.1592, Val Loss: 0.1690, Val Acc: 0.9357, Val AUC: 0.9850\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9430, AUC: 0.9869\n",
      "Sensitivity: 0.9613, Specificity: 0.9239\n",
      "Confusion Matrix:\n",
      "[[2100  173]\n",
      " [  92 2284]]\n",
      "Epoch [6/30] - Train Loss: 0.1501, Val Loss: 0.1471, Val Acc: 0.9430, Val AUC: 0.9869\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9408, AUC: 0.9861\n",
      "Sensitivity: 0.9588, Specificity: 0.9221\n",
      "Confusion Matrix:\n",
      "[[2096  177]\n",
      " [  98 2278]]\n",
      "Epoch [7/30] - Train Loss: 0.1412, Val Loss: 0.1511, Val Acc: 0.9408, Val AUC: 0.9861\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9099, AUC: 0.9772\n",
      "Sensitivity: 0.8489, Specificity: 0.9736\n",
      "Confusion Matrix:\n",
      "[[2213   60]\n",
      " [ 359 2017]]\n",
      "Epoch [8/30] - Train Loss: 0.1363, Val Loss: 0.2287, Val Acc: 0.9099, Val AUC: 0.9772\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9393, AUC: 0.9854\n",
      "Sensitivity: 0.9390, Specificity: 0.9397\n",
      "Confusion Matrix:\n",
      "[[2136  137]\n",
      " [ 145 2231]]\n",
      "Epoch [9/30] - Train Loss: 0.1515, Val Loss: 0.1523, Val Acc: 0.9393, Val AUC: 0.9854\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9479, AUC: 0.9894\n",
      "Sensitivity: 0.9322, Specificity: 0.9644\n",
      "Confusion Matrix:\n",
      "[[2192   81]\n",
      " [ 161 2215]]\n",
      "Epoch [10/30] - Train Loss: 0.1284, Val Loss: 0.1329, Val Acc: 0.9479, Val AUC: 0.9894\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9482, AUC: 0.9901\n",
      "Sensitivity: 0.9558, Specificity: 0.9402\n",
      "Confusion Matrix:\n",
      "[[2137  136]\n",
      " [ 105 2271]]\n",
      "Epoch [11/30] - Train Loss: 0.1137, Val Loss: 0.1305, Val Acc: 0.9482, Val AUC: 0.9901\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9507, AUC: 0.9899\n",
      "Sensitivity: 0.9482, Specificity: 0.9534\n",
      "Confusion Matrix:\n",
      "[[2167  106]\n",
      " [ 123 2253]]\n",
      "Epoch [12/30] - Train Loss: 0.1068, Val Loss: 0.1278, Val Acc: 0.9507, Val AUC: 0.9899\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9495, AUC: 0.9903\n",
      "Sensitivity: 0.9444, Specificity: 0.9547\n",
      "Confusion Matrix:\n",
      "[[2170  103]\n",
      " [ 132 2244]]\n",
      "Epoch [13/30] - Train Loss: 0.0989, Val Loss: 0.1287, Val Acc: 0.9495, Val AUC: 0.9903\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9529, AUC: 0.9909\n",
      "Sensitivity: 0.9407, Specificity: 0.9657\n",
      "Confusion Matrix:\n",
      "[[2195   78]\n",
      " [ 141 2235]]\n",
      "Epoch [14/30] - Train Loss: 0.0943, Val Loss: 0.1273, Val Acc: 0.9529, Val AUC: 0.9909\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9542, AUC: 0.9900\n",
      "Sensitivity: 0.9487, Specificity: 0.9600\n",
      "Confusion Matrix:\n",
      "[[2182   91]\n",
      " [ 122 2254]]\n",
      "Epoch [15/30] - Train Loss: 0.0858, Val Loss: 0.1282, Val Acc: 0.9542, Val AUC: 0.9900\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9516, AUC: 0.9907\n",
      "Sensitivity: 0.9617, Specificity: 0.9410\n",
      "Confusion Matrix:\n",
      "[[2139  134]\n",
      " [  91 2285]]\n",
      "Epoch [16/30] - Train Loss: 0.0815, Val Loss: 0.1274, Val Acc: 0.9516, Val AUC: 0.9907\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9529, AUC: 0.9910\n",
      "Sensitivity: 0.9533, Specificity: 0.9525\n",
      "Confusion Matrix:\n",
      "[[2165  108]\n",
      " [ 111 2265]]\n",
      "Epoch [17/30] - Train Loss: 0.0786, Val Loss: 0.1337, Val Acc: 0.9529, Val AUC: 0.9910\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9570, AUC: 0.9916\n",
      "Sensitivity: 0.9617, Specificity: 0.9520\n",
      "Confusion Matrix:\n",
      "[[2164  109]\n",
      " [  91 2285]]\n",
      "Epoch [18/30] - Train Loss: 0.0716, Val Loss: 0.1190, Val Acc: 0.9570, Val AUC: 0.9916\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9576, AUC: 0.9916\n",
      "Sensitivity: 0.9592, Specificity: 0.9560\n",
      "Confusion Matrix:\n",
      "[[2173  100]\n",
      " [  97 2279]]\n",
      "Epoch [19/30] - Train Loss: 0.0730, Val Loss: 0.1248, Val Acc: 0.9576, Val AUC: 0.9916\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9516, AUC: 0.9906\n",
      "Sensitivity: 0.9718, Specificity: 0.9305\n",
      "Confusion Matrix:\n",
      "[[2115  158]\n",
      " [  67 2309]]\n",
      "Epoch [20/30] - Train Loss: 0.0702, Val Loss: 0.1350, Val Acc: 0.9516, Val AUC: 0.9906\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9432, AUC: 0.9887\n",
      "Sensitivity: 0.9171, Specificity: 0.9705\n",
      "Confusion Matrix:\n",
      "[[2206   67]\n",
      " [ 197 2179]]\n",
      "Epoch [21/30] - Train Loss: 0.0610, Val Loss: 0.1762, Val Acc: 0.9432, Val AUC: 0.9887\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9467, AUC: 0.9909\n",
      "Sensitivity: 0.9209, Specificity: 0.9736\n",
      "Confusion Matrix:\n",
      "[[2213   60]\n",
      " [ 188 2188]]\n",
      "Epoch [22/30] - Train Loss: 0.0609, Val Loss: 0.1466, Val Acc: 0.9467, Val AUC: 0.9909\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9527, AUC: 0.9907\n",
      "Sensitivity: 0.9436, Specificity: 0.9622\n",
      "Confusion Matrix:\n",
      "[[2187   86]\n",
      " [ 134 2242]]\n",
      "Epoch [23/30] - Train Loss: 0.0605, Val Loss: 0.1390, Val Acc: 0.9527, Val AUC: 0.9907\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9561, AUC: 0.9917\n",
      "Sensitivity: 0.9478, Specificity: 0.9648\n",
      "Confusion Matrix:\n",
      "[[2193   80]\n",
      " [ 124 2252]]\n",
      "Epoch [24/30] - Train Loss: 0.0555, Val Loss: 0.1357, Val Acc: 0.9561, Val AUC: 0.9917\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9533, AUC: 0.9910\n",
      "Sensitivity: 0.9415, Specificity: 0.9657\n",
      "Confusion Matrix:\n",
      "[[2195   78]\n",
      " [ 139 2237]]\n",
      "Epoch [25/30] - Train Loss: 0.0566, Val Loss: 0.1496, Val Acc: 0.9533, Val AUC: 0.9910\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9540, AUC: 0.9916\n",
      "Sensitivity: 0.9381, Specificity: 0.9705\n",
      "Confusion Matrix:\n",
      "[[2206   67]\n",
      " [ 147 2229]]\n",
      "Epoch [26/30] - Train Loss: 0.0511, Val Loss: 0.1340, Val Acc: 0.9540, Val AUC: 0.9916\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9518, AUC: 0.9900\n",
      "Sensitivity: 0.9453, Specificity: 0.9586\n",
      "Confusion Matrix:\n",
      "[[2179   94]\n",
      " [ 130 2246]]\n",
      "Epoch [27/30] - Train Loss: 0.0484, Val Loss: 0.1425, Val Acc: 0.9518, Val AUC: 0.9900\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9520, AUC: 0.9904\n",
      "Sensitivity: 0.9461, Specificity: 0.9582\n",
      "Confusion Matrix:\n",
      "[[2178   95]\n",
      " [ 128 2248]]\n",
      "Epoch [28/30] - Train Loss: 0.0477, Val Loss: 0.1432, Val Acc: 0.9520, Val AUC: 0.9904\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9527, AUC: 0.9896\n",
      "Sensitivity: 0.9596, Specificity: 0.9454\n",
      "Confusion Matrix:\n",
      "[[2149  124]\n",
      " [  96 2280]]\n",
      "Epoch [29/30] - Train Loss: 0.0493, Val Loss: 0.1476, Val Acc: 0.9527, Val AUC: 0.9896\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9531, AUC: 0.9906\n",
      "Sensitivity: 0.9503, Specificity: 0.9560\n",
      "Confusion Matrix:\n",
      "[[2173  100]\n",
      " [ 118 2258]]\n",
      "Epoch [30/30] - Train Loss: 0.0561, Val Loss: 0.1464, Val Acc: 0.9531, Val AUC: 0.9906\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8636, AUC: 0.9528\n",
      "Sensitivity: 0.9512, Specificity: 0.7872\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 2 39]]\n",
      "Test Loss: 0.4828, Test Accuracy: 0.8636, Test AUC: 0.9528\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000.0\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attention/BiLSTM_Attention_Optuna_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model-bilstm_attention.pt')\n",
    "\n",
    "\n",
    "model = BiLSTMWithAttentionClassifier(\n",
    "    input_dim=20,\n",
    "    hidden_dim=bilstm_attn_best_param['hidden_dim'],\n",
    "    num_layers=bilstm_attn_best_param['num_layers'],\n",
    "    dropout=bilstm_attn_best_param['dropout']\n",
    ")\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=30, lr=bilstm_attn_best_param['lr'],\n",
    "                      weight_decay=bilstm_attn_best_param['weight_decay'], verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import torch\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Suppress print output during training/evaluation\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Run config\n",
    "n_runs = 10\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=bilstm_attn_best_param['hidden_dim'],\n",
    "        num_layers=bilstm_attn_best_param['num_layers'],\n",
    "        dropout=bilstm_attn_best_param['dropout']\n",
    "    )\n",
    "\n",
    "    with suppress_stdout():\n",
    "        history = train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=30,\n",
    "            lr=bilstm_attn_best_param['lr'],\n",
    "            weight_decay=bilstm_attn_best_param['weight_decay'],\n",
    "            verbose=False,\n",
    "            train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Manually compute Sensitivity & Specificity\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n📊 Summary across 10 runs (BiLSTM with Attention):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer learning to TB data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "{'E', 'X', 'H', 'M', 'P', 'T', 'N', 'V', 'G', 'L', 'A', 'K', 'F', 'Q', 'Y', 'C', 'W', 'S', 'I', 'R', 'D'}\n",
      "21\n",
      "{'X'}\n",
      "Number of 'B' values: 0\n",
      "Dataset sizes:\n",
      "Train: 264\n",
      "Validation: 88\n",
      "Test: 88\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/all_seq702.csv')\n",
    "df = df.drop_duplicates(subset='Sequences')\n",
    "max_length = df['Sequences'].str.len().max()\n",
    "print(max_length)\n",
    "# df['Sequences'] = df['Sequences'].apply(lambda x: x.ljust(max_length, 'X'))\n",
    "\n",
    "unique_letters = set(''.join(df[\"Sequences\"]))\n",
    "print(unique_letters)\n",
    "print(len(unique_letters))\n",
    "amino_acids = set(\"ACDEFGHIKLMNPQRSTVWY\")\n",
    "non_standard_amino_acids = unique_letters - amino_acids\n",
    "print(non_standard_amino_acids)\n",
    "b_count = df[\"Sequences\"].str.count('B').sum()\n",
    "print(f\"Number of 'B' values: {b_count}\")\n",
    "# manually replaced one of the B with D and the other with N\n",
    "\n",
    "df = df[\n",
    "    (df['Sequences'].str.len() >= 10) &\n",
    "    (df['Sequences'].apply(lambda x: len(set(x)) > 1)) &\n",
    "    (~df['Sequences'].str.contains('X'))\n",
    "]\n",
    "\n",
    "X = df[\"Sequences\"]\n",
    "y = df[\"AMP\"]\n",
    "\n",
    "\n",
    "# Split into train (70%), validation (15%), test (15%)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.4, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "# Step 2: Split train+val into train and val (stratified)\n",
    "X_test, X_val, y_test, y_val = train_test_split(\n",
    "    X_test, y_test, test_size=0.5, random_state=42, stratify=y_test\n",
    ")  # 0.1765 to maintain 15% of original dataset\n",
    "\n",
    "# Define DataLoaders\n",
    "\n",
    "train_dataset = SequenceDataset(X_train, y_train)\n",
    "val_dataset = SequenceDataset(X_val, y_val)\n",
    "test_dataset = SequenceDataset(X_test, y_test)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_and_pack)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_and_pack)\n",
    "\n",
    "    \n",
    "# Display dataset sizes\n",
    "dataset_sizes = {\n",
    "    \"Train\": len(train_dataset),\n",
    "    \"Validation\": len(val_dataset),\n",
    "    \"Test\": len(test_dataset)\n",
    "}\n",
    "print(\"Dataset sizes:\")\n",
    "for name, size in dataset_sizes.items():\n",
    "    print(f\"{name}: {size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:58:20,696] A new study created in memory with name: no-name-6db92da7-b5af-496a-84f1-0213556a7a4d\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:58:25,097] Trial 0 finished with value: 0.5409349600474039 and parameters: {'lr': 0.00012509103329235658, 'weight_decay': 1.4929411779198368e-05, 'dropout': 0.21143539063754804}. Best is trial 0 with value: 0.5409349600474039.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:58:29,531] Trial 1 finished with value: 0.2456193963686625 and parameters: {'lr': 0.005306668899987968, 'weight_decay': 4.906247393759226e-06, 'dropout': 0.32225704736591265}. Best is trial 1 with value: 0.2456193963686625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:58:33,553] Trial 2 finished with value: 0.31032859285672504 and parameters: {'lr': 0.002707164124895946, 'weight_decay': 0.0001765598750101452, 'dropout': 0.14489281565170914}. Best is trial 1 with value: 0.2456193963686625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:58:38,201] Trial 3 finished with value: 0.2884900818268458 and parameters: {'lr': 0.0030329310500049776, 'weight_decay': 0.009427195597100996, 'dropout': 0.15133185928280182}. Best is trial 1 with value: 0.2456193963686625.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:58:42,682] Trial 4 finished with value: 0.2396086553732554 and parameters: {'lr': 0.0066101594972914745, 'weight_decay': 8.347644017658696e-05, 'dropout': 0.4607953373609135}. Best is trial 4 with value: 0.2396086553732554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:58:47,268] Trial 5 finished with value: 0.44319814443588257 and parameters: {'lr': 0.0009803809739409846, 'weight_decay': 8.44486539542473e-06, 'dropout': 0.2822213149282786}. Best is trial 4 with value: 0.2396086553732554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:58:51,754] Trial 6 finished with value: 0.40571147700150806 and parameters: {'lr': 0.0015378714377452766, 'weight_decay': 6.498554934494249e-06, 'dropout': 0.46886380822163576}. Best is trial 4 with value: 0.2396086553732554.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:58:56,198] Trial 7 finished with value: 0.22415248304605484 and parameters: {'lr': 0.008123388837591653, 'weight_decay': 7.334990796719557e-06, 'dropout': 0.2045053078692307}. Best is trial 7 with value: 0.22415248304605484.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:59:00,660] Trial 8 finished with value: 0.5561400353908539 and parameters: {'lr': 1.0337945808821513e-05, 'weight_decay': 2.3971122913467665e-06, 'dropout': 0.4356810074157905}. Best is trial 7 with value: 0.22415248304605484.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:59:05,134] Trial 9 finished with value: 0.5519718130429586 and parameters: {'lr': 4.3330963612671414e-05, 'weight_decay': 1.4389489946980833e-05, 'dropout': 0.4847093555996397}. Best is trial 7 with value: 0.22415248304605484.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:59:09,621] Trial 10 finished with value: 0.509053111076355 and parameters: {'lr': 0.0003936746709649615, 'weight_decay': 0.0003034913477327827, 'dropout': 0.33699014680491285}. Best is trial 7 with value: 0.22415248304605484.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:59:14,038] Trial 11 finished with value: 0.22342105954885483 and parameters: {'lr': 0.009830552693825352, 'weight_decay': 7.033717508210554e-05, 'dropout': 0.3902000388108825}. Best is trial 11 with value: 0.22342105954885483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:59:18,520] Trial 12 finished with value: 0.22765806317329407 and parameters: {'lr': 0.009308705304827233, 'weight_decay': 5.8463269605642904e-05, 'dropout': 0.3880267089442155}. Best is trial 11 with value: 0.22342105954885483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:59:23,066] Trial 13 finished with value: 0.4875611861546834 and parameters: {'lr': 0.0005484684962408032, 'weight_decay': 0.000815460134062046, 'dropout': 0.23410090540676953}. Best is trial 11 with value: 0.22342105954885483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:59:27,589] Trial 14 finished with value: 0.542270561059316 and parameters: {'lr': 0.00011904957161277323, 'weight_decay': 3.3870819387606545e-05, 'dropout': 0.3869598215573734}. Best is trial 11 with value: 0.22342105954885483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:59:32,215] Trial 15 finished with value: 0.3279346227645874 and parameters: {'lr': 0.0024473602569847434, 'weight_decay': 1.3399905299970106e-06, 'dropout': 0.22222323059809057}. Best is trial 11 with value: 0.22342105954885483.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:59:36,799] Trial 16 finished with value: 0.21945489446322122 and parameters: {'lr': 0.009721760525684369, 'weight_decay': 0.0017251614959305703, 'dropout': 0.273304750162406}. Best is trial 16 with value: 0.21945489446322122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:59:40,936] Trial 17 finished with value: 0.4451230565706889 and parameters: {'lr': 0.0009486291032989516, 'weight_decay': 0.002344131523258068, 'dropout': 0.279408301202803}. Best is trial 16 with value: 0.21945489446322122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:59:45,641] Trial 18 finished with value: 0.2837814340988795 and parameters: {'lr': 0.003462298742698603, 'weight_decay': 0.0009014528021113066, 'dropout': 0.37513058102389524}. Best is trial 16 with value: 0.21945489446322122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:59:50,096] Trial 19 finished with value: 0.41799702246983844 and parameters: {'lr': 0.0012189878951022154, 'weight_decay': 0.0075347875182017856, 'dropout': 0.42309093160503264}. Best is trial 16 with value: 0.21945489446322122.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.009721760525684369, 'weight_decay': 0.0017251614959305703, 'dropout': 0.273304750162406}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation function with detailed output\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    # print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    # print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    # print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function for frozen encoder\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-lstm-trans-frozen/FrozenEncoder_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        # if val_auc > best_val_auc:\n",
    "        #     best_val_auc = val_auc\n",
    "        #     torch.save(model.state_dict(), 'best_model_frozen.pt')\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path, max_layers=None):\n",
    "    \"\"\"\n",
    "    Load up to `max_layers` compatible layers from a checkpoint into the model.\n",
    "    If max_layers is None, load all compatible layers.\n",
    "    \"\"\"\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter compatible layers\n",
    "    compatible_items = [\n",
    "        (k, v) for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    ]\n",
    "\n",
    "    # Limit number of layers to load\n",
    "    if max_layers is not None:\n",
    "        compatible_items = compatible_items[:max_layers]\n",
    "\n",
    "    # Convert list of tuples back to dict\n",
    "    compatible_dict = dict(compatible_items)\n",
    "\n",
    "    # Update model state dict\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "import optuna\n",
    "lstm_best_param = {'hidden_dim': 95, 'num_layers': 1, 'dropout': 0.20287746211724011, 'lr': 0.004396992152527415, 'weight_decay': 3.999214064585909e-05}\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=dropout)\n",
    "    # model.load_state_dict(torch.load('best_model_lstm_1.pt'))\n",
    "    model = load_partial_weights(model, 'best_model_lstm_1.pt', 6)\n",
    "\n",
    "    \n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_frozen_best_param = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_best_param = {'hidden_dim': 95, 'num_layers': 1, 'dropout': 0.20287746211724011, 'lr': 0.004396992152527415, 'weight_decay': 3.999214064585909e-05}\n",
    "lstm_frozen_best_param = {'lr': 0.009721760525684369, 'weight_decay': 0.0017251614959305703, 'dropout': 0.273304750162406}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8636, AUC: 0.9502\n",
      "Sensitivity: 0.9512, Specificity: 0.7872\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 2 39]]\n",
      "Epoch [1/20] - Train Loss: 0.4483, Val Loss: 0.3497, Val Acc: 0.8636, Val AUC: 0.9502\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9528\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [2/20] - Train Loss: 0.4470, Val Loss: 0.3107, Val Acc: 0.8864, Val AUC: 0.9528\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9554\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [3/20] - Train Loss: 0.3985, Val Loss: 0.2898, Val Acc: 0.8864, Val AUC: 0.9554\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9554\n",
      "Sensitivity: 0.9512, Specificity: 0.8298\n",
      "Confusion Matrix:\n",
      "[[39  8]\n",
      " [ 2 39]]\n",
      "Epoch [4/20] - Train Loss: 0.3918, Val Loss: 0.2790, Val Acc: 0.8864, Val AUC: 0.9554\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9580\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [5/20] - Train Loss: 0.3733, Val Loss: 0.2715, Val Acc: 0.8977, Val AUC: 0.9580\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9585\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [6/20] - Train Loss: 0.4072, Val Loss: 0.2660, Val Acc: 0.8977, Val AUC: 0.9585\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9611\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [7/20] - Train Loss: 0.3315, Val Loss: 0.2633, Val Acc: 0.8977, Val AUC: 0.9611\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9642\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [8/20] - Train Loss: 0.3418, Val Loss: 0.2601, Val Acc: 0.9091, Val AUC: 0.9642\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9663\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [9/20] - Train Loss: 0.3881, Val Loss: 0.2553, Val Acc: 0.9091, Val AUC: 0.9663\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9683\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [10/20] - Train Loss: 0.3520, Val Loss: 0.2545, Val Acc: 0.9091, Val AUC: 0.9683\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9694\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [11/20] - Train Loss: 0.3358, Val Loss: 0.2556, Val Acc: 0.9091, Val AUC: 0.9694\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9699\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [12/20] - Train Loss: 0.3005, Val Loss: 0.2549, Val Acc: 0.9091, Val AUC: 0.9699\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9699\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [13/20] - Train Loss: 0.3167, Val Loss: 0.2547, Val Acc: 0.9091, Val AUC: 0.9699\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.9091, AUC: 0.9709\n",
      "Sensitivity: 0.9512, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 2 39]]\n",
      "Epoch [14/20] - Train Loss: 0.3146, Val Loss: 0.2532, Val Acc: 0.9091, Val AUC: 0.9709\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9709\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [15/20] - Train Loss: 0.3502, Val Loss: 0.2495, Val Acc: 0.8977, Val AUC: 0.9709\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9709\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [16/20] - Train Loss: 0.3210, Val Loss: 0.2463, Val Acc: 0.8977, Val AUC: 0.9709\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9709\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [17/20] - Train Loss: 0.3148, Val Loss: 0.2451, Val Acc: 0.8977, Val AUC: 0.9709\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9725\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [18/20] - Train Loss: 0.3099, Val Loss: 0.2456, Val Acc: 0.8977, Val AUC: 0.9725\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9720\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [19/20] - Train Loss: 0.2928, Val Loss: 0.2428, Val Acc: 0.8977, Val AUC: 0.9720\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8977, AUC: 0.9725\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Epoch [20/20] - Train Loss: 0.3112, Val Loss: 0.2407, Val Acc: 0.8977, Val AUC: 0.9725\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8864, AUC: 0.9673\n",
      "Sensitivity: 0.9024, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 4 37]]\n",
      "Test Loss: 0.2463, Test Accuracy: 0.8864, Test AUC: 0.9673\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm-trans-frozen/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            # torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Best hyperparameters: {'lr': 0.009940295438316211, 'weight_decay': 1.4383289881186473e-05, 'dropout': 0.22563027249521914}\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=lstm_frozen_best_param['dropout'])\n",
    "# model.load_state_dict(torch.load('best_model_lstm_1.pt')) \n",
    "model = load_partial_weights(model, 'best_model_lstm_1.pt', 6)\n",
    "\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_frozen_best_param['lr'],\n",
    "                      weight_decay=lstm_frozen_best_param['weight_decay'], verbose=True)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Run 1/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 2/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 3/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 4/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 5/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 6/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 7/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 8/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 9/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 10/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "📊 Summary across runs:\n",
      "Accuracy:       0.8795 ± 0.0116\n",
      "AUC:            0.9679 ± 0.0010\n",
      "Sensitivity:    0.9098 ± 0.0112\n",
      "Specificity:    0.8532 ± 0.0177\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Helper to suppress prints\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Number of repeated runs\n",
    "n_runs = 10\n",
    "\n",
    "# Metrics\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = LSTMClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=lstm_best_param['hidden_dim'],\n",
    "        num_layers=lstm_best_param['num_layers'],\n",
    "        dropout=lstm_frozen_best_param['dropout']\n",
    "    )\n",
    "    model = load_partial_weights(model, 'best_model_lstm_1.pt', 6)\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    # Suppress stdout during training/evaluation\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=20,\n",
    "            lr=lstm_frozen_best_param['lr'],\n",
    "            weight_decay=lstm_frozen_best_param['weight_decay'],\n",
    "            verbose=False\n",
    "        )\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Manually calculate confusion matrix\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    # Store metrics\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n📊 Summary across runs:\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-29 22:10:55,717] A new study created in memory with name: no-name-aed5ef02-5b53-40e1-ba2f-1e7b3602bf3e\n",
      "[I 2025-04-29 22:10:59,473] Trial 0 finished with value: 0.21000881989796957 and parameters: {'lr': 0.003638914711863913, 'weight_decay': 1.908177732451063e-06, 'dropout': 0.10566476075698641}. Best is trial 0 with value: 0.21000881989796957.\n",
      "[I 2025-04-29 22:11:03,125] Trial 1 finished with value: 0.2008835350473722 and parameters: {'lr': 0.0022261021284847387, 'weight_decay': 0.004372737378638452, 'dropout': 0.171167212301504}. Best is trial 1 with value: 0.2008835350473722.\n",
      "[I 2025-04-29 22:11:06,760] Trial 2 finished with value: 0.5003370443979899 and parameters: {'lr': 1.782791261327648e-05, 'weight_decay': 2.9545661586020474e-05, 'dropout': 0.243207621630648}. Best is trial 1 with value: 0.2008835350473722.\n",
      "[I 2025-04-29 22:11:10,419] Trial 3 finished with value: 0.20486789445082346 and parameters: {'lr': 0.0017625842351926672, 'weight_decay': 0.00038375384209343716, 'dropout': 0.25995088197314786}. Best is trial 1 with value: 0.2008835350473722.\n",
      "[I 2025-04-29 22:11:13,933] Trial 4 finished with value: 0.20870461563269296 and parameters: {'lr': 0.004611202628781895, 'weight_decay': 4.869293425677768e-06, 'dropout': 0.47337228461632663}. Best is trial 1 with value: 0.2008835350473722.\n",
      "[I 2025-04-29 22:11:17,499] Trial 5 finished with value: 0.16278578589359918 and parameters: {'lr': 0.0032156567293269668, 'weight_decay': 0.007051238524047399, 'dropout': 0.45440393197543916}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:11:21,063] Trial 6 finished with value: 0.18727222084999084 and parameters: {'lr': 0.0036916869028061185, 'weight_decay': 5.526668688031519e-05, 'dropout': 0.2634535635094343}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:11:24,648] Trial 7 finished with value: 0.42977269490559894 and parameters: {'lr': 4.782301367232561e-05, 'weight_decay': 0.0006338172575163281, 'dropout': 0.1589997638371613}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:11:28,206] Trial 8 finished with value: 0.417380357782046 and parameters: {'lr': 5.461405409703398e-05, 'weight_decay': 0.0004418107111323672, 'dropout': 0.19927653817359792}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:11:31,685] Trial 9 finished with value: 0.22950767974058786 and parameters: {'lr': 0.0006759051865556818, 'weight_decay': 9.603314793906388e-05, 'dropout': 0.13227232728088958}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:11:35,238] Trial 10 finished with value: 0.2715782970190048 and parameters: {'lr': 0.0002699917672155051, 'weight_decay': 0.006384916380104736, 'dropout': 0.41104015948334666}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:11:38,753] Trial 11 finished with value: 0.17791409293810526 and parameters: {'lr': 0.00882228317289317, 'weight_decay': 2.3199158834645607e-05, 'dropout': 0.35018794905132467}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:11:42,294] Trial 12 finished with value: 0.19605727742115656 and parameters: {'lr': 0.006822223508567665, 'weight_decay': 1.1199738483990568e-05, 'dropout': 0.36317303218978075}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:11:45,850] Trial 13 finished with value: 0.17062300443649292 and parameters: {'lr': 0.008855893163739845, 'weight_decay': 0.0022480747141169037, 'dropout': 0.35745812498208496}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:11:49,510] Trial 14 finished with value: 0.22337727000315985 and parameters: {'lr': 0.0006720691392264638, 'weight_decay': 0.0017979728142763004, 'dropout': 0.4690259390098778}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:11:53,172] Trial 15 finished with value: 0.197411447763443 and parameters: {'lr': 0.0009654646920234787, 'weight_decay': 0.009512006903850937, 'dropout': 0.41363991144386086}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:11:56,759] Trial 16 finished with value: 0.25197326640288037 and parameters: {'lr': 0.00031414841076898994, 'weight_decay': 0.001699623913814942, 'dropout': 0.32467019951647896}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:12:01,035] Trial 17 finished with value: 0.16765321418642998 and parameters: {'lr': 0.009901240455297665, 'weight_decay': 0.0019313713161614543, 'dropout': 0.4034306293279939}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:12:04,616] Trial 18 finished with value: 0.31896023948987323 and parameters: {'lr': 0.00014493235869636052, 'weight_decay': 0.00026533121491225607, 'dropout': 0.422824011291635}. Best is trial 5 with value: 0.16278578589359918.\n",
      "[I 2025-04-29 22:12:08,196] Trial 19 finished with value: 0.21420582135518393 and parameters: {'lr': 0.0016299097941217416, 'weight_decay': 0.001076686845981584, 'dropout': 0.49948241686723144}. Best is trial 5 with value: 0.16278578589359918.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.0032156567293269668, 'weight_decay': 0.007051238524047399, 'dropout': 0.45440393197543916}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM Classifier (same as before)\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=False,\n",
    "            dropout=dropout if num_layers > 1 else 0  # LSTM dropout applies only between layers\n",
    "        )\n",
    "\n",
    "        # Dropout after LSTM (even if 1 layer)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # Fully connected classifier\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        packed_output, (hn, cn) = self.lstm(packed_input)\n",
    "\n",
    "        # Get the last hidden state from the last layer\n",
    "        last_hidden = hn[-1]  # shape: [batch_size, hidden_dim]\n",
    "\n",
    "        # Apply dropout\n",
    "        dropped = self.dropout(last_hidden)\n",
    "\n",
    "        # Fully connected + sigmoid\n",
    "        out = self.fc(dropped)       # shape: [batch_size, 1]\n",
    "        out = self.sigmoid(out).squeeze(1)  # shape: [batch_size]\n",
    "        return out\n",
    "\n",
    "\n",
    "# Function to freeze the encoder (LSTM)\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation function with detailed output\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    # print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    # print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    # print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    # print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training function for frozen encoder\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000.0\n",
    "    log_dir = f\"runs-lstm-transfer-fullbackprop/fullbackprop_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        # if val_auc > best_val_auc:\n",
    "        #     best_val_auc = val_auc\n",
    "        #     torch.save(model.state_dict(), 'best_model_frozen.pt')\n",
    "            \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_full_backprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# # Load the best pretrained model and fine-tune\n",
    "# def finetune_with_frozen_encoder(pretrained_model_path, train_loader, val_loader, hidden_dim, num_layers, dropout):\n",
    "#     model = LSTMClassifier(input_dim=20, hidden_dim=hidden_dim, num_layers=num_layers, dropout=dropout)\n",
    "#     model.load_state_dict(torch.load(pretrained_model_path))\n",
    "#     # freeze_encoder(model)\n",
    "\n",
    "#     best_auc = train_finetune_model(\n",
    "#         model=model,\n",
    "#         train_loader=train_loader,\n",
    "#         val_loader=val_loader,\n",
    "#         num_epochs=10,\n",
    "#         lr=1e-3,\n",
    "#         weight_decay=1e-4\n",
    "#     )\n",
    "\n",
    "#     model.load_state_dict(torch.load('best_model_frozen.pt'))\n",
    "#     evaluate_model(model, val_loader, nn.BCELoss())\n",
    "\n",
    "#     return model, best_auc\n",
    "\n",
    "# model, best_auc = finetune_with_frozen_encoder(\n",
    "#     pretrained_model_path='best_model-lstm.pt',\n",
    "#     train_loader=train_loader,\n",
    "#     val_loader=val_loader,\n",
    "#     hidden_dim=47,  # or from Optuna\n",
    "#     num_layers=2,\n",
    "#     dropout=0.3\n",
    "# )\n",
    "\n",
    "\n",
    "# lstm_best_param = {'hidden_dim': 74,\n",
    "#  'num_layers': 3,\n",
    "#  'dropout': 0.3037059572844035,\n",
    "#  'lr': 0.00774103421243492,\n",
    "#  'weight_decay': 2.4221276513292614e-05}\n",
    "\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMClassifier(input_dim=20, hidden_dim=lstm_best_param['hidden_dim'], num_layers=lstm_best_param['num_layers'], dropout=dropout)\n",
    "    model.load_state_dict(torch.load('best_model_lstm_1.pt'))\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_fullbackprop_best_param = study.best_trial.params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_fullbackprop_best_param = {'lr': 0.0032156567293269668, 'weight_decay': 0.007051238524047399, 'dropout': 0.45440393197543916}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7348\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [1/19] - Train Loss: 0.6901, Val Loss: 334.0213, Val Acc: 0.5341, Val AUC: 0.7348\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7509\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [2/19] - Train Loss: 0.6835, Val Loss: 334.0184, Val Acc: 0.5341, Val AUC: 0.7509\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5341, AUC: 0.7509\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Epoch [3/19] - Train Loss: 0.6815, Val Loss: 334.0142, Val Acc: 0.5341, Val AUC: 0.7509\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5455, AUC: 0.7680\n",
      "Sensitivity: 0.0732, Specificity: 0.9574\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [38  3]]\n",
      "Epoch [4/19] - Train Loss: 0.6725, Val Loss: 334.0074, Val Acc: 0.5455, Val AUC: 0.7680\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.5682, AUC: 0.7841\n",
      "Sensitivity: 0.1463, Specificity: 0.9362\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [35  6]]\n",
      "Epoch [5/19] - Train Loss: 0.6575, Val Loss: 333.9853, Val Acc: 0.5682, Val AUC: 0.7841\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7500, AUC: 0.8158\n",
      "Sensitivity: 0.6829, Specificity: 0.8085\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [13 28]]\n",
      "Epoch [6/19] - Train Loss: 0.6326, Val Loss: 333.9299, Val Acc: 0.7500, Val AUC: 0.8158\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7614, AUC: 0.8298\n",
      "Sensitivity: 0.7073, Specificity: 0.8085\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [12 29]]\n",
      "Epoch [7/19] - Train Loss: 0.5993, Val Loss: 333.9469, Val Acc: 0.7614, Val AUC: 0.8298\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7955, AUC: 0.8583\n",
      "Sensitivity: 0.8293, Specificity: 0.7660\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 7 34]]\n",
      "Epoch [8/19] - Train Loss: 0.5489, Val Loss: 333.8637, Val Acc: 0.7955, Val AUC: 0.8583\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7841, AUC: 0.8236\n",
      "Sensitivity: 0.6829, Specificity: 0.8723\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [13 28]]\n",
      "Epoch [9/19] - Train Loss: 0.5184, Val Loss: 333.9017, Val Acc: 0.7841, Val AUC: 0.8236\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7045, AUC: 0.8625\n",
      "Sensitivity: 0.9512, Specificity: 0.4894\n",
      "Confusion Matrix:\n",
      "[[23 24]\n",
      " [ 2 39]]\n",
      "Epoch [10/19] - Train Loss: 0.5423, Val Loss: 333.9173, Val Acc: 0.7045, Val AUC: 0.8625\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7386, AUC: 0.8371\n",
      "Sensitivity: 0.5366, Specificity: 0.9149\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [19 22]]\n",
      "Epoch [11/19] - Train Loss: 0.5802, Val Loss: 333.9240, Val Acc: 0.7386, Val AUC: 0.8371\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7386, AUC: 0.8033\n",
      "Sensitivity: 0.5610, Specificity: 0.8936\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [18 23]]\n",
      "Epoch [12/19] - Train Loss: 0.5317, Val Loss: 333.9486, Val Acc: 0.7386, Val AUC: 0.8033\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8068, AUC: 0.8677\n",
      "Sensitivity: 0.9268, Specificity: 0.7021\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [ 3 38]]\n",
      "Epoch [13/19] - Train Loss: 0.4477, Val Loss: 333.8024, Val Acc: 0.8068, Val AUC: 0.8677\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8068, AUC: 0.8443\n",
      "Sensitivity: 0.8049, Specificity: 0.8085\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 8 33]]\n",
      "Epoch [14/19] - Train Loss: 0.4556, Val Loss: 333.8458, Val Acc: 0.8068, Val AUC: 0.8443\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8182, AUC: 0.8838\n",
      "Sensitivity: 0.9756, Specificity: 0.6809\n",
      "Confusion Matrix:\n",
      "[[32 15]\n",
      " [ 1 40]]\n",
      "Epoch [15/19] - Train Loss: 0.4594, Val Loss: 333.8064, Val Acc: 0.8182, Val AUC: 0.8838\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7955, AUC: 0.8822\n",
      "Sensitivity: 0.8293, Specificity: 0.7660\n",
      "Confusion Matrix:\n",
      "[[36 11]\n",
      " [ 7 34]]\n",
      "Epoch [16/19] - Train Loss: 0.5290, Val Loss: 333.7920, Val Acc: 0.7955, Val AUC: 0.8822\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8068, AUC: 0.8640\n",
      "Sensitivity: 0.8049, Specificity: 0.8085\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 8 33]]\n",
      "Epoch [17/19] - Train Loss: 0.4790, Val Loss: 333.8172, Val Acc: 0.8068, Val AUC: 0.8640\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7841, AUC: 0.8537\n",
      "Sensitivity: 0.8293, Specificity: 0.7447\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 7 34]]\n",
      "Epoch [18/19] - Train Loss: 0.4552, Val Loss: 333.8398, Val Acc: 0.7841, Val AUC: 0.8537\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.8068, AUC: 0.8749\n",
      "Sensitivity: 0.9268, Specificity: 0.7021\n",
      "Confusion Matrix:\n",
      "[[33 14]\n",
      " [ 3 38]]\n",
      "Epoch [19/19] - Train Loss: 0.5231, Val Loss: 333.8680, Val Acc: 0.8068, Val AUC: 0.8749\n",
      "\n",
      "Final Evaluation on Best Model:\n",
      "Accuracy: 0.7841, AUC: 0.8365\n",
      "Sensitivity: 0.8537, Specificity: 0.7234\n",
      "Confusion Matrix:\n",
      "[[34 13]\n",
      " [ 6 35]]\n",
      "Test Loss: 333.8707, Test Accuracy: 0.7841, Test AUC: 0.8365\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "    # Optional: For workers in DataLoader\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "\n",
    "    g = torch.Generator()\n",
    "    g.manual_seed(seed)\n",
    "\n",
    "    return seed_worker, g\n",
    "\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 1000.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    print(f\"\\nFinal Evaluation on Best Model:\")\n",
    "    print(f\"Accuracy: {acc:.4f}, AUC: {auc:.4f}\")\n",
    "    print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "    print(f\"Confusion Matrix:\\n{cm}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Best hyperparameters: {'lr': 0.008986542560528932, 'weight_decay': 2.3033044758439348e-06, 'dropout': 0.17164705350229123}\n",
    "\n",
    "model = LSTMClassifier(input_dim=20, hidden_dim=113, num_layers=1, dropout=lstm_fullbackprop_best_param['dropout'])\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=19, lr=lstm_fullbackprop_best_param['lr'],\n",
    "                      weight_decay=lstm_fullbackprop_best_param['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Run 1/10\n",
      "\n",
      "🔁 Run 2/10\n",
      "\n",
      "🔁 Run 3/10\n",
      "\n",
      "🔁 Run 4/10\n",
      "\n",
      "🔁 Run 5/10\n",
      "\n",
      "🔁 Run 6/10\n",
      "\n",
      "🔁 Run 7/10\n",
      "\n",
      "🔁 Run 8/10\n",
      "\n",
      "🔁 Run 9/10\n",
      "\n",
      "🔁 Run 10/10\n",
      "\n",
      "📊 Summary across runs (Full Backpropagation Model):\n",
      "Accuracy:       0.7943 ± 0.0596\n",
      "AUC:            0.8597 ± 0.0230\n",
      "Sensitivity:    0.8293 ± 0.0408\n",
      "Specificity:    0.7638 ± 0.1344\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Helper to suppress prints\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Number of repeated runs\n",
    "n_runs = 10\n",
    "\n",
    "# Metrics\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = LSTMClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=113,\n",
    "        num_layers=1,\n",
    "        dropout=lstm_fullbackprop_best_param['dropout']\n",
    "    )\n",
    "\n",
    "    # Suppress stdout during training and evaluation\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=19,\n",
    "            lr=lstm_fullbackprop_best_param['lr'],\n",
    "            weight_decay=lstm_fullbackprop_best_param['weight_decay'],\n",
    "            verbose=False,\n",
    "            train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Manually calculate confusion matrix\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    # Store metrics\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n📊 Summary across runs (Full Backpropagation Model):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:21:50,353] A new study created in memory with name: no-name-5b7b9158-2c95-45e5-8606-63e37fe2c6b9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:21:55,279] Trial 0 finished with value: 0.6758488416671753 and parameters: {'lr': 2.534958014869424e-05, 'weight_decay': 5.775892490287451e-06, 'dropout': 0.47044132331479194}. Best is trial 0 with value: 0.6758488416671753.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:21:59,783] Trial 1 finished with value: 0.3800984521706899 and parameters: {'lr': 0.0009222464469545083, 'weight_decay': 0.003685347601942048, 'dropout': 0.2798552175051443}. Best is trial 1 with value: 0.3800984521706899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:04,239] Trial 2 finished with value: 0.6288740833600363 and parameters: {'lr': 8.372552686429017e-05, 'weight_decay': 0.00016289020356816616, 'dropout': 0.10072481950675467}. Best is trial 1 with value: 0.3800984521706899.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:08,807] Trial 3 finished with value: 0.24294357374310493 and parameters: {'lr': 0.004837242820313321, 'weight_decay': 2.03252861488189e-06, 'dropout': 0.4939852104684296}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:13,321] Trial 4 finished with value: 0.6850630839665731 and parameters: {'lr': 1.0527937585663375e-05, 'weight_decay': 0.001708621952299083, 'dropout': 0.4666507585113433}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:17,882] Trial 5 finished with value: 0.3745337128639221 and parameters: {'lr': 0.0011722529196370862, 'weight_decay': 8.048278633277898e-06, 'dropout': 0.1515965521268682}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:22,596] Trial 6 finished with value: 0.47653934359550476 and parameters: {'lr': 0.0002960287195176182, 'weight_decay': 0.0003671978912784095, 'dropout': 0.26206207816825255}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:27,299] Trial 7 finished with value: 0.658479650815328 and parameters: {'lr': 5.387194863178497e-05, 'weight_decay': 4.560847872197345e-06, 'dropout': 0.4926631250252146}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:31,814] Trial 8 finished with value: 0.2901750902334849 and parameters: {'lr': 0.0038245641496077658, 'weight_decay': 1.4064561809619517e-06, 'dropout': 0.1380299222181979}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:36,332] Trial 9 finished with value: 0.34490164120992023 and parameters: {'lr': 0.0014760765335687716, 'weight_decay': 3.790189709338313e-05, 'dropout': 0.20762947327028114}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:40,977] Trial 10 finished with value: 0.258048386623462 and parameters: {'lr': 0.008615096803226382, 'weight_decay': 3.1643483102718286e-05, 'dropout': 0.38750793280278395}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:46,007] Trial 11 finished with value: 0.2891167104244232 and parameters: {'lr': 0.009747807202707017, 'weight_decay': 3.595043545313575e-05, 'dropout': 0.38243509970707334}. Best is trial 3 with value: 0.24294357374310493.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:50,713] Trial 12 finished with value: 0.2363847754895687 and parameters: {'lr': 0.009981673641982228, 'weight_decay': 1.5244563205828208e-06, 'dropout': 0.38560031839200354}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:22:55,369] Trial 13 finished with value: 0.29435809701681137 and parameters: {'lr': 0.00265186525287254, 'weight_decay': 1.1576991126832467e-06, 'dropout': 0.37902768424249167}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:23:00,064] Trial 14 finished with value: 0.428882896900177 and parameters: {'lr': 0.00044234575302963883, 'weight_decay': 3.310321963477673e-06, 'dropout': 0.41532635632373033}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:23:04,939] Trial 15 finished with value: 0.24932534930606684 and parameters: {'lr': 0.00519679053034669, 'weight_decay': 1.1648959481001289e-05, 'dropout': 0.3225174342173375}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:23:09,778] Trial 16 finished with value: 0.271179494758447 and parameters: {'lr': 0.002625673993374063, 'weight_decay': 1.210976339858137e-06, 'dropout': 0.4298577593923169}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:23:14,316] Trial 17 finished with value: 0.46983620524406433 and parameters: {'lr': 0.0003176335273641833, 'weight_decay': 0.00044223216635708146, 'dropout': 0.34063831470826716}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:23:19,014] Trial 18 finished with value: 0.26390478014945984 and parameters: {'lr': 0.005400587032946064, 'weight_decay': 1.5738645643086362e-05, 'dropout': 0.43652416422873896}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:23:23,675] Trial 19 finished with value: 0.41215527057647705 and parameters: {'lr': 0.0006554042007735221, 'weight_decay': 2.715948234261771e-06, 'dropout': 0.49822741500077933}. Best is trial 12 with value: 0.2363847754895687.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.009981673641982228, 'weight_decay': 1.5244563205828208e-06, 'dropout': 0.38560031839200354}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm-trans-frozen/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path, max_layers=None):\n",
    "    \"\"\"\n",
    "    Load up to `max_layers` compatible layers from a checkpoint into the model.\n",
    "    If max_layers is None, load all compatible layers.\n",
    "    \"\"\"\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter compatible layers\n",
    "    compatible_items = [\n",
    "        (k, v) for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    ]\n",
    "\n",
    "    # Limit number of layers to load\n",
    "    if max_layers is not None:\n",
    "        compatible_items = compatible_items[:max_layers]\n",
    "\n",
    "    # Convert list of tuples back to dict\n",
    "    compatible_dict = dict(compatible_items)\n",
    "\n",
    "    # Update model state dict\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def freeze_encoder(model, num_layers_to_freeze):\n",
    "    \"\"\"\n",
    "    Freezes the first `num_layers_to_freeze` LSTM layers of the model.\n",
    "    Assumes parameter names follow standard PyTorch LSTM naming.\n",
    "    \"\"\"\n",
    "    if num_layers_to_freeze <= 0:\n",
    "        print(\"⚠️ No LSTM layers frozen.\")\n",
    "        return\n",
    "\n",
    "    layer_prefixes = [f'lstm.weight_ih_l{i}' for i in range(num_layers_to_freeze)]\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(prefix in name for prefix in layer_prefixes):\n",
    "            param.requires_grad = False\n",
    "\n",
    "    print(f\"✅ Frozen first {num_layers_to_freeze} LSTM layers.\")\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=dropout, max_seq_len=100)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt', 4)\n",
    "    freeze_encoder(model, 4)\n",
    "\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### running repeats, accounting for stochastic results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_frozen_best_parameters = {'lr': 0.009981673641982228,\n",
    " 'weight_decay': 1.5244563205828208e-06,\n",
    " 'dropout': 0.38560031839200354}\n",
    "\n",
    "bilstm_best_param = {'hidden_dim': 59, 'num_layers': 1, 'dropout': 0.19595557432276017, 'lr': 0.009816503475479747, 'weight_decay': 2.600777275437832e-05}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[25 22]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.5319\n",
      "Epoch [1/15] - Train Loss: 0.5974, Val Loss: 0.4955, Val Acc: 0.7273, Val AUC: 0.9185\n",
      "\n",
      "Confusion Matrix:\n",
      "[[37 10]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.7872\n",
      "Epoch [2/15] - Train Loss: 0.3407, Val Loss: 0.3751, Val Acc: 0.8636, Val AUC: 0.9388\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8936\n",
      "Epoch [3/15] - Train Loss: 0.2933, Val Loss: 0.2824, Val Acc: 0.8636, Val AUC: 0.9559\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.8511\n",
      "Epoch [4/15] - Train Loss: 0.1991, Val Loss: 0.2963, Val Acc: 0.8750, Val AUC: 0.9538\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 3 38]]\n",
      "Sensitivity: 0.9268, Specificity: 0.8511\n",
      "Epoch [5/15] - Train Loss: 0.1611, Val Loss: 0.3151, Val Acc: 0.8864, Val AUC: 0.9585\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8936\n",
      "Epoch [6/15] - Train Loss: 0.1474, Val Loss: 0.2946, Val Acc: 0.8636, Val AUC: 0.9554\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.8936\n",
      "Epoch [7/15] - Train Loss: 0.0780, Val Loss: 0.3116, Val Acc: 0.8977, Val AUC: 0.9585\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 4 37]]\n",
      "Sensitivity: 0.9024, Specificity: 0.9149\n",
      "Epoch [8/15] - Train Loss: 0.0615, Val Loss: 0.2856, Val Acc: 0.9091, Val AUC: 0.9663\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.9149\n",
      "Epoch [9/15] - Train Loss: 0.0327, Val Loss: 0.3162, Val Acc: 0.9318, Val AUC: 0.9715\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8511\n",
      "Epoch [10/15] - Train Loss: 0.0155, Val Loss: 0.3495, Val Acc: 0.8977, Val AUC: 0.9751\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.9362\n",
      "Epoch [11/15] - Train Loss: 0.0120, Val Loss: 0.2921, Val Acc: 0.9432, Val AUC: 0.9782\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Epoch [12/15] - Train Loss: 0.0077, Val Loss: 0.2787, Val Acc: 0.9205, Val AUC: 0.9741\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.8936\n",
      "Epoch [13/15] - Train Loss: 0.0048, Val Loss: 0.3251, Val Acc: 0.9205, Val AUC: 0.9782\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.9362\n",
      "Epoch [14/15] - Train Loss: 0.0016, Val Loss: 0.2991, Val Acc: 0.9432, Val AUC: 0.9808\n",
      "\n",
      "Confusion Matrix:\n",
      "[[44  3]\n",
      " [ 2 39]]\n",
      "Sensitivity: 0.9512, Specificity: 0.9362\n",
      "Epoch [15/15] - Train Loss: 0.0008, Val Loss: 0.3113, Val Acc: 0.9432, Val AUC: 0.9829\n",
      "\n",
      "Confusion Matrix:\n",
      "[[43  4]\n",
      " [ 9 32]]\n",
      "Sensitivity: 0.7805, Specificity: 0.9149\n",
      "Test Loss: 1.0462, Test Accuracy: 0.8523, Test AUC: 0.9201\n"
     ]
    }
   ],
   "source": [
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze encoder\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path, max_layers=None):\n",
    "    \"\"\"\n",
    "    Load up to `max_layers` compatible layers from a checkpoint into the model.\n",
    "    If max_layers is None, load all compatible layers.\n",
    "    \"\"\"\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter compatible layers\n",
    "    compatible_items = [\n",
    "        (k, v) for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    ]\n",
    "\n",
    "    # Limit number of layers to load\n",
    "    if max_layers is not None:\n",
    "        compatible_items = compatible_items[:max_layers]\n",
    "\n",
    "    # Convert list of tuples back to dict\n",
    "    compatible_dict = dict(compatible_items)\n",
    "\n",
    "    # Update model state dict\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def freeze_encoder(model, num_layers_to_freeze):\n",
    "    \"\"\"\n",
    "    Freezes the first `num_layers_to_freeze` LSTM layers of the model.\n",
    "    Assumes parameter names follow standard PyTorch LSTM naming.\n",
    "    \"\"\"\n",
    "    if num_layers_to_freeze <= 0:\n",
    "        print(\"⚠️ No LSTM layers frozen.\")\n",
    "        return\n",
    "\n",
    "    layer_prefixes = [f'lstm.weight_ih_l{i}' for i in range(num_layers_to_freeze)]\n",
    "    for name, param in model.named_parameters():\n",
    "        if any(prefix in name for prefix in layer_prefixes):\n",
    "            param.requires_grad = False\n",
    "\n",
    "    print(f\"✅ Frozen first {num_layers_to_freeze} LSTM layers.\")\n",
    "    \n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# model = LSTMClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'])\n",
    "model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'], max_seq_len=100)\n",
    "\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt', 4)\n",
    "# freeze_encoder(model)\n",
    "freeze_encoder(model, 4)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=15, lr=bilstm_frozen_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Run 1/10\n",
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n",
      "\n",
      "🔁 Run 2/10\n",
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n",
      "\n",
      "🔁 Run 3/10\n",
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n",
      "\n",
      "🔁 Run 4/10\n",
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n",
      "\n",
      "🔁 Run 5/10\n",
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n",
      "\n",
      "🔁 Run 6/10\n",
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n",
      "\n",
      "🔁 Run 7/10\n",
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n",
      "\n",
      "🔁 Run 8/10\n",
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n",
      "\n",
      "🔁 Run 9/10\n",
      "✅ Loaded 4 matching layers from checkpoint.\n",
      "✅ Frozen first 4 LSTM layers.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Helper to suppress prints\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Number of repeated runs\n",
    "n_runs = 10\n",
    "\n",
    "# Metrics\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=bilstm_best_param['hidden_dim'],\n",
    "        num_layers=bilstm_best_param['num_layers'],\n",
    "        dropout=bilstm_frozen_best_parameters['dropout'],\n",
    "        max_seq_len=100\n",
    "    )\n",
    "\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt', 4)\n",
    "    freeze_encoder(model, 4)\n",
    "\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=30,\n",
    "            lr=bilstm_frozen_best_parameters['lr'],\n",
    "            weight_decay=bilstm_frozen_best_parameters['weight_decay'],\n",
    "            verbose=False,\n",
    "            train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Calculate sensitivity and specificity\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    # Store metrics\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n📊 Summary across runs (BiLSTM with Flatten & Frozen Encoder):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:22,477] A new study created in memory with name: no-name-55b57740-d999-48bf-bfc7-ed8bf7f76410\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:28,751] Trial 0 finished with value: 0.6472770969072977 and parameters: {'lr': 2.7468020893093288e-05, 'weight_decay': 1.9823888137898948e-05, 'dropout': 0.3614048468861061}. Best is trial 0 with value: 0.6472770969072977.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:34,951] Trial 1 finished with value: 0.5378240942955017 and parameters: {'lr': 0.00011050232434267229, 'weight_decay': 0.002430591078406706, 'dropout': 0.44456875419188324}. Best is trial 1 with value: 0.5378240942955017.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:41,269] Trial 2 finished with value: 0.40694797535737354 and parameters: {'lr': 0.00040441757443146373, 'weight_decay': 0.0007244162314611645, 'dropout': 0.2543936645248715}. Best is trial 2 with value: 0.40694797535737354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:47,574] Trial 3 finished with value: 0.6023574670155843 and parameters: {'lr': 5.552704854986336e-05, 'weight_decay': 5.1417614802022544e-05, 'dropout': 0.24668768873375035}. Best is trial 2 with value: 0.40694797535737354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:53,829] Trial 4 finished with value: 0.41414891680081684 and parameters: {'lr': 0.0001858988397340464, 'weight_decay': 9.383893221533215e-05, 'dropout': 0.10417637110993555}. Best is trial 2 with value: 0.40694797535737354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:30:59,670] Trial 5 finished with value: 0.6527635852495829 and parameters: {'lr': 2.417719030171581e-05, 'weight_decay': 4.461338135656818e-06, 'dropout': 0.12875554245744866}. Best is trial 2 with value: 0.40694797535737354.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:05,847] Trial 6 finished with value: 0.31085051223635674 and parameters: {'lr': 0.00926595862633317, 'weight_decay': 2.0121744840074475e-05, 'dropout': 0.46616976355966055}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:12,176] Trial 7 finished with value: 0.6513139009475708 and parameters: {'lr': 2.7391283145004265e-05, 'weight_decay': 0.0030115904593620655, 'dropout': 0.40326048753502586}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:18,357] Trial 8 finished with value: 0.3694236824909846 and parameters: {'lr': 0.0024766730126500607, 'weight_decay': 0.00023421014949312951, 'dropout': 0.4115624504911498}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:24,634] Trial 9 finished with value: 0.46098150809605914 and parameters: {'lr': 0.00018670885203011189, 'weight_decay': 0.004383737500132786, 'dropout': 0.20913943083997089}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:30,951] Trial 10 finished with value: 0.3658249229192734 and parameters: {'lr': 0.008125491550040213, 'weight_decay': 1.6142305843854046e-06, 'dropout': 0.49824230257692326}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:37,836] Trial 11 finished with value: 0.3162108088533084 and parameters: {'lr': 0.009357507940506937, 'weight_decay': 1.1938352503357078e-06, 'dropout': 0.49880941136603546}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:44,780] Trial 12 finished with value: 0.31403346980611485 and parameters: {'lr': 0.009659907971180574, 'weight_decay': 8.925280816078873e-06, 'dropout': 0.49912495000798535}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:31:53,491] Trial 13 finished with value: 0.38085274398326874 and parameters: {'lr': 0.001595409448541909, 'weight_decay': 1.1779100082492163e-05, 'dropout': 0.315948475819138}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:32:00,650] Trial 14 finished with value: 0.3830023507277171 and parameters: {'lr': 0.0025971744526537134, 'weight_decay': 1.0979574052414046e-05, 'dropout': 0.4516587988428966}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:32:09,012] Trial 15 finished with value: 0.39438508947690326 and parameters: {'lr': 0.0008109533614610762, 'weight_decay': 2.6958452498126817e-05, 'dropout': 0.3488070866340102}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:32:16,277] Trial 16 finished with value: 0.35764744877815247 and parameters: {'lr': 0.0047700970692806915, 'weight_decay': 4.393842864792311e-06, 'dropout': 0.4536291628595147}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:32:22,962] Trial 17 finished with value: 0.40200090408325195 and parameters: {'lr': 0.0009538114802220414, 'weight_decay': 0.00024331960402126517, 'dropout': 0.3978024930675375}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:32:33,702] Trial 18 finished with value: 0.3766237248977025 and parameters: {'lr': 0.004245728608577005, 'weight_decay': 3.8442241149315545e-06, 'dropout': 0.4758037419174857}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 17:32:42,339] Trial 19 finished with value: 0.40634193023045856 and parameters: {'lr': 0.0005770924461404385, 'weight_decay': 5.446901693669708e-05, 'dropout': 0.3651356881275941}. Best is trial 6 with value: 0.31085051223635674.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.00926595862633317, 'weight_decay': 2.0121744840074475e-05, 'dropout': 0.46616976355966055}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "class BiLSTMWithFlattenClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3, max_seq_len=100):\n",
    "        super(BiLSTMWithFlattenClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.max_seq_len = max_seq_len\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(max_seq_len * hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        batch_size, seq_len, feature_dim = lstm_out.size()\n",
    "\n",
    "        if seq_len < self.max_seq_len:\n",
    "            pad_len = self.max_seq_len - seq_len\n",
    "            pad = torch.zeros(batch_size, pad_len, feature_dim, device=lstm_out.device)\n",
    "            lstm_out = torch.cat([lstm_out, pad], dim=1)\n",
    "        elif seq_len > self.max_seq_len:\n",
    "            lstm_out = lstm_out[:, :self.max_seq_len, :]\n",
    "\n",
    "        dropped = self.dropout(lstm_out)\n",
    "        flat = dropped.contiguous().view(batch_size, -1)\n",
    "        out = self.fc(flat)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm-trans-fullbackprop/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def load_partial_weights(model, checkpoint_path, max_layers=None):\n",
    "    \"\"\"\n",
    "    Load up to `max_layers` compatible layers from a checkpoint into the model.\n",
    "    If max_layers is None, load all compatible layers.\n",
    "    \"\"\"\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    # Filter compatible layers\n",
    "    compatible_items = [\n",
    "        (k, v) for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    ]\n",
    "\n",
    "    # Limit number of layers to load\n",
    "    if max_layers is not None:\n",
    "        compatible_items = compatible_items[:max_layers]\n",
    "\n",
    "    # Convert list of tuples back to dict\n",
    "    compatible_dict = dict(compatible_items)\n",
    "\n",
    "    # Update model state dict\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=dropout, max_seq_len=100)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt', 6)\n",
    "\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=20, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_fullbackprop_best_parameters = {'lr': 0.00926595862633317, 'weight_decay': 2.0121744840074475e-05, 'dropout': 0.46616976355966055}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8085\n",
      "Epoch [1/10] - Train Loss: 0.5303, Val Loss: 0.3890, Val Acc: 0.8295, Val AUC: 0.9165\n",
      "\n",
      "Confusion Matrix:\n",
      "[[35 12]\n",
      " [ 5 36]]\n",
      "Sensitivity: 0.8780, Specificity: 0.7447\n",
      "Epoch [2/10] - Train Loss: 0.2880, Val Loss: 0.4410, Val Acc: 0.8068, Val AUC: 0.9180\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8936\n",
      "Epoch [3/10] - Train Loss: 0.1930, Val Loss: 0.3648, Val Acc: 0.8750, Val AUC: 0.9325\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8511\n",
      "Epoch [4/10] - Train Loss: 0.0979, Val Loss: 0.4236, Val Acc: 0.8523, Val AUC: 0.9268\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8723\n",
      "Epoch [5/10] - Train Loss: 0.0683, Val Loss: 0.4882, Val Acc: 0.8523, Val AUC: 0.9284\n",
      "\n",
      "Confusion Matrix:\n",
      "[[38  9]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8085\n",
      "Epoch [6/10] - Train Loss: 0.0456, Val Loss: 0.7827, Val Acc: 0.8295, Val AUC: 0.9170\n",
      "\n",
      "Confusion Matrix:\n",
      "[[41  6]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8723\n",
      "Epoch [7/10] - Train Loss: 0.0255, Val Loss: 0.5153, Val Acc: 0.8523, Val AUC: 0.9310\n",
      "\n",
      "Confusion Matrix:\n",
      "[[40  7]\n",
      " [ 6 35]]\n",
      "Sensitivity: 0.8537, Specificity: 0.8511\n",
      "Epoch [8/10] - Train Loss: 0.0130, Val Loss: 0.5593, Val Acc: 0.8523, Val AUC: 0.9414\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8936\n",
      "Epoch [9/10] - Train Loss: 0.0094, Val Loss: 0.6924, Val Acc: 0.8636, Val AUC: 0.9341\n",
      "\n",
      "Confusion Matrix:\n",
      "[[42  5]\n",
      " [ 7 34]]\n",
      "Sensitivity: 0.8293, Specificity: 0.8936\n",
      "Epoch [10/10] - Train Loss: 0.0049, Val Loss: 0.7135, Val Acc: 0.8636, Val AUC: 0.9398\n",
      "\n",
      "Confusion Matrix:\n",
      "[[45  2]\n",
      " [ 8 33]]\n",
      "Sensitivity: 0.8049, Specificity: 0.9574\n",
      "Test Loss: 0.6370, Test Accuracy: 0.8864, Test AUC: 0.9528\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "\n",
    "# to rerun\n",
    "model = BiLSTMWithFlattenClassifier(input_dim=20, hidden_dim=bilstm_best_param['hidden_dim'], num_layers=bilstm_best_param['num_layers'], dropout=bilstm_frozen_best_parameters['dropout'], max_seq_len=100)\n",
    "# model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "model = load_partial_weights(model, 'best_model-bilstm.pt', 6)\n",
    "\n",
    "# freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=10, lr=bilstm_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Run 1/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 2/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 3/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 4/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 5/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 6/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 7/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 8/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 9/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 10/10\n",
      "✅ Loaded 6 matching layers from checkpoint.\n",
      "\n",
      "📊 Summary across runs (BiLSTM + Flatten, Full Backprop):\n",
      "Accuracy:       0.8818 ± 0.0239\n",
      "AUC:            0.9519 ± 0.0080\n",
      "Sensitivity:    0.8171 ± 0.0225\n",
      "Specificity:    0.9383 ± 0.0349\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Suppress stdout context manager\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Number of repeated runs\n",
    "n_runs = 10\n",
    "\n",
    "# Metric containers\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = BiLSTMWithFlattenClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=bilstm_best_param['hidden_dim'],\n",
    "        num_layers=bilstm_best_param['num_layers'],\n",
    "        dropout=bilstm_frozen_best_parameters['dropout'],\n",
    "        max_seq_len=100\n",
    "    )\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt', 6)\n",
    "    # Not freezing encoder (full backprop setting)\n",
    "\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=10,\n",
    "            lr=bilstm_fullbackprop_best_parameters['lr'],\n",
    "            weight_decay=bilstm_fullbackprop_best_parameters['weight_decay'],\n",
    "            verbose=False,\n",
    "            train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Calculate confusion matrix for sensitivity/specificity\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    # Store metrics\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Print results\n",
    "print(\"\\n📊 Summary across runs (BiLSTM + Flatten, Full Backprop):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### lstm + attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:21,490] A new study created in memory with name: no-name-a7d840e6-6b44-4aac-bb65-e37e9b4eab81\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:25,753] Trial 0 finished with value: 0.7674408356348673 and parameters: {'lr': 0.0013602003194664756, 'weight_decay': 1.5712677101296663e-05, 'dropout': 0.4169138472289945}. Best is trial 0 with value: 0.7674408356348673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:30,111] Trial 1 finished with value: 0.895917534828186 and parameters: {'lr': 0.00013430607061839717, 'weight_decay': 0.0042018236062449435, 'dropout': 0.15353305409980217}. Best is trial 0 with value: 0.7674408356348673.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:34,547] Trial 2 finished with value: 0.692759116490682 and parameters: {'lr': 0.003780147510933303, 'weight_decay': 4.147651339063238e-05, 'dropout': 0.15484650861534519}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:38,612] Trial 3 finished with value: 0.7345146139462789 and parameters: {'lr': 0.0016964887404221295, 'weight_decay': 0.0025817663584606415, 'dropout': 0.3348237829311172}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:42,484] Trial 4 finished with value: 0.8995350400606791 and parameters: {'lr': 4.9828507299876767e-05, 'weight_decay': 1.0333765354408845e-05, 'dropout': 0.47991499384796643}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:46,517] Trial 5 finished with value: 0.8928668101628622 and parameters: {'lr': 0.00011988681439998793, 'weight_decay': 0.0002677504823600038, 'dropout': 0.1378990760927295}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:50,750] Trial 6 finished with value: 0.9062303105990092 and parameters: {'lr': 3.1463892370362697e-05, 'weight_decay': 0.006769770872029123, 'dropout': 0.1857350862362233}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:54,930] Trial 7 finished with value: 0.9076404372851054 and parameters: {'lr': 1.0546460736553949e-05, 'weight_decay': 2.218658426160035e-05, 'dropout': 0.1200372432481383}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:43:59,195] Trial 8 finished with value: 0.893417239189148 and parameters: {'lr': 2.5182458713641945e-05, 'weight_decay': 0.001656918686782995, 'dropout': 0.1668816125518131}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:44:03,516] Trial 9 finished with value: 0.9056477149327596 and parameters: {'lr': 4.6009844895884564e-05, 'weight_decay': 0.0006708921290933786, 'dropout': 0.19471185829869456}. Best is trial 2 with value: 0.692759116490682.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:44:07,962] Trial 10 finished with value: 0.6912072499593099 and parameters: {'lr': 0.009315794991764154, 'weight_decay': 1.3701958442320686e-06, 'dropout': 0.26912897628898586}. Best is trial 10 with value: 0.6912072499593099.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:44:12,344] Trial 11 finished with value: 0.691220243771871 and parameters: {'lr': 0.009073800913924703, 'weight_decay': 1.3122066920704696e-06, 'dropout': 0.27779103538774164}. Best is trial 10 with value: 0.6912072499593099.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.009315794991764154, 'weight_decay': 1.3701958442320686e-06, 'dropout': 0.26912897628898586}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM with Attention\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze LSTM encoder\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-att-trans/LSTMAttTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_att_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Load compatible weights\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "# Freeze LSTM encoder\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=dropout)\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=12)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_att_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_att_frozen_best_parameters = {'lr': 0.009315794991764154,\n",
    " 'weight_decay': 1.3701958442320686e-06,\n",
    " 'dropout': 0.26912897628898586}\n",
    "\n",
    "lstm_attn_best_param = {'hidden_dim': 85, 'num_layers': 1, 'dropout': 0.4548453210586073, 'lr': 0.005881852254528488, 'weight_decay': 1.22922875003128e-06}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [1/20] - Train Loss: 0.8695, Val Loss: 0.8136, Val Acc: 0.4659, Val AUC: 0.6118\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [2/20] - Train Loss: 0.7840, Val Loss: 0.7531, Val Acc: 0.4659, Val AUC: 0.6020\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [3/20] - Train Loss: 0.7417, Val Loss: 0.7190, Val Acc: 0.4659, Val AUC: 0.6051\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [4/20] - Train Loss: 0.7128, Val Loss: 0.7005, Val Acc: 0.4659, Val AUC: 0.6108\n",
      "\n",
      "Confusion Matrix:\n",
      "[[ 0 47]\n",
      " [ 0 41]]\n",
      "Sensitivity: 1.0000, Specificity: 0.0000\n",
      "Epoch [5/20] - Train Loss: 0.6984, Val Loss: 0.6934, Val Acc: 0.4659, Val AUC: 0.6129\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/20] - Train Loss: 0.6919, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6186\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/20] - Train Loss: 0.6926, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6274\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [8/20] - Train Loss: 0.6953, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6321\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [9/20] - Train Loss: 0.6877, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6321\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [10/20] - Train Loss: 0.6874, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6380\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [11/20] - Train Loss: 0.6915, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6393\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [12/20] - Train Loss: 0.6838, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6476\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [13/20] - Train Loss: 0.6956, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6523\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [14/20] - Train Loss: 0.6889, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6554\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [15/20] - Train Loss: 0.6889, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6596\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [16/20] - Train Loss: 0.6899, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6627\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [17/20] - Train Loss: 0.6931, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6679\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [18/20] - Train Loss: 0.6835, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6637\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [19/20] - Train Loss: 0.6951, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.6653\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [20/20] - Train Loss: 0.6879, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.6663\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Test Loss: 0.6923, Test Accuracy: 0.5341, Test AUC: 0.5724\n"
     ]
    }
   ],
   "source": [
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "# Freeze LSTM encoder\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# to rerun\n",
    "model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=lstm_att_frozen_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_att_frozen_best_parameters['lr'],\n",
    "                      weight_decay=lstm_att_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Run 1/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 2/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 3/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 4/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 5/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 6/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 7/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 8/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 9/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 10/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "📊 Summary across runs (LSTM + Attention, Frozen Encoder):\n",
      "Accuracy:       0.5341 ± 0.0000\n",
      "AUC:            0.4684 ± 0.1193\n",
      "Sensitivity:    0.0000 ± 0.0000\n",
      "Specificity:    1.0000 ± 0.0000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Suppress print helper\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Number of repeated runs\n",
    "n_runs = 10\n",
    "\n",
    "# Containers for metrics\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=69,\n",
    "        num_layers=2,\n",
    "        dropout=lstm_att_frozen_best_parameters['dropout']\n",
    "    )\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=20,\n",
    "            lr=lstm_att_frozen_best_parameters['lr'],\n",
    "            weight_decay=lstm_att_frozen_best_parameters['weight_decay'],\n",
    "            verbose=False,\n",
    "            train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Gather predictions for confusion matrix\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Report results\n",
    "print(\"\\n📊 Summary across runs (LSTM + Attention, Frozen Encoder):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full back prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:45:47,883] A new study created in memory with name: no-name-76b0eca7-46cb-4162-8dba-40a3f14a8454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:45:52,416] Trial 0 finished with value: 0.4709471066792806 and parameters: {'lr': 0.00442440456463527, 'weight_decay': 0.0008226696089013947, 'dropout': 0.30955810155370944}. Best is trial 0 with value: 0.4709471066792806.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:45:56,951] Trial 1 finished with value: 0.4392253557840983 and parameters: {'lr': 0.003661950605748677, 'weight_decay': 1.6001438653977643e-06, 'dropout': 0.4778793892133558}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:01,439] Trial 2 finished with value: 0.8960553606351217 and parameters: {'lr': 3.032894962764756e-05, 'weight_decay': 0.008733957208604901, 'dropout': 0.44586469529109785}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:06,022] Trial 3 finished with value: 0.48641664783159894 and parameters: {'lr': 0.0017527997703626144, 'weight_decay': 1.491832353517625e-06, 'dropout': 0.10531062549569291}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:10,496] Trial 4 finished with value: 0.6915572881698608 and parameters: {'lr': 0.00020896990510620403, 'weight_decay': 0.00124155424080721, 'dropout': 0.3618972607271408}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:14,442] Trial 5 finished with value: 0.8237904111544291 and parameters: {'lr': 7.533312097210972e-05, 'weight_decay': 0.00317457248026703, 'dropout': 0.37683960635297187}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:18,810] Trial 6 finished with value: 0.8280006845792135 and parameters: {'lr': 8.40556321905403e-05, 'weight_decay': 0.003096918223786138, 'dropout': 0.22357908876439977}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:23,312] Trial 7 finished with value: 0.6959420045216879 and parameters: {'lr': 9.169637422694053e-05, 'weight_decay': 1.0675046993269794e-05, 'dropout': 0.22834866915121146}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:27,884] Trial 8 finished with value: 0.6871453921000162 and parameters: {'lr': 0.0005610513630148932, 'weight_decay': 0.0004256836841638055, 'dropout': 0.18336677071677587}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:32,063] Trial 9 finished with value: 0.44175009926160175 and parameters: {'lr': 0.0033097919893395897, 'weight_decay': 3.749354030450557e-05, 'dropout': 0.4276219521659518}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:36,405] Trial 10 finished with value: 0.6771309971809387 and parameters: {'lr': 0.0009671460718926826, 'weight_decay': 1.1083631440082833e-06, 'dropout': 0.4784058248877746}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:46:40,698] Trial 11 finished with value: 0.4735666811466217 and parameters: {'lr': 0.009846905354476123, 'weight_decay': 3.597164154555872e-05, 'dropout': 0.4219270858412268}. Best is trial 1 with value: 0.4392253557840983.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.003661950605748677, 'weight_decay': 1.6001438653977643e-06, 'dropout': 0.4778793892133558}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import datetime\n",
    "import optuna\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# LSTM with Attention\n",
    "class LSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(LSTMWithAttentionClassifier, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=False\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attn = nn.Linear(hidden_dim, 1)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        attn_weights = self.attn(lstm_out).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_weights, dim=1)\n",
    "        attn_applied = torch.sum(lstm_out * attn_weights.unsqueeze(-1), dim=1)\n",
    "\n",
    "        dropped = self.dropout(attn_applied)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "\n",
    "# Freeze LSTM encoder\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    log_dir = f\"runs-lstm-att-trans-fullback/LSTMAttTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_lstm_att_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Load compatible weights\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "    compatible_dict = {k: v for k, v in pretrained_dict.items() if k in model_dict and model_dict[k].shape == v.shape}\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=dropout)\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "    # freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "# Run optimization\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=12)\n",
    "\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "lstm_att_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_att_fullbackprop_best_parameters =  {'lr': 0.003661950605748677, 'weight_decay': 1.6001438653977643e-06, 'dropout': 0.4778793892133558}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-lstm/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = LSTMWithAttentionClassifier(input_dim=20, hidden_dim=69, num_layers=2, dropout=lstm_att_fullbackprop_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=20, lr=lstm_att_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=lstm_att_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Run 1/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 2/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 3/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 4/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 5/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 6/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 7/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 8/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 9/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 10/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "📊 Summary across runs (LSTM + Attention, Full Backprop):\n",
      "Accuracy:       0.8375 ± 0.0599\n",
      "AUC:            0.9135 ± 0.0346\n",
      "Sensitivity:    0.7439 ± 0.1485\n",
      "Specificity:    0.9191 ± 0.0845\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "import optuna\n",
    "\n",
    "# Suppress print helper\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "\n",
    "n_runs = 10\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=69,\n",
    "        num_layers=2,\n",
    "        dropout=lstm_att_fullbackprop_best_parameters['dropout']\n",
    "    )\n",
    "\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')  # adjust if needed\n",
    "\n",
    "    with suppress_stdout():\n",
    "        train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=20,\n",
    "            lr=lstm_att_fullbackprop_best_parameters['lr'],\n",
    "            weight_decay=lstm_att_fullbackprop_best_parameters['weight_decay'],\n",
    "            verbose=False,\n",
    "            train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "print(\"\\n📊 Summary across runs (LSTM + Attention, Full Backprop):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BiLSTM + attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:31,867] A new study created in memory with name: no-name-6f67a702-3116-4719-a1f3-fabc0ce7b300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:36,317] Trial 0 finished with value: 0.6913818120956421 and parameters: {'lr': 0.00010396705802047225, 'weight_decay': 2.606722106246024e-05, 'dropout': 0.27484829374411573}. Best is trial 0 with value: 0.6913818120956421.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:40,608] Trial 1 finished with value: 0.6909677386283875 and parameters: {'lr': 0.001603372192461072, 'weight_decay': 2.7392594780050672e-05, 'dropout': 0.35549236639803505}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:45,004] Trial 2 finished with value: 0.6912177602450053 and parameters: {'lr': 0.001028278887873135, 'weight_decay': 2.7421423263802487e-06, 'dropout': 0.3645635923789524}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:49,294] Trial 3 finished with value: 0.6910070578257242 and parameters: {'lr': 0.009932459261507275, 'weight_decay': 0.0010550357036470013, 'dropout': 0.4391675249545186}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:53,716] Trial 4 finished with value: 0.6915017167727152 and parameters: {'lr': 4.4562322008763944e-05, 'weight_decay': 0.0011432662106658348, 'dropout': 0.1812756174961075}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:47:58,132] Trial 5 finished with value: 0.6912265221277872 and parameters: {'lr': 6.892477912590756e-05, 'weight_decay': 0.0010490351025928817, 'dropout': 0.36219203807204225}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:02,570] Trial 6 finished with value: 0.6912133097648621 and parameters: {'lr': 0.00027652020377975293, 'weight_decay': 0.005115876742108018, 'dropout': 0.21390603329189248}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:06,418] Trial 7 finished with value: 0.6912065744400024 and parameters: {'lr': 0.008822275785741721, 'weight_decay': 0.00013168808579933257, 'dropout': 0.25332206236182137}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:10,417] Trial 8 finished with value: 0.6913290421168009 and parameters: {'lr': 0.00014108015644636373, 'weight_decay': 0.0005935795593691928, 'dropout': 0.11745937162575203}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:14,267] Trial 9 finished with value: 0.6910712718963623 and parameters: {'lr': 0.0019145400759922671, 'weight_decay': 1.3258180609809638e-06, 'dropout': 0.17373351254916286}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:18,210] Trial 10 finished with value: 0.6919327179590861 and parameters: {'lr': 1.387364637976795e-05, 'weight_decay': 1.441591616962705e-05, 'dropout': 0.4993958742066162}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:22,101] Trial 11 finished with value: 0.6910941203435262 and parameters: {'lr': 0.009644222341479193, 'weight_decay': 8.642345129084171e-05, 'dropout': 0.46389529216658204}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:26,005] Trial 12 finished with value: 0.6910844445228577 and parameters: {'lr': 0.0024496849132849673, 'weight_decay': 0.006860473021053124, 'dropout': 0.4014735436121461}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:30,002] Trial 13 finished with value: 0.6913674672444662 and parameters: {'lr': 0.0008034113243593062, 'weight_decay': 0.00017901786277895685, 'dropout': 0.4267274754567762}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:33,943] Trial 14 finished with value: 0.6910897493362427 and parameters: {'lr': 0.003944461039328581, 'weight_decay': 1.205315555437901e-05, 'dropout': 0.3224619251674377}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:37,986] Trial 15 finished with value: 0.6911685268084208 and parameters: {'lr': 0.0004764677716866242, 'weight_decay': 4.112887309935719e-05, 'dropout': 0.33150172104875636}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:42,037] Trial 16 finished with value: 0.6910964250564575 and parameters: {'lr': 0.005196798019258291, 'weight_decay': 0.00040071957730875225, 'dropout': 0.4193980346042576}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:45,943] Trial 17 finished with value: 0.69124835729599 and parameters: {'lr': 0.0016855656468750069, 'weight_decay': 6.06374318250068e-06, 'dropout': 0.4647991614128991}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:49,833] Trial 18 finished with value: 0.6911240418752035 and parameters: {'lr': 0.004291227007661284, 'weight_decay': 0.0021430179976216273, 'dropout': 0.3917833116789575}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:48:54,293] Trial 19 finished with value: 0.6911638975143433 and parameters: {'lr': 0.00037995929454300183, 'weight_decay': 0.00031219145374794077, 'dropout': 0.28841007466281104}. Best is trial 1 with value: 0.6909677386283875.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.001603372192461072, 'weight_decay': 2.7392594780050672e-05, 'dropout': 0.35549236639803505}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm_attn-trans-frozen/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_attn_frozen.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=dropout)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_frozen_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### frozen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_attn_best_param = {'hidden_dim': 33, 'num_layers': 2, 'dropout': 0.10065981308640495, 'lr': 0.009726729721310196, 'weight_decay': 1.9050323459033188e-05}\n",
    "bilstm_attn_frozen_best_parameters = {'lr': 0.001603372192461072, 'weight_decay': 2.7392594780050672e-05, 'dropout': 0.35549236639803505}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attn-trans-frozen/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# to rerun\n",
    "model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=bilstm_attn_frozen_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=50, lr=bilstm_attn_frozen_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_attn_frozen_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Run 1/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 2/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 3/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 4/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 5/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 6/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 7/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 8/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 9/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 10/10\n",
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "📊 Summary across 10 runs (LSTM + Attention, Full Backprop, no freezing):\n",
      "Accuracy:       0.8318 ± 0.0425\n",
      "AUC:            0.8887 ± 0.0642\n",
      "Sensitivity:    0.8390 ± 0.0749\n",
      "Specificity:    0.8255 ± 0.1362\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Suppress print helper\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Run settings\n",
    "n_runs = 10\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = LSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=69,\n",
    "        num_layers=2,\n",
    "        dropout=lstm_att_fullbackprop_best_parameters['dropout']\n",
    "    )\n",
    "    model = load_partial_weights(model, 'best_model-lstm_attention.pt')\n",
    "\n",
    "    with suppress_stdout():\n",
    "        history = train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=20,\n",
    "            lr=lstm_att_fullbackprop_best_parameters['lr'],\n",
    "            weight_decay=lstm_att_fullbackprop_best_parameters['weight_decay'],\n",
    "            verbose=False,\n",
    "            train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Compute confusion matrix metrics\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Summary\n",
    "print(\"\\n📊 Summary across 10 runs (LSTM + Attention, Full Backprop, no freezing):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### full back prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:08,676] A new study created in memory with name: no-name-bb29b48d-f95e-4f82-bd4f-627eb384a827\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:12,600] Trial 0 finished with value: 0.6919108629226685 and parameters: {'lr': 7.861113045771693e-05, 'weight_decay': 1.4127732016108189e-06, 'dropout': 0.3592199466945646}. Best is trial 0 with value: 0.6919108629226685.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:16,574] Trial 1 finished with value: 0.6915312012036642 and parameters: {'lr': 3.626443880830906e-05, 'weight_decay': 1.2961818653876822e-05, 'dropout': 0.3464371450549254}. Best is trial 1 with value: 0.6915312012036642.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:20,716] Trial 2 finished with value: 0.6911666790644327 and parameters: {'lr': 2.7565591674814572e-05, 'weight_decay': 0.00016592249203051755, 'dropout': 0.3963016233506764}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:24,764] Trial 3 finished with value: 0.6911691228548685 and parameters: {'lr': 0.001956726158117691, 'weight_decay': 2.0522585791503508e-05, 'dropout': 0.33785336420806644}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:29,197] Trial 4 finished with value: 0.6911851763725281 and parameters: {'lr': 0.0018456346129685436, 'weight_decay': 1.3374238068711262e-05, 'dropout': 0.4166964491092321}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:33,644] Trial 5 finished with value: 0.6916813254356384 and parameters: {'lr': 1.645749040342259e-05, 'weight_decay': 0.006048875394564759, 'dropout': 0.3382932817294666}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:37,619] Trial 6 finished with value: 0.6912331978480021 and parameters: {'lr': 2.3006779878890447e-05, 'weight_decay': 0.0005144087182014116, 'dropout': 0.1673895308849253}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:41,541] Trial 7 finished with value: 0.691234310468038 and parameters: {'lr': 0.0005244862073215171, 'weight_decay': 4.783349577976966e-06, 'dropout': 0.1578606009585733}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:45,443] Trial 8 finished with value: 0.6911967794100443 and parameters: {'lr': 4.44117100165219e-05, 'weight_decay': 2.1866785989418446e-06, 'dropout': 0.30810178545726064}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:49,611] Trial 9 finished with value: 0.691170891125997 and parameters: {'lr': 5.923540180131155e-05, 'weight_decay': 3.974123505671287e-05, 'dropout': 0.4864122476074447}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:53,925] Trial 10 finished with value: 0.6913396914800009 and parameters: {'lr': 0.00024242668811504204, 'weight_decay': 0.000269896650980584, 'dropout': 0.22681777131386222}. Best is trial 2 with value: 0.6911666790644327.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:49:57,986] Trial 11 finished with value: 0.6911083658536276 and parameters: {'lr': 0.009023890166622224, 'weight_decay': 0.00010779058678715841, 'dropout': 0.4535673475985836}. Best is trial 11 with value: 0.6911083658536276.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:02,056] Trial 12 finished with value: 0.6910683314005533 and parameters: {'lr': 0.006575991272077654, 'weight_decay': 0.0012171960694781323, 'dropout': 0.49038586373028537}. Best is trial 12 with value: 0.6910683314005533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:05,891] Trial 13 finished with value: 0.691091517607371 and parameters: {'lr': 0.008979998474401603, 'weight_decay': 0.0016277825579878843, 'dropout': 0.49510349781243346}. Best is trial 12 with value: 0.6910683314005533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:10,215] Trial 14 finished with value: 0.6910977959632874 and parameters: {'lr': 0.008378064727562523, 'weight_decay': 0.0018987159227296963, 'dropout': 0.48731650267050336}. Best is trial 12 with value: 0.6910683314005533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:14,570] Trial 15 finished with value: 0.6911635796229044 and parameters: {'lr': 0.002924883905604112, 'weight_decay': 0.0013506202633295314, 'dropout': 0.4384318444073999}. Best is trial 12 with value: 0.6910683314005533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:18,760] Trial 16 finished with value: 0.6911545395851135 and parameters: {'lr': 0.003904690298858426, 'weight_decay': 0.009615624075107606, 'dropout': 0.25972480298293266}. Best is trial 12 with value: 0.6910683314005533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:22,773] Trial 17 finished with value: 0.6912190715471903 and parameters: {'lr': 0.0007172112286993747, 'weight_decay': 0.0019101752799604538, 'dropout': 0.10642009632957608}. Best is trial 12 with value: 0.6910683314005533.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:26,835] Trial 18 finished with value: 0.6910647551218668 and parameters: {'lr': 0.004843097327507198, 'weight_decay': 0.0006598357188430301, 'dropout': 0.49558732282929335}. Best is trial 18 with value: 0.6910647551218668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-04-30 15:50:31,364] Trial 19 finished with value: 0.691186785697937 and parameters: {'lr': 0.0010800143901562472, 'weight_decay': 0.0005160704668832621, 'dropout': 0.3869721104280036}. Best is trial 18 with value: 0.6910647551218668.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters: {'lr': 0.004843097327507198, 'weight_decay': 0.0006598357188430301, 'dropout': 0.49558732282929335}\n"
     ]
    }
   ],
   "source": [
    "# Updated BiLSTM with Flatten Transfer Learning\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, confusion_matrix\n",
    "from torch.nn.utils.rnn import pad_packed_sequence\n",
    "import datetime\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import optuna\n",
    "\n",
    "class BiLSTMWithAttentionClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=20, hidden_dim=64, num_layers=1, dropout=0.3):\n",
    "        super(BiLSTMWithAttentionClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.attention = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, packed_input):\n",
    "        unpacked, lengths = pad_packed_sequence(packed_input, batch_first=True)\n",
    "        lstm_out, _ = self.lstm(unpacked)\n",
    "\n",
    "        # Attention mechanism\n",
    "        attn_weights = torch.softmax(self.attention(lstm_out), dim=1)\n",
    "        context_vector = torch.sum(attn_weights * lstm_out, dim=1)\n",
    "\n",
    "        dropped = self.dropout(context_vector)\n",
    "        out = self.fc(dropped)\n",
    "        return self.sigmoid(out).squeeze(1)\n",
    "# Freeze encoder\n",
    "\n",
    "def freeze_encoder(model):\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'lstm' in name:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Evaluation\n",
    "\n",
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=True):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "# Training for fine-tuning\n",
    "\n",
    "def train_finetune_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4,\n",
    "                         device='cuda' if torch.cuda.is_available() else 'cpu'):\n",
    "    model.to(device)\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=lr, weight_decay=weight_decay)\n",
    "\n",
    "    best_val_loss = 1000\n",
    "    log_dir = f\"runs-bilstm_attn-trans-fullbackprop/BiLSTMTransfer_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=False)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model_bilstm_attn_fullbackprop.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "# Optuna objective for BiLSTM transfer\n",
    "def load_partial_weights(model, checkpoint_path):\n",
    "    pretrained_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "    model_dict = model.state_dict()\n",
    "\n",
    "    compatible_dict = {\n",
    "        k: v for k, v in pretrained_dict.items()\n",
    "        if k in model_dict and model_dict[k].shape == v.shape\n",
    "    }\n",
    "\n",
    "    model_dict.update(compatible_dict)\n",
    "    model.load_state_dict(model_dict)\n",
    "    print(f\"✅ Loaded {len(compatible_dict)} matching layers from checkpoint.\")\n",
    "    return model\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-2, log=True)\n",
    "    weight_decay = trial.suggest_float(\"weight_decay\", 1e-6, 1e-2, log=True)\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=dropout)\n",
    "    # model.load_state_dict(torch.load('best_model-bilstm.pt'))\n",
    "    model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "\n",
    "    freeze_encoder(model)\n",
    "\n",
    "    return train_finetune_model(\n",
    "        model, train_loader, val_loader,\n",
    "        num_epochs=15, lr=lr, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "study = optuna.create_study(direction=\"minimize\")\n",
    "study.optimize(objective, n_trials=20)\n",
    "print(\"Best hyperparameters:\", study.best_trial.params)\n",
    "bilstm_attn_fullbackprop_best_parameters = study.best_trial.params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "bilstm_attn_fullbackprop_best_parameters = {'lr': 0.004843097327507198, 'weight_decay': 0.0006598357188430301, 'dropout': 0.49558732282929335}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Loaded 2 matching layers from checkpoint.\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [1/120] - Train Loss: 0.6911, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.2714\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [2/120] - Train Loss: 0.6905, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.2875\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [3/120] - Train Loss: 0.6904, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.3083\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [4/120] - Train Loss: 0.6912, Val Loss: 0.6914, Val Acc: 0.5341, Val AUC: 0.3285\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [5/120] - Train Loss: 0.6925, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.3664\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [6/120] - Train Loss: 0.6928, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4074\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [7/120] - Train Loss: 0.6887, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4406\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [8/120] - Train Loss: 0.6913, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.4619\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [9/120] - Train Loss: 0.6885, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5091\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [10/120] - Train Loss: 0.6913, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5143\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [11/120] - Train Loss: 0.6920, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.5138\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [12/120] - Train Loss: 0.6902, Val Loss: 0.6916, Val Acc: 0.5341, Val AUC: 0.5169\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [13/120] - Train Loss: 0.6899, Val Loss: 0.6914, Val Acc: 0.5341, Val AUC: 0.5314\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [14/120] - Train Loss: 0.6927, Val Loss: 0.6915, Val Acc: 0.5341, Val AUC: 0.5480\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [15/120] - Train Loss: 0.6936, Val Loss: 0.6915, Val Acc: 0.5341, Val AUC: 0.5426\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [16/120] - Train Loss: 0.6916, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.5402\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [17/120] - Train Loss: 0.6911, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.5470\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [18/120] - Train Loss: 0.6885, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.5620\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [19/120] - Train Loss: 0.6930, Val Loss: 0.6914, Val Acc: 0.5341, Val AUC: 0.5908\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [20/120] - Train Loss: 0.6906, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6149\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [21/120] - Train Loss: 0.6971, Val Loss: 0.6914, Val Acc: 0.5341, Val AUC: 0.6282\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [22/120] - Train Loss: 0.6970, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6367\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [23/120] - Train Loss: 0.6916, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6448\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [24/120] - Train Loss: 0.6909, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6471\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [25/120] - Train Loss: 0.6919, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6492\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [26/120] - Train Loss: 0.6892, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6502\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [27/120] - Train Loss: 0.6903, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6539\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [28/120] - Train Loss: 0.6897, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6679\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [29/120] - Train Loss: 0.6947, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.6700\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [30/120] - Train Loss: 0.6842, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6754\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [31/120] - Train Loss: 0.6890, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.6772\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [32/120] - Train Loss: 0.6884, Val Loss: 0.6914, Val Acc: 0.5341, Val AUC: 0.6788\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [33/120] - Train Loss: 0.6913, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6746\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [34/120] - Train Loss: 0.6907, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6689\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [35/120] - Train Loss: 0.6962, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6723\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [36/120] - Train Loss: 0.6861, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6679\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [37/120] - Train Loss: 0.6896, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6710\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [38/120] - Train Loss: 0.6882, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6736\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [39/120] - Train Loss: 0.6887, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6751\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [40/120] - Train Loss: 0.6916, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6762\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [41/120] - Train Loss: 0.6883, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6762\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [42/120] - Train Loss: 0.6915, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6772\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [43/120] - Train Loss: 0.6889, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6762\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [44/120] - Train Loss: 0.6897, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6793\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [45/120] - Train Loss: 0.6931, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6798\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [46/120] - Train Loss: 0.6917, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6819\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [47/120] - Train Loss: 0.6953, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6876\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [48/120] - Train Loss: 0.6956, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6884\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [49/120] - Train Loss: 0.6900, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6902\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [50/120] - Train Loss: 0.6949, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.6897\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [51/120] - Train Loss: 0.6897, Val Loss: 0.6912, Val Acc: 0.5341, Val AUC: 0.6938\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [52/120] - Train Loss: 0.6904, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6969\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [53/120] - Train Loss: 0.6922, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6990\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [54/120] - Train Loss: 0.6932, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6975\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [55/120] - Train Loss: 0.6855, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.6995\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [56/120] - Train Loss: 0.6895, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7078\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [57/120] - Train Loss: 0.6929, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7109\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [58/120] - Train Loss: 0.6904, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7161\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [59/120] - Train Loss: 0.6900, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7229\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [60/120] - Train Loss: 0.6897, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7265\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [61/120] - Train Loss: 0.6958, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7255\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [62/120] - Train Loss: 0.6920, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7281\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [63/120] - Train Loss: 0.6858, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7338\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [64/120] - Train Loss: 0.6906, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7369\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [65/120] - Train Loss: 0.6894, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7340\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [66/120] - Train Loss: 0.6872, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7374\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [67/120] - Train Loss: 0.6886, Val Loss: 0.6916, Val Acc: 0.5341, Val AUC: 0.7390\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [68/120] - Train Loss: 0.6940, Val Loss: 0.6916, Val Acc: 0.5341, Val AUC: 0.7468\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [69/120] - Train Loss: 0.6859, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.7545\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [70/120] - Train Loss: 0.6889, Val Loss: 0.6914, Val Acc: 0.5341, Val AUC: 0.7504\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [71/120] - Train Loss: 0.6892, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.7509\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [72/120] - Train Loss: 0.6980, Val Loss: 0.6913, Val Acc: 0.5341, Val AUC: 0.7592\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [73/120] - Train Loss: 0.6918, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7680\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [74/120] - Train Loss: 0.6862, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7691\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [75/120] - Train Loss: 0.6900, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7743\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [76/120] - Train Loss: 0.6871, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7701\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [77/120] - Train Loss: 0.6901, Val Loss: 0.6911, Val Acc: 0.5341, Val AUC: 0.7623\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [78/120] - Train Loss: 0.6880, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7639\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [79/120] - Train Loss: 0.6869, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7686\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [80/120] - Train Loss: 0.6903, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7737\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [81/120] - Train Loss: 0.6903, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7740\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [82/120] - Train Loss: 0.6925, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7727\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [83/120] - Train Loss: 0.6901, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7758\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [84/120] - Train Loss: 0.6913, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7763\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [85/120] - Train Loss: 0.6901, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7758\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [86/120] - Train Loss: 0.6893, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7805\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [87/120] - Train Loss: 0.6907, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7784\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [88/120] - Train Loss: 0.6925, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7722\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [89/120] - Train Loss: 0.6918, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7743\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [90/120] - Train Loss: 0.6890, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7763\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [91/120] - Train Loss: 0.6906, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7758\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [92/120] - Train Loss: 0.6853, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7758\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [93/120] - Train Loss: 0.6926, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7800\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [94/120] - Train Loss: 0.6945, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7794\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [95/120] - Train Loss: 0.6878, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7794\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [96/120] - Train Loss: 0.6908, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7883\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [97/120] - Train Loss: 0.6917, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7867\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [98/120] - Train Loss: 0.6890, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7857\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [99/120] - Train Loss: 0.6948, Val Loss: 0.6910, Val Acc: 0.5341, Val AUC: 0.7857\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [100/120] - Train Loss: 0.6896, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7883\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [101/120] - Train Loss: 0.6904, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7883\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [102/120] - Train Loss: 0.6924, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7898\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [103/120] - Train Loss: 0.6915, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7883\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [104/120] - Train Loss: 0.6926, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.7914\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [105/120] - Train Loss: 0.6918, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7914\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [106/120] - Train Loss: 0.6879, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7955\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [107/120] - Train Loss: 0.6966, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7981\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [108/120] - Train Loss: 0.6922, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7992\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [109/120] - Train Loss: 0.6911, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7997\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [110/120] - Train Loss: 0.6889, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.8054\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [111/120] - Train Loss: 0.6873, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.8067\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [112/120] - Train Loss: 0.6916, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.8080\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [113/120] - Train Loss: 0.6915, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.8085\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [114/120] - Train Loss: 0.6866, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.8049\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [115/120] - Train Loss: 0.6895, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.8095\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [116/120] - Train Loss: 0.6916, Val Loss: 0.6909, Val Acc: 0.5341, Val AUC: 0.8095\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [117/120] - Train Loss: 0.6994, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.7987\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [118/120] - Train Loss: 0.6875, Val Loss: 0.6907, Val Acc: 0.5341, Val AUC: 0.7992\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [119/120] - Train Loss: 0.6922, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.8007\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Epoch [120/120] - Train Loss: 0.6906, Val Loss: 0.6908, Val Acc: 0.5341, Val AUC: 0.8033\n",
      "\n",
      "Confusion Matrix:\n",
      "[[47  0]\n",
      " [41  0]]\n",
      "Sensitivity: 0.0000, Specificity: 1.0000\n",
      "Test Loss: 0.6922, Test Accuracy: 0.5341, Test AUC: 0.7566\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model(model, data_loader, criterion, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    total_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for packed_input, labels in data_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            total_loss += loss.item()\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "    acc = accuracy_score(all_labels, pred_labels)\n",
    "    try:\n",
    "        auc = roc_auc_score(all_labels, all_preds)\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "\n",
    "    cm = confusion_matrix(all_labels, pred_labels)\n",
    "    tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "    sensitivity = tp / (tp + fn) if (tp + fn) > 0 else float('nan')\n",
    "    specificity = tn / (tn + fp) if (tn + fp) > 0 else float('nan')\n",
    "    if verbose:\n",
    "        print(f\"\\nConfusion Matrix:\\n{cm}\")\n",
    "        print(f\"Sensitivity: {sensitivity:.4f}, Specificity: {specificity:.4f}\")\n",
    "\n",
    "    return avg_loss, acc, auc\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=10, lr=1e-3, weight_decay=1e-4, device='cuda' if torch.cuda.is_available() else 'cpu', verbose=False, train=False):\n",
    "    model.to(device)\n",
    "    if not train:\n",
    "        model.eval()\n",
    "    criterion = nn.BCELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "    best_val_loss = 1000\n",
    "\n",
    "    log_dir = f\"runs-bilstm_attn-trans-fullbackprop/AMP_LSTM_{datetime.datetime.now().strftime('%Y%m%d-%H%M%S')}\"\n",
    "    writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        for packed_input, labels in train_loader:\n",
    "            labels = labels.to(device)\n",
    "            packed_input = packed_input.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(packed_input)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = epoch_loss / len(train_loader)\n",
    "        val_loss, val_acc, val_auc = evaluate_model(model, val_loader, criterion, device, verbose=verbose)\n",
    "\n",
    "        writer.add_scalar('Loss/Train', avg_train_loss, epoch)\n",
    "        writer.add_scalar('Loss/Validation', val_loss, epoch)\n",
    "        writer.add_scalar('Accuracy/Validation', val_acc, epoch)\n",
    "        writer.add_scalar('AUC/Validation', val_auc, epoch)\n",
    "\n",
    "        if verbose:\n",
    "            print(f\"Epoch [{epoch}/{num_epochs}] - \"\n",
    "                  f\"Train Loss: {avg_train_loss:.4f}, \"\n",
    "                  f\"Val Loss: {val_loss:.4f}, \"\n",
    "                  f\"Val Acc: {val_acc:.4f}, \"\n",
    "                  f\"Val AUC: {val_auc:.4f}\")\n",
    "\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            if train:\n",
    "                torch.save(model.state_dict(), 'best_model_lstm.pt')\n",
    "\n",
    "    writer.close()\n",
    "    return best_val_loss\n",
    "\n",
    "model = BiLSTMWithAttentionClassifier(input_dim=20, hidden_dim=bilstm_attn_best_param['hidden_dim'], num_layers=bilstm_attn_best_param['num_layers'], dropout=bilstm_attn_fullbackprop_best_parameters['dropout'])\n",
    "model = load_partial_weights(model, 'best_model-bilstm_attention.pt')\n",
    "freeze_encoder(model)\n",
    "\n",
    "history = train_model(model, train_loader, val_loader, num_epochs=120, lr=bilstm_attn_fullbackprop_best_parameters['lr'],\n",
    "                      weight_decay=bilstm_attn_fullbackprop_best_parameters['weight_decay'], verbose=True, train=False)\n",
    "\n",
    "criterion = nn.BCELoss()\n",
    "val_loss, val_acc, val_auc = evaluate_model(model, test_loader, criterion, verbose=True)\n",
    "print(f\"Test Loss: {val_loss:.4f}, Test Accuracy: {val_acc:.4f}, Test AUC: {val_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Run 1/10\n",
      "✅ Loaded 9 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 2/10\n",
      "✅ Loaded 9 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 3/10\n",
      "✅ Loaded 9 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 4/10\n",
      "✅ Loaded 9 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 5/10\n",
      "✅ Loaded 9 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 6/10\n",
      "✅ Loaded 9 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 7/10\n",
      "✅ Loaded 9 matching layers from checkpoint.\n",
      "\n",
      "🔁 Run 8/10\n",
      "✅ Loaded 9 matching layers from checkpoint.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import contextlib\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Helper: suppress printing\n",
    "@contextlib.contextmanager\n",
    "def suppress_stdout():\n",
    "    with open(os.devnull, \"w\") as devnull:\n",
    "        old_stdout = sys.stdout\n",
    "        sys.stdout = devnull\n",
    "        try:\n",
    "            yield\n",
    "        finally:\n",
    "            sys.stdout = old_stdout\n",
    "\n",
    "# Metrics storage\n",
    "n_runs = 10\n",
    "acc_list, auc_list, sens_list, spec_list = [], [], [], []\n",
    "\n",
    "for run in range(n_runs):\n",
    "    print(f\"\\n🔁 Run {run + 1}/{n_runs}\")\n",
    "\n",
    "    model = BiLSTMWithAttentionClassifier(\n",
    "        input_dim=20,\n",
    "        hidden_dim=bilstm_best_param['hidden_dim'],\n",
    "        num_layers=bilstm_best_param['num_layers'],\n",
    "        dropout=bilstm_attn_fullbackprop_best_parameters['dropout']\n",
    "    )\n",
    "    model = load_partial_weights(model, 'best_model-bilstm.pt')\n",
    "\n",
    "    with suppress_stdout():\n",
    "        history = train_model(\n",
    "            model, train_loader, val_loader,\n",
    "            num_epochs=120,\n",
    "            lr=bilstm_attn_fullbackprop_best_parameters['lr'],\n",
    "            weight_decay=bilstm_attn_fullbackprop_best_parameters['weight_decay'],\n",
    "            verbose=False,\n",
    "            train=False\n",
    "        )\n",
    "\n",
    "        criterion = nn.BCELoss()\n",
    "        val_loss, acc, auc = evaluate_model(model, test_loader, criterion, verbose=False)\n",
    "\n",
    "        # Manually compute confusion matrix\n",
    "        model.eval()\n",
    "        all_labels, all_preds = [], []\n",
    "        with torch.no_grad():\n",
    "            for packed_input, labels in test_loader:\n",
    "                packed_input = packed_input.to(model.fc.weight.device)\n",
    "                labels = labels.to(model.fc.weight.device)\n",
    "                outputs = model(packed_input)\n",
    "                all_labels.extend(labels.cpu().numpy())\n",
    "                all_preds.extend(outputs.cpu().numpy())\n",
    "\n",
    "        pred_labels = [1 if p > 0.5 else 0 for p in all_preds]\n",
    "        cm = confusion_matrix(all_labels, pred_labels)\n",
    "        tn, fp, fn, tp = cm.ravel() if cm.size == 4 else (0, 0, 0, 0)\n",
    "        sens = tp / (tp + fn) if (tp + fn) > 0 else np.nan\n",
    "        spec = tn / (tn + fp) if (tn + fp) > 0 else np.nan\n",
    "\n",
    "    acc_list.append(acc)\n",
    "    auc_list.append(auc)\n",
    "    sens_list.append(sens)\n",
    "    spec_list.append(spec)\n",
    "\n",
    "# Print summary\n",
    "print(\"\\n📊 Summary across 10 runs (BiLSTM + Attention, Full Backprop):\")\n",
    "print(f\"Accuracy:       {np.mean(acc_list):.4f} ± {np.std(acc_list):.4f}\")\n",
    "print(f\"AUC:            {np.mean(auc_list):.4f} ± {np.std(auc_list):.4f}\")\n",
    "print(f\"Sensitivity:    {np.mean(sens_list):.4f} ± {np.std(sens_list):.4f}\")\n",
    "print(f\"Specificity:    {np.mean(spec_list):.4f} ± {np.std(spec_list):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "new-ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
